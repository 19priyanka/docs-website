{
  "/docs/browser/new-relic-browser/troubleshooting/browser-javascript-injection-causes-problems-page": [
    {
      "sections": [
        "AJAX call fails with a CORS redirect error message",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "AJAX call fails with a CORS redirect error message",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "a4e478428acefb454ab8969cccb666d03ae458f1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/ajax-call-fails-cors-redirect-error-message/",
      "published_at": "2022-01-12T09:12:16Z",
      "updated_at": "2021-11-13T07:04:35Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem A redirected AJAX call is being rejected with a CORS error message, for example: Access to XMLHttpRequest at 'https://my-domain-2/path' (redirected from 'https://my-domain-1/path') from origin 'https://my-website-domain' has been blocked by CORS policy: Request header field x-newrelic-id is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The browser agent automatically adds custom headers to outgoing same-origin AJAX calls in order to support the Distributed Tracing feature. When the server that receives the AJAX call responds with a redirect status code (such as 302), the browser will automatically make the same AJAX call to the redirected URL. And if this new URL is on a different origin and the call does not pass the CORS preflight, the browser will fail the call with the error message listed above.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.0676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The <em>browser</em> agent"
      },
      "id": "603eb41ce7b9d2ce042a07db"
    },
    {
      "sections": [
        "Troubleshoot AJAX data collection",
        "Problem",
        "Solution",
        "1. Verify you use XMLHttpRequest.",
        "2. Verify the object is instrumented.",
        "3. Verify network access.",
        "JSONP requirements"
      ],
      "title": "Troubleshoot AJAX data collection",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "8dc486fa0fbb0a07f5e93c3cf75590e3e03bba2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/troubleshoot-ajax-data-collection/",
      "published_at": "2022-01-12T07:46:12Z",
      "updated_at": "2021-11-13T07:03:46Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are not seeing AJAX data for your browser app. Solution If your application is instrumented with browser monitoring and is correctly collecting data for other Pro features, follow these steps: 1. Verify you use XMLHttpRequest. Check whether your application uses the XMLHttpRequest object to make AJAX calls. Browser monitoring: Other methods (including the newer Fetch API) currently are not supported when using browser Pro. Single-page app monitoring: Fetch is supported for AJAX requests within a browser interaction with SPA monitoring. If you are making requests using JSONP, see JSONP requirements. 2. Verify the object is instrumented. If you are using XMLHttpRequest, use your browser's dev console to verify that the object has been instrumented by New Relic. Enter the object name at your console. If instrumentation has succeeded, the console should return something like: function (t){var e=new p(t);try{u.emit(\"new-xhr\"... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see Troubleshooting browser monitoring installation. If you see a different response, you may be using another script or library that is conflicting with New Relic instrumentation. Contact support at support.newrelic.com. 3. Verify network access. If the object is properly instrumented, try triggering an AJAX call in your application while monitoring network traffic in the browser's developer tools. Wait up to one minute, and look for a call to bam.nr-data.net/jserrors with an xhr parameter. If the call fails, check for network issues. If you don't see this call, if it fails with an error not related to network access, or if it succeeds but you still aren't seeing data, get support at support.newrelic.com. If your requests use JSONP, see requirements and notes on functionality below: JSONP requirements If your requests use JSONP, these requests will not appear on the AJAX UI page. However, you can view them as assets within session traces. If using SPA monitoring, you can view them on the Breakdown tab of the Page views page. Requirements for JSONP to be recognized: Each JSONP request must use a unique callback function. Most popular libraries (like jQuery) generate a unique callback function dynamically for each request. The query string callback must be named \"callback\" or \"cb\" in order to be recognized by New Relic. This is the default behavior in most popular libraries.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.06745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Troubleshoot</em> AJAX data collection",
        "sections": "<em>Troubleshoot</em> AJAX data collection",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": ": function (t){var e=new p(t);try{u.emit(&quot;new-xhr&quot;... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see <em>Troubleshooting</em> <em>browser</em> <em>monitoring</em> installation. If you see a different response, you may"
      },
      "id": "603e902d196a6762dea83d8a"
    },
    {
      "sections": [
        "Browser data doesn't match other analytics tools",
        "Problem",
        "Solution"
      ],
      "title": "Browser data doesn't match other analytics tools ",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "02439fadf41050ba60397b865b87f79576143295",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/browser-data-doesnt-match-other-analytics-tools/",
      "published_at": "2022-01-12T09:12:57Z",
      "updated_at": "2021-07-09T13:47:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Problem You see different data in browser monitoring than other analytics tools you use. Common symptoms include: Browser records fewer page views than you expect. Browser's geographical distribution for your users differs from the locations of your expected user base. Browser shows a different number of active sessions than you see elsewhere. Solution Follow these troubleshooting steps for browser monitoring: Check whether browser monitoring is properly installed in all of your application's pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic, follow the steps for troubleshooting browser monitoring installation. If you have a single-page style application and are expecting to see your route changes as views, consider using browser SPA monitoring, which provides an integrated view of initial page loads and route changes. If browser monitoring is working properly, you may be seeing different results because New Relic collects and presents information differently from other analytics tools. For more information about how we collects data, see: Page load timing process Instrumentation for browser monitoring Understand SPA data collection",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.38531,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> data doesn&#x27;t match other analytics tools ",
        "sections": "<em>Browser</em> data doesn&#x27;t match other analytics tools",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " number of active sessions than you see elsewhere. Solution Follow these <em>troubleshooting</em> steps for <em>browser</em> <em>monitoring</em>: Check whether <em>browser</em> <em>monitoring</em> is properly installed in all of your application&#x27;s pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic"
      },
      "id": "6043ecc328ccbc6f972e1dcc"
    }
  ],
  "/docs/browser/new-relic-browser/troubleshooting/get-browser-side-troubleshooting-details-har-file": [
    {
      "sections": [
        "AJAX call fails with a CORS redirect error message",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "AJAX call fails with a CORS redirect error message",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "a4e478428acefb454ab8969cccb666d03ae458f1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/ajax-call-fails-cors-redirect-error-message/",
      "published_at": "2022-01-12T09:12:16Z",
      "updated_at": "2021-11-13T07:04:35Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem A redirected AJAX call is being rejected with a CORS error message, for example: Access to XMLHttpRequest at 'https://my-domain-2/path' (redirected from 'https://my-domain-1/path') from origin 'https://my-website-domain' has been blocked by CORS policy: Request header field x-newrelic-id is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The browser agent automatically adds custom headers to outgoing same-origin AJAX calls in order to support the Distributed Tracing feature. When the server that receives the AJAX call responds with a redirect status code (such as 302), the browser will automatically make the same AJAX call to the redirected URL. And if this new URL is on a different origin and the call does not pass the CORS preflight, the browser will fail the call with the error message listed above.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.0676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The <em>browser</em> agent"
      },
      "id": "603eb41ce7b9d2ce042a07db"
    },
    {
      "sections": [
        "Troubleshoot AJAX data collection",
        "Problem",
        "Solution",
        "1. Verify you use XMLHttpRequest.",
        "2. Verify the object is instrumented.",
        "3. Verify network access.",
        "JSONP requirements"
      ],
      "title": "Troubleshoot AJAX data collection",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "8dc486fa0fbb0a07f5e93c3cf75590e3e03bba2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/troubleshoot-ajax-data-collection/",
      "published_at": "2022-01-12T07:46:12Z",
      "updated_at": "2021-11-13T07:03:46Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are not seeing AJAX data for your browser app. Solution If your application is instrumented with browser monitoring and is correctly collecting data for other Pro features, follow these steps: 1. Verify you use XMLHttpRequest. Check whether your application uses the XMLHttpRequest object to make AJAX calls. Browser monitoring: Other methods (including the newer Fetch API) currently are not supported when using browser Pro. Single-page app monitoring: Fetch is supported for AJAX requests within a browser interaction with SPA monitoring. If you are making requests using JSONP, see JSONP requirements. 2. Verify the object is instrumented. If you are using XMLHttpRequest, use your browser's dev console to verify that the object has been instrumented by New Relic. Enter the object name at your console. If instrumentation has succeeded, the console should return something like: function (t){var e=new p(t);try{u.emit(\"new-xhr\"... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see Troubleshooting browser monitoring installation. If you see a different response, you may be using another script or library that is conflicting with New Relic instrumentation. Contact support at support.newrelic.com. 3. Verify network access. If the object is properly instrumented, try triggering an AJAX call in your application while monitoring network traffic in the browser's developer tools. Wait up to one minute, and look for a call to bam.nr-data.net/jserrors with an xhr parameter. If the call fails, check for network issues. If you don't see this call, if it fails with an error not related to network access, or if it succeeds but you still aren't seeing data, get support at support.newrelic.com. If your requests use JSONP, see requirements and notes on functionality below: JSONP requirements If your requests use JSONP, these requests will not appear on the AJAX UI page. However, you can view them as assets within session traces. If using SPA monitoring, you can view them on the Breakdown tab of the Page views page. Requirements for JSONP to be recognized: Each JSONP request must use a unique callback function. Most popular libraries (like jQuery) generate a unique callback function dynamically for each request. The query string callback must be named \"callback\" or \"cb\" in order to be recognized by New Relic. This is the default behavior in most popular libraries.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.06745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Troubleshoot</em> AJAX data collection",
        "sections": "<em>Troubleshoot</em> AJAX data collection",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": ": function (t){var e=new p(t);try{u.emit(&quot;new-xhr&quot;... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see <em>Troubleshooting</em> <em>browser</em> <em>monitoring</em> installation. If you see a different response, you may"
      },
      "id": "603e902d196a6762dea83d8a"
    },
    {
      "sections": [
        "Browser data doesn't match other analytics tools",
        "Problem",
        "Solution"
      ],
      "title": "Browser data doesn't match other analytics tools ",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "02439fadf41050ba60397b865b87f79576143295",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/browser-data-doesnt-match-other-analytics-tools/",
      "published_at": "2022-01-12T09:12:57Z",
      "updated_at": "2021-07-09T13:47:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Problem You see different data in browser monitoring than other analytics tools you use. Common symptoms include: Browser records fewer page views than you expect. Browser's geographical distribution for your users differs from the locations of your expected user base. Browser shows a different number of active sessions than you see elsewhere. Solution Follow these troubleshooting steps for browser monitoring: Check whether browser monitoring is properly installed in all of your application's pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic, follow the steps for troubleshooting browser monitoring installation. If you have a single-page style application and are expecting to see your route changes as views, consider using browser SPA monitoring, which provides an integrated view of initial page loads and route changes. If browser monitoring is working properly, you may be seeing different results because New Relic collects and presents information differently from other analytics tools. For more information about how we collects data, see: Page load timing process Instrumentation for browser monitoring Understand SPA data collection",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.38531,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> data doesn&#x27;t match other analytics tools ",
        "sections": "<em>Browser</em> data doesn&#x27;t match other analytics tools",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " number of active sessions than you see elsewhere. Solution Follow these <em>troubleshooting</em> steps for <em>browser</em> <em>monitoring</em>: Check whether <em>browser</em> <em>monitoring</em> is properly installed in all of your application&#x27;s pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic"
      },
      "id": "6043ecc328ccbc6f972e1dcc"
    }
  ],
  "/docs/browser/new-relic-browser/troubleshooting/not-seeing-specific-page-or-endpoint-names-browser-data": [
    {
      "sections": [
        "AJAX call fails with a CORS redirect error message",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "AJAX call fails with a CORS redirect error message",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "a4e478428acefb454ab8969cccb666d03ae458f1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/ajax-call-fails-cors-redirect-error-message/",
      "published_at": "2022-01-12T09:12:16Z",
      "updated_at": "2021-11-13T07:04:35Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem A redirected AJAX call is being rejected with a CORS error message, for example: Access to XMLHttpRequest at 'https://my-domain-2/path' (redirected from 'https://my-domain-1/path') from origin 'https://my-website-domain' has been blocked by CORS policy: Request header field x-newrelic-id is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The browser agent automatically adds custom headers to outgoing same-origin AJAX calls in order to support the Distributed Tracing feature. When the server that receives the AJAX call responds with a redirect status code (such as 302), the browser will automatically make the same AJAX call to the redirected URL. And if this new URL is on a different origin and the call does not pass the CORS preflight, the browser will fail the call with the error message listed above.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.0676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The <em>browser</em> agent"
      },
      "id": "603eb41ce7b9d2ce042a07db"
    },
    {
      "sections": [
        "Troubleshoot AJAX data collection",
        "Problem",
        "Solution",
        "1. Verify you use XMLHttpRequest.",
        "2. Verify the object is instrumented.",
        "3. Verify network access.",
        "JSONP requirements"
      ],
      "title": "Troubleshoot AJAX data collection",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "8dc486fa0fbb0a07f5e93c3cf75590e3e03bba2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/troubleshoot-ajax-data-collection/",
      "published_at": "2022-01-12T07:46:12Z",
      "updated_at": "2021-11-13T07:03:46Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are not seeing AJAX data for your browser app. Solution If your application is instrumented with browser monitoring and is correctly collecting data for other Pro features, follow these steps: 1. Verify you use XMLHttpRequest. Check whether your application uses the XMLHttpRequest object to make AJAX calls. Browser monitoring: Other methods (including the newer Fetch API) currently are not supported when using browser Pro. Single-page app monitoring: Fetch is supported for AJAX requests within a browser interaction with SPA monitoring. If you are making requests using JSONP, see JSONP requirements. 2. Verify the object is instrumented. If you are using XMLHttpRequest, use your browser's dev console to verify that the object has been instrumented by New Relic. Enter the object name at your console. If instrumentation has succeeded, the console should return something like: function (t){var e=new p(t);try{u.emit(\"new-xhr\"... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see Troubleshooting browser monitoring installation. If you see a different response, you may be using another script or library that is conflicting with New Relic instrumentation. Contact support at support.newrelic.com. 3. Verify network access. If the object is properly instrumented, try triggering an AJAX call in your application while monitoring network traffic in the browser's developer tools. Wait up to one minute, and look for a call to bam.nr-data.net/jserrors with an xhr parameter. If the call fails, check for network issues. If you don't see this call, if it fails with an error not related to network access, or if it succeeds but you still aren't seeing data, get support at support.newrelic.com. If your requests use JSONP, see requirements and notes on functionality below: JSONP requirements If your requests use JSONP, these requests will not appear on the AJAX UI page. However, you can view them as assets within session traces. If using SPA monitoring, you can view them on the Breakdown tab of the Page views page. Requirements for JSONP to be recognized: Each JSONP request must use a unique callback function. Most popular libraries (like jQuery) generate a unique callback function dynamically for each request. The query string callback must be named \"callback\" or \"cb\" in order to be recognized by New Relic. This is the default behavior in most popular libraries.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.06745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Troubleshoot</em> AJAX data collection",
        "sections": "<em>Troubleshoot</em> AJAX data collection",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": ": function (t){var e=new p(t);try{u.emit(&quot;new-xhr&quot;... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see <em>Troubleshooting</em> <em>browser</em> <em>monitoring</em> installation. If you see a different response, you may"
      },
      "id": "603e902d196a6762dea83d8a"
    },
    {
      "sections": [
        "Browser data doesn't match other analytics tools",
        "Problem",
        "Solution"
      ],
      "title": "Browser data doesn't match other analytics tools ",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "02439fadf41050ba60397b865b87f79576143295",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/browser-data-doesnt-match-other-analytics-tools/",
      "published_at": "2022-01-12T09:12:57Z",
      "updated_at": "2021-07-09T13:47:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Problem You see different data in browser monitoring than other analytics tools you use. Common symptoms include: Browser records fewer page views than you expect. Browser's geographical distribution for your users differs from the locations of your expected user base. Browser shows a different number of active sessions than you see elsewhere. Solution Follow these troubleshooting steps for browser monitoring: Check whether browser monitoring is properly installed in all of your application's pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic, follow the steps for troubleshooting browser monitoring installation. If you have a single-page style application and are expecting to see your route changes as views, consider using browser SPA monitoring, which provides an integrated view of initial page loads and route changes. If browser monitoring is working properly, you may be seeing different results because New Relic collects and presents information differently from other analytics tools. For more information about how we collects data, see: Page load timing process Instrumentation for browser monitoring Understand SPA data collection",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.3853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> data doesn&#x27;t match other analytics tools ",
        "sections": "<em>Browser</em> data doesn&#x27;t match other analytics tools",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " number of active sessions than you see elsewhere. Solution Follow these <em>troubleshooting</em> steps for <em>browser</em> <em>monitoring</em>: Check whether <em>browser</em> <em>monitoring</em> is properly installed in all of your application&#x27;s pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic"
      },
      "id": "6043ecc328ccbc6f972e1dcc"
    }
  ],
  "/docs/browser/new-relic-browser/troubleshooting/third-party-js-errors-missing-stack-traces": [
    {
      "sections": [
        "AJAX call fails with a CORS redirect error message",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "AJAX call fails with a CORS redirect error message",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "a4e478428acefb454ab8969cccb666d03ae458f1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/ajax-call-fails-cors-redirect-error-message/",
      "published_at": "2022-01-12T09:12:16Z",
      "updated_at": "2021-11-13T07:04:35Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem A redirected AJAX call is being rejected with a CORS error message, for example: Access to XMLHttpRequest at 'https://my-domain-2/path' (redirected from 'https://my-domain-1/path') from origin 'https://my-website-domain' has been blocked by CORS policy: Request header field x-newrelic-id is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The browser agent automatically adds custom headers to outgoing same-origin AJAX calls in order to support the Distributed Tracing feature. When the server that receives the AJAX call responds with a redirect status code (such as 302), the browser will automatically make the same AJAX call to the redirected URL. And if this new URL is on a different origin and the call does not pass the CORS preflight, the browser will fail the call with the error message listed above.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.0676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The <em>browser</em> agent"
      },
      "id": "603eb41ce7b9d2ce042a07db"
    },
    {
      "sections": [
        "Troubleshoot AJAX data collection",
        "Problem",
        "Solution",
        "1. Verify you use XMLHttpRequest.",
        "2. Verify the object is instrumented.",
        "3. Verify network access.",
        "JSONP requirements"
      ],
      "title": "Troubleshoot AJAX data collection",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "8dc486fa0fbb0a07f5e93c3cf75590e3e03bba2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/troubleshoot-ajax-data-collection/",
      "published_at": "2022-01-12T07:46:12Z",
      "updated_at": "2021-11-13T07:03:46Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are not seeing AJAX data for your browser app. Solution If your application is instrumented with browser monitoring and is correctly collecting data for other Pro features, follow these steps: 1. Verify you use XMLHttpRequest. Check whether your application uses the XMLHttpRequest object to make AJAX calls. Browser monitoring: Other methods (including the newer Fetch API) currently are not supported when using browser Pro. Single-page app monitoring: Fetch is supported for AJAX requests within a browser interaction with SPA monitoring. If you are making requests using JSONP, see JSONP requirements. 2. Verify the object is instrumented. If you are using XMLHttpRequest, use your browser's dev console to verify that the object has been instrumented by New Relic. Enter the object name at your console. If instrumentation has succeeded, the console should return something like: function (t){var e=new p(t);try{u.emit(\"new-xhr\"... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see Troubleshooting browser monitoring installation. If you see a different response, you may be using another script or library that is conflicting with New Relic instrumentation. Contact support at support.newrelic.com. 3. Verify network access. If the object is properly instrumented, try triggering an AJAX call in your application while monitoring network traffic in the browser's developer tools. Wait up to one minute, and look for a call to bam.nr-data.net/jserrors with an xhr parameter. If the call fails, check for network issues. If you don't see this call, if it fails with an error not related to network access, or if it succeeds but you still aren't seeing data, get support at support.newrelic.com. If your requests use JSONP, see requirements and notes on functionality below: JSONP requirements If your requests use JSONP, these requests will not appear on the AJAX UI page. However, you can view them as assets within session traces. If using SPA monitoring, you can view them on the Breakdown tab of the Page views page. Requirements for JSONP to be recognized: Each JSONP request must use a unique callback function. Most popular libraries (like jQuery) generate a unique callback function dynamically for each request. The query string callback must be named \"callback\" or \"cb\" in order to be recognized by New Relic. This is the default behavior in most popular libraries.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.06745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Troubleshoot</em> AJAX data collection",
        "sections": "<em>Troubleshoot</em> AJAX data collection",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": ": function (t){var e=new p(t);try{u.emit(&quot;new-xhr&quot;... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see <em>Troubleshooting</em> <em>browser</em> <em>monitoring</em> installation. If you see a different response, you may"
      },
      "id": "603e902d196a6762dea83d8a"
    },
    {
      "sections": [
        "Browser data doesn't match other analytics tools",
        "Problem",
        "Solution"
      ],
      "title": "Browser data doesn't match other analytics tools ",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "02439fadf41050ba60397b865b87f79576143295",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/browser-data-doesnt-match-other-analytics-tools/",
      "published_at": "2022-01-12T09:12:57Z",
      "updated_at": "2021-07-09T13:47:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Problem You see different data in browser monitoring than other analytics tools you use. Common symptoms include: Browser records fewer page views than you expect. Browser's geographical distribution for your users differs from the locations of your expected user base. Browser shows a different number of active sessions than you see elsewhere. Solution Follow these troubleshooting steps for browser monitoring: Check whether browser monitoring is properly installed in all of your application's pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic, follow the steps for troubleshooting browser monitoring installation. If you have a single-page style application and are expecting to see your route changes as views, consider using browser SPA monitoring, which provides an integrated view of initial page loads and route changes. If browser monitoring is working properly, you may be seeing different results because New Relic collects and presents information differently from other analytics tools. For more information about how we collects data, see: Page load timing process Instrumentation for browser monitoring Understand SPA data collection",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.3853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> data doesn&#x27;t match other analytics tools ",
        "sections": "<em>Browser</em> data doesn&#x27;t match other analytics tools",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " number of active sessions than you see elsewhere. Solution Follow these <em>troubleshooting</em> steps for <em>browser</em> <em>monitoring</em>: Check whether <em>browser</em> <em>monitoring</em> is properly installed in all of your application&#x27;s pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic"
      },
      "id": "6043ecc328ccbc6f972e1dcc"
    }
  ],
  "/docs/browser/new-relic-browser/troubleshooting/troubleshoot-ajax-data-collection": [
    {
      "sections": [
        "AJAX call fails with a CORS redirect error message",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "AJAX call fails with a CORS redirect error message",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "a4e478428acefb454ab8969cccb666d03ae458f1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/ajax-call-fails-cors-redirect-error-message/",
      "published_at": "2022-01-12T09:12:16Z",
      "updated_at": "2021-11-13T07:04:35Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem A redirected AJAX call is being rejected with a CORS error message, for example: Access to XMLHttpRequest at 'https://my-domain-2/path' (redirected from 'https://my-domain-1/path') from origin 'https://my-website-domain' has been blocked by CORS policy: Request header field x-newrelic-id is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The browser agent automatically adds custom headers to outgoing same-origin AJAX calls in order to support the Distributed Tracing feature. When the server that receives the AJAX call responds with a redirect status code (such as 302), the browser will automatically make the same AJAX call to the redirected URL. And if this new URL is on a different origin and the call does not pass the CORS preflight, the browser will fail the call with the error message listed above.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.06759,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The <em>browser</em> agent"
      },
      "id": "603eb41ce7b9d2ce042a07db"
    },
    {
      "sections": [
        "Browser data doesn't match other analytics tools",
        "Problem",
        "Solution"
      ],
      "title": "Browser data doesn't match other analytics tools ",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "02439fadf41050ba60397b865b87f79576143295",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/browser-data-doesnt-match-other-analytics-tools/",
      "published_at": "2022-01-12T09:12:57Z",
      "updated_at": "2021-07-09T13:47:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Problem You see different data in browser monitoring than other analytics tools you use. Common symptoms include: Browser records fewer page views than you expect. Browser's geographical distribution for your users differs from the locations of your expected user base. Browser shows a different number of active sessions than you see elsewhere. Solution Follow these troubleshooting steps for browser monitoring: Check whether browser monitoring is properly installed in all of your application's pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic, follow the steps for troubleshooting browser monitoring installation. If you have a single-page style application and are expecting to see your route changes as views, consider using browser SPA monitoring, which provides an integrated view of initial page loads and route changes. If browser monitoring is working properly, you may be seeing different results because New Relic collects and presents information differently from other analytics tools. For more information about how we collects data, see: Page load timing process Instrumentation for browser monitoring Understand SPA data collection",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.3853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> data doesn&#x27;t match other analytics tools ",
        "sections": "<em>Browser</em> data doesn&#x27;t match other analytics tools",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " number of active sessions than you see elsewhere. Solution Follow these <em>troubleshooting</em> steps for <em>browser</em> <em>monitoring</em>: Check whether <em>browser</em> <em>monitoring</em> is properly installed in all of your application&#x27;s pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic"
      },
      "id": "6043ecc328ccbc6f972e1dcc"
    },
    {
      "sections": [
        "Troubleshoot your browser monitoring installation",
        "Problem",
        "Solution",
        "Deployment via APM agent",
        "Enable your app",
        "Update your APM agent",
        "Tip",
        "Verify the JavaScript snippets",
        "Check other JavaScript error monitors",
        "Configure auto-instrumentation",
        "Important",
        "Verify manual API instrumentation",
        "Restart your APM app",
        "Checkpoint: Verify JavaScript on page",
        "Verify end-user network access",
        "Deployment via JavaScript copy/paste",
        "Clear any cached versions of your pages",
        "Check the JavaScript placement and completeness",
        "Razor framework: Check for a parser error message"
      ],
      "title": "Troubleshoot your browser monitoring installation",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "38dd3206f05a9dda3e4561843c1488ecb0f3c276",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/browser-monitoring/troubleshooting/troubleshoot-your-browser-monitoring-installation/",
      "published_at": "2022-01-12T08:51:03Z",
      "updated_at": "2021-07-27T10:04:27Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem If you have just configured your application with browser monitoring, wait a few minutes for data to appear. If you have already waited a few minutes and you still do not see any data, try these troubleshooting tips. Some of these tips depend on whether you deployed your app by allowing browser monitoring to automatically insert the JavaScript snippet into your APM-monitored app or by inserting the JavaScript snippet yourself. Solution Recommendation: Before following specific troubleshooting procedures, run the Diagnostics browser checks. Once Diagnostics CLI is downloaded, target the browser checks by running the command-line options: /nrdiag -browser-url YOUR-WEBSITE-URL -suites browser. The information returned from Diagnostics CLI can be used when communicating with New Relic Support. If your browser monitoring agent was deployed for an account monitored with an APM agent, see APM deployments. If your browser monitoring agent was deployed using the JavaScript copy/paste method, see Copy/paste deployment. If you are unsure which you have, the Diagnostics tool will return the deployment method. You can also troubleshoot situations where you are missing only AJAX or session trace data. Deployment via APM agent These troubleshooting steps apply to problems when the browser monitoring agent is installed on an app already being monitored by an APM agent: Enable your app Verify that browser monitoring has been enabled in your application's Browser settings: Go to one.newrelic.com > Browser > (select an app) > Settings. Follow standard procedures to enable browser settings for each app. Flush your webserver cache. Wait a few more minutes for data to arrive. Try the next steps if you still do not see any data. Update your APM agent More recent APM agents place the page load timing scripts more accurately. Before continuing with these troubleshooting tips, make sure you have the most recent release for your APM agent. Tip For agent version requirements for page load timing, see Compatibility and requirements. Verify the JavaScript snippets Check your page's source for the two script elements required to capture metrics and send them to New Relic. In a browser, view the source of your page, and look for a script element near the beginning like this: <script type=\"text/javascript\"> (window.NREUM||(NREUM={})).loader_config={xpid:\"VRUGVVJS\";window.NREUM||(NREUM={}),__nr_require=function a(b,c,d){ ... Copy OR like this: <script type=\"text/javascript\"> window.NREUM||(NREUM={}),__nr_require=function a (b,c,d){ ... Copy Also verify that a second script element exists in one of two locations, depending on the app server agent language. C SDK: n/a Go: n/a Java: Before the </body> tag (which must be added to the page if missing) .NET: Immediately before the first script element Node.js: Immediately before the first script element PHP: At the end of the body element Python: In the head element or at the end of the body element Ruby: Immediately before the first script element The second tag contains configuration and timing data, and looks like this: <script type=\"text/javascript\"> window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\"... Copy If either script element is missing, continue troubleshooting with the following steps. If the script elements are present and data does not appear after several minutes, get support at support.newrelic.com. Check other JavaScript error monitors If you see JavaScript errors on your webpage, or if you do not see any data on your JavaScript errors page in the New Relic UI, check if any other JavaScript error monitoring frameworks have been enabled for your app. Some JavaScript error monitors may interfere with functionality for browser monitoring. Configure auto-instrumentation If you are using New Relic's automatic instrumentation feature, ensure your agent is configured properly. Each agent has a configuration file setting and specific instructions to turn auto-instrumentation on or off: C SDK: n/a Go: n/a Java .NET Node.js agent: Currently not supported; see manual instrumentation procedures PHP Python Ruby Important If you modify your agent's configuration file, be sure to restart your app. Verify manual API instrumentation If you are manually calling the New Relic agent API to generate and insert the JavaScript, verify that the calls are actually being made. The APIs and how to use them are specific to your agent: C SDK: n/a Go: n/a Java agent .NET agent Node.js agent (see also the Node.js troubleshooting procedures) PHP agent Python agent Ruby agent Restart your APM app If you modified your APM agent's configuration file, be sure to restart your application so that it picks up the modified settings. Java agent: Flush the app server’s \"work\" cache. This forces the app server to recompile. .NET agent: Make sure your asp.net cache directory is clean by using the command flush_dotnet_temp.cmd. This forces the app server to recompile ASPs with page load timing instrumentation. Checkpoint: Verify JavaScript on page Verify that the JavaScript is on the page as detailed in the previous step Verify the JavaScript snippets. If it is not on the page, use the manual instrumentation as detailed in Verify manual API instrumentation. The Javascript must be present on your page before you can proceed to the next step. Verify end-user network access If your application is loaded primarily within a secured local network, ensure that your users can reach the necessary network endpoints to report browser data. This includes New Relic's CDNs and beacon. Deployment via JavaScript copy/paste These troubleshooting steps apply to problems when New Relic's browser monitoring agent was deployed using the HTML copy/paste method: Enable your app Verify that browser monitoring has been enabled in your application's Browser settings: Go to one.newrelic.com > Browser > (select an app) > Settings. Follow standard procedures to enable browser settings for each app. Flush your webserver cache. Wait a few more minutes for data to arrive. Try the next steps if you still do not see any data. Verify the JavaScript snippets Check your page's source for the script element required to capture metrics and send them to New Relic. In a browser, view the source of your page, and look for a script element near the beginning like this: <script type=\"text/javascript\"> window.NREUM||(NREUM={}),__nr_require=function a (b,c,d){ ... Copy If the script element is missing, make sure that you have pasted it in and deployed your new code. If you have not yet pasted in the JavaScript snippet, you can copy it again from your browser app's Settings page. If applicable, restart the application serving these pages. If you still don't see the script, continue troubleshooting with the following steps. Clear any cached versions of your pages For the Javascript snippets to appear in pages served by your application, ensure that any cached versions of the pages from before you added the code have been cleared. Check any CDN caches. Flush your webserver cache. Check the JavaScript placement and completeness Insert the JavaScript as close to the top of the HEAD as possible, but after any position-sensitive META tags (X-UA-Compatible and charset). It must be outside any comments. Here is an example of the general format: <!DOCTYPE html> <html> <head> <meta tags> {PLACE NEW RELIC SCRIPT TAG HERE} <script and link tags> </head> <body> ... </body> </html> Copy If the Javascript is correctly placed, compare it with the original snippet you generated to ensure it is complete and identical. Incomplete or modified snippets may not report correctly. Check other JavaScript error monitors If you see JavaScript errors on your webpage, or if you do not see any data on your JavaScript errors page in the New Relic UI, check if any other JavaScript error monitoring frameworks have been enabled for your app. Some JavaScript error monitors may interfere with functionality for browser monitoring. Verify end-user network access If your application is loaded primarily within a secured local network, ensure that your users can reach the necessary network endpoints to report browser data. This includes New Relic's CDNs and beacon. Razor framework: Check for a parser error message If you use the copy/paste installation method in a .NET app that uses the Razor Framework, this may result in the following error: Parser Error Message: \"\").pop().split(\"\" is not valid at the start of a code block. Only identifiers, keywords, comments, \"(\" and \"{\" are valid. Copy The error is caused by an @ symbol in theJavaScript snippet for browser monitoring . The @ symbol represents the beginning of a code block in Razor. The line that causes the problem is: .split(\"@\").pop().split(\":\") Copy To fix this issue, use one of these workarounds: Wrap the browser JavaScript snippet in <text></text> tags to force it to be interpreted as content. OR Add another @ to the line to escape it. Check that the line looks like this: .split(\"@@\").pop().split(\":\") Copy Use only one of the workarounds. Using both will break the code again.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.19076,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Troubleshoot</em> your <em>browser</em> <em>monitoring</em> installation",
        "sections": "<em>Troubleshoot</em> your <em>browser</em> <em>monitoring</em> installation",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": "Problem If you have just configured your application with <em>browser</em> <em>monitoring</em>, wait a few minutes for data to appear. If you have already waited a few minutes and you still do not see any data, try these <em>troubleshooting</em> tips. Some of these tips depend on whether you deployed your app by allowing"
      },
      "id": "603e8f2864441fa5024e8877"
    }
  ],
  "/docs/browser/new-relic-browser/troubleshooting/troubleshooting-session-trace-collection": [
    {
      "sections": [
        "AJAX call fails with a CORS redirect error message",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "AJAX call fails with a CORS redirect error message",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "a4e478428acefb454ab8969cccb666d03ae458f1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/ajax-call-fails-cors-redirect-error-message/",
      "published_at": "2022-01-12T09:12:16Z",
      "updated_at": "2021-11-13T07:04:35Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem A redirected AJAX call is being rejected with a CORS error message, for example: Access to XMLHttpRequest at 'https://my-domain-2/path' (redirected from 'https://my-domain-1/path') from origin 'https://my-website-domain' has been blocked by CORS policy: Request header field x-newrelic-id is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The browser agent automatically adds custom headers to outgoing same-origin AJAX calls in order to support the Distributed Tracing feature. When the server that receives the AJAX call responds with a redirect status code (such as 302), the browser will automatically make the same AJAX call to the redirected URL. And if this new URL is on a different origin and the call does not pass the CORS preflight, the browser will fail the call with the error message listed above.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.06759,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The <em>browser</em> agent"
      },
      "id": "603eb41ce7b9d2ce042a07db"
    },
    {
      "sections": [
        "Troubleshoot AJAX data collection",
        "Problem",
        "Solution",
        "1. Verify you use XMLHttpRequest.",
        "2. Verify the object is instrumented.",
        "3. Verify network access.",
        "JSONP requirements"
      ],
      "title": "Troubleshoot AJAX data collection",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "8dc486fa0fbb0a07f5e93c3cf75590e3e03bba2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/troubleshoot-ajax-data-collection/",
      "published_at": "2022-01-12T07:46:12Z",
      "updated_at": "2021-11-13T07:03:46Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are not seeing AJAX data for your browser app. Solution If your application is instrumented with browser monitoring and is correctly collecting data for other Pro features, follow these steps: 1. Verify you use XMLHttpRequest. Check whether your application uses the XMLHttpRequest object to make AJAX calls. Browser monitoring: Other methods (including the newer Fetch API) currently are not supported when using browser Pro. Single-page app monitoring: Fetch is supported for AJAX requests within a browser interaction with SPA monitoring. If you are making requests using JSONP, see JSONP requirements. 2. Verify the object is instrumented. If you are using XMLHttpRequest, use your browser's dev console to verify that the object has been instrumented by New Relic. Enter the object name at your console. If instrumentation has succeeded, the console should return something like: function (t){var e=new p(t);try{u.emit(\"new-xhr\"... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see Troubleshooting browser monitoring installation. If you see a different response, you may be using another script or library that is conflicting with New Relic instrumentation. Contact support at support.newrelic.com. 3. Verify network access. If the object is properly instrumented, try triggering an AJAX call in your application while monitoring network traffic in the browser's developer tools. Wait up to one minute, and look for a call to bam.nr-data.net/jserrors with an xhr parameter. If the call fails, check for network issues. If you don't see this call, if it fails with an error not related to network access, or if it succeeds but you still aren't seeing data, get support at support.newrelic.com. If your requests use JSONP, see requirements and notes on functionality below: JSONP requirements If your requests use JSONP, these requests will not appear on the AJAX UI page. However, you can view them as assets within session traces. If using SPA monitoring, you can view them on the Breakdown tab of the Page views page. Requirements for JSONP to be recognized: Each JSONP request must use a unique callback function. Most popular libraries (like jQuery) generate a unique callback function dynamically for each request. The query string callback must be named \"callback\" or \"cb\" in order to be recognized by New Relic. This is the default behavior in most popular libraries.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.067444,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Troubleshoot</em> AJAX data collection",
        "sections": "<em>Troubleshoot</em> AJAX data collection",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": ": function (t){var e=new p(t);try{u.emit(&quot;new-xhr&quot;... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see <em>Troubleshooting</em> <em>browser</em> <em>monitoring</em> installation. If you see a different response, you may"
      },
      "id": "603e902d196a6762dea83d8a"
    },
    {
      "sections": [
        "Browser data doesn't match other analytics tools",
        "Problem",
        "Solution"
      ],
      "title": "Browser data doesn't match other analytics tools ",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "02439fadf41050ba60397b865b87f79576143295",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/browser-data-doesnt-match-other-analytics-tools/",
      "published_at": "2022-01-12T09:12:57Z",
      "updated_at": "2021-07-09T13:47:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Problem You see different data in browser monitoring than other analytics tools you use. Common symptoms include: Browser records fewer page views than you expect. Browser's geographical distribution for your users differs from the locations of your expected user base. Browser shows a different number of active sessions than you see elsewhere. Solution Follow these troubleshooting steps for browser monitoring: Check whether browser monitoring is properly installed in all of your application's pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic, follow the steps for troubleshooting browser monitoring installation. If you have a single-page style application and are expecting to see your route changes as views, consider using browser SPA monitoring, which provides an integrated view of initial page loads and route changes. If browser monitoring is working properly, you may be seeing different results because New Relic collects and presents information differently from other analytics tools. For more information about how we collects data, see: Page load timing process Instrumentation for browser monitoring Understand SPA data collection",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.3853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> data doesn&#x27;t match other analytics tools ",
        "sections": "<em>Browser</em> data doesn&#x27;t match other analytics tools",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " number of active sessions than you see elsewhere. Solution Follow these <em>troubleshooting</em> steps for <em>browser</em> <em>monitoring</em>: Check whether <em>browser</em> <em>monitoring</em> is properly installed in all of your application&#x27;s pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic"
      },
      "id": "6043ecc328ccbc6f972e1dcc"
    }
  ],
  "/docs/browser/new-relic-browser/troubleshooting/view-detailed-error-logs-browser": [
    {
      "sections": [
        "AJAX call fails with a CORS redirect error message",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "AJAX call fails with a CORS redirect error message",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "a4e478428acefb454ab8969cccb666d03ae458f1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/ajax-call-fails-cors-redirect-error-message/",
      "published_at": "2022-01-12T09:12:16Z",
      "updated_at": "2021-11-13T07:04:35Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem A redirected AJAX call is being rejected with a CORS error message, for example: Access to XMLHttpRequest at 'https://my-domain-2/path' (redirected from 'https://my-domain-1/path') from origin 'https://my-website-domain' has been blocked by CORS policy: Request header field x-newrelic-id is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The browser agent automatically adds custom headers to outgoing same-origin AJAX calls in order to support the Distributed Tracing feature. When the server that receives the AJAX call responds with a redirect status code (such as 302), the browser will automatically make the same AJAX call to the redirected URL. And if this new URL is on a different origin and the call does not pass the CORS preflight, the browser will fail the call with the error message listed above.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.06759,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " is not allowed by Access-Control-Allow-Headers in preflight response. Copy Solution To resolve this error, update your code to make the AJAX call to the new URL provided by the redirect. For more information, see the MDN article CORS request external redirect not allowed. Cause The <em>browser</em> agent"
      },
      "id": "603eb41ce7b9d2ce042a07db"
    },
    {
      "sections": [
        "Troubleshoot AJAX data collection",
        "Problem",
        "Solution",
        "1. Verify you use XMLHttpRequest.",
        "2. Verify the object is instrumented.",
        "3. Verify network access.",
        "JSONP requirements"
      ],
      "title": "Troubleshoot AJAX data collection",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "8dc486fa0fbb0a07f5e93c3cf75590e3e03bba2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/troubleshoot-ajax-data-collection/",
      "published_at": "2022-01-12T07:46:12Z",
      "updated_at": "2021-11-13T07:03:46Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are not seeing AJAX data for your browser app. Solution If your application is instrumented with browser monitoring and is correctly collecting data for other Pro features, follow these steps: 1. Verify you use XMLHttpRequest. Check whether your application uses the XMLHttpRequest object to make AJAX calls. Browser monitoring: Other methods (including the newer Fetch API) currently are not supported when using browser Pro. Single-page app monitoring: Fetch is supported for AJAX requests within a browser interaction with SPA monitoring. If you are making requests using JSONP, see JSONP requirements. 2. Verify the object is instrumented. If you are using XMLHttpRequest, use your browser's dev console to verify that the object has been instrumented by New Relic. Enter the object name at your console. If instrumentation has succeeded, the console should return something like: function (t){var e=new p(t);try{u.emit(\"new-xhr\"... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see Troubleshooting browser monitoring installation. If you see a different response, you may be using another script or library that is conflicting with New Relic instrumentation. Contact support at support.newrelic.com. 3. Verify network access. If the object is properly instrumented, try triggering an AJAX call in your application while monitoring network traffic in the browser's developer tools. Wait up to one minute, and look for a call to bam.nr-data.net/jserrors with an xhr parameter. If the call fails, check for network issues. If you don't see this call, if it fails with an error not related to network access, or if it succeeds but you still aren't seeing data, get support at support.newrelic.com. If your requests use JSONP, see requirements and notes on functionality below: JSONP requirements If your requests use JSONP, these requests will not appear on the AJAX UI page. However, you can view them as assets within session traces. If using SPA monitoring, you can view them on the Breakdown tab of the Page views page. Requirements for JSONP to be recognized: Each JSONP request must use a unique callback function. Most popular libraries (like jQuery) generate a unique callback function dynamically for each request. The query string callback must be named \"callback\" or \"cb\" in order to be recognized by New Relic. This is the default behavior in most popular libraries.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.067444,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Troubleshoot</em> AJAX data collection",
        "sections": "<em>Troubleshoot</em> AJAX data collection",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": ": function (t){var e=new p(t);try{u.emit(&quot;new-xhr&quot;... Copy If instrumentation failed, you will see something like: function XMLHttpRequest() { [native_code] } Copy If you see this type of failure response, see <em>Troubleshooting</em> <em>browser</em> <em>monitoring</em> installation. If you see a different response, you may"
      },
      "id": "603e902d196a6762dea83d8a"
    },
    {
      "sections": [
        "Browser data doesn't match other analytics tools",
        "Problem",
        "Solution"
      ],
      "title": "Browser data doesn't match other analytics tools ",
      "type": "docs",
      "tags": [
        "Browser",
        "Browser monitoring",
        "Troubleshooting"
      ],
      "external_id": "02439fadf41050ba60397b865b87f79576143295",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/new-relic-browser/troubleshooting/browser-data-doesnt-match-other-analytics-tools/",
      "published_at": "2022-01-12T09:12:57Z",
      "updated_at": "2021-07-09T13:47:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Problem You see different data in browser monitoring than other analytics tools you use. Common symptoms include: Browser records fewer page views than you expect. Browser's geographical distribution for your users differs from the locations of your expected user base. Browser shows a different number of active sessions than you see elsewhere. Solution Follow these troubleshooting steps for browser monitoring: Check whether browser monitoring is properly installed in all of your application's pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic, follow the steps for troubleshooting browser monitoring installation. If you have a single-page style application and are expecting to see your route changes as views, consider using browser SPA monitoring, which provides an integrated view of initial page loads and route changes. If browser monitoring is working properly, you may be seeing different results because New Relic collects and presents information differently from other analytics tools. For more information about how we collects data, see: Page load timing process Instrumentation for browser monitoring Understand SPA data collection",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.3853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> data doesn&#x27;t match other analytics tools ",
        "sections": "<em>Browser</em> data doesn&#x27;t match other analytics tools",
        "tags": "<em>Browser</em> <em>monitoring</em>",
        "body": " number of active sessions than you see elsewhere. Solution Follow these <em>troubleshooting</em> steps for <em>browser</em> <em>monitoring</em>: Check whether <em>browser</em> <em>monitoring</em> is properly installed in all of your application&#x27;s pages. If the JavaScript snippets are missing, or if users are unable to send data to New Relic"
      },
      "id": "6043ecc328ccbc6f972e1dcc"
    }
  ],
  "/docs/browser/single-page-app-monitoring/get-started/install-single-page-app-monitoring": [
    {
      "sections": [
        "Introduction to Single Page App monitoring",
        "Enable SPA monitoring",
        "Analyze throughput and performance data",
        "Browser SPA features"
      ],
      "title": "Introduction to Single Page App monitoring",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "6dedda52851e1ca1f180c8d88bdcb7038c4d1b5d",
      "image": "https://docs.newrelic.com/static/98d434a02c314f2bd2ce9828aa7b755d/c1b63/browser_SPA.png",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/introduction-single-page-app-monitoring/",
      "published_at": "2022-01-12T02:17:37Z",
      "updated_at": "2021-07-21T20:07:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic browser monitoring has a single-page application (SPA) monitoring feature that provides deeper visibility and actionable insights into real user interactions with single-page apps, and for any app that uses AJAX requests. In addition to monitoring route changes automatically, our SPA API allows you to monitor virtually anything that executes inside the browser. This allows developers and their team to: Create faster, more responsive, highly interactive apps. Monitor the throughput and performance that real users are experiencing. Troubleshoot and resolve problems within the context of the page load. Query your data to assist with business decisions. Bring better apps to the marketplace more quickly. Enable SPA monitoring SPA monitoring is enabled by default for new browser agent installations. The SPA-enabled version of the agent gives access to other powerful New Relic features, like distributed tracing. For more information, see Enable browser monitoring. For compatability information for SPA-related features, see SPA requirements. Analyze throughput and performance data Improving on traditional industry standards for measuring page load timing, we give you a complete picture of the activity, both synchronous and asynchronous, associated with page loads and route changes. one.newrelic.com > Browser > (select an app) > Page views: Use browser monitoring's SPA monitoring to examine the throughput and performance of your SPA-architecture app. SPA data monitored by browser monitoring includes: Performance data and throughput for page loads and route changes AJAX request data JavaScript activity, both synchronous and asynchronous Dynamic page updates, monitored using the SPA API With this data, you will gain a clear understanding of how your users experience your app's page loads and route changes, and be able to solve bottlenecks and troubleshoot errors. For more about how New Relic handles SPA data, see Understand SPA data collection. Browser SPA features Here is a summary of SPA monitoring features: Single-page app monitoring Take advantage of these features Robust views in browser's UI When a user initiates a page load or route change, New Relic begins to monitor all subsequent JavaScript, and ends the timing once all AJAX events are complete. This provides a more accurate view of when a page is actually ready for a user compared to the traditional method of ending the timing when the window load event is fired. When SPA monitoring is enabled, the Page views page in browser shows event-driven data about application usage levels (throughput) and user experience (performance), including: Charts with drill-down details about initial page load performance, route changes, and historical performance Sort, search, and filter options, including custom attributes Additional AJAX breakdown data for all initial page loads and route changes For an explanation of how SPA monitoring will impact your existing browser account's data usage, see SPA and browser data usage. Data analysis with data explorer The data explorer supports three SPA-specific event types: BrowserInteraction, AjaxRequest, and BrowserTiming. You can query these events in the query builder to analyze your app's performance and make business decisions. Customized data from API Use SPA API to obtain the specific data you need, such as custom naming, custom timing, finishline API, or other custom attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.0102,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "sections": "Introduction to <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "New Relic <em>browser</em> <em>monitoring</em> has a <em>single</em>-<em>page</em> application (SPA) <em>monitoring</em> feature that provides deeper visibility and actionable insights into real user interactions with <em>single</em>-<em>page</em> apps, and for any <em>app</em> that uses AJAX requests. In addition to <em>monitoring</em> route changes automatically, our SPA API"
      },
      "id": "604408d328ccbcf69e2c6064"
    },
    {
      "sections": [
        "SPA compatibility and requirements",
        "Browser agent version",
        "Browser types",
        "Framework requirements",
        "Security when collecting hash fragments"
      ],
      "title": "SPA compatibility and requirements",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "17d916a952f7b86a1da190a9d7236072eff12361",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/spa-compatibility-requirements/",
      "published_at": "2022-01-12T03:27:26Z",
      "updated_at": "2021-07-09T07:41:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to set up Single Page Application (SPA) monitoring for browser monitoring, make sure your app meets these SPA monitoring requirements. Browser agent version SPA monitoring requires an SPA-specific version of the browser snippet, available for browser agent version 885 or higher. To activate this snippet version for your application, enable your application for SPA monitoring. To check your version and integrate the updated snippet, follow the appropriate upgrade instructions. Browser types SPA monitoring requires the addEventListener browser API and the Navigation Timing API. Both APIs are available in all modern browsers, including Google Chrome, Mozilla Firefox, Apple Safari, and Microsoft Internet Explorer (IE) versions 9 or higher. Framework requirements Because SPA instrumentation works by wrapping low-level browser APIs, it is framework-agnostic. SPA instrumentation is compatible with most SPA frameworks, such as Angular, Backbone, Ember, and React. It can also instrument requests made using JSONP. Below are known compatibility issues: If your application uses AngularJS and you want to use browser's SPA monitoring capabilities, Zone.js versions 0.6.18-0.6.24 are not compatible with the SPA agent. The html2pdf.js library is not compatible with the SPA agent. Security when collecting hash fragments New Relic collects and saves hash fragments from route change URLs. If you use hashes to pass private or sensitive data, that data may be visible to your New Relic account users. Follow browser's guidelines for security with data collection and reporting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 185.33209,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Browser</em> agent version",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "In order to set up <em>Single</em> <em>Page</em> Application (SPA) <em>monitoring</em> for <em>browser</em> <em>monitoring</em>, make sure your <em>app</em> meets these SPA <em>monitoring</em> requirements. <em>Browser</em> agent version SPA <em>monitoring</em> requires an SPA-specific version of the <em>browser</em> snippet, available for <em>browser</em> agent version 885 or higher"
      },
      "id": "6044095ee7b9d20d555799f3"
    },
    {
      "sections": [
        "Missing route changes with SPA agent",
        "Problem",
        "Solution",
        "Short term solutions",
        "Support",
        "Cause"
      ],
      "title": "Missing route changes with SPA agent",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Troubleshooting"
      ],
      "external_id": "9ca088a0459684464512ee51dfb7ffca22e26c14",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/troubleshooting/missing-route-changes-spa-agent/",
      "published_at": "2022-01-12T03:23:27Z",
      "updated_at": "2021-08-20T21:36:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are using the Pro + SPA agent, but you are not seeing all of the route change browser interactions you expect. We are aware that this can be frustrating. Our goal throughout 2021 is to reevaluate the SPA feature functionality, making it simpler and more reliable, starting with the methods we use to detect and capture route changes. Additionally, we plan to add new frameworks and use cases to our testing suite based on your feedback and examples. Likely this work will include new APIs as well as framework-specific plug-ins. Check our release notes for the latest updates. Solution Short term solutions To make sure all route changes are captured, you can use our SPA interaction() API. Using the interaction API will categorize the BrowserInteraction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework is exposing events that represent router activity, you can use custom instrumentation in these events. Here is an example using our API with the Angular router: router.events.subscribe( (event: Event) => { if (event instanceof NavigationStart) { let i = newrelic.interaction() i.setName(event.url) i.save() } }); Copy In this example, the router object is an instance of the Angular router (from the @angular/router module). The setName call sets the name attribute of the BrowserInteraction event to the new URL, and the save call ensures that the interaction is captured. You will need to adapt this for the needs of your own application’s framework. If your framework does not provide routing events, then you can add this code in the event handler of the original interaction event such as click): myButton.addEventListener('click', function () { let i = newrelic.interaction() i.setName(‘new URL') i.save() }); Copy Recommendation: If you do not have access to router events nor the interaction event handler, implement this as soon as possible in code that you know is the result of a user interaction. An alternative to using the SPA API is to capture routes as PageAction events. PageAction events can be used to capture any custom data. We recommend this option as a fallback in case using the SPA interaction API does not work as expected, or to completely separate the custom instrumentation from built-in BrowserInteraction events. Both of these solutions can ensure these events are captured, either as a BrowserInteraction event or as a PageAction event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations where you are seeing most route changes, but none for a particular route change you expect, attempt to evaluate the difference in the implementation of the code for that particular route. Is there something non-standard or unique about that route that you could provide to our support team? Document the frameworks and any libraries that might be of interest. If this is a new problem, has anything changed in your environment that has led to these interactions suddenly not being tracked? Note the browser agent version you are using. If you are more than a few releases behind, we will recommend that you update to the latest release, as we may have already resolved a similar issue. Be aware that due to the complexity of diagnosing these issues, the team will likely need access to an environment and code that demonstrates the problem for testing and research. Cause The browser agent attempts to be framework agnostic, as well as support coding best practices. However, there are often edge cases that will be missed that lead to you not collecting the route changes you expect. The implementation is based on instrumenting most common asynchronous browser APIs. There are cases when a web application uses some asynchronous API or uses custom or third-party code that we do not instrument, and this can result in inaccurate or missed route changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.34016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": " are captured, either as a <em>Browser</em>Interaction event or as a <em>Page</em>Action event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations"
      },
      "id": "603e937628ccbcd4efeba750"
    }
  ],
  "/docs/browser/single-page-app-monitoring/get-started/introduction-single-page-app-monitoring": [
    {
      "sections": [
        "Install Single Page App monitoring",
        "Requirements",
        "Enable or disable SPA monitoring"
      ],
      "title": "Install Single Page App monitoring",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "04501b8d90b2c9b3bf3fa29f1662596a1379e2b6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/install-single-page-app-monitoring/",
      "published_at": "2022-01-12T02:17:36Z",
      "updated_at": "2021-07-27T14:14:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Single page app (SPA) monitoring comes with the default install of the browser agent. Requirements You can review compability and requirements for SPA monitoring here. When you set up your first monitored app in a New Relic account, you must agree to the Terms of Service. By agreeing to the terms, you authorize New Relic to collect hash fragments from URLs. You only need to select the checkbox option once for an account. Enable or disable SPA monitoring When you enable browser monitoring, SPA monitoring is included by default because it gives access to a range of our most recent features, including distributed tracing. Some older agent installations may need to be upgraded. Read more about browser agent types.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.35419,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "sections": "Install <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "<em>Single</em> <em>page</em> <em>app</em> (SPA) <em>monitoring</em> comes with the default install of the <em>browser</em> agent. Requirements You can review compability and requirements for SPA <em>monitoring</em> here. When you set up your first monitored <em>app</em> in a New Relic account, you must agree to the Terms of Service. By agreeing to the terms"
      },
      "id": "6043f16664441f56fa378eec"
    },
    {
      "sections": [
        "SPA compatibility and requirements",
        "Browser agent version",
        "Browser types",
        "Framework requirements",
        "Security when collecting hash fragments"
      ],
      "title": "SPA compatibility and requirements",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "17d916a952f7b86a1da190a9d7236072eff12361",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/spa-compatibility-requirements/",
      "published_at": "2022-01-12T03:27:26Z",
      "updated_at": "2021-07-09T07:41:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to set up Single Page Application (SPA) monitoring for browser monitoring, make sure your app meets these SPA monitoring requirements. Browser agent version SPA monitoring requires an SPA-specific version of the browser snippet, available for browser agent version 885 or higher. To activate this snippet version for your application, enable your application for SPA monitoring. To check your version and integrate the updated snippet, follow the appropriate upgrade instructions. Browser types SPA monitoring requires the addEventListener browser API and the Navigation Timing API. Both APIs are available in all modern browsers, including Google Chrome, Mozilla Firefox, Apple Safari, and Microsoft Internet Explorer (IE) versions 9 or higher. Framework requirements Because SPA instrumentation works by wrapping low-level browser APIs, it is framework-agnostic. SPA instrumentation is compatible with most SPA frameworks, such as Angular, Backbone, Ember, and React. It can also instrument requests made using JSONP. Below are known compatibility issues: If your application uses AngularJS and you want to use browser's SPA monitoring capabilities, Zone.js versions 0.6.18-0.6.24 are not compatible with the SPA agent. The html2pdf.js library is not compatible with the SPA agent. Security when collecting hash fragments New Relic collects and saves hash fragments from route change URLs. If you use hashes to pass private or sensitive data, that data may be visible to your New Relic account users. Follow browser's guidelines for security with data collection and reporting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 185.33209,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Browser</em> agent version",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "In order to set up <em>Single</em> <em>Page</em> Application (SPA) <em>monitoring</em> for <em>browser</em> <em>monitoring</em>, make sure your <em>app</em> meets these SPA <em>monitoring</em> requirements. <em>Browser</em> agent version SPA <em>monitoring</em> requires an SPA-specific version of the <em>browser</em> snippet, available for <em>browser</em> agent version 885 or higher"
      },
      "id": "6044095ee7b9d20d555799f3"
    },
    {
      "sections": [
        "Missing route changes with SPA agent",
        "Problem",
        "Solution",
        "Short term solutions",
        "Support",
        "Cause"
      ],
      "title": "Missing route changes with SPA agent",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Troubleshooting"
      ],
      "external_id": "9ca088a0459684464512ee51dfb7ffca22e26c14",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/troubleshooting/missing-route-changes-spa-agent/",
      "published_at": "2022-01-12T03:23:27Z",
      "updated_at": "2021-08-20T21:36:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are using the Pro + SPA agent, but you are not seeing all of the route change browser interactions you expect. We are aware that this can be frustrating. Our goal throughout 2021 is to reevaluate the SPA feature functionality, making it simpler and more reliable, starting with the methods we use to detect and capture route changes. Additionally, we plan to add new frameworks and use cases to our testing suite based on your feedback and examples. Likely this work will include new APIs as well as framework-specific plug-ins. Check our release notes for the latest updates. Solution Short term solutions To make sure all route changes are captured, you can use our SPA interaction() API. Using the interaction API will categorize the BrowserInteraction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework is exposing events that represent router activity, you can use custom instrumentation in these events. Here is an example using our API with the Angular router: router.events.subscribe( (event: Event) => { if (event instanceof NavigationStart) { let i = newrelic.interaction() i.setName(event.url) i.save() } }); Copy In this example, the router object is an instance of the Angular router (from the @angular/router module). The setName call sets the name attribute of the BrowserInteraction event to the new URL, and the save call ensures that the interaction is captured. You will need to adapt this for the needs of your own application’s framework. If your framework does not provide routing events, then you can add this code in the event handler of the original interaction event such as click): myButton.addEventListener('click', function () { let i = newrelic.interaction() i.setName(‘new URL') i.save() }); Copy Recommendation: If you do not have access to router events nor the interaction event handler, implement this as soon as possible in code that you know is the result of a user interaction. An alternative to using the SPA API is to capture routes as PageAction events. PageAction events can be used to capture any custom data. We recommend this option as a fallback in case using the SPA interaction API does not work as expected, or to completely separate the custom instrumentation from built-in BrowserInteraction events. Both of these solutions can ensure these events are captured, either as a BrowserInteraction event or as a PageAction event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations where you are seeing most route changes, but none for a particular route change you expect, attempt to evaluate the difference in the implementation of the code for that particular route. Is there something non-standard or unique about that route that you could provide to our support team? Document the frameworks and any libraries that might be of interest. If this is a new problem, has anything changed in your environment that has led to these interactions suddenly not being tracked? Note the browser agent version you are using. If you are more than a few releases behind, we will recommend that you update to the latest release, as we may have already resolved a similar issue. Be aware that due to the complexity of diagnosing these issues, the team will likely need access to an environment and code that demonstrates the problem for testing and research. Cause The browser agent attempts to be framework agnostic, as well as support coding best practices. However, there are often edge cases that will be missed that lead to you not collecting the route changes you expect. The implementation is based on instrumenting most common asynchronous browser APIs. There are cases when a web application uses some asynchronous API or uses custom or third-party code that we do not instrument, and this can result in inaccurate or missed route changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.34016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": " are captured, either as a <em>Browser</em>Interaction event or as a <em>Page</em>Action event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations"
      },
      "id": "603e937628ccbcd4efeba750"
    }
  ],
  "/docs/browser/single-page-app-monitoring/get-started/spa-compatibility-requirements": [
    {
      "sections": [
        "Install Single Page App monitoring",
        "Requirements",
        "Enable or disable SPA monitoring"
      ],
      "title": "Install Single Page App monitoring",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "04501b8d90b2c9b3bf3fa29f1662596a1379e2b6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/install-single-page-app-monitoring/",
      "published_at": "2022-01-12T02:17:36Z",
      "updated_at": "2021-07-27T14:14:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Single page app (SPA) monitoring comes with the default install of the browser agent. Requirements You can review compability and requirements for SPA monitoring here. When you set up your first monitored app in a New Relic account, you must agree to the Terms of Service. By agreeing to the terms, you authorize New Relic to collect hash fragments from URLs. You only need to select the checkbox option once for an account. Enable or disable SPA monitoring When you enable browser monitoring, SPA monitoring is included by default because it gives access to a range of our most recent features, including distributed tracing. Some older agent installations may need to be upgraded. Read more about browser agent types.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.35419,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "sections": "Install <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "<em>Single</em> <em>page</em> <em>app</em> (SPA) <em>monitoring</em> comes with the default install of the <em>browser</em> agent. Requirements You can review compability and requirements for SPA <em>monitoring</em> here. When you set up your first monitored <em>app</em> in a New Relic account, you must agree to the Terms of Service. By agreeing to the terms"
      },
      "id": "6043f16664441f56fa378eec"
    },
    {
      "sections": [
        "Introduction to Single Page App monitoring",
        "Enable SPA monitoring",
        "Analyze throughput and performance data",
        "Browser SPA features"
      ],
      "title": "Introduction to Single Page App monitoring",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "6dedda52851e1ca1f180c8d88bdcb7038c4d1b5d",
      "image": "https://docs.newrelic.com/static/98d434a02c314f2bd2ce9828aa7b755d/c1b63/browser_SPA.png",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/introduction-single-page-app-monitoring/",
      "published_at": "2022-01-12T02:17:37Z",
      "updated_at": "2021-07-21T20:07:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic browser monitoring has a single-page application (SPA) monitoring feature that provides deeper visibility and actionable insights into real user interactions with single-page apps, and for any app that uses AJAX requests. In addition to monitoring route changes automatically, our SPA API allows you to monitor virtually anything that executes inside the browser. This allows developers and their team to: Create faster, more responsive, highly interactive apps. Monitor the throughput and performance that real users are experiencing. Troubleshoot and resolve problems within the context of the page load. Query your data to assist with business decisions. Bring better apps to the marketplace more quickly. Enable SPA monitoring SPA monitoring is enabled by default for new browser agent installations. The SPA-enabled version of the agent gives access to other powerful New Relic features, like distributed tracing. For more information, see Enable browser monitoring. For compatability information for SPA-related features, see SPA requirements. Analyze throughput and performance data Improving on traditional industry standards for measuring page load timing, we give you a complete picture of the activity, both synchronous and asynchronous, associated with page loads and route changes. one.newrelic.com > Browser > (select an app) > Page views: Use browser monitoring's SPA monitoring to examine the throughput and performance of your SPA-architecture app. SPA data monitored by browser monitoring includes: Performance data and throughput for page loads and route changes AJAX request data JavaScript activity, both synchronous and asynchronous Dynamic page updates, monitored using the SPA API With this data, you will gain a clear understanding of how your users experience your app's page loads and route changes, and be able to solve bottlenecks and troubleshoot errors. For more about how New Relic handles SPA data, see Understand SPA data collection. Browser SPA features Here is a summary of SPA monitoring features: Single-page app monitoring Take advantage of these features Robust views in browser's UI When a user initiates a page load or route change, New Relic begins to monitor all subsequent JavaScript, and ends the timing once all AJAX events are complete. This provides a more accurate view of when a page is actually ready for a user compared to the traditional method of ending the timing when the window load event is fired. When SPA monitoring is enabled, the Page views page in browser shows event-driven data about application usage levels (throughput) and user experience (performance), including: Charts with drill-down details about initial page load performance, route changes, and historical performance Sort, search, and filter options, including custom attributes Additional AJAX breakdown data for all initial page loads and route changes For an explanation of how SPA monitoring will impact your existing browser account's data usage, see SPA and browser data usage. Data analysis with data explorer The data explorer supports three SPA-specific event types: BrowserInteraction, AjaxRequest, and BrowserTiming. You can query these events in the query builder to analyze your app's performance and make business decisions. Customized data from API Use SPA API to obtain the specific data you need, such as custom naming, custom timing, finishline API, or other custom attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.0102,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "sections": "Introduction to <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "New Relic <em>browser</em> <em>monitoring</em> has a <em>single</em>-<em>page</em> application (SPA) <em>monitoring</em> feature that provides deeper visibility and actionable insights into real user interactions with <em>single</em>-<em>page</em> apps, and for any <em>app</em> that uses AJAX requests. In addition to <em>monitoring</em> route changes automatically, our SPA API"
      },
      "id": "604408d328ccbcf69e2c6064"
    },
    {
      "sections": [
        "Missing route changes with SPA agent",
        "Problem",
        "Solution",
        "Short term solutions",
        "Support",
        "Cause"
      ],
      "title": "Missing route changes with SPA agent",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Troubleshooting"
      ],
      "external_id": "9ca088a0459684464512ee51dfb7ffca22e26c14",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/troubleshooting/missing-route-changes-spa-agent/",
      "published_at": "2022-01-12T03:23:27Z",
      "updated_at": "2021-08-20T21:36:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are using the Pro + SPA agent, but you are not seeing all of the route change browser interactions you expect. We are aware that this can be frustrating. Our goal throughout 2021 is to reevaluate the SPA feature functionality, making it simpler and more reliable, starting with the methods we use to detect and capture route changes. Additionally, we plan to add new frameworks and use cases to our testing suite based on your feedback and examples. Likely this work will include new APIs as well as framework-specific plug-ins. Check our release notes for the latest updates. Solution Short term solutions To make sure all route changes are captured, you can use our SPA interaction() API. Using the interaction API will categorize the BrowserInteraction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework is exposing events that represent router activity, you can use custom instrumentation in these events. Here is an example using our API with the Angular router: router.events.subscribe( (event: Event) => { if (event instanceof NavigationStart) { let i = newrelic.interaction() i.setName(event.url) i.save() } }); Copy In this example, the router object is an instance of the Angular router (from the @angular/router module). The setName call sets the name attribute of the BrowserInteraction event to the new URL, and the save call ensures that the interaction is captured. You will need to adapt this for the needs of your own application’s framework. If your framework does not provide routing events, then you can add this code in the event handler of the original interaction event such as click): myButton.addEventListener('click', function () { let i = newrelic.interaction() i.setName(‘new URL') i.save() }); Copy Recommendation: If you do not have access to router events nor the interaction event handler, implement this as soon as possible in code that you know is the result of a user interaction. An alternative to using the SPA API is to capture routes as PageAction events. PageAction events can be used to capture any custom data. We recommend this option as a fallback in case using the SPA interaction API does not work as expected, or to completely separate the custom instrumentation from built-in BrowserInteraction events. Both of these solutions can ensure these events are captured, either as a BrowserInteraction event or as a PageAction event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations where you are seeing most route changes, but none for a particular route change you expect, attempt to evaluate the difference in the implementation of the code for that particular route. Is there something non-standard or unique about that route that you could provide to our support team? Document the frameworks and any libraries that might be of interest. If this is a new problem, has anything changed in your environment that has led to these interactions suddenly not being tracked? Note the browser agent version you are using. If you are more than a few releases behind, we will recommend that you update to the latest release, as we may have already resolved a similar issue. Be aware that due to the complexity of diagnosing these issues, the team will likely need access to an environment and code that demonstrates the problem for testing and research. Cause The browser agent attempts to be framework agnostic, as well as support coding best practices. However, there are often edge cases that will be missed that lead to you not collecting the route changes you expect. The implementation is based on instrumenting most common asynchronous browser APIs. There are cases when a web application uses some asynchronous API or uses custom or third-party code that we do not instrument, and this can result in inaccurate or missed route changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.34016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": " are captured, either as a <em>Browser</em>Interaction event or as a <em>Page</em>Action event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations"
      },
      "id": "603e937628ccbcd4efeba750"
    }
  ],
  "/docs/browser/single-page-app-monitoring/troubleshooting/missing-route-changes-spa-agent": [
    {
      "sections": [
        "Install Single Page App monitoring",
        "Requirements",
        "Enable or disable SPA monitoring"
      ],
      "title": "Install Single Page App monitoring",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "04501b8d90b2c9b3bf3fa29f1662596a1379e2b6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/install-single-page-app-monitoring/",
      "published_at": "2022-01-12T02:17:36Z",
      "updated_at": "2021-07-27T14:14:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Single page app (SPA) monitoring comes with the default install of the browser agent. Requirements You can review compability and requirements for SPA monitoring here. When you set up your first monitored app in a New Relic account, you must agree to the Terms of Service. By agreeing to the terms, you authorize New Relic to collect hash fragments from URLs. You only need to select the checkbox option once for an account. Enable or disable SPA monitoring When you enable browser monitoring, SPA monitoring is included by default because it gives access to a range of our most recent features, including distributed tracing. Some older agent installations may need to be upgraded. Read more about browser agent types.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.59322,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "sections": "Install <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "<em>Single</em> <em>page</em> <em>app</em> (SPA) <em>monitoring</em> comes with the default install of the <em>browser</em> agent. Requirements You can review compability and requirements for SPA <em>monitoring</em> here. When you set up your first monitored <em>app</em> in a New Relic account, you must agree to the Terms of Service. By agreeing to the terms"
      },
      "id": "6043f16664441f56fa378eec"
    },
    {
      "sections": [
        "Introduction to Single Page App monitoring",
        "Enable SPA monitoring",
        "Analyze throughput and performance data",
        "Browser SPA features"
      ],
      "title": "Introduction to Single Page App monitoring",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "6dedda52851e1ca1f180c8d88bdcb7038c4d1b5d",
      "image": "https://docs.newrelic.com/static/98d434a02c314f2bd2ce9828aa7b755d/c1b63/browser_SPA.png",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/introduction-single-page-app-monitoring/",
      "published_at": "2022-01-12T02:17:37Z",
      "updated_at": "2021-07-21T20:07:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic browser monitoring has a single-page application (SPA) monitoring feature that provides deeper visibility and actionable insights into real user interactions with single-page apps, and for any app that uses AJAX requests. In addition to monitoring route changes automatically, our SPA API allows you to monitor virtually anything that executes inside the browser. This allows developers and their team to: Create faster, more responsive, highly interactive apps. Monitor the throughput and performance that real users are experiencing. Troubleshoot and resolve problems within the context of the page load. Query your data to assist with business decisions. Bring better apps to the marketplace more quickly. Enable SPA monitoring SPA monitoring is enabled by default for new browser agent installations. The SPA-enabled version of the agent gives access to other powerful New Relic features, like distributed tracing. For more information, see Enable browser monitoring. For compatability information for SPA-related features, see SPA requirements. Analyze throughput and performance data Improving on traditional industry standards for measuring page load timing, we give you a complete picture of the activity, both synchronous and asynchronous, associated with page loads and route changes. one.newrelic.com > Browser > (select an app) > Page views: Use browser monitoring's SPA monitoring to examine the throughput and performance of your SPA-architecture app. SPA data monitored by browser monitoring includes: Performance data and throughput for page loads and route changes AJAX request data JavaScript activity, both synchronous and asynchronous Dynamic page updates, monitored using the SPA API With this data, you will gain a clear understanding of how your users experience your app's page loads and route changes, and be able to solve bottlenecks and troubleshoot errors. For more about how New Relic handles SPA data, see Understand SPA data collection. Browser SPA features Here is a summary of SPA monitoring features: Single-page app monitoring Take advantage of these features Robust views in browser's UI When a user initiates a page load or route change, New Relic begins to monitor all subsequent JavaScript, and ends the timing once all AJAX events are complete. This provides a more accurate view of when a page is actually ready for a user compared to the traditional method of ending the timing when the window load event is fired. When SPA monitoring is enabled, the Page views page in browser shows event-driven data about application usage levels (throughput) and user experience (performance), including: Charts with drill-down details about initial page load performance, route changes, and historical performance Sort, search, and filter options, including custom attributes Additional AJAX breakdown data for all initial page loads and route changes For an explanation of how SPA monitoring will impact your existing browser account's data usage, see SPA and browser data usage. Data analysis with data explorer The data explorer supports three SPA-specific event types: BrowserInteraction, AjaxRequest, and BrowserTiming. You can query these events in the query builder to analyze your app's performance and make business decisions. Customized data from API Use SPA API to obtain the specific data you need, such as custom naming, custom timing, finishline API, or other custom attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.31706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "sections": "Introduction to <em>Single</em> <em>Page</em> <em>App</em> <em>monitoring</em>",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": " your <em>app</em>&#x27;s <em>page</em> loads and route changes, and be able to solve bottlenecks and <em>troubleshoot</em> errors. For more about how New Relic handles SPA data, see Understand SPA data collection. <em>Browser</em> SPA features Here is a summary of SPA <em>monitoring</em> features: <em>Single</em>-<em>page</em> <em>app</em> <em>monitoring</em> Take advantage"
      },
      "id": "604408d328ccbcf69e2c6064"
    },
    {
      "sections": [
        "SPA compatibility and requirements",
        "Browser agent version",
        "Browser types",
        "Framework requirements",
        "Security when collecting hash fragments"
      ],
      "title": "SPA compatibility and requirements",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Get started"
      ],
      "external_id": "17d916a952f7b86a1da190a9d7236072eff12361",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/get-started/spa-compatibility-requirements/",
      "published_at": "2022-01-12T03:27:26Z",
      "updated_at": "2021-07-09T07:41:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to set up Single Page Application (SPA) monitoring for browser monitoring, make sure your app meets these SPA monitoring requirements. Browser agent version SPA monitoring requires an SPA-specific version of the browser snippet, available for browser agent version 885 or higher. To activate this snippet version for your application, enable your application for SPA monitoring. To check your version and integrate the updated snippet, follow the appropriate upgrade instructions. Browser types SPA monitoring requires the addEventListener browser API and the Navigation Timing API. Both APIs are available in all modern browsers, including Google Chrome, Mozilla Firefox, Apple Safari, and Microsoft Internet Explorer (IE) versions 9 or higher. Framework requirements Because SPA instrumentation works by wrapping low-level browser APIs, it is framework-agnostic. SPA instrumentation is compatible with most SPA frameworks, such as Angular, Backbone, Ember, and React. It can also instrument requests made using JSONP. Below are known compatibility issues: If your application uses AngularJS and you want to use browser's SPA monitoring capabilities, Zone.js versions 0.6.18-0.6.24 are not compatible with the SPA agent. The html2pdf.js library is not compatible with the SPA agent. Security when collecting hash fragments New Relic collects and saves hash fragments from route change URLs. If you use hashes to pass private or sensitive data, that data may be visible to your New Relic account users. Follow browser's guidelines for security with data collection and reporting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.77274,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Browser</em> agent version",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "In order to set up <em>Single</em> <em>Page</em> Application (SPA) <em>monitoring</em> for <em>browser</em> <em>monitoring</em>, make sure your <em>app</em> meets these SPA <em>monitoring</em> requirements. <em>Browser</em> agent version SPA <em>monitoring</em> requires an SPA-specific version of the <em>browser</em> snippet, available for <em>browser</em> agent version 885 or higher"
      },
      "id": "6044095ee7b9d20d555799f3"
    }
  ],
  "/docs/browser/single-page-app-monitoring/use-spa-data/spa-data-collection": [
    {
      "sections": [
        "View SPA data in Browser UI",
        "Single-page app (SPA) data",
        "Filter SPA views",
        "Group SPA views",
        "SPA view details",
        "Initial page load performance details",
        "Route change performance details"
      ],
      "title": "View SPA data in Browser UI",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Use SPA data"
      ],
      "external_id": "0ab30db71f34da6376ff5e71734292b247754ca4",
      "image": "https://docs.newrelic.com/static/04bcea9186a93fc786a6db3469765824/c1b63/spa_overview.png",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/use-spa-data/view-spa-data-browser-ui/",
      "published_at": "2022-01-12T02:11:51Z",
      "updated_at": "2021-07-09T10:04:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have opted in to SPA (single-page app) monitoring, the browser Page views page will include data on SPA route changes and initial page loads. one.newrelic.com > Browser > (select an app) > Page views: When you opt in to SPA monitoring, the browser Page views page will display SPA data like route changes and associated asynchronous browser activity. Single-page app (SPA) data To view SPA data: Go to one.newrelic.com > Browser > (select an app) > Page views. Initial page loads and route changes are automatically grouped by browser interaction name. You can adjust this with your allow list settings for segments. If you set custom route names with the SPA API, the custom route names will be displayed. You can change how the page loads and route changes are grouped by using the Group page by dropdown. By default, the list of page loads and route changes displays the most time consuming views at the top of the list. You can also sort by average response time, median response time, and throughput per minute by using the Sort by dropdown. To search for specific views by grouped URL, type in the search bar below the Sort by dropdown. For example, to find URLs that represent your checkout page, search for checkout. The charts on the initial Page view page display: The five views with the slowest average response times The five views with the highest throughput To change the range of time being examined, use the time picker near the top of the page. (If you choose a time range more than eight days in the past, some filtering and grouping functionality won't be available.) Filter SPA views one.newrelic.com > Browser > (select an app) > Page views > Filter: Use the Filter to filter for route changes, initial page loads, and other attributes like location and browser type. To view only initial page loads or only route changes, use the Filter dropdown. For example, to view only route changes, select Filter > Route change. The filter also gives you the ability to filter by other attributes of page loads and route changes, such as app name, geographical location of the browser, and browser type. For example, to see only page loads and route changes that occurred on browsers in the city of Portland, Oregon, select Filter > City > Portland. Group SPA views You can use the Group page by dropdown to group the list of page views by any attribute. For example, if you want to compare the average response times by browser type, select Group page by > userAgent. The combination of filtering and grouping lets you quickly find very specific data. For example, to compare how a specific URL is loading on different browsers: From the Filter dropdown, select targetURL, then select the URL you want to study. From the Group page by dropdown, select userAgent. SPA view details one.newrelic.com > Browser > (select an app) > Page views > (select a view): Select a view from the list to see assorted details and breakdowns. Select an individual page load or route change to see details. Selecting either will provide a breakdown of where time was spent for a browser interaction, and display that data over a time series matching the window selected in the time picker. Every route change view can theoretically also be an initial page load. (For example, when a route change URL is sent to someone else and they load it, that will now be considered an initial page load to New Relic.) This is why the SPA view details page has charts for both initial page loads and route changes. This allows you to compare how a view performs as an initial page load to how its performance as a route change. There are three chart display options, selectable with the icons to the right of the Avg initial page load time chart title. The default display is the color-coded stacked area chart. You can also switch to a Histogram display or a percentile line graph. Also on the details page is a Throughput chart that combines initial page loads and route changes. The chart displays the 5 pages with the highest throughput, which are listed beneath the chart, and consolidates all other pages into Other. Here are details on the specific performance data displayed for both page loads and route changes: Initial page load performance details For initial page loads, the performance details include the average back end time, front end time, and the window onload event: Back end time includes network, web app time, and request queuing. Front end time includes DOM processing, page rendering, and the time to complete all XHRs. A horizontal red line shows when the window load event is fired. This corresponds to the traditional page load timing measured by the browser agent without SPA monitoring enabled. With SPA monitoring it is common to have a window load event before the front end time is complete. (For more about how SPA page load timing differs from traditional page load timing, see Understand SPA data collection.) Route change performance details For route changes, the performance chart displays JS duration and waiting time. JS Duration is the sum of all JavaScript execution time during the interaction, which is synchronous by definition. The remaining time is called Waiting time and is derived by subtracting JS duration from the total duration. The Historical performance and Breakdown details are similar for both page loads and route changes: Detail tab Comments Historical data The Historical performance tab displays throughput (views per minute) and response time charted against the same time period yesterday and last week. Breakdowns The Breakdowns tab lists the various components individually timed as part of an interaction. By default, all XHRs are captured and timed. You can also use the SPA API to include additional elements for a route change or page load.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.14165,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View <em>SPA</em> <em>data</em> in <em>Browser</em> UI",
        "sections": "<em>Single</em>-<em>page</em> <em>app</em> (<em>SPA</em>) <em>data</em>",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "If you have opted in to <em>SPA</em> (<em>single</em>-<em>page</em> <em>app</em>) <em>monitoring</em>, the <em>browser</em> <em>Page</em> views <em>page</em> will include <em>data</em> on <em>SPA</em> route changes and initial <em>page</em> loads. one.newrelic.com &gt; <em>Browser</em> &gt; (select an <em>app</em>) &gt; <em>Page</em> views: When you opt in to <em>SPA</em> <em>monitoring</em>, the <em>browser</em> <em>Page</em> views <em>page</em> will display <em>SPA</em> <em>data</em> like"
      },
      "id": "60440de328ccbc26592c60be"
    },
    {
      "sections": [
        "Use SPA API"
      ],
      "title": "Use SPA API",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Use SPA data"
      ],
      "external_id": "85ba9b61e8ba08112a3a276d186fbe7af894251d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/use-spa-data/use-spa-api/",
      "published_at": "2022-01-12T03:13:49Z",
      "updated_at": "2021-03-11T07:35:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Browser's single-page application (SPA) monitoring includes an API to add custom monitoring of specific browser interactions. This is useful for monitoring interactions that New Relic does not record automatically because they do not result in route changes, such as a dynamically-updated widget. The SPA API also allows you to turn off default monitoring for interactions that you do not consider important enough to monitor. For more information about the SPA API, including specific API calls, see the Browser agent and SPA API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.9067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Use</em> <em>SPA</em> API",
        "sections": "<em>Use</em> <em>SPA</em> API",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "<em>Browser</em>&#x27;s <em>single</em>-<em>page</em> application (<em>SPA</em>) <em>monitoring</em> includes an API to add custom <em>monitoring</em> of specific <em>browser</em> interactions. This is useful for <em>monitoring</em> interactions that New Relic does not record automatically because they do not result in route changes, such as a dynamically-updated widget"
      },
      "id": "60440de328ccbc04a23025de"
    },
    {
      "sections": [
        "Missing route changes with SPA agent",
        "Problem",
        "Solution",
        "Short term solutions",
        "Support",
        "Cause"
      ],
      "title": "Missing route changes with SPA agent",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Troubleshooting"
      ],
      "external_id": "9ca088a0459684464512ee51dfb7ffca22e26c14",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/troubleshooting/missing-route-changes-spa-agent/",
      "published_at": "2022-01-12T03:23:27Z",
      "updated_at": "2021-08-20T21:36:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are using the Pro + SPA agent, but you are not seeing all of the route change browser interactions you expect. We are aware that this can be frustrating. Our goal throughout 2021 is to reevaluate the SPA feature functionality, making it simpler and more reliable, starting with the methods we use to detect and capture route changes. Additionally, we plan to add new frameworks and use cases to our testing suite based on your feedback and examples. Likely this work will include new APIs as well as framework-specific plug-ins. Check our release notes for the latest updates. Solution Short term solutions To make sure all route changes are captured, you can use our SPA interaction() API. Using the interaction API will categorize the BrowserInteraction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework is exposing events that represent router activity, you can use custom instrumentation in these events. Here is an example using our API with the Angular router: router.events.subscribe( (event: Event) => { if (event instanceof NavigationStart) { let i = newrelic.interaction() i.setName(event.url) i.save() } }); Copy In this example, the router object is an instance of the Angular router (from the @angular/router module). The setName call sets the name attribute of the BrowserInteraction event to the new URL, and the save call ensures that the interaction is captured. You will need to adapt this for the needs of your own application’s framework. If your framework does not provide routing events, then you can add this code in the event handler of the original interaction event such as click): myButton.addEventListener('click', function () { let i = newrelic.interaction() i.setName(‘new URL') i.save() }); Copy Recommendation: If you do not have access to router events nor the interaction event handler, implement this as soon as possible in code that you know is the result of a user interaction. An alternative to using the SPA API is to capture routes as PageAction events. PageAction events can be used to capture any custom data. We recommend this option as a fallback in case using the SPA interaction API does not work as expected, or to completely separate the custom instrumentation from built-in BrowserInteraction events. Both of these solutions can ensure these events are captured, either as a BrowserInteraction event or as a PageAction event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations where you are seeing most route changes, but none for a particular route change you expect, attempt to evaluate the difference in the implementation of the code for that particular route. Is there something non-standard or unique about that route that you could provide to our support team? Document the frameworks and any libraries that might be of interest. If this is a new problem, has anything changed in your environment that has led to these interactions suddenly not being tracked? Note the browser agent version you are using. If you are more than a few releases behind, we will recommend that you update to the latest release, as we may have already resolved a similar issue. Be aware that due to the complexity of diagnosing these issues, the team will likely need access to an environment and code that demonstrates the problem for testing and research. Cause The browser agent attempts to be framework agnostic, as well as support coding best practices. However, there are often edge cases that will be missed that lead to you not collecting the route changes you expect. The implementation is based on instrumenting most common asynchronous browser APIs. There are cases when a web application uses some asynchronous API or uses custom or third-party code that we do not instrument, and this can result in inaccurate or missed route changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.98947,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Missing route changes with <em>SPA</em> agent",
        "sections": "Missing route changes with <em>SPA</em> agent",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": " Short term solutions To make sure all route changes are captured, you can <em>use</em> our <em>SPA</em> interaction() API. Using the interaction API will categorize the <em>Browser</em>Interaction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework"
      },
      "id": "603e937628ccbcd4efeba750"
    }
  ],
  "/docs/browser/single-page-app-monitoring/use-spa-data/use-spa-api": [
    {
      "sections": [
        "View SPA data in Browser UI",
        "Single-page app (SPA) data",
        "Filter SPA views",
        "Group SPA views",
        "SPA view details",
        "Initial page load performance details",
        "Route change performance details"
      ],
      "title": "View SPA data in Browser UI",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Use SPA data"
      ],
      "external_id": "0ab30db71f34da6376ff5e71734292b247754ca4",
      "image": "https://docs.newrelic.com/static/04bcea9186a93fc786a6db3469765824/c1b63/spa_overview.png",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/use-spa-data/view-spa-data-browser-ui/",
      "published_at": "2022-01-12T02:11:51Z",
      "updated_at": "2021-07-09T10:04:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have opted in to SPA (single-page app) monitoring, the browser Page views page will include data on SPA route changes and initial page loads. one.newrelic.com > Browser > (select an app) > Page views: When you opt in to SPA monitoring, the browser Page views page will display SPA data like route changes and associated asynchronous browser activity. Single-page app (SPA) data To view SPA data: Go to one.newrelic.com > Browser > (select an app) > Page views. Initial page loads and route changes are automatically grouped by browser interaction name. You can adjust this with your allow list settings for segments. If you set custom route names with the SPA API, the custom route names will be displayed. You can change how the page loads and route changes are grouped by using the Group page by dropdown. By default, the list of page loads and route changes displays the most time consuming views at the top of the list. You can also sort by average response time, median response time, and throughput per minute by using the Sort by dropdown. To search for specific views by grouped URL, type in the search bar below the Sort by dropdown. For example, to find URLs that represent your checkout page, search for checkout. The charts on the initial Page view page display: The five views with the slowest average response times The five views with the highest throughput To change the range of time being examined, use the time picker near the top of the page. (If you choose a time range more than eight days in the past, some filtering and grouping functionality won't be available.) Filter SPA views one.newrelic.com > Browser > (select an app) > Page views > Filter: Use the Filter to filter for route changes, initial page loads, and other attributes like location and browser type. To view only initial page loads or only route changes, use the Filter dropdown. For example, to view only route changes, select Filter > Route change. The filter also gives you the ability to filter by other attributes of page loads and route changes, such as app name, geographical location of the browser, and browser type. For example, to see only page loads and route changes that occurred on browsers in the city of Portland, Oregon, select Filter > City > Portland. Group SPA views You can use the Group page by dropdown to group the list of page views by any attribute. For example, if you want to compare the average response times by browser type, select Group page by > userAgent. The combination of filtering and grouping lets you quickly find very specific data. For example, to compare how a specific URL is loading on different browsers: From the Filter dropdown, select targetURL, then select the URL you want to study. From the Group page by dropdown, select userAgent. SPA view details one.newrelic.com > Browser > (select an app) > Page views > (select a view): Select a view from the list to see assorted details and breakdowns. Select an individual page load or route change to see details. Selecting either will provide a breakdown of where time was spent for a browser interaction, and display that data over a time series matching the window selected in the time picker. Every route change view can theoretically also be an initial page load. (For example, when a route change URL is sent to someone else and they load it, that will now be considered an initial page load to New Relic.) This is why the SPA view details page has charts for both initial page loads and route changes. This allows you to compare how a view performs as an initial page load to how its performance as a route change. There are three chart display options, selectable with the icons to the right of the Avg initial page load time chart title. The default display is the color-coded stacked area chart. You can also switch to a Histogram display or a percentile line graph. Also on the details page is a Throughput chart that combines initial page loads and route changes. The chart displays the 5 pages with the highest throughput, which are listed beneath the chart, and consolidates all other pages into Other. Here are details on the specific performance data displayed for both page loads and route changes: Initial page load performance details For initial page loads, the performance details include the average back end time, front end time, and the window onload event: Back end time includes network, web app time, and request queuing. Front end time includes DOM processing, page rendering, and the time to complete all XHRs. A horizontal red line shows when the window load event is fired. This corresponds to the traditional page load timing measured by the browser agent without SPA monitoring enabled. With SPA monitoring it is common to have a window load event before the front end time is complete. (For more about how SPA page load timing differs from traditional page load timing, see Understand SPA data collection.) Route change performance details For route changes, the performance chart displays JS duration and waiting time. JS Duration is the sum of all JavaScript execution time during the interaction, which is synchronous by definition. The remaining time is called Waiting time and is derived by subtracting JS duration from the total duration. The Historical performance and Breakdown details are similar for both page loads and route changes: Detail tab Comments Historical data The Historical performance tab displays throughput (views per minute) and response time charted against the same time period yesterday and last week. Breakdowns The Breakdowns tab lists the various components individually timed as part of an interaction. By default, all XHRs are captured and timed. You can also use the SPA API to include additional elements for a route change or page load.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.14165,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View <em>SPA</em> <em>data</em> in <em>Browser</em> UI",
        "sections": "<em>Single</em>-<em>page</em> <em>app</em> (<em>SPA</em>) <em>data</em>",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "If you have opted in to <em>SPA</em> (<em>single</em>-<em>page</em> <em>app</em>) <em>monitoring</em>, the <em>browser</em> <em>Page</em> views <em>page</em> will include <em>data</em> on <em>SPA</em> route changes and initial <em>page</em> loads. one.newrelic.com &gt; <em>Browser</em> &gt; (select an <em>app</em>) &gt; <em>Page</em> views: When you opt in to <em>SPA</em> <em>monitoring</em>, the <em>browser</em> <em>Page</em> views <em>page</em> will display <em>SPA</em> <em>data</em> like"
      },
      "id": "60440de328ccbc26592c60be"
    },
    {
      "sections": [
        "SPA data collection",
        "Browser interactions",
        "Types of SPA data reporting",
        "Initial page loads",
        "Route changes",
        "Custom monitoring",
        "Difference from traditional page load timing",
        "Tip",
        "Timers",
        "Events and attributes"
      ],
      "title": "SPA data collection",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Use SPA data"
      ],
      "external_id": "d42d239aca2ea13a37fd926dca3672fcf83d73dd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/use-spa-data/spa-data-collection/",
      "published_at": "2022-01-12T03:27:25Z",
      "updated_at": "2021-07-09T08:08:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document explains how browser collects and stores your asynchronous single page app (SPA) data. This will give you a better understanding of the SPA data you see in the browser UI. This will also help you more easily add custom monitoring with the SPA API. Browser interactions At the heart of SPA monitoring is the concept of the browser interaction. New Relic defines a browser interaction as anything that occurs in the app user's browser; for example: A user interaction that leads to a page load or route change A scheduled, dynamic update to an app's widget A browser interaction includes not just the initial triggering event, but also the activity caused by that event, such as AJAX requests and both synchronous and asynchronous JavaScript. By tracking not just the cause but also the effects of a browser interaction, we help you understand how users experience your application's views and route changes. All apps are different and have different monitoring needs. That's why we include default monitoring as well as the ability to set up custom monitoring for any browser interactions you choose. Types of SPA data reporting Three major categories of single page app data can be reported to New Relic: Initial page loads Route changes Custom browser interactions created via the SPA API Each of these creates a BrowserInteraction event. If one or more AJAX requests are part of an interaction, then associated AjaxRequest events are also created. These events and their attributes can be queried in the query builder. Initial page loads An initial page load is a traditional URL change, stemming from a complete load or reload of a URL. This is indicated in the browser when a page load event fires (the window.onload event). Initial page loads appear along with route changes in the browser UI. Route changes SPA users experience dynamic route changes in a similar way to page loads. Visitors to a site or app generally do not care how a new view was delivered; they simply know that when they perform an action, a new view appears. For this reason, we treat route changes in a similar way to page loads in the UI. In order to optimally monitor single page applications, we start monitoring many browser interactions that could theoretically lead to route changes. If these interactions do not lead to route changes, browser initiates monitoring but then discards them. If these interactions do lead to a route change, browser saves the interaction sequence as a BrowserInteraction event, including information about both synchronous and asynchronous activity. An interaction is considered a route change and saved as a BrowserInteraction event when one of the following occurs: The URL hash changes (usually using window.location.hash). A popstate event fires during a callback associated with an interaction. A pushState or replaceState API is called. Route changes appear along with initial page loads in the browser UI. We receive and save hash fragments from route change URLs. If you use hashes to pass private or sensitive data, that data may be visible to your New Relic account users. For more information about data collection and reporting, see Security for browser. Custom monitoring You can use the SPA API to set up custom monitoring of browser interactions that are not monitored by default. You can also use the API to disable default monitoring. Custom events are saved as BrowserInteraction events and have the following attributes: The category attribute will have the value Custom. The trigger attribute will have the value api. (This is the default value but can be changed with the API.) Difference from traditional page load timing To provide optimized data for single page app monitoring, we measure page load timing in a new way: by wrapping low level browser functions, both synchronous and asynchronous. This gives a fuller depiction of how long it takes to complete the changes required for a new view. This is different from the traditional method for page load timing. Traditional page load timing uses the firing of the window.onload event to determine when a page is loaded. This is not an optimal way to measure view change timing because web apps often have asynchronous code that runs for a significant amount of time after the window.onload event occurs. Tip Browser's standard, non-SPA Page views page displays different page load times than when SPA monitoring is enabled. Because SPA monitoring is measuring all asynchronous activity, the SPA load times will generally be longer than standard page load times. The traditional window.onload page load timing still appears on the SPA Page views page. When you select a specific page load event, Window onload appears as a red line in the page load time chart. You can also select Switch to standard page views to return to traditional load timing displays. Timers The agent monitors all asynchronous calls, including timers. Timers with durations shorter than one second are wrapped. Timers longer than one second are not wrapped because usually they are meant for non-web transactions, such as background work or polling that is unrelated to a user interaction. Events and attributes We save browser interactions that lead to route changes and page loads as BrowserInteraction events, and AJAX requests as AjaxRequest events. You can query these events in the query builder.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.13705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>SPA</em> <em>data</em> collection",
        "sections": "<em>SPA</em> <em>data</em> collection",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "This document explains how <em>browser</em> collects and stores your asynchronous <em>single</em> <em>page</em> <em>app</em> (<em>SPA</em>) <em>data</em>. This will give you a better understanding of the <em>SPA</em> <em>data</em> you see in the <em>browser</em> UI. This will also help you more easily add custom <em>monitoring</em> with the <em>SPA</em> API. <em>Browser</em> interactions At the heart"
      },
      "id": "60440d9b196a672eb1960f6d"
    },
    {
      "sections": [
        "Missing route changes with SPA agent",
        "Problem",
        "Solution",
        "Short term solutions",
        "Support",
        "Cause"
      ],
      "title": "Missing route changes with SPA agent",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Troubleshooting"
      ],
      "external_id": "9ca088a0459684464512ee51dfb7ffca22e26c14",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/troubleshooting/missing-route-changes-spa-agent/",
      "published_at": "2022-01-12T03:23:27Z",
      "updated_at": "2021-08-20T21:36:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are using the Pro + SPA agent, but you are not seeing all of the route change browser interactions you expect. We are aware that this can be frustrating. Our goal throughout 2021 is to reevaluate the SPA feature functionality, making it simpler and more reliable, starting with the methods we use to detect and capture route changes. Additionally, we plan to add new frameworks and use cases to our testing suite based on your feedback and examples. Likely this work will include new APIs as well as framework-specific plug-ins. Check our release notes for the latest updates. Solution Short term solutions To make sure all route changes are captured, you can use our SPA interaction() API. Using the interaction API will categorize the BrowserInteraction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework is exposing events that represent router activity, you can use custom instrumentation in these events. Here is an example using our API with the Angular router: router.events.subscribe( (event: Event) => { if (event instanceof NavigationStart) { let i = newrelic.interaction() i.setName(event.url) i.save() } }); Copy In this example, the router object is an instance of the Angular router (from the @angular/router module). The setName call sets the name attribute of the BrowserInteraction event to the new URL, and the save call ensures that the interaction is captured. You will need to adapt this for the needs of your own application’s framework. If your framework does not provide routing events, then you can add this code in the event handler of the original interaction event such as click): myButton.addEventListener('click', function () { let i = newrelic.interaction() i.setName(‘new URL') i.save() }); Copy Recommendation: If you do not have access to router events nor the interaction event handler, implement this as soon as possible in code that you know is the result of a user interaction. An alternative to using the SPA API is to capture routes as PageAction events. PageAction events can be used to capture any custom data. We recommend this option as a fallback in case using the SPA interaction API does not work as expected, or to completely separate the custom instrumentation from built-in BrowserInteraction events. Both of these solutions can ensure these events are captured, either as a BrowserInteraction event or as a PageAction event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations where you are seeing most route changes, but none for a particular route change you expect, attempt to evaluate the difference in the implementation of the code for that particular route. Is there something non-standard or unique about that route that you could provide to our support team? Document the frameworks and any libraries that might be of interest. If this is a new problem, has anything changed in your environment that has led to these interactions suddenly not being tracked? Note the browser agent version you are using. If you are more than a few releases behind, we will recommend that you update to the latest release, as we may have already resolved a similar issue. Be aware that due to the complexity of diagnosing these issues, the team will likely need access to an environment and code that demonstrates the problem for testing and research. Cause The browser agent attempts to be framework agnostic, as well as support coding best practices. However, there are often edge cases that will be missed that lead to you not collecting the route changes you expect. The implementation is based on instrumenting most common asynchronous browser APIs. There are cases when a web application uses some asynchronous API or uses custom or third-party code that we do not instrument, and this can result in inaccurate or missed route changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.98947,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Missing route changes with <em>SPA</em> agent",
        "sections": "Missing route changes with <em>SPA</em> agent",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": " Short term solutions To make sure all route changes are captured, you can <em>use</em> our <em>SPA</em> interaction() API. Using the interaction API will categorize the <em>Browser</em>Interaction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework"
      },
      "id": "603e937628ccbcd4efeba750"
    }
  ],
  "/docs/browser/single-page-app-monitoring/use-spa-data/view-spa-data-browser-ui": [
    {
      "sections": [
        "SPA data collection",
        "Browser interactions",
        "Types of SPA data reporting",
        "Initial page loads",
        "Route changes",
        "Custom monitoring",
        "Difference from traditional page load timing",
        "Tip",
        "Timers",
        "Events and attributes"
      ],
      "title": "SPA data collection",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Use SPA data"
      ],
      "external_id": "d42d239aca2ea13a37fd926dca3672fcf83d73dd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/use-spa-data/spa-data-collection/",
      "published_at": "2022-01-12T03:27:25Z",
      "updated_at": "2021-07-09T08:08:11Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document explains how browser collects and stores your asynchronous single page app (SPA) data. This will give you a better understanding of the SPA data you see in the browser UI. This will also help you more easily add custom monitoring with the SPA API. Browser interactions At the heart of SPA monitoring is the concept of the browser interaction. New Relic defines a browser interaction as anything that occurs in the app user's browser; for example: A user interaction that leads to a page load or route change A scheduled, dynamic update to an app's widget A browser interaction includes not just the initial triggering event, but also the activity caused by that event, such as AJAX requests and both synchronous and asynchronous JavaScript. By tracking not just the cause but also the effects of a browser interaction, we help you understand how users experience your application's views and route changes. All apps are different and have different monitoring needs. That's why we include default monitoring as well as the ability to set up custom monitoring for any browser interactions you choose. Types of SPA data reporting Three major categories of single page app data can be reported to New Relic: Initial page loads Route changes Custom browser interactions created via the SPA API Each of these creates a BrowserInteraction event. If one or more AJAX requests are part of an interaction, then associated AjaxRequest events are also created. These events and their attributes can be queried in the query builder. Initial page loads An initial page load is a traditional URL change, stemming from a complete load or reload of a URL. This is indicated in the browser when a page load event fires (the window.onload event). Initial page loads appear along with route changes in the browser UI. Route changes SPA users experience dynamic route changes in a similar way to page loads. Visitors to a site or app generally do not care how a new view was delivered; they simply know that when they perform an action, a new view appears. For this reason, we treat route changes in a similar way to page loads in the UI. In order to optimally monitor single page applications, we start monitoring many browser interactions that could theoretically lead to route changes. If these interactions do not lead to route changes, browser initiates monitoring but then discards them. If these interactions do lead to a route change, browser saves the interaction sequence as a BrowserInteraction event, including information about both synchronous and asynchronous activity. An interaction is considered a route change and saved as a BrowserInteraction event when one of the following occurs: The URL hash changes (usually using window.location.hash). A popstate event fires during a callback associated with an interaction. A pushState or replaceState API is called. Route changes appear along with initial page loads in the browser UI. We receive and save hash fragments from route change URLs. If you use hashes to pass private or sensitive data, that data may be visible to your New Relic account users. For more information about data collection and reporting, see Security for browser. Custom monitoring You can use the SPA API to set up custom monitoring of browser interactions that are not monitored by default. You can also use the API to disable default monitoring. Custom events are saved as BrowserInteraction events and have the following attributes: The category attribute will have the value Custom. The trigger attribute will have the value api. (This is the default value but can be changed with the API.) Difference from traditional page load timing To provide optimized data for single page app monitoring, we measure page load timing in a new way: by wrapping low level browser functions, both synchronous and asynchronous. This gives a fuller depiction of how long it takes to complete the changes required for a new view. This is different from the traditional method for page load timing. Traditional page load timing uses the firing of the window.onload event to determine when a page is loaded. This is not an optimal way to measure view change timing because web apps often have asynchronous code that runs for a significant amount of time after the window.onload event occurs. Tip Browser's standard, non-SPA Page views page displays different page load times than when SPA monitoring is enabled. Because SPA monitoring is measuring all asynchronous activity, the SPA load times will generally be longer than standard page load times. The traditional window.onload page load timing still appears on the SPA Page views page. When you select a specific page load event, Window onload appears as a red line in the page load time chart. You can also select Switch to standard page views to return to traditional load timing displays. Timers The agent monitors all asynchronous calls, including timers. Timers with durations shorter than one second are wrapped. Timers longer than one second are not wrapped because usually they are meant for non-web transactions, such as background work or polling that is unrelated to a user interaction. Events and attributes We save browser interactions that lead to route changes and page loads as BrowserInteraction events, and AJAX requests as AjaxRequest events. You can query these events in the query builder.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.13705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>SPA</em> <em>data</em> collection",
        "sections": "<em>SPA</em> <em>data</em> collection",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "This document explains how <em>browser</em> collects and stores your asynchronous <em>single</em> <em>page</em> <em>app</em> (<em>SPA</em>) <em>data</em>. This will give you a better understanding of the <em>SPA</em> <em>data</em> you see in the <em>browser</em> UI. This will also help you more easily add custom <em>monitoring</em> with the <em>SPA</em> API. <em>Browser</em> interactions At the heart"
      },
      "id": "60440d9b196a672eb1960f6d"
    },
    {
      "sections": [
        "Use SPA API"
      ],
      "title": "Use SPA API",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Use SPA data"
      ],
      "external_id": "85ba9b61e8ba08112a3a276d186fbe7af894251d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/use-spa-data/use-spa-api/",
      "published_at": "2022-01-12T03:13:49Z",
      "updated_at": "2021-03-11T07:35:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Browser's single-page application (SPA) monitoring includes an API to add custom monitoring of specific browser interactions. This is useful for monitoring interactions that New Relic does not record automatically because they do not result in route changes, such as a dynamically-updated widget. The SPA API also allows you to turn off default monitoring for interactions that you do not consider important enough to monitor. For more information about the SPA API, including specific API calls, see the Browser agent and SPA API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.9067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Use</em> <em>SPA</em> API",
        "sections": "<em>Use</em> <em>SPA</em> API",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": "<em>Browser</em>&#x27;s <em>single</em>-<em>page</em> application (<em>SPA</em>) <em>monitoring</em> includes an API to add custom <em>monitoring</em> of specific <em>browser</em> interactions. This is useful for <em>monitoring</em> interactions that New Relic does not record automatically because they do not result in route changes, such as a dynamically-updated widget"
      },
      "id": "60440de328ccbc04a23025de"
    },
    {
      "sections": [
        "Missing route changes with SPA agent",
        "Problem",
        "Solution",
        "Short term solutions",
        "Support",
        "Cause"
      ],
      "title": "Missing route changes with SPA agent",
      "type": "docs",
      "tags": [
        "Browser",
        "Single page app monitoring",
        "Troubleshooting"
      ],
      "external_id": "9ca088a0459684464512ee51dfb7ffca22e26c14",
      "image": "",
      "url": "https://docs.newrelic.com/docs/browser/single-page-app-monitoring/troubleshooting/missing-route-changes-spa-agent/",
      "published_at": "2022-01-12T03:23:27Z",
      "updated_at": "2021-08-20T21:36:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are using the Pro + SPA agent, but you are not seeing all of the route change browser interactions you expect. We are aware that this can be frustrating. Our goal throughout 2021 is to reevaluate the SPA feature functionality, making it simpler and more reliable, starting with the methods we use to detect and capture route changes. Additionally, we plan to add new frameworks and use cases to our testing suite based on your feedback and examples. Likely this work will include new APIs as well as framework-specific plug-ins. Check our release notes for the latest updates. Solution Short term solutions To make sure all route changes are captured, you can use our SPA interaction() API. Using the interaction API will categorize the BrowserInteraction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework is exposing events that represent router activity, you can use custom instrumentation in these events. Here is an example using our API with the Angular router: router.events.subscribe( (event: Event) => { if (event instanceof NavigationStart) { let i = newrelic.interaction() i.setName(event.url) i.save() } }); Copy In this example, the router object is an instance of the Angular router (from the @angular/router module). The setName call sets the name attribute of the BrowserInteraction event to the new URL, and the save call ensures that the interaction is captured. You will need to adapt this for the needs of your own application’s framework. If your framework does not provide routing events, then you can add this code in the event handler of the original interaction event such as click): myButton.addEventListener('click', function () { let i = newrelic.interaction() i.setName(‘new URL') i.save() }); Copy Recommendation: If you do not have access to router events nor the interaction event handler, implement this as soon as possible in code that you know is the result of a user interaction. An alternative to using the SPA API is to capture routes as PageAction events. PageAction events can be used to capture any custom data. We recommend this option as a fallback in case using the SPA interaction API does not work as expected, or to completely separate the custom instrumentation from built-in BrowserInteraction events. Both of these solutions can ensure these events are captured, either as a BrowserInteraction event or as a PageAction event. However, they will not address recording the correct duration and related AJAX calls. Support If this solution does not resolve your issue, please file a support ticket, and have the following information available: For situations where you are seeing most route changes, but none for a particular route change you expect, attempt to evaluate the difference in the implementation of the code for that particular route. Is there something non-standard or unique about that route that you could provide to our support team? Document the frameworks and any libraries that might be of interest. If this is a new problem, has anything changed in your environment that has led to these interactions suddenly not being tracked? Note the browser agent version you are using. If you are more than a few releases behind, we will recommend that you update to the latest release, as we may have already resolved a similar issue. Be aware that due to the complexity of diagnosing these issues, the team will likely need access to an environment and code that demonstrates the problem for testing and research. Cause The browser agent attempts to be framework agnostic, as well as support coding best practices. However, there are often edge cases that will be missed that lead to you not collecting the route changes you expect. The implementation is based on instrumenting most common asynchronous browser APIs. There are cases when a web application uses some asynchronous API or uses custom or third-party code that we do not instrument, and this can result in inaccurate or missed route changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.98947,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Missing route changes with <em>SPA</em> agent",
        "sections": "Missing route changes with <em>SPA</em> agent",
        "tags": "<em>Single</em> <em>page</em> <em>app</em> <em>monitoring</em>",
        "body": " Short term solutions To make sure all route changes are captured, you can <em>use</em> our <em>SPA</em> interaction() API. Using the interaction API will categorize the <em>Browser</em>Interaction event (in the category attribute) as custom rather than a route change if no route change is in fact detected. If your framework"
      },
      "id": "603e937628ccbcd4efeba750"
    }
  ],
  "/docs/codestream/codestream-integrations/msteams-integration": [
    {
      "image": "https://docs.newrelic.com/static/8945e0a9c512b8638ebf8165d47aee04/69902/QS-SignUp3.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-user-guide/",
      "sections": [
        "New Relic CodeStream user guide",
        "Jump to a topic",
        "1. Install the CodeStream extension in your IDE and sign up.",
        "2. Connect your tools",
        "3. Discuss any block of code, at any time",
        "4. Get feedback on your work in progress",
        "5. Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "New Relic CodeStream user guide",
      "updated_at": "2021-11-24T04:44:31Z",
      "type": "docs",
      "external_id": "fa9af0118a8872fea89fda91482c44fb69913ea2",
      "document_type": "page",
      "popularity": 1,
      "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic CodeStream. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. 1. Install the CodeStream extension in your IDE and sign up. Install CodeStream for VS Code, Visual Studio or JetBrains. The CodeStream pane automatically appears in the sidebar for VS Code or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you're the first person from your team to join CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on GitHub, GitLab or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click on your headshot at the top of the CodeStream pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, just select the code and ask your question. Learn more about discussing code. 4. Get feedback on your work in progress Select Request Feedback from the + menu at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code!) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. 5. Create or review a pull request Look for the Pull Requests section of the CodeStream sidebar to review an open pull request. Just click on a pull request (or load one from URL) to get a complete GitHub experience right in your IDE! Note that you can create a pull request in GitHub, GitLab or Bitbucket, but support for reviewing pull requests is currently only available for GitHub (cloud or Enterprise). Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1085.1608,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic <em>CodeStream</em> user guide",
        "sections": "1. Install the <em>CodeStream</em> extension in your IDE <em>and</em> sign up.",
        "body": " discussions on Slack or <em>Microsoft</em> <em>Teams</em>. <em>CodeStream</em> brings the tools you use every day together in your IDE. Click on your headshot at the top of the <em>CodeStream</em> pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of <em>code</em>, at any time Whether you&#x27;re trying"
      },
      "id": "61744137e7b9d2428b13c6a0"
    },
    {
      "image": "https://docs.newrelic.com/static/5c1d085b14abf961ca66b96285f0c0fa/69902/QS-Integrations.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/install-codestream/",
      "sections": [
        "Install New Relic CodeStream",
        "Install CodeStream",
        "Instant Observability (I/O) quickstart",
        "Visual Studio Code",
        "Visual Studio",
        "JetBrains",
        "Connect your tools",
        "Tip",
        "Discuss any block of code, at any time",
        "Get feedback on your work in progress",
        "Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T06:15:45Z",
      "title": "Install New Relic CodeStream",
      "updated_at": "2021-11-24T09:39:10Z",
      "type": "docs",
      "external_id": "5d431c8f9a2690b64d26ac9fc173b18085153aac",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that makes it easy to discuss and review code in a more natural and contextual way. Once connected to New Relic, collaborate on your application errors directly in your IDE. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Install CodeStream You can install CodeStream for your specific IDE or install it through our Instant Observability (I/O) quickstart. Instant Observability (I/O) quickstart Install CodeStream with its Instant Observability (I/O) quickstart to connect CodeStream to your New Relic account via your user key. Visual Studio Code Download and install CodeStream for Visual Studio Code. You can also install it directly in Visual Studio Code via the extensions marketplace. Visual Studio Download and install CodeStream for Visual Studio. You can also install it directly in Visual Studio via the extensions marketplace. JetBrains Download and install CodeStream for JetBrains. You can also install it from the JetBrains plugins menu. Connect your tools Create and review pull requests on GitHub, GitLab, or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Investigate errors reported to New Relic One. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click your headshot at the top of the CodeStream pane, then click Integrations to connect all of your tools to CodeStream. Tip Once you've installed CodeStream, to connect to New Relic, you'll need your New Relic user key. Go here to learn more about finding or creating your user key. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, select the code and click the comment button to ask your question. Learn more about discussing code. Get feedback on your work in progress Click the + menu then click Request Feedback at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. Create or review a pull request In the CodeStream sidebar, look for the Pull Requests section to review an open pull request. Select a pull request (or load one from URL) to get a complete GitHub experience right in your IDE. You can create a pull request in GitHub, GitLab, or Bitbucket, but support for reviewing pull requests is currently only available for GitHub. Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 988.53906,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install New Relic <em>CodeStream</em>",
        "sections": "Install New Relic <em>CodeStream</em>",
        "body": ", and other issue trackers. Investigate errors reported to New Relic One. Share <em>code</em> discussions on Slack or <em>Microsoft</em> <em>Teams</em>. <em>CodeStream</em> brings the tools you use every day together in your IDE. Click your headshot at the top of the <em>CodeStream</em> pane, then click Integrations to connect all of your tools"
      },
      "id": "6174400564441ff1025fd832"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.64365,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    }
  ],
  "/docs/codestream/codestream-integrations/notifications": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.3441,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.84872,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/79d35d62b4c952d3f8b6131bbcfce9e7/f96db/Sidebar3.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/codestream-sidebar/",
      "sections": [
        "CodeStream sidebar overview",
        "Sidebar overview",
        "The username menu",
        "Header menu items"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream sidebar overview",
      "updated_at": "2021-11-13T21:09:54Z",
      "type": "docs",
      "external_id": "d3bb6106b4e568060453b597ac08e4a51471f3a3",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic CodeStream sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here's a quick overview of the sidebar sections: Pull requests: If your team uses GitHub or GitHub Enterprise to host your code, you'll see all of your open pull requests listed here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you've been assigned a feedback request or you've requested feedback from someone else, find those listed here. Codemarks: Codemarks are the discussions that annotate your codebase. Codemarks are created pull requests, feedback requests, or through ad hoc code comments/issues. All of the codemarks in your current repository are listed for reference. Observability: Track errors assigned to you in New Relic One and discover recent errors in the repositories you have open in your IDE. Issues: See all of your open issues, across multiple services, in one place. Click a specific issue to update its status, create a feature branch to do your work, and update your status on Slack. The CodeStream sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen. This is useful when you're looking at a longer list. Click it again to return to your previous view. The username menu The username menu gives you options for managing your account, your organization, how you receive notifications, and what sections are visible. Here are descriptions of each menu item: Account: View your profile. Includes various options for customizing the profile photo, email, username, and full name for your account. View: Uncheck sections you're not interested in seeing. Notifications: Manage how and what notifications you receive. Organization admin: Manage your organization settings. Also, export your data and delete your organization. Switch organization: Use this if you're a member of more than one organization. Integrations: Connect CodeStream to the code host, issue, and messaging providers you use. New Relic setup: Connect your New Relic account to CodeStream to get the most out of New Relic CodeStream's observability tools. Feedback: Got feedback for us? Write a GitHub issue. Help: Links to documentation, our video library, keybindings, the CodeStream workflow, what's new, and reporting an issue. Header menu items The header menu items provide different options for creating and discovering content in your organization. From left to right: Create, Activity feed, My organization, and Filter & search. + (Compose): Click to create a code comment/issue, request feedback on changes, or to create a pull request. Activity feed: The activity feed will let you know about new code comments/issues and feedback requests, as well as replies to existing ones. My organization: See who is in your CodeStream organization, invite new members, and create blame maps. Filter & search: The filter a search tools enable you to slice and dice your team’s collection of code comments, issues, and feature requests however you see fit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.88374,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> sidebar overview",
        "sections": "<em>CodeStream</em> sidebar overview",
        "body": "The New Relic <em>CodeStream</em> sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here&#x27;s a quick overview of the sidebar"
      },
      "id": "617cbd1b28ccbc18bf7fef35"
    }
  ],
  "/docs/codestream/codestream-integrations/slack-integration": [
    {
      "image": "https://developer.newrelic.com/static/31a54fffa55465d7c2b36f21218a43d6/0086b/filters-pane.png",
      "url": "https://developer.newrelic.com/automate-workflows/error-inbox/manage-errors/",
      "sections": [
        "Manage your triaged errors",
        "lab",
        "View triaged errors",
        "Tip",
        "Optional: Integrate Errors Inbox with Slack, Jira, and CodeStream",
        "Summary",
        "Homework"
      ],
      "published_at": "2022-01-12T01:51:54Z",
      "title": "Manage your triaged errors",
      "updated_at": "2021-12-19T01:46:41Z",
      "type": "developer",
      "external_id": "fb7cac4eab154359e99dfdfb32af2536f217b82b",
      "document_type": "page",
      "popularity": 1,
      "info": "Managed your triaged errors in Errors Inbox",
      "body": "lab This procedure is part of a lab that teaches you how to manage errors using Errors Inbox. Each procedure in the lab builds upon the last, so make sure you've triaged your errors before starting this one. You're now observing Geek's Movie Shop's errors in Errors Inbox, and you're trying to debug your application before pushing your site live. With your errors triaged, you can track their progress, look at who's working on a bug, or even create tasks in Jira to resolve them. View triaged errors Change the filter in Errors Inbox to view your triaged errors Step 1 of 3 In Errors Inbox, find the filter pane below the top navigation bar. Step 2 of 3 Click Unresolved to change the filter value. Here, you see three options in the dropdown: Resolved Unresolved Ignored Step 3 of 3 Select Resolved. Errors Inbox now shows you all your resolved error groups. If you only resolved pika.exceptions:ChannelWrongStateError, you don't see any resolved errors here because Errors Inbox unresolved that one when it saw another occurrence. Tip If you want to observe your ignored error groups instead of resolved ones, filter by Ignored. Optional: Integrate Errors Inbox with Slack, Jira, and CodeStream Being able to view resolved and ignored errors is useful, but you're trying to squash the bugs in your application before you deploy it to production. To help you manage this, connect your inbox to Slack, Jira, and CodeStream. Summary In this lab, you set up Errors Inbox to proactively observe and catch errors from across your stack. You analyzed the errors in full context and triaged them before they could affect your customers. You also managed your errors in Errors Inbox and integrated your inbox with Jira, CodeStream, and Slack to help you collaborate and resolve errors faster. Once you resolve your high priority errors, you'll be more confident in your production release. But Errors Inbox is helpful even when you're in production, because you'll be able to see, triage, and manage errors that come from your customers as well. Homework Now that you know how to track and triage errors using Errors Inbox, here are some other resources you can use to familiarize yourself even more with Errors Inbox. Read our documentation on Errors Inbox Read our blog Collaborate and fix errors quickly with Errors Inbox and workloads Read our blog Error Tracking Across Your Entire Stack with New Relic Errors Inbox",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 855.31647,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Optional: Integrate Errors Inbox with <em>Slack</em>, Jira, <em>and</em> <em>CodeStream</em>",
        "body": " pika.exceptions:ChannelWrongStateError, you don&#x27;t see any resolved errors here because Errors Inbox unresolved that one when it saw another occurrence. Tip If you want to observe your ignored error groups instead of resolved ones, filter by Ignored. Optional: Integrate Errors Inbox with <em>Slack</em>, Jira, and <em>CodeStream</em>"
      },
      "id": "61be8f01196a67e048eef29c"
    },
    {
      "image": "https://docs.newrelic.com/static/79d35d62b4c952d3f8b6131bbcfce9e7/f96db/Sidebar3.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/codestream-sidebar/",
      "sections": [
        "CodeStream sidebar overview",
        "Sidebar overview",
        "The username menu",
        "Header menu items"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream sidebar overview",
      "updated_at": "2021-11-13T21:09:54Z",
      "type": "docs",
      "external_id": "d3bb6106b4e568060453b597ac08e4a51471f3a3",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic CodeStream sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here's a quick overview of the sidebar sections: Pull requests: If your team uses GitHub or GitHub Enterprise to host your code, you'll see all of your open pull requests listed here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you've been assigned a feedback request or you've requested feedback from someone else, find those listed here. Codemarks: Codemarks are the discussions that annotate your codebase. Codemarks are created pull requests, feedback requests, or through ad hoc code comments/issues. All of the codemarks in your current repository are listed for reference. Observability: Track errors assigned to you in New Relic One and discover recent errors in the repositories you have open in your IDE. Issues: See all of your open issues, across multiple services, in one place. Click a specific issue to update its status, create a feature branch to do your work, and update your status on Slack. The CodeStream sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen. This is useful when you're looking at a longer list. Click it again to return to your previous view. The username menu The username menu gives you options for managing your account, your organization, how you receive notifications, and what sections are visible. Here are descriptions of each menu item: Account: View your profile. Includes various options for customizing the profile photo, email, username, and full name for your account. View: Uncheck sections you're not interested in seeing. Notifications: Manage how and what notifications you receive. Organization admin: Manage your organization settings. Also, export your data and delete your organization. Switch organization: Use this if you're a member of more than one organization. Integrations: Connect CodeStream to the code host, issue, and messaging providers you use. New Relic setup: Connect your New Relic account to CodeStream to get the most out of New Relic CodeStream's observability tools. Feedback: Got feedback for us? Write a GitHub issue. Help: Links to documentation, our video library, keybindings, the CodeStream workflow, what's new, and reporting an issue. Header menu items The header menu items provide different options for creating and discovering content in your organization. From left to right: Create, Activity feed, My organization, and Filter & search. + (Compose): Click to create a code comment/issue, request feedback on changes, or to create a pull request. Activity feed: The activity feed will let you know about new code comments/issues and feedback requests, as well as replies to existing ones. My organization: See who is in your CodeStream organization, invite new members, and create blame maps. Filter & search: The filter a search tools enable you to slice and dice your team’s collection of code comments, issues, and feature requests however you see fit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 576.60364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> sidebar overview",
        "sections": "<em>CodeStream</em> sidebar overview",
        "body": " status on <em>Slack</em>. The <em>CodeStream</em> sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen"
      },
      "id": "617cbd1b28ccbc18bf7fef35"
    },
    {
      "image": "https://docs.newrelic.com/static/8945e0a9c512b8638ebf8165d47aee04/69902/QS-SignUp3.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-user-guide/",
      "sections": [
        "New Relic CodeStream user guide",
        "Jump to a topic",
        "1. Install the CodeStream extension in your IDE and sign up.",
        "2. Connect your tools",
        "3. Discuss any block of code, at any time",
        "4. Get feedback on your work in progress",
        "5. Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "New Relic CodeStream user guide",
      "updated_at": "2021-11-24T04:44:31Z",
      "type": "docs",
      "external_id": "fa9af0118a8872fea89fda91482c44fb69913ea2",
      "document_type": "page",
      "popularity": 1,
      "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic CodeStream. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. 1. Install the CodeStream extension in your IDE and sign up. Install CodeStream for VS Code, Visual Studio or JetBrains. The CodeStream pane automatically appears in the sidebar for VS Code or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you're the first person from your team to join CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on GitHub, GitLab or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click on your headshot at the top of the CodeStream pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, just select the code and ask your question. Learn more about discussing code. 4. Get feedback on your work in progress Select Request Feedback from the + menu at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code!) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. 5. Create or review a pull request Look for the Pull Requests section of the CodeStream sidebar to review an open pull request. Just click on a pull request (or load one from URL) to get a complete GitHub experience right in your IDE! Note that you can create a pull request in GitHub, GitLab or Bitbucket, but support for reviewing pull requests is currently only available for GitHub (cloud or Enterprise). Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 470.90216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic <em>CodeStream</em> user guide",
        "sections": "1. Install the <em>CodeStream</em> extension in your IDE <em>and</em> sign up.",
        "body": " discussions on <em>Slack</em> or Microsoft Teams. <em>CodeStream</em> brings the tools you use every day together in your IDE. Click on your headshot at the top of the <em>CodeStream</em> pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of <em>code</em>, at any time Whether you&#x27;re trying"
      },
      "id": "61744137e7b9d2428b13c6a0"
    }
  ],
  "/docs/codestream/codestream-settings/account-settings": [
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.91594,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One <em>account</em>"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/79d35d62b4c952d3f8b6131bbcfce9e7/f96db/Sidebar3.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/codestream-sidebar/",
      "sections": [
        "CodeStream sidebar overview",
        "Sidebar overview",
        "The username menu",
        "Header menu items"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream sidebar overview",
      "updated_at": "2021-11-13T21:09:54Z",
      "type": "docs",
      "external_id": "d3bb6106b4e568060453b597ac08e4a51471f3a3",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic CodeStream sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here's a quick overview of the sidebar sections: Pull requests: If your team uses GitHub or GitHub Enterprise to host your code, you'll see all of your open pull requests listed here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you've been assigned a feedback request or you've requested feedback from someone else, find those listed here. Codemarks: Codemarks are the discussions that annotate your codebase. Codemarks are created pull requests, feedback requests, or through ad hoc code comments/issues. All of the codemarks in your current repository are listed for reference. Observability: Track errors assigned to you in New Relic One and discover recent errors in the repositories you have open in your IDE. Issues: See all of your open issues, across multiple services, in one place. Click a specific issue to update its status, create a feature branch to do your work, and update your status on Slack. The CodeStream sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen. This is useful when you're looking at a longer list. Click it again to return to your previous view. The username menu The username menu gives you options for managing your account, your organization, how you receive notifications, and what sections are visible. Here are descriptions of each menu item: Account: View your profile. Includes various options for customizing the profile photo, email, username, and full name for your account. View: Uncheck sections you're not interested in seeing. Notifications: Manage how and what notifications you receive. Organization admin: Manage your organization settings. Also, export your data and delete your organization. Switch organization: Use this if you're a member of more than one organization. Integrations: Connect CodeStream to the code host, issue, and messaging providers you use. New Relic setup: Connect your New Relic account to CodeStream to get the most out of New Relic CodeStream's observability tools. Feedback: Got feedback for us? Write a GitHub issue. Help: Links to documentation, our video library, keybindings, the CodeStream workflow, what's new, and reporting an issue. Header menu items The header menu items provide different options for creating and discovering content in your organization. From left to right: Create, Activity feed, My organization, and Filter & search. + (Compose): Click to create a code comment/issue, request feedback on changes, or to create a pull request. Activity feed: The activity feed will let you know about new code comments/issues and feedback requests, as well as replies to existing ones. My organization: See who is in your CodeStream organization, invite new members, and create blame maps. Filter & search: The filter a search tools enable you to slice and dice your team’s collection of code comments, issues, and feature requests however you see fit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.21432,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> sidebar overview",
        "sections": "<em>CodeStream</em> sidebar overview",
        "body": " <em>settings</em>. Also, export your data and delete your organization. Switch organization: Use this if you&#x27;re a member of more than one organization. Integrations: Connect <em>CodeStream</em> to the <em>code</em> host, issue, and messaging providers you use. New Relic setup: Connect your New Relic <em>account</em> to <em>CodeStream</em> to get"
      },
      "id": "617cbd1b28ccbc18bf7fef35"
    },
    {
      "sections": [
        "Default time zone setting",
        "Change your default time zone",
        "Exceptions"
      ],
      "title": "Default time zone setting",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "3a7abaee77b5d140836c96007766fa8eb9109b6b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/general-account-settings/default-time-zone-setting/",
      "published_at": "2022-01-12T08:16:27Z",
      "updated_at": "2022-01-08T04:21:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Your personal timezone setting controls most time-related settings in the New Relic UI, with a few exceptions, as explained in this document. If you change your timezone setting, this may take up to 24 hours to be reflected in the UI. Change your default time zone To change your default time zone for your New Relic account: Go to one.newrelic.com. Select the account dropdown, then select User preferences. Exceptions Users managed via automated user management can't change their time zone in the UI. That must be configured in your identity provider. Some New Relic features do not rely on the User preferences time zone settings. The following use Coordinated Universal Time (UTC) and aren't affected by user preferences: Alerts REST API v2 There may be other features where the time zone doesn't rely on your default time zone settings.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.39813,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default time zone <em>setting</em>",
        "sections": "Default time zone <em>setting</em>",
        "tags": "General <em>account</em> <em>settings</em>",
        "body": "Your personal timezone setting controls most time-related <em>settings</em> in the New Relic UI, with a few exceptions, as explained in this document. If you change your timezone setting, this may take up to 24 hours to be reflected in the UI. Change your default time zone To change your default time zone"
      },
      "id": "6043f38a28ccbc97e62c6090"
    }
  ],
  "/docs/codestream/codestream-settings/team-administration": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.3441,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Account ID"
      ],
      "title": "Account ID",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account setup"
      ],
      "external_id": "3771d53b48cfcf2f29d303b1d77a3884e4bae480",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/account-id/",
      "published_at": "2022-01-12T08:15:05Z",
      "updated_at": "2022-01-12T08:15:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some New Relic procedures require use of your account ID (for example, some API calls). Your account ID is the ID number we assign to a New Relic account. Note that some New Relic organizations contain multiple accounts. Options for finding your account ID: If your organization has multiple accounts: From one.newrelic.com, use the account selector dropdown near the top of the page to switch between accounts and see their IDs. There are other options, depending on which user model you're on: New Relic One user model: Click the account dropdown, and then go to: Administration > Organization and access > Accounts to see account IDs. Original user model: Click the account dropdown, click Account settings, and then click API keys. The account ID is displayed there. For more on how account access works, see Factors affecting access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 230.30515,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "Accounts <em>and</em> billing",
        "body": " accounts: From one.newrelic.com, use the account selector dropdown near the top of the page to switch between accounts and see their IDs. There are other options, depending on which user model you&#x27;re on: New Relic One user model: Click the account dropdown, and then go to: <em>Administration</em> &gt; <em>Organization</em>"
      },
      "id": "61271c84196a67ed0c00b367"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/authentication-domains-saml-sso-scim-more/",
      "sections": [
        "Authentication domain settings: SAML SSO, SCIM, and more",
        "Important",
        "Requirements",
        "What is an authentication domain?",
        "Create and configure an authentication domain",
        "Source of users: Manual versus SCIM",
        "Tip",
        "Authentication",
        "Set up SAML SSO authentication",
        "Azure app",
        "Okta app",
        "OneLogin app",
        "Session duration and timeout",
        "Manage user type",
        "Manage user type in New Relic (default)",
        "Manage user type with SCIM"
      ],
      "published_at": "2022-01-12T08:17:53Z",
      "title": "Authentication domain settings: SAML SSO, SCIM, and more",
      "updated_at": "2022-01-12T08:17:53Z",
      "type": "docs",
      "external_id": "8690bc2587ac53c35203d51075e1a2df0de33bc4",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for managing users on the New Relic One user model. For managing users on our original user model, see Original users. Learn about user models. To manage their users, New Relic organizations can configure one or more authentication domains, which control how users are added to a New Relic account, how they’re authenticated, and more. Requirements To check if you have access to these features, you can go to the authentication domain settings UI and see if you can configure settings. Requirements to configure these settings: These features are for managing users on the New Relic One user model. For users on our original user model, see Original account management. Configuring these settings requires Pro or Enterprise edition. To edit these settings, you must be in a group with the Authentication domain manager role. SCIM provisioning, also known as automated user management requires Pro or Enterprise edition. SAML SSO requires Pro or Enterprise edition. SAML support includes: Active Directory Federation Services (ADFS) Auth0 Azure AD (Microsoft Azure Active Directory) Okta OneLogin Ping Identity Salesforce Generic support for SSO systems that use SAML 2.0 What is an authentication domain? An \"authentication domain\" is a grouping of New Relic users governed by the same user management settings, like how they're provisioned (added and updated), how they're authenticated (logged in), session settings, and how user upgrades are managed. When someone creates a New Relic account, the default authentication settings are: Users are manually added to New Relic Users manually log in using their email and password Those default settings would be under one \"authentication domain.\" Another authentication domain might be set up like this: Users are added and managed from an identity provider using SCIM provisioning Users are logged in using SAML single sign-on (SSO) from an identity provider When you add users to New Relic, they’re added to a specific authentication domain. Typically organizations have either one or two authentication domains: one for the manual, default methods and one for the methods associated with an identity provider. Learn more in this short video (4:26 minutes): Create and configure an authentication domain If you meet the requirements, you can add and manage authentication domains. To view and configure authentication domains: from the account dropdown, go to Administration > Organization and access > Authentication domains. If you have existing domains, they'll be on the left. Note that most organizations will have, at most, two or three domains: one with the manual, default settings and one or two for the identity provider-associated settings. To create a new domain from the authentication domain UI page, click Create new. For more about the configuration options, keep reading. Source of users: Manual versus SCIM Tip For an introduction to our SAML SSO and SCIM offerings, please read Get started with SSO and SCIM. From the authentication domain UI, you can set one of two options for how users are added to New Relic: Manual: This means that your users are added manually to New Relic from the User management UI. SCIM: Our automated user management feature allows you to use SCIM provisioning to import and manage users from your identity provider. Notes on these settings: You can't toggle Source of users. This means if you want to change this for an authentication domain that's already been set up, you'll need to create a new one. When you first enable SCIM, the bearer token is generated and only shown once. If you need to view a bearer token later, the only way to do this is to generate a new one, which will invalidate the old one and any integrations using the old token. For how to set up SCIM, see Automated user management. Authentication The authentication method is the way in which New Relic users log in to New Relic. All users in an authentication domain have a single authentication method. There are two authentication options: Username/password: Your users log in via email and password. SAML SSO: Your users log in via SAML single sign-on (SSO) via your identity provider. To learn how to set that up, keep reading. Set up SAML SSO authentication Before enabling SAML SSO using the instructions below, here are some things to understand and consider: For an introduction to our SAML SSO and SCIM offerings, please read Get started with SSO and SCIM. We recommend reviewing the SAML SSO requirements. Note that your SSO-enabled users won't receive email verification notifications from New Relic because the login and password information is handled by your identity provider. Consult your identity provider docs because they may have New Relic-specific instructions. If you're setting up SCIM provisioning: If you use Azure, Okta, or OneLogin, follow these procedures first: Azure | OneLogin | Okta. If you use a different identity provider, follow the SAML procedures below and use our SCIM API to enable SCIM. If you only want to enable SAML SSO and not SCIM, and if you use Azure, Okta, or OneLogin, follow these instructions for configuring the relevant app: Azure app Azure AD provides an application gallery, which includes various integrations for Azure AD, including the ones that New Relic offers. Add the New Relic SCIM/SSO application to your list of applications. Go to the Azure Active Directory admin center, and sign in if necessary. aad.portal.azure.com/ Click on All services in the left hand menu. In the main pane, click on Enterprise applications. Click on +New application. Find our SCIM/SSO application by entering New Relic in the name search box, and click on the application New Relic by organization (not New Relic by account). Click on Add. Okta app Add the New Relic SCIM/SSO application to your Okta applications. Go to okta.com/ and sign in with an account that has administrator permissions. From the Okta home page, click on Admin. From the Okta admin Dashboard, choose the Applications page. Click Add Application. In the search field on the Okta Add applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the New Relic by Organization page, click on Add. From the Add New Relic by Organization page, check the two Application visibility \"Do not display...\" checkboxes and click on Done. We will make the application visible later after configuration is complete and provisioning has begun. OneLogin app Add the New Relic SCIM/SSO application to your OneLogin applications. Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click on Administration. From the OneLogin Administration page, choose the Applications menu. From the OneLogin Applications page, click on Add app. In the search field on the OneLogin Find Applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the Add New Relic by organization page, click on Save. If you're implementing SAML using a different identity provider not mentioned above, you'll need to attempt to integrate using the SAML instructions below. Note that your identity provider must use the SAML 2.0 protocol, and must require signed SAML assertions. Next, you'll go to our authentication domain UI. From the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new domain to be used for your SAML-authenticating users. Under Authentication, click Configure. Under Method of authenticating users, select SAML SSO. If you're using the Okta, OneLogin, or Azure AD app, you can skip this step. Under Provided by New Relic, we have some New Relic-specific information. You'll need to place these in the relevant fields in your identity provider service. If you're not sure where they go, consult your identity provider docs. Under Provided by you, input the Source of SAML metadata. This URL is supplied by your identity provider and may be called something else. It should conform to SAML V2.0 metadata standards. If your identity provider doesn't support dynamic configuration, you can do this by using Upload a certificate. This should be a PEM encoded x509 certificate. Under Provided by you, set the SSO target URL supplied by your identity provider. You can find this by going to the Source of SAML metadata and finding the POST binding URL. It looks like: https://newrelic.oktapreview.com/app/newreliclr/1234567890abcdefghij/sso/saml. If your identity provider has a redirect URL for logout, enter it in the Logout redirect URL; otherwise, leave it blank. If you’re using an identity provider app, you’ll need to input the authentication domain ID in the app. That ID is found at the top of New Relic’s authentication domain UI page. Optional: In New Relic’s authentication domain UI, you can adjust other settings, like browser session length and user upgrade method. You can adjust these settings at any time. If you're enabling SAML only, you need to create groups and assign access grants in New Relic. (If you enabled SCIM, you've already completed this step.) Access grants are what give your users access to New Relic accounts. Without access grants, your users are provisioned in New Relic but have no account access. To learn how to do this: Learn how access grants work Read the access grant tutorial. Okta only: Return to Okta's New Relic app and, from the Add New Relic by organization page, uncheck the two Application visibility \"Do not display...\" checkboxes and click on Done. To verify it's been set up correctly, see if your users can log in to New Relic via your identity provider and ensure they have access to their accounts. Session duration and timeout In the authentication domain UI, under Management, you can control some other settings for the users in that domain, including: Length of time users can remain logged in. Amount of idle time before a user's session expires. User upgrade requests Manage user type In the authentication domain UI, under Management, you can control how your users' user type is managed. This includes how the user type can be edited and how basic user requests to become full platform users are handled. There are two main settings: Manage user type in New Relic: This is the default option. It allows you to manage your users' user type from New Relic. Manage user type with SCIM: Enabling this means that you can no longer manage user type from New Relic. You'd only be able to change and manage it from your identity provider. More on these two options: Manage user type in New Relic (default) Our default functionality is to manage user type from New Relic. This is the option you'd use if you weren't managing user type via SCIM. The management options governed by this include: The ability to change user type in the User management UI. The ability to manage how basic users' requests to become full platform users are handled. These options include: Allow basic users to self-upgrade: this allows basic users to be able to quickly become full platform users on their own, in order to quickly respond to issues. Require review by admins: in this case, admins refer to users with the Authentication domain manager role. With this option, admins receive an email when an upgrade request is made, and can approve or deny those requests in the User management UI. Manage user type with SCIM Our SCIM API lets you manage user type (basic and full platform) from your identity provider instead of in the New Relic user management UI. When you enable this, you can no longer change or manage your users' user type from New Relic. To enable this setting, your authentication domain Source of users must have SCIM enabled. Currently, we only have instructions for Okta. For other identity provider services, you can configure your identity provider to send us user type using our SCIM API user type specifications. How basic users' requests to become full platform users are handled: When you enable Manage user type with SCIM, the default option is that your basic users are shown a message explaining that they need to \"reach out to your manager or IT department\" to become full platform users. Optionally, you can customize that message and add a link to a customized page. For example, you might add a custom message that explains to contact a specific person or department. Or you might use our SCIM API specifications to set up an implementation that programmatically handles user type change requests and automatically changes basic users to full platform users. For more about basic users and full platform users, see User type. Note that if you're on our original user model, upgrades work differently.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.36206,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Authentication domain <em>settings</em>: SAML SSO, SCIM, <em>and</em> more",
        "sections": "Authentication domain <em>settings</em>: SAML SSO, SCIM, <em>and</em> more",
        "body": ": from the account dropdown, go to <em>Administration</em> &gt; <em>Organization</em> and access &gt; Authentication domains. If you have existing domains, they&#x27;ll be on the left. Note that most organizations will have, at most, two or three domains: one with the manual, default <em>settings</em> and one or two for the identity"
      },
      "id": "6043f69a196a67380e960f4c"
    }
  ],
  "/docs/codestream/codestream-ui-overview/activity-feed": [
    {
      "image": "https://docs.newrelic.com/static/30e00c292c5aa1c5d702d67be5021a45/f96db/CreateAnAccount6.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/sign-up-codestream/",
      "sections": [
        "Sign up for CodeStream",
        "Create an account",
        "CodeStream organizations",
        "Create or join an organization",
        "Invite your team"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Sign up for CodeStream",
      "updated_at": "2021-11-24T09:42:56Z",
      "type": "docs",
      "external_id": "2f8eda03703523f62844512a3b8ef005b624ebc2",
      "document_type": "page",
      "popularity": 1,
      "body": "To get the most out of New Relic CodeStream's collaboration tools, create an organization and then invite your team members to it. If you haven't already, sign up for a free New Relic account to jump from your application's stack trace errors in New Relic One directly to the line of code responsible in your IDE. Create an account If you already have the CodeStream extension installed in your IDE, you can start the sign up process from the CodeStream pane. There are two ways to sign up for CodeStream. You can sign up with a set of CodeStream credentials (such as, email address and password). Alternatively, you can sign up using your GitHub, GitLab, or Bitbucket (cloud versions only) account. Signing up via your code host also connects your repositories to CodeStream. If the email address you're using with your code host isn't your work email, you should create CodeStream-specific credentials instead. If you sign up via CodeStream, your next step will be to confirm your email address by entering a code sent to you via email. You can paste the code into any of the boxes rather than typing each number individually. CodeStream organizations A CodeStream organization is where you and your teammates will discuss code. Similar to a Slack workspace, all of the developers in your company are in the same CodeStream organization. CodeStream's activity feed keeps things relevant for you by showing activity related to the code you have open in your IDE. The discussions about code build up a knowledge base that is a company-wide resource, so the only reason to have multiple organizations on CodeStream is if you truly need separation. For example, you might have an organization for your day job and another for an open-source project you work on. Or maybe you're a consultant that's a member of different CodeStream organizations for each of your clients. Create or join an organization If you're invited to join an organization on CodeStream, sign up with the same email address the invitation was sent to. You'll automatically be added to that organization. If you're the first person from your company to sign up for CodeStream you can go ahead and create a new organization. Otherwise, there may be existing CodeStream organizations available for you to join based on your email domain. If you think your company is already on CodeStream, but don't see an organization to join, make sure that you've signed up with your work email. If you decide to create an organization you'll be asked to give it a name and, if you signed up with your work email address, anyone else with that email domain can join the organization. Invite your team Collaboration is a team sport so invite your teammates to join you on CodeStream. CodeStream will offer up some suggestions based on the commit history of the repositories you have open in your IDE. Now you're ready to start using CodeStream.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1594.1421,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Sign up for <em>CodeStream</em>",
        "sections": "Sign up for <em>CodeStream</em>",
        "body": " and your teammates will discuss <em>code</em>. Similar to a Slack workspace, all of the developers in your company are in the same <em>CodeStream</em> organization. <em>CodeStream</em>&#x27;s <em>activity</em> <em>feed</em> keeps things relevant for you by showing <em>activity</em> related to the <em>code</em> you have open in your IDE. The discussions about <em>code</em> build up"
      },
      "id": "617440e3196a6782592f011c"
    },
    {
      "image": "https://docs.newrelic.com/static/eeb160b7c86583a706f3b8b4cc3d5f41/f96db/AccountSettings2.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-settings/account-settings/",
      "sections": [
        "CodeStream account settings and profile pages",
        "Account overview",
        "Account profile"
      ],
      "published_at": "2022-01-12T07:46:12Z",
      "title": "CodeStream account settings and profile pages",
      "updated_at": "2021-11-13T21:05:57Z",
      "type": "docs",
      "external_id": "83e4ba3ea989b1f2a0bce57b59bf3a7ca92ad2cc",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream account settings let you make changes to your personal account. Account overview The Account menu is located under the ellipses menu at the top of the CodeStream pane. The account menu includes options for viewing your profile and changing your profile photo, username, full name, and email address. When you change your email address, the old one will remain in effect until you click the link in the confirmation email that's sent to you. If you'd like to cancel your CodeStream account, find that option under Other Actions. Account profile If you click a headshot from anywhere in CodeStream (such as, the activity feed or a discussion thread) you're taken to that person's profile page. When viewing your own profile page, hover over entries for username, email address, phone number, works on, or the profile photo to see a pencil icon that enables you to make changes to those fields.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1381.7737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> account settings and profile pages",
        "sections": "<em>CodeStream</em> account settings and profile pages",
        "body": " anywhere in <em>CodeStream</em> (such as, the <em>activity</em> <em>feed</em> or a discussion thread) you&#x27;re taken to that person&#x27;s profile page. When viewing your own profile page, hover over entries for username, email address, phone number, works on, or the profile photo to see a pencil icon that enables you to make changes to those fields."
      },
      "id": "61743fc5e7b9d2b9e813cf20"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.64365,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    }
  ],
  "/docs/codestream/codestream-ui-overview/codemarks-section": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.64365,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/74c274508fc745275751f72d3e01ea33/f96db/CodemarkAddRange1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/discuss-code/",
      "sections": [
        "Discuss code on CodeStream",
        "What is a codemark?",
        "Create a codemark",
        "Comment codemarks",
        "Issue codemarks",
        "Bring the right people into the discussion",
        "Work with different versions of the code",
        "Resolve codemarks",
        "Advanced features",
        "Multiple ranges",
        "File attachments",
        "Add tags",
        "Related codemarks",
        "Manage codemarks"
      ],
      "published_at": "2022-01-12T08:21:20Z",
      "title": "Discuss code on CodeStream",
      "updated_at": "2021-11-13T21:16:02Z",
      "type": "docs",
      "external_id": "9169c03142311fe838566856d0ff154df0e6d555",
      "document_type": "page",
      "popularity": 1,
      "body": "What is a codemark? Quite simply, a codemark is a discussion connected to the code. It could be a question, a suggestion, a bug report, or documentation. All of these discussions are saved, anchored to the blocks of code they refer to, so that they can be leveraged in the future. It could be a new developer joining the team, a developer trying to fix a bug in someone else’s code, or even just you trying to remember why you made that change six months ago. Whatever the case, CodeStream helps you understand the code by surfacing the discussions in a contextual way. Even as a file changes over time, the codemarks remain connected to the code. Add some new lines of code above the code block, make edits to the code, or even cut-and-paste the entire block to a different section of the file, and you’ll see the codemark move along with your changes. Create a codemark To create a codemark, select a block of code in your editor and then click one of the icons that appears in the CodeStream pane next to your selection. If you're using a JetBrains IDE, such as IntelliJ, you can also create a codemark via the + button that appears in the editor's gutter when you select a block of code. When you're viewing a diff, for either a feedback request or a pull request, the button will also appear when you hover over the gutter to make it easy to comment on a single line. Even when the CodeStream pane is closed or not in view, you can create a codemark via the CodeStream options in either the lightbulb or context menus. You can also look for the + menu at the top of the CodeStream pane. Need to reach teammates that don’t spend a lot of time in the IDE? Or maybe some teammates that aren’t yet on CodeStream? You can optionally share a codemark out to Slack or Microsoft Teams. The Slack integration even allows your teammates to reply directly from Slack. Comment codemarks Comment codemarks are the all-purpose codemark for linking any type of discussion to a block of code. Ask a question. Make a suggestion. Document some code. Make note of key sections of the codebase. The possibilities are endless. Issue codemarks When something needs to get done there’s a better chance of it happening if it’s captured as an issue with someone’s name attached. Assign issues as a way of reporting bugs or manage your tech debt by capturing items as tracked issues instead of inline FIXMEs. If your team uses Asana, Azure DevOps, Bitbucket (cloud), Clubhouse, GitHub (cloud or Enterprise), GitLab (cloud or Self-Managed), Jira (cloud or Server), Linear, Trello, or YouTrack (cloud) for tracking issues, you can create an issue on one of those services directly from CodeStream. Select the service you use from the dropdown at the top of the codemark form. After going through the authentication process with the selected service, you can select a destination for your issue. For example, with Jira you'll be able to select the appropriate issue type and project. Once the issue has been created on CodeStream, it includes a link to the issue that was created on the external service. In the example, you'll see the URL for the issue on Jira. The issue on Jira includes a link to open the relevant code in your IDE. Bring the right people into the discussion When you create a codemark, CodeStream automatically mentions the people that most recently touched the code you're commenting on. They may be the best people to answer your question, but you can, of course, remove those mentions and manually mention someone else if appropriate. It may be the case that the people that have touched the code aren't yet on CodeStream, in which case CodeStream will provide checkboxes to have them notified via email. They can reply to the email to have their comment posted to CodeStream and, of course, they can install CodeStream to participate from their IDE. Work with different versions of the code Maybe you’re on a feature branch, have local changes, or simply haven’t pulled in a while. There are countless reasons why the code you’re looking at might be different than what a teammate is looking at. As a result, there will be plenty of times when the code referenced in a codemark doesn’t match what you have locally. CodeStream recognizes these situations and includes the original version of the code block (such as, at the time the codemark was created), the current version, and a diff. Keep in mind that with CodeStream you can discuss any line of code, in any source file, at any time, even if it’s code that you just typed into your editor and haven’t yet committed. CodeStream empowers you to discuss code at the very earliest stages of the development process. Resolve codemarks Although not required, both comment and issue codemarks can be resolved. The codemarks section of the CodeStream pane breaks out codemarks into open, resolved and archived sections. Green, purple, and gray icons are used to represent those different states. If you see a lot of open/green codemarks in the CodeStream pane, that means that your teammates are being blocked by discussions and issues that haven't been resolved. You can add a comment at the same time you resolve the codemark and you can also archive the codemark at the same time. Advanced features Advanced features include multiple range codemarks, file attachments, tags, and related codemarks. Multiple ranges Many discussions about code involve more than just one block of code and concepts are often best presented when you can refer to multiple code locations at once. Here are a few examples of multi-range codemark at work: A change to a function is being contemplated that will impact its name. Each instance of the function call can now be referenced in one discussion. A React component and its CSS styling aren’t interacting well and you want to ask the team for input. You might select the div and the CSS rules you think should apply, so your teammates know exactly what you’re talking about. Clients which make API calls to the server might get an unexpected result. Select the code where you’re making the API call, and the handler in the API server, to connect the two actions together. To create a multi-range codemark, click + Add Code Block. Then select another block of code from the same file, a different file, or even a different repo. You can intersperse the difference code blocks in your post by referring to each one as [#N] (or click the pin icon from one of the code blocks to insert the markdown for you), as in the following example. Here's how that example is rendered. Once you've created the codemark, you can jump between the different locations by clicking the jump icon at the bottom right of each code block. When you edit a codemark, you can add and remove code blocks and you can change the location of any of the code blocks by clicking the dashed square icon. File attachments Enrich your discussions about code by attaching files directly to code blocks. Think about how much more compelling your comments and documentation become when you attach: A spec to guide the development of a new feature A log file to help debug an issue in the code A mockup to help clarify some UI work A screenshot to highlight a problem When creating a code comment or issue, you can attach a file by dragging-and-dropping onto the description field, pasting from your clipboard, or by clicking the paperclip icon. Images can even be displayed inline using markdown. Click the pin icon to the right of the attachment and CodeStream will insert the markdown for you. Now your teammate knows exactly what you’re looking for. You can click on files in the attachments section to either download it or open it in the appropriate application. Add tags Look for the tag icon inside the codemark compose box to either select a tag or create a tag using any combination of color and text label. Tags are a great way to broadly organize and group your organization's codemarks and the possibilities here are endless. You can also filter by tag on the Filter & Search page. Related codemarks Click the CodeStream icon in the codemark compose form to select other related codemarks to attach them to the current codemark. This establishes a connection between different parts of a codebase. For example, when a change to one part of the codebase would require a change to another part, identify the dependency by creating two related codemarks. Once you’ve added the related codemarks they’ll be displayed in a related section and you can click on any one to jump to that codemark and the corresponding section of the code. Manage codemarks Click the ellipses menu for any codemark and you'll see options to manage the codemark. Share: In addition to sharing to Slack or Teams at the time you create a codemark, you can also share it anytime later. Follow/Unfollow: Follow a codemark to be notified when its updated. Unfollow to stop receiving notifications. Copy link: Get a permalink for the codemark to share it anywhere. Archive: If there’s a codemark that you don’t think is important enough to be on permanent display in a given file, but you don’t want to completely delete it, you can archive it instead. Settings in the codemarks section allow you to easily see all archived codemarks. Edit: Only the codemark's author can edit it. Delete: Only the codemark's author can delete it, but we encourage you to archive instead of deleting unless you're positive the codemark won't have any future value. Inject as Inline Comment: If you'd like a specific codemark to become part of the repo use this option to have it added as an inline comment. You can select the appropriate format, and then indicate if you want to include timestamps, replies, or to have the comment wrapped at 80 characters. You can also elect to have the codemark archived once it's been added as an inline comment. Reposition codemark: In most cases, a codemark will automatically remain linked to the block of code it refers to as the file changes over time. For example, if you cut the block of code and paste it at a different location in the file, the codemark will move right along with it. There are some scenarios, however, that CodeStream isn't able to handle automatically. For example, if you pasted the block of code into a different file. In these cases, the Reposition codemark allows you to select the new location of the block of code so that the codemark is displayed properly.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 258.07617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discuss <em>code</em> on <em>CodeStream</em>",
        "sections": "Discuss <em>code</em> on <em>CodeStream</em>",
        "body": " if it’s <em>code</em> that you just typed into your editor and haven’t yet committed. <em>CodeStream</em> empowers you to discuss <em>code</em> at the very earliest stages of the development process. Resolve <em>codemarks</em> Although not required, both comment and issue <em>codemarks</em> can be resolved. The <em>codemarks</em> <em>section</em> of the <em>CodeStream</em>"
      },
      "id": "6174400564441fe8685fd746"
    },
    {
      "image": "https://docs.newrelic.com/static/79d35d62b4c952d3f8b6131bbcfce9e7/f96db/Sidebar3.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/codestream-sidebar/",
      "sections": [
        "CodeStream sidebar overview",
        "Sidebar overview",
        "The username menu",
        "Header menu items"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream sidebar overview",
      "updated_at": "2021-11-13T21:09:54Z",
      "type": "docs",
      "external_id": "d3bb6106b4e568060453b597ac08e4a51471f3a3",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic CodeStream sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here's a quick overview of the sidebar sections: Pull requests: If your team uses GitHub or GitHub Enterprise to host your code, you'll see all of your open pull requests listed here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you've been assigned a feedback request or you've requested feedback from someone else, find those listed here. Codemarks: Codemarks are the discussions that annotate your codebase. Codemarks are created pull requests, feedback requests, or through ad hoc code comments/issues. All of the codemarks in your current repository are listed for reference. Observability: Track errors assigned to you in New Relic One and discover recent errors in the repositories you have open in your IDE. Issues: See all of your open issues, across multiple services, in one place. Click a specific issue to update its status, create a feature branch to do your work, and update your status on Slack. The CodeStream sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen. This is useful when you're looking at a longer list. Click it again to return to your previous view. The username menu The username menu gives you options for managing your account, your organization, how you receive notifications, and what sections are visible. Here are descriptions of each menu item: Account: View your profile. Includes various options for customizing the profile photo, email, username, and full name for your account. View: Uncheck sections you're not interested in seeing. Notifications: Manage how and what notifications you receive. Organization admin: Manage your organization settings. Also, export your data and delete your organization. Switch organization: Use this if you're a member of more than one organization. Integrations: Connect CodeStream to the code host, issue, and messaging providers you use. New Relic setup: Connect your New Relic account to CodeStream to get the most out of New Relic CodeStream's observability tools. Feedback: Got feedback for us? Write a GitHub issue. Help: Links to documentation, our video library, keybindings, the CodeStream workflow, what's new, and reporting an issue. Header menu items The header menu items provide different options for creating and discovering content in your organization. From left to right: Create, Activity feed, My organization, and Filter & search. + (Compose): Click to create a code comment/issue, request feedback on changes, or to create a pull request. Activity feed: The activity feed will let you know about new code comments/issues and feedback requests, as well as replies to existing ones. My organization: See who is in your CodeStream organization, invite new members, and create blame maps. Filter & search: The filter a search tools enable you to slice and dice your team’s collection of code comments, issues, and feature requests however you see fit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 249.82129,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> sidebar overview",
        "sections": "<em>CodeStream</em> sidebar overview",
        "body": "The New Relic <em>CodeStream</em> sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, <em>codemarks</em>, observability, and issues. Sidebar overview Here&#x27;s a quick overview of the sidebar"
      },
      "id": "617cbd1b28ccbc18bf7fef35"
    }
  ],
  "/docs/codestream/codestream-ui-overview/codestream-keyboard-shortcuts": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.64365,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.75354,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/595e873b1bfe20aa5d39a66adf18d0c9/f96db/RequestFeedback.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/request-feedback/",
      "sections": [
        "Request feedback on CodeStream",
        "Request feedback",
        "Tip",
        "Provide feedback",
        "Comments and change requests",
        "Add more code changes"
      ],
      "published_at": "2022-01-12T08:21:20Z",
      "title": "Request feedback on CodeStream",
      "updated_at": "2021-11-13T21:19:49Z",
      "type": "docs",
      "external_id": "752fa4dd9516d616b763d1552836fc30a93ccc7b",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream's feedback requests are powerful enough to use for traditional end-of-cycle code reviews, but at the same time they're so easy and flexible that you can use them throughout the development process to get quick feedback on work-in-progress. You can even use feedback requests for uncommitted your changes. Traditional code review happens at the end of the development cycle, when you’re looking to get the changes merged. Not only are end-of-cycle code reviews much more burdensome on your teammates, but you run the risk of identifying issues so late in the game that you end up having to decide between blowing up your schedule or taking on technical debt. Whether you’re at the beginning of a project, with just some stubbed out functions, are mid-way through a work in progress, or are ready for a final review of a finished project, CodeStream enables feedback at any point during the development cycle. CodeStream handles the complexity of sharing your current status, including pushed commits, local commits, and staged and saved changes. Your teammates can provide feedback from their IDE, with no need to switch applications, and no need to switch branches or pull changes. By the time you get to the formal code review/pull request at the end of the development cycle, it’s far less painful and more of a formality because issues have been raised, discussed, and resolved all along the way. Request feedback To request feedback at any time, regardless of the current state of your work, click the + Create button at the top of the CodeStream pane, or the + Request Feedback button in the header of the Feedback Requests section. You can also use a keyboard shortcut (ctlr+shift+/ r or ctrl+/ r on a Mac). With a single click you can name the feedback request based on the last commit message, the branch name, or, if you started work by selecting a ticket, the ticket title. CodeStream assumes that you are requesting feedback on changes in the repository/branch of the file you've currently selected in your editor. If you have multiple repositories open in your IDE, you can change this via the repository dropdown at the very top of the feedback request form. Depending on your organization's settings, CodeStream may suggest specific reviewers. Based on the commit history of the code being changed, the suggestions may even include someone that isn't yet on your CodeStream team. In that case, they'd be notified by email. Hover over a reviewer’s name to see more details or to remove them. If multiple reviewers are assigned you may also have the option to determine whether any of them can approve the review or if each one has to approve it individually. The Changed Files section lists all of the files that have been added, removed, or modified. Click any file to view a diff if you want to review your changes before submitting the feedback request. If you have a file that’s not suitable for review, such as a checked-in binary file, you can hover over any file and click the x to exclude that file from the feedback request. That file will be moved to a list below the form. New files are, by default, excluded from the feedback request, but you can hover over their entry in the list and click + to add them. Hover over an excluded file and click the trashcan to permanently exclude it from all future feedback requests. Permanently excluding files creates a .codestreamignore file in the repository. If you think your teammates will also want to exclude these files (for example, a package-lock.json or other system-generated file), you can commit and push the file so that they can make use of it as well. The changes represented across the selected files are broken out into four different categories, allowing you to select exactly what you would like to include in the feedback request. This includes changes that haven't been pushed, or even committed. The four categories are: Saved changes Staged changes Local commits Pushed commits Commits are listed in descending order across the Local Commits and Pushed Commits sections. If you uncheck the box for a commit, it will automatically uncheck the boxes for all of its preceding commits. In other words, the commits included in the feedback request must be consecutive. Only your commits are checked by default, but you can include any of them in your review. Tip Make sure the email address in your git configuration matches your CodeStream email address. Or set up a blame map to map your git email address to you CodeStream email address. Optionally, you can share your feedback request out to either Slack or Microsoft Teams. When you submit your feedback request, your teammates will be notified via the activity feed, with anyone assigned as a reviewer being @mentioned so that they’ll also receive an email notification. Provide feedback The best part of CodeStream's feedback requests is that having your teammates look over your code doesn't put any extra burden on them. There's no need for them to set aside their own work to switch branches or pull changes and no need to for them to leave their IDE. As long as they have the appropriate repository, they can open the feedback request and start reviewing your changes. Click any file in the Changed Files section to review the changes. The changes are presented with a diff in your editor. You can step through the changes in the file using your IDE's native navigation or click the up/down arrows at the top of your IDE. For JetBrains IDEs, CodeStream only supports the side-by-side diff viewer. Typically, the diff will represent the changes in the branch associated with the feedback request (such as, a feature/topic branch) against the base branch, at the point at which the feature branch was created. With CodeStream diffs this may not always be the case, because the developer may not have included all of their changes in the feedback request. As a result, the version of the files that the changes are being diff’ed against may, in fact, also include changes that aren’t in the base branch. This is important in order to provide continuity. Comments and change requests If you have a general comment about the changes, add a reply to the feedback request's thread. If you want to comment on the actual changes, select some code from the right side of the diff and then click the comment button that appears in the CodeStream pane next to your selection. You can also use a keyboard shortcut (ctlr+shift+/ c or ctrl+/ c on a Mac) after selecting some code. Since you have the full file context, you aren’t limited to commenting on just the lines of code that were changed. For example, you might notice another part of the file that needs work as well or that you simply want to reference. Whether it’s a general comment or a comment on code, you can mark it as a change request to let the developer know that it’s required before you’ll approve the changes. While you're providing feedback, you can even comment on files that aren't part of the changeset and they'll get added as a reply to the review. This is helpful to be able to point your teammate to another location in the codebase that might need improving. All of the change requests associated with the the feedback request are summarized in a section at the top, in addition to being part of the discussion thread. This is where they'll get marked complete when the work is done. Look for the green and red buttons at the top to either approve the changes or request additional changes. If there are any open change requests, the approve button will be replaced by a blue button that shows the number. You can still approve the changes, but we wanted to make sure you were aware of the outstanding work. When there are multiple reviewers, and an approval is required from each, CodeStream makes it very clear when there are still outstanding approvals. The blue button at the top right shows how many approvals are outstanding. The green thumbs up on the headshots of reviewers indicates that they've already approved your changes. Add more code changes A typical workflow involves the reviewer leaving some comments or suggesting some changes and then the developer responding to that feedback with more changes to the code. To continue the process, click the blue Amend button to add your changes. Similar to when you originally submitted the feedback request, you can choose from your saved and staged changes and your local and pushed commits. Any open change requests are also listed so you can mark off any that are addressed by your update. By default, when the reviewer goes back into the feedback request, they’ll be looking at the complete changeset (such as, changes across all updates) as they go through the diffs for each file. They can also view the diffs for any individual update. The feedback review process can continue across as many updates as needed to get to the final approval of your changes. Once the feedback request has been approved, you can create a pull request from within CodeStream to get your code merged. Tip The feedback request can't be amended or reopened once a pull request has been created.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.82474,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Request feedback on <em>CodeStream</em>",
        "sections": "Request feedback on <em>CodeStream</em>",
        "body": " state of your work, click the + Create button at the top of the <em>CodeStream</em> pane, or the + Request Feedback button in the header of the Feedback Requests section. You can also use a <em>keyboard</em> <em>shortcut</em> (ctlr+shift+&#x2F; r or ctrl+&#x2F; r on a Mac). With a single click you can name the feedback request based"
      },
      "id": "6174403d28ccbc9b20c6cca0"
    }
  ],
  "/docs/codestream/codestream-ui-overview/codestream-sidebar": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.3441,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.84854,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/8945e0a9c512b8638ebf8165d47aee04/69902/QS-SignUp3.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-user-guide/",
      "sections": [
        "New Relic CodeStream user guide",
        "Jump to a topic",
        "1. Install the CodeStream extension in your IDE and sign up.",
        "2. Connect your tools",
        "3. Discuss any block of code, at any time",
        "4. Get feedback on your work in progress",
        "5. Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "New Relic CodeStream user guide",
      "updated_at": "2021-11-24T04:44:31Z",
      "type": "docs",
      "external_id": "fa9af0118a8872fea89fda91482c44fb69913ea2",
      "document_type": "page",
      "popularity": 1,
      "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic CodeStream. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. 1. Install the CodeStream extension in your IDE and sign up. Install CodeStream for VS Code, Visual Studio or JetBrains. The CodeStream pane automatically appears in the sidebar for VS Code or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you're the first person from your team to join CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on GitHub, GitLab or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click on your headshot at the top of the CodeStream pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, just select the code and ask your question. Learn more about discussing code. 4. Get feedback on your work in progress Select Request Feedback from the + menu at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code!) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. 5. Create or review a pull request Look for the Pull Requests section of the CodeStream sidebar to review an open pull request. Just click on a pull request (or load one from URL) to get a complete GitHub experience right in your IDE! Note that you can create a pull request in GitHub, GitLab or Bitbucket, but support for reviewing pull requests is currently only available for GitHub (cloud or Enterprise). Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.05827,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic <em>CodeStream</em> user guide",
        "sections": "New Relic <em>CodeStream</em> user guide",
        "body": " in your IDE and sign up. Install <em>CodeStream</em> for VS <em>Code</em>, Visual Studio or JetBrains. The <em>CodeStream</em> pane automatically appears in the <em>sidebar</em> for VS <em>Code</em> or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you&#x27;re the first person from your team to join"
      },
      "id": "61744137e7b9d2428b13c6a0"
    }
  ],
  "/docs/codestream/codestream-ui-overview/feedback-requests-section": [
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1493.913,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to New Relic <em>CodeStream</em>",
        "sections": "Get <em>feedback</em> on work-in-progress with pre-PR <em>code</em> review",
        "body": " For most development teams, the final step in the development process is a pull <em>request</em>. Even if your team has decided to use <em>CodeStream</em>&#x27;s <em>feedback</em> <em>requests</em> as a replacement for, and not just a precursor to, your end-of-cycle PR-based <em>code</em> reviews, you can create and review pull <em>requests</em> right inside"
      },
      "id": "617440e3e7b9d2836c13c43c"
    },
    {
      "image": "https://docs.newrelic.com/static/595e873b1bfe20aa5d39a66adf18d0c9/f96db/RequestFeedback.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/request-feedback/",
      "sections": [
        "Request feedback on CodeStream",
        "Request feedback",
        "Tip",
        "Provide feedback",
        "Comments and change requests",
        "Add more code changes"
      ],
      "published_at": "2022-01-12T08:21:20Z",
      "title": "Request feedback on CodeStream",
      "updated_at": "2021-11-13T21:19:49Z",
      "type": "docs",
      "external_id": "752fa4dd9516d616b763d1552836fc30a93ccc7b",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream's feedback requests are powerful enough to use for traditional end-of-cycle code reviews, but at the same time they're so easy and flexible that you can use them throughout the development process to get quick feedback on work-in-progress. You can even use feedback requests for uncommitted your changes. Traditional code review happens at the end of the development cycle, when you’re looking to get the changes merged. Not only are end-of-cycle code reviews much more burdensome on your teammates, but you run the risk of identifying issues so late in the game that you end up having to decide between blowing up your schedule or taking on technical debt. Whether you’re at the beginning of a project, with just some stubbed out functions, are mid-way through a work in progress, or are ready for a final review of a finished project, CodeStream enables feedback at any point during the development cycle. CodeStream handles the complexity of sharing your current status, including pushed commits, local commits, and staged and saved changes. Your teammates can provide feedback from their IDE, with no need to switch applications, and no need to switch branches or pull changes. By the time you get to the formal code review/pull request at the end of the development cycle, it’s far less painful and more of a formality because issues have been raised, discussed, and resolved all along the way. Request feedback To request feedback at any time, regardless of the current state of your work, click the + Create button at the top of the CodeStream pane, or the + Request Feedback button in the header of the Feedback Requests section. You can also use a keyboard shortcut (ctlr+shift+/ r or ctrl+/ r on a Mac). With a single click you can name the feedback request based on the last commit message, the branch name, or, if you started work by selecting a ticket, the ticket title. CodeStream assumes that you are requesting feedback on changes in the repository/branch of the file you've currently selected in your editor. If you have multiple repositories open in your IDE, you can change this via the repository dropdown at the very top of the feedback request form. Depending on your organization's settings, CodeStream may suggest specific reviewers. Based on the commit history of the code being changed, the suggestions may even include someone that isn't yet on your CodeStream team. In that case, they'd be notified by email. Hover over a reviewer’s name to see more details or to remove them. If multiple reviewers are assigned you may also have the option to determine whether any of them can approve the review or if each one has to approve it individually. The Changed Files section lists all of the files that have been added, removed, or modified. Click any file to view a diff if you want to review your changes before submitting the feedback request. If you have a file that’s not suitable for review, such as a checked-in binary file, you can hover over any file and click the x to exclude that file from the feedback request. That file will be moved to a list below the form. New files are, by default, excluded from the feedback request, but you can hover over their entry in the list and click + to add them. Hover over an excluded file and click the trashcan to permanently exclude it from all future feedback requests. Permanently excluding files creates a .codestreamignore file in the repository. If you think your teammates will also want to exclude these files (for example, a package-lock.json or other system-generated file), you can commit and push the file so that they can make use of it as well. The changes represented across the selected files are broken out into four different categories, allowing you to select exactly what you would like to include in the feedback request. This includes changes that haven't been pushed, or even committed. The four categories are: Saved changes Staged changes Local commits Pushed commits Commits are listed in descending order across the Local Commits and Pushed Commits sections. If you uncheck the box for a commit, it will automatically uncheck the boxes for all of its preceding commits. In other words, the commits included in the feedback request must be consecutive. Only your commits are checked by default, but you can include any of them in your review. Tip Make sure the email address in your git configuration matches your CodeStream email address. Or set up a blame map to map your git email address to you CodeStream email address. Optionally, you can share your feedback request out to either Slack or Microsoft Teams. When you submit your feedback request, your teammates will be notified via the activity feed, with anyone assigned as a reviewer being @mentioned so that they’ll also receive an email notification. Provide feedback The best part of CodeStream's feedback requests is that having your teammates look over your code doesn't put any extra burden on them. There's no need for them to set aside their own work to switch branches or pull changes and no need to for them to leave their IDE. As long as they have the appropriate repository, they can open the feedback request and start reviewing your changes. Click any file in the Changed Files section to review the changes. The changes are presented with a diff in your editor. You can step through the changes in the file using your IDE's native navigation or click the up/down arrows at the top of your IDE. For JetBrains IDEs, CodeStream only supports the side-by-side diff viewer. Typically, the diff will represent the changes in the branch associated with the feedback request (such as, a feature/topic branch) against the base branch, at the point at which the feature branch was created. With CodeStream diffs this may not always be the case, because the developer may not have included all of their changes in the feedback request. As a result, the version of the files that the changes are being diff’ed against may, in fact, also include changes that aren’t in the base branch. This is important in order to provide continuity. Comments and change requests If you have a general comment about the changes, add a reply to the feedback request's thread. If you want to comment on the actual changes, select some code from the right side of the diff and then click the comment button that appears in the CodeStream pane next to your selection. You can also use a keyboard shortcut (ctlr+shift+/ c or ctrl+/ c on a Mac) after selecting some code. Since you have the full file context, you aren’t limited to commenting on just the lines of code that were changed. For example, you might notice another part of the file that needs work as well or that you simply want to reference. Whether it’s a general comment or a comment on code, you can mark it as a change request to let the developer know that it’s required before you’ll approve the changes. While you're providing feedback, you can even comment on files that aren't part of the changeset and they'll get added as a reply to the review. This is helpful to be able to point your teammate to another location in the codebase that might need improving. All of the change requests associated with the the feedback request are summarized in a section at the top, in addition to being part of the discussion thread. This is where they'll get marked complete when the work is done. Look for the green and red buttons at the top to either approve the changes or request additional changes. If there are any open change requests, the approve button will be replaced by a blue button that shows the number. You can still approve the changes, but we wanted to make sure you were aware of the outstanding work. When there are multiple reviewers, and an approval is required from each, CodeStream makes it very clear when there are still outstanding approvals. The blue button at the top right shows how many approvals are outstanding. The green thumbs up on the headshots of reviewers indicates that they've already approved your changes. Add more code changes A typical workflow involves the reviewer leaving some comments or suggesting some changes and then the developer responding to that feedback with more changes to the code. To continue the process, click the blue Amend button to add your changes. Similar to when you originally submitted the feedback request, you can choose from your saved and staged changes and your local and pushed commits. Any open change requests are also listed so you can mark off any that are addressed by your update. By default, when the reviewer goes back into the feedback request, they’ll be looking at the complete changeset (such as, changes across all updates) as they go through the diffs for each file. They can also view the diffs for any individual update. The feedback review process can continue across as many updates as needed to get to the final approval of your changes. Once the feedback request has been approved, you can create a pull request from within CodeStream to get your code merged. Tip The feedback request can't be amended or reopened once a pull request has been created.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1144.4497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Request</em> <em>feedback</em> on <em>CodeStream</em>",
        "sections": "<em>Request</em> <em>feedback</em> on <em>CodeStream</em>",
        "body": " state of your work, click the + Create button at the top of the <em>CodeStream</em> pane, or the + <em>Request</em> <em>Feedback</em> button in the header of the <em>Feedback</em> <em>Requests</em> section. You can also use a keyboard shortcut (ctlr+shift+&#x2F; r or ctrl+&#x2F; r on a Mac). With a single click you can name the <em>feedback</em> <em>request</em> based"
      },
      "id": "6174403d28ccbc9b20c6cca0"
    },
    {
      "image": "https://docs.newrelic.com/static/dd18b67123e9d4b7d40b56a8653a1f6b/f96db/OpenPullRequest1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/pull-requests/",
      "sections": [
        "Manage pull requests in CodeStream",
        "Pull request workflow",
        "Create a pull request",
        "Review a pull request",
        "Caution",
        "Leverage pull request comments"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Manage pull requests in CodeStream",
      "updated_at": "2021-11-13T21:14:26Z",
      "type": "docs",
      "external_id": "7e35f2ff4f06799fe492ffb6b2fedbb52e898b69",
      "document_type": "page",
      "popularity": 1,
      "body": "For most development teams, the final step in the development process is a pull request. Even if your team has decided to use New Relic CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, CodeStream allows you to keep all of that workflow inside your IDE. Pull request workflow There are four elements of CodeStream's pull request workflow. The following table outlines which code-hosting services are supported for each element. Feature Supported Services Create a pull request GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed, Bitbucket, Bitbucket Server Create a pull request across forks GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed Review and edit a pull request GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed Display pull request comments as code annotations GitHub, GitLab, Bitbucket Create a pull request To open a pull request at any time, click the + button at the top of the CodeStream pane or the + button in the header of the Pull Requests section. You can also use a keyboard shortcut (ctlr+shift+/ p, ctrl+/ p on a Mac, and m if you're a GitLab user). CodeStream provides you with tree view, list view, and diff view options for reviewing your changes before opening the pull request. With a single click you can name the pull request based on the last commit message, the branch name, or, if you started work by selecting a ticket, the ticket title. If you have a ticket selected, you can also explicitly tie the ticket to the pull request and CodeStream will include a link to the ticket in the pull request's description. Before submitting the pull request, you can review your changes by clicking on any of the files listed below the form. To create a pull request across forks, click the compare across forks link at the top of the page and the form will update to allow you to select both the base and head repositories. You can also create a pull request from within a CodeStream feedback request. Once the feedback request has been approved, you’ll see an option to open a pull request at the top. Before you can create a pull request, make sure that any changes included in the feedback request have been committed and pushed. Also, if the feature branch you’re working on doesn’t have a remote tracking branch you’ll be given the option to set that as part of creating the pull request. When you create a pull request from a feedback request, CodeStream connects the dots between the two by adding a link to the pull request in the feedback request. Add a link to the feedback request, along with information about who did the review and when, in the description of the pull request. Review a pull request Caution The ability to review pull requests is currently not available for Bitbucket. Regardless of where the pull request was created, you can edit, review, and even merge it from within CodeStream. CodeStream brings GitHub and GitLab into your IDE, so there's zero learning curve. If you know how to work with pull requests on GitHub or GitLab, you'll know how to do it in CodeStream as well. You can edit a GitHub pull request's details, such as reviewers, assignees and labels. For a GitLab merge request, you can use edit mode (via the dropdown at the top of the page) or use the sidebar. By default, you can only add a single reviewer and a single assignee to a GitLab merge request. If your organization supports multiple reviewers and assignees, click the gear menu in the heading of the Merge Requests section of the CodeStream pane to enable this. Review the conversation and add comments with the ability to @mention your collaborators. View the changes, add comments, and submit a review. CodeStream does improve upon the GitHub/GitLab experience in a couple of important ways. On GitHub and GitLab you can only view the changes as a series of diff hunks. CodeStream provides that view as well, but if you'd prefer to see the changes in the context of the full file you can use either list view or tree view. Select the code you want to comment on and then click the Comment button (or select Comment from the context menu). When commenting, you can either add a single comment or start a review. With CodeStream, you can comment on lines of code that haven't changed. You can select any lines of code in the diff and not just those that are part of the changeset. These comments are added as a single comment to the pull request and aren't part of any review you may have in progress. All the power of GitHub pull requests and GitLab merge requests, and then some, right in your IDE. Leverage pull request comments Once the pull request has been approved and the code has been merged, that's usually the end of life for any comments in that pull request. Although there's often useful information in those comments that may have long-term value, they're rarely seen again. CodeStream gives those comments a second life by displaying them alongside the blocks of code that they refer to. To have pull request comments displayed as annotations in your codemarks, as well as in the Codemarks section of the CodeStream pane, click the gear icon in that section and check Show comments from pull requests. When you first check that box, if you haven’t already authenticated with your code-hosting service, you’ll be prompted to do so. Comments from merged PRs will appear next to the blocks of code they refer to. Comments from open PRs will also be included if you are on a relevant branch. For example, if the open PR is a request to merge the feature/some-name branch into main, you’ll see comments from that PR if you've checked out either feature/some-name or main, but not when you’re on any other branch. As the code evolves, the location of each comment is automatically updated so that it remains linked to the block of code it refers to. PR comments for a given file are updated roughly every 30 minutes, so new comments may not appear right away. You can force an update by restarting your IDE.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 960.5154,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage pull <em>requests</em> in <em>CodeStream</em>",
        "sections": "Manage pull <em>requests</em> in <em>CodeStream</em>",
        "body": "For most development teams, the final step in the development process is a pull <em>request</em>. Even if your team has decided to use New Relic <em>CodeStream</em>&#x27;s <em>feedback</em> <em>requests</em> as a replacement for, and not just a precursor to, your end-of-cycle PR-based <em>code</em> reviews, <em>CodeStream</em> allows you to keep all"
      },
      "id": "61744006196a67ee542f0555"
    }
  ],
  "/docs/codestream/codestream-ui-overview/filter-search": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.64362,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.75342,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/79d35d62b4c952d3f8b6131bbcfce9e7/f96db/Sidebar3.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/codestream-sidebar/",
      "sections": [
        "CodeStream sidebar overview",
        "Sidebar overview",
        "The username menu",
        "Header menu items"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream sidebar overview",
      "updated_at": "2021-11-13T21:09:54Z",
      "type": "docs",
      "external_id": "d3bb6106b4e568060453b597ac08e4a51471f3a3",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic CodeStream sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here's a quick overview of the sidebar sections: Pull requests: If your team uses GitHub or GitHub Enterprise to host your code, you'll see all of your open pull requests listed here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you've been assigned a feedback request or you've requested feedback from someone else, find those listed here. Codemarks: Codemarks are the discussions that annotate your codebase. Codemarks are created pull requests, feedback requests, or through ad hoc code comments/issues. All of the codemarks in your current repository are listed for reference. Observability: Track errors assigned to you in New Relic One and discover recent errors in the repositories you have open in your IDE. Issues: See all of your open issues, across multiple services, in one place. Click a specific issue to update its status, create a feature branch to do your work, and update your status on Slack. The CodeStream sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen. This is useful when you're looking at a longer list. Click it again to return to your previous view. The username menu The username menu gives you options for managing your account, your organization, how you receive notifications, and what sections are visible. Here are descriptions of each menu item: Account: View your profile. Includes various options for customizing the profile photo, email, username, and full name for your account. View: Uncheck sections you're not interested in seeing. Notifications: Manage how and what notifications you receive. Organization admin: Manage your organization settings. Also, export your data and delete your organization. Switch organization: Use this if you're a member of more than one organization. Integrations: Connect CodeStream to the code host, issue, and messaging providers you use. New Relic setup: Connect your New Relic account to CodeStream to get the most out of New Relic CodeStream's observability tools. Feedback: Got feedback for us? Write a GitHub issue. Help: Links to documentation, our video library, keybindings, the CodeStream workflow, what's new, and reporting an issue. Header menu items The header menu items provide different options for creating and discovering content in your organization. From left to right: Create, Activity feed, My organization, and Filter & search. + (Compose): Click to create a code comment/issue, request feedback on changes, or to create a pull request. Activity feed: The activity feed will let you know about new code comments/issues and feedback requests, as well as replies to existing ones. My organization: See who is in your CodeStream organization, invite new members, and create blame maps. Filter & search: The filter a search tools enable you to slice and dice your team’s collection of code comments, issues, and feature requests however you see fit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.47932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> sidebar overview",
        "sections": "<em>CodeStream</em> sidebar overview",
        "body": " know about new <em>code</em> comments&#x2F;issues and feedback requests, as well as replies to existing ones. My organization: See who is in your <em>CodeStream</em> organization, invite new members, and create blame maps. <em>Filter</em> &amp; <em>search</em>: The <em>filter</em> a <em>search</em> tools enable you to slice and dice your team’s collection of <em>code</em> comments, issues, and feature requests however you see fit."
      },
      "id": "617cbd1b28ccbc18bf7fef35"
    }
  ],
  "/docs/codestream/codestream-ui-overview/issues-section": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.64362,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/48721d72a10a277b2ef615b62a07f0fe/f96db/GitHubAuth.png",
      "url": "https://docs.newrelic.com/docs/codestream/troubleshooting/github-org-repos/",
      "sections": [
        "Why aren't PRs and issues from all of my GitHub organizations listed in CodeStream?"
      ],
      "published_at": "2022-01-12T08:22:40Z",
      "title": "Why aren't PRs and issues from all of my GitHub organizations listed in CodeStream?",
      "updated_at": "2021-11-13T20:01:28Z",
      "type": "docs",
      "external_id": "aa4cd2a3acc746bc90aa1cf6663335e36084c002",
      "document_type": "page",
      "popularity": 1,
      "body": "When you connect to GitHub you should see all of your open pull requests in the Pull Requests section of the CodeStream pane, as well as all issues assigned to you in the Issues section. If pull requests or issues from any of your GitHub organizations are missing, it's probably because at the time you authenticated with GitHub and you didn't grant access to all of your organizations. If you didn't click the Grant button at authentication time, on GitHub go to Settings > Applications and click the Authorized OAuth Apps tab. From there, click the CodeStream application. On the following page, click the Grant button next to any organizations that you'd like to be able to access from CodeStream. In some instances, you'll see a Request button instead of Grant, which means that the owner of your GitHub organization will need to grant you access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 342.14612,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Why aren&#x27;t PRs and <em>issues</em> from all of my GitHub organizations listed in <em>CodeStream</em>?",
        "sections": "Why aren&#x27;t PRs and <em>issues</em> from all of my GitHub organizations listed in <em>CodeStream</em>?",
        "body": "When you connect to GitHub you should see all of your open pull requests in the Pull Requests section of the <em>CodeStream</em> pane, as well as all <em>issues</em> assigned to you in the <em>Issues</em> section. If pull requests or <em>issues</em> from any of your GitHub organizations are missing, it&#x27;s probably because at the time"
      },
      "id": "617441c3e7b9d2478513cf8c"
    },
    {
      "image": "https://docs.newrelic.com/static/ec73595e0bcde8b47ae3040cc556ddd1/f96db/NotificationSettings4.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-integrations/notifications/",
      "sections": [
        "CodeStream notifications",
        "Notification settings",
        "Follow or unfollow",
        "Desktop notifications",
        "Other notifications"
      ],
      "published_at": "2022-01-12T08:54:27Z",
      "title": "CodeStream notifications",
      "updated_at": "2021-11-13T21:01:16Z",
      "type": "docs",
      "external_id": "0af3b7458032e1b89f8e3cc7225d7c7b0272354a",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream will notify you about comments, issues, and feedback requests that you follow, and you can choose whether you want to be notified via email, desktop (for the VS Code and JetBrains extensions only), or both. Look for the notifications option under your user profile menu at the top of the CodeStream pane. Notification settings By default, you're set to automatically follow any comment, issue, or feedback request that you created, where you’ve been mentioned (either in the original post or in a subsequent reply), or to which you’ve replied. You can always choose to follow or unfollow any individual comment, issue, or feedback request via its ellipses menu. Follow or unfollow Email notifications are sent immediately. You can participate in the discussion by replying to the email. Your reply will get added to CodeStream as a reply to the appropriate comment, issue, or feedback request. Be sure that when you reply you're doing so from the same email address where the notification was sent (that is, your email address listed in My Organization on CodeStream). You can unfollow a codemark or code review by clicking the link at the bottom of the email. Desktop notifications If you’re using CodeStream in VS Code or a JetBrains IDE you can also receive desktop notifications in the IDE for comment, issue, or feedback requests that you follow. Click the Open button to open the discussion so that you can participate. Other notifications CodeStream offers the following notifications that can be turned on and off via the checkboxes at the bottom of the page. Notify me about outstanding feedback requests: Get an email reminder about open feedback requests assigned to you that you haven't responded to in the last 24 hours. Notify me about new unreviewed commits from teammates when I pull: Any time you pull, if there are new commits from a teammate on the current branch you'll get a toast notification. Click the Review button to start reviewing your teammate's changes and provide feedback. Send me weekly emails summarizing my activity: Sent every Monday with information about you and your organization's activity for the previous week. If you've connected to GitHub or GitHub Enterprise to leverage CodeStream's pull request integration, you'll also be notified when a pull request is assigned to you or you are added as a reviewer. Click the Open button to open the pull request right in your IDE where you can review the changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 264.06808,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> notifications",
        "sections": "<em>CodeStream</em> notifications",
        "body": "New Relic <em>CodeStream</em> will notify you about comments, <em>issues</em>, and feedback requests that you follow, and you can choose whether you want to be notified via email, desktop (for the VS <em>Code</em> and JetBrains extensions only), or both. Look for the notifications option under your user profile menu"
      },
      "id": "6174407564441f5c515fcf66"
    }
  ],
  "/docs/codestream/codestream-ui-overview/my-organization": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.34406,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/0d940de4d107acece21b6e518f1a2c53/d10fb/OrganizationSettings.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-settings/team-administration/",
      "sections": [
        "CodeStream organization settings and administration",
        "Manage people and roles",
        "Use blame map to reassign code",
        "Organization settings",
        "Onboarding settings",
        "Feedback request assignment and approval",
        "Change your organization's name",
        "Export your data"
      ],
      "published_at": "2022-01-12T07:46:11Z",
      "title": "CodeStream organization settings and administration",
      "updated_at": "2021-11-13T21:06:47Z",
      "type": "docs",
      "external_id": "643e62ff1fc920ca94e3bcd2e27e3a9be4317515",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream provides several tools for managing the teammates in your organization, whether they have accounts with New Relic One or not. Manage people and roles Click My Organization menu at the top of the CodeStream pane to invite people to your organization and assign or remove admin privileges. Use blame map to reassign code Click Blame Map to define code ownership with your organization. By default, when you comment on code, CodeStream mentions (or offers to email) the author or authors of the code you're commenting on. If that person has left the company it might not be the right thing to do. Non-admins are able to set up blame maps for themselves to handle situations where the email address they use to commit code is different than the one they used to sign up for CodeStream. Organization settings If you're an organization admin, look for the Organization Admin menu under the headshot menu at the top of the CodeStream pane. Onboarding settings Domain-based joining allows anyone with email addresses on the specified domains to join your CodeStream organization without being first invited. Not only does this make it very easy to get your teammates on board, but it ensures that they'll be part of your organization (as opposed to accidentally creating their own). Feedback request assignment and approval Admins can control how both feedback request assignments and approvals work for your organization. By default, the person requesting feedback can decide how approvals work, but you can, instead, set a default behavior for all feedback requests for the organization. Any reviewer can approve: Anyone can approve the feedback request, regardless of how many reviewers are assigned. All reviewers must approve individually: Each assigned reviewer must individually approve the feedback request before it’s considered approved. You can also decide if and how CodeStream suggests reviewers. Round-robin will cycle through all developers in the organization. Random will randomly assign the feedback request to any developer in the organization. The Authorship options will suggest up to three reviewers based on the developers who wrote the lines of code impacted by the changes, as well as other developers who may have committed to the branch. Change your organization's name Update the name of your CodeStream organization at any time. Export your data There's a lightweight export tool for getting your organization's discussions out of CodeStream. Click the icon to copy all of your data to the clipboard so that you can paste it elsewhere.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.13231,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> <em>organization</em> settings and administration",
        "sections": "<em>CodeStream</em> <em>organization</em> settings and administration",
        "body": "New Relic <em>CodeStream</em> provides several tools for managing the teammates in <em>your</em> <em>organization</em>, whether they have accounts with New Relic One or not. <em>Manage</em> people and roles Click My <em>Organization</em> menu at the top of the <em>CodeStream</em> pane to invite people to <em>your</em> <em>organization</em> and assign or remove admin"
      },
      "id": "61743ee264441ff1025fd5b5"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.85812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how <em>your</em> <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into <em>your</em> IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to <em>your</em> New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    }
  ],
  "/docs/codestream/codestream-ui-overview/observability-section": [
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 878.71783,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": " in the <em>CodeStream</em> <em>observability</em> section. In addition to recent errors in your repos, see any specific errors assigned to you. Use <em>CodeStream</em>&#x27;s <em>observability</em> section to keep up to date with recent and assigned stack trace errors. Error details No matter how you&#x27;ve arrived at an error in your IDE"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 531.5562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to New Relic <em>CodeStream</em>",
        "sections": "Intro to New Relic <em>CodeStream</em>",
        "body": " knowledge that is currently being lost in Slack channels and emails. Not only that, our <em>observability</em> solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic <em>CodeStream</em> to discover, troubleshoot, and triage errors in your IDE. (2:27"
      },
      "id": "617440e3e7b9d2836c13c43c"
    },
    {
      "image": "https://docs.newrelic.com/static/79d35d62b4c952d3f8b6131bbcfce9e7/f96db/Sidebar3.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/codestream-sidebar/",
      "sections": [
        "CodeStream sidebar overview",
        "Sidebar overview",
        "The username menu",
        "Header menu items"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream sidebar overview",
      "updated_at": "2021-11-13T21:09:54Z",
      "type": "docs",
      "external_id": "d3bb6106b4e568060453b597ac08e4a51471f3a3",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic CodeStream sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here's a quick overview of the sidebar sections: Pull requests: If your team uses GitHub or GitHub Enterprise to host your code, you'll see all of your open pull requests listed here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you've been assigned a feedback request or you've requested feedback from someone else, find those listed here. Codemarks: Codemarks are the discussions that annotate your codebase. Codemarks are created pull requests, feedback requests, or through ad hoc code comments/issues. All of the codemarks in your current repository are listed for reference. Observability: Track errors assigned to you in New Relic One and discover recent errors in the repositories you have open in your IDE. Issues: See all of your open issues, across multiple services, in one place. Click a specific issue to update its status, create a feature branch to do your work, and update your status on Slack. The CodeStream sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen. This is useful when you're looking at a longer list. Click it again to return to your previous view. The username menu The username menu gives you options for managing your account, your organization, how you receive notifications, and what sections are visible. Here are descriptions of each menu item: Account: View your profile. Includes various options for customizing the profile photo, email, username, and full name for your account. View: Uncheck sections you're not interested in seeing. Notifications: Manage how and what notifications you receive. Organization admin: Manage your organization settings. Also, export your data and delete your organization. Switch organization: Use this if you're a member of more than one organization. Integrations: Connect CodeStream to the code host, issue, and messaging providers you use. New Relic setup: Connect your New Relic account to CodeStream to get the most out of New Relic CodeStream's observability tools. Feedback: Got feedback for us? Write a GitHub issue. Help: Links to documentation, our video library, keybindings, the CodeStream workflow, what's new, and reporting an issue. Header menu items The header menu items provide different options for creating and discovering content in your organization. From left to right: Create, Activity feed, My organization, and Filter & search. + (Compose): Click to create a code comment/issue, request feedback on changes, or to create a pull request. Activity feed: The activity feed will let you know about new code comments/issues and feedback requests, as well as replies to existing ones. My organization: See who is in your CodeStream organization, invite new members, and create blame maps. Filter & search: The filter a search tools enable you to slice and dice your team’s collection of code comments, issues, and feature requests however you see fit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 509.38724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> sidebar overview",
        "sections": "<em>CodeStream</em> sidebar overview",
        "body": "The New Relic <em>CodeStream</em> sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, <em>observability</em>, and issues. Sidebar overview Here&#x27;s a quick overview of the sidebar"
      },
      "id": "617cbd1b28ccbc18bf7fef35"
    }
  ],
  "/docs/codestream/codestream-ui-overview/permalinks": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.34406,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.84836,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.39215,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to New Relic <em>CodeStream</em>",
        "sections": "Intro to New Relic <em>CodeStream</em>",
        "body": "New Relic <em>CodeStream</em> is a developer collaboration platform that enables your development team to discuss and review <em>code</em> in a natural and contextual way. <em>CodeStream</em> not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional"
      },
      "id": "617440e3e7b9d2836c13c43c"
    }
  ],
  "/docs/codestream/codestream-ui-overview/pull-requests-section": [
    {
      "image": "https://docs.newrelic.com/static/5c1d085b14abf961ca66b96285f0c0fa/69902/QS-Integrations.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/install-codestream/",
      "sections": [
        "Install New Relic CodeStream",
        "Install CodeStream",
        "Instant Observability (I/O) quickstart",
        "Visual Studio Code",
        "Visual Studio",
        "JetBrains",
        "Connect your tools",
        "Tip",
        "Discuss any block of code, at any time",
        "Get feedback on your work in progress",
        "Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T06:15:45Z",
      "title": "Install New Relic CodeStream",
      "updated_at": "2021-11-24T09:39:10Z",
      "type": "docs",
      "external_id": "5d431c8f9a2690b64d26ac9fc173b18085153aac",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that makes it easy to discuss and review code in a more natural and contextual way. Once connected to New Relic, collaborate on your application errors directly in your IDE. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Install CodeStream You can install CodeStream for your specific IDE or install it through our Instant Observability (I/O) quickstart. Instant Observability (I/O) quickstart Install CodeStream with its Instant Observability (I/O) quickstart to connect CodeStream to your New Relic account via your user key. Visual Studio Code Download and install CodeStream for Visual Studio Code. You can also install it directly in Visual Studio Code via the extensions marketplace. Visual Studio Download and install CodeStream for Visual Studio. You can also install it directly in Visual Studio via the extensions marketplace. JetBrains Download and install CodeStream for JetBrains. You can also install it from the JetBrains plugins menu. Connect your tools Create and review pull requests on GitHub, GitLab, or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Investigate errors reported to New Relic One. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click your headshot at the top of the CodeStream pane, then click Integrations to connect all of your tools to CodeStream. Tip Once you've installed CodeStream, to connect to New Relic, you'll need your New Relic user key. Go here to learn more about finding or creating your user key. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, select the code and click the comment button to ask your question. Learn more about discussing code. Get feedback on your work in progress Click the + menu then click Request Feedback at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. Create or review a pull request In the CodeStream sidebar, look for the Pull Requests section to review an open pull request. Select a pull request (or load one from URL) to get a complete GitHub experience right in your IDE. You can create a pull request in GitHub, GitLab, or Bitbucket, but support for reviewing pull requests is currently only available for GitHub. Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1445.8774,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install New Relic <em>CodeStream</em>",
        "sections": "Create or review a <em>pull</em> <em>request</em>",
        "body": " uncommitted <em>code</em>) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback <em>requests</em>. Create or review a <em>pull</em> <em>request</em> In the <em>CodeStream</em> sidebar, look for the <em>Pull</em> <em>Requests</em>"
      },
      "id": "6174400564441ff1025fd832"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.34406,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.44077,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to New Relic <em>CodeStream</em>",
        "sections": "Create and review <em>pull</em> <em>requests</em>",
        "body": " For most development teams, the final step in the development process is a <em>pull</em> <em>request</em>. Even if your team has decided to use <em>CodeStream</em>&#x27;s feedback <em>requests</em> as a replacement for, and not just a precursor to, your end-of-cycle PR-based <em>code</em> reviews, you can create and review <em>pull</em> <em>requests</em> right inside"
      },
      "id": "617440e3e7b9d2836c13c43c"
    }
  ],
  "/docs/codestream/how-use-codestream/discuss-code": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 448.61673,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.69958,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.22443,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to New Relic <em>CodeStream</em>",
        "sections": "<em>Discuss</em> <em>code</em> just like commenting <em>on</em> a Google Doc",
        "body": "New Relic <em>CodeStream</em> is a developer collaboration platform that enables your development team to <em>discuss</em> and review <em>code</em> in a natural and contextual way. <em>CodeStream</em> not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional"
      },
      "id": "617440e3e7b9d2836c13c43c"
    }
  ],
  "/docs/codestream/how-use-codestream/performance-monitoring": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.64362,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.09267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to New Relic <em>CodeStream</em>",
        "sections": "<em>Monitor</em> your <em>code’s</em> <em>performance</em> in production",
        "body": " your IDE. <em>CodeStream</em> shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. <em>Monitor</em> your <em>code</em>’s <em>performance</em> in production Your pursuit of software quality doesn’t end once the <em>code</em> has been merged. Connect <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617440e3e7b9d2836c13c43c"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/whats-new/2021/10/wn-codestream-1021/",
      "sections": [
        "Simplify code collaboration and review with New Relic CodeStream"
      ],
      "published_at": "2022-01-12T11:41:29Z",
      "title": "Simplify code collaboration and review with New Relic CodeStream",
      "updated_at": "2021-12-10T12:55:48Z",
      "type": "docs",
      "external_id": "f9d13af696d67a12cebebfc6698e916f3a5a11c9",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "Now you can resolve production errors faster and improve software performance with New Relic CodeStream. CodeStream is an exciting new IDE extension that helps developers discuss, review, and understand their code by integrating with popular dev tools and providing advanced in-IDE commenting. With the latest integration to New Relic One, CodeStream surfaces production errors inside your development environment for faster debugging. A free preview of the New Relic integration for CodeStream is available for everyone until February 28, 2022!",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.8521,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Simplify <em>code</em> collaboration and review <em>with</em> New Relic <em>CodeStream</em>",
        "sections": "Simplify <em>code</em> collaboration and review <em>with</em> New Relic <em>CodeStream</em>",
        "body": "Now you can resolve production errors faster and improve software <em>performance</em> with New Relic <em>CodeStream</em>. <em>CodeStream</em> is an exciting new IDE extension that helps developers discuss, review, and understand their <em>code</em> by integrating with popular dev tools and providing advanced in-IDE commenting"
      },
      "id": "617434da196a67c6212f0dde"
    }
  ],
  "/docs/codestream/how-use-codestream/pull-requests": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.34406,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/79d35d62b4c952d3f8b6131bbcfce9e7/f96db/Sidebar3.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/codestream-sidebar/",
      "sections": [
        "CodeStream sidebar overview",
        "Sidebar overview",
        "The username menu",
        "Header menu items"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream sidebar overview",
      "updated_at": "2021-11-13T21:09:54Z",
      "type": "docs",
      "external_id": "d3bb6106b4e568060453b597ac08e4a51471f3a3",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic CodeStream sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here's a quick overview of the sidebar sections: Pull requests: If your team uses GitHub or GitHub Enterprise to host your code, you'll see all of your open pull requests listed here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you've been assigned a feedback request or you've requested feedback from someone else, find those listed here. Codemarks: Codemarks are the discussions that annotate your codebase. Codemarks are created pull requests, feedback requests, or through ad hoc code comments/issues. All of the codemarks in your current repository are listed for reference. Observability: Track errors assigned to you in New Relic One and discover recent errors in the repositories you have open in your IDE. Issues: See all of your open issues, across multiple services, in one place. Click a specific issue to update its status, create a feature branch to do your work, and update your status on Slack. The CodeStream sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen. This is useful when you're looking at a longer list. Click it again to return to your previous view. The username menu The username menu gives you options for managing your account, your organization, how you receive notifications, and what sections are visible. Here are descriptions of each menu item: Account: View your profile. Includes various options for customizing the profile photo, email, username, and full name for your account. View: Uncheck sections you're not interested in seeing. Notifications: Manage how and what notifications you receive. Organization admin: Manage your organization settings. Also, export your data and delete your organization. Switch organization: Use this if you're a member of more than one organization. Integrations: Connect CodeStream to the code host, issue, and messaging providers you use. New Relic setup: Connect your New Relic account to CodeStream to get the most out of New Relic CodeStream's observability tools. Feedback: Got feedback for us? Write a GitHub issue. Help: Links to documentation, our video library, keybindings, the CodeStream workflow, what's new, and reporting an issue. Header menu items The header menu items provide different options for creating and discovering content in your organization. From left to right: Create, Activity feed, My organization, and Filter & search. + (Compose): Click to create a code comment/issue, request feedback on changes, or to create a pull request. Activity feed: The activity feed will let you know about new code comments/issues and feedback requests, as well as replies to existing ones. My organization: See who is in your CodeStream organization, invite new members, and create blame maps. Filter & search: The filter a search tools enable you to slice and dice your team’s collection of code comments, issues, and feature requests however you see fit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.92987,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> sidebar overview",
        "sections": "<em>CodeStream</em> sidebar overview",
        "body": "The New Relic <em>CodeStream</em> sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: <em>pull</em> <em>requests</em>, feedback <em>requests</em>, codemarks, observability, and issues. Sidebar overview Here&#x27;s a quick overview of the sidebar"
      },
      "id": "617cbd1b28ccbc18bf7fef35"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.76483,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Intro</em> to New Relic <em>CodeStream</em>",
        "sections": "<em>Intro</em> to New Relic <em>CodeStream</em>",
        "body": " For most development teams, the final step in the development process is a <em>pull</em> <em>request</em>. Even if your team has decided to use <em>CodeStream</em>&#x27;s feedback <em>requests</em> as a replacement for, and not just a precursor to, your end-of-cycle PR-based <em>code</em> reviews, you can create and review <em>pull</em> <em>requests</em> right inside"
      },
      "id": "617440e3e7b9d2836c13c43c"
    }
  ],
  "/docs/codestream/how-use-codestream/request-feedback": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.34406,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/dd18b67123e9d4b7d40b56a8653a1f6b/f96db/OpenPullRequest1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/pull-requests/",
      "sections": [
        "Manage pull requests in CodeStream",
        "Pull request workflow",
        "Create a pull request",
        "Review a pull request",
        "Caution",
        "Leverage pull request comments"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Manage pull requests in CodeStream",
      "updated_at": "2021-11-13T21:14:26Z",
      "type": "docs",
      "external_id": "7e35f2ff4f06799fe492ffb6b2fedbb52e898b69",
      "document_type": "page",
      "popularity": 1,
      "body": "For most development teams, the final step in the development process is a pull request. Even if your team has decided to use New Relic CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, CodeStream allows you to keep all of that workflow inside your IDE. Pull request workflow There are four elements of CodeStream's pull request workflow. The following table outlines which code-hosting services are supported for each element. Feature Supported Services Create a pull request GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed, Bitbucket, Bitbucket Server Create a pull request across forks GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed Review and edit a pull request GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed Display pull request comments as code annotations GitHub, GitLab, Bitbucket Create a pull request To open a pull request at any time, click the + button at the top of the CodeStream pane or the + button in the header of the Pull Requests section. You can also use a keyboard shortcut (ctlr+shift+/ p, ctrl+/ p on a Mac, and m if you're a GitLab user). CodeStream provides you with tree view, list view, and diff view options for reviewing your changes before opening the pull request. With a single click you can name the pull request based on the last commit message, the branch name, or, if you started work by selecting a ticket, the ticket title. If you have a ticket selected, you can also explicitly tie the ticket to the pull request and CodeStream will include a link to the ticket in the pull request's description. Before submitting the pull request, you can review your changes by clicking on any of the files listed below the form. To create a pull request across forks, click the compare across forks link at the top of the page and the form will update to allow you to select both the base and head repositories. You can also create a pull request from within a CodeStream feedback request. Once the feedback request has been approved, you’ll see an option to open a pull request at the top. Before you can create a pull request, make sure that any changes included in the feedback request have been committed and pushed. Also, if the feature branch you’re working on doesn’t have a remote tracking branch you’ll be given the option to set that as part of creating the pull request. When you create a pull request from a feedback request, CodeStream connects the dots between the two by adding a link to the pull request in the feedback request. Add a link to the feedback request, along with information about who did the review and when, in the description of the pull request. Review a pull request Caution The ability to review pull requests is currently not available for Bitbucket. Regardless of where the pull request was created, you can edit, review, and even merge it from within CodeStream. CodeStream brings GitHub and GitLab into your IDE, so there's zero learning curve. If you know how to work with pull requests on GitHub or GitLab, you'll know how to do it in CodeStream as well. You can edit a GitHub pull request's details, such as reviewers, assignees and labels. For a GitLab merge request, you can use edit mode (via the dropdown at the top of the page) or use the sidebar. By default, you can only add a single reviewer and a single assignee to a GitLab merge request. If your organization supports multiple reviewers and assignees, click the gear menu in the heading of the Merge Requests section of the CodeStream pane to enable this. Review the conversation and add comments with the ability to @mention your collaborators. View the changes, add comments, and submit a review. CodeStream does improve upon the GitHub/GitLab experience in a couple of important ways. On GitHub and GitLab you can only view the changes as a series of diff hunks. CodeStream provides that view as well, but if you'd prefer to see the changes in the context of the full file you can use either list view or tree view. Select the code you want to comment on and then click the Comment button (or select Comment from the context menu). When commenting, you can either add a single comment or start a review. With CodeStream, you can comment on lines of code that haven't changed. You can select any lines of code in the diff and not just those that are part of the changeset. These comments are added as a single comment to the pull request and aren't part of any review you may have in progress. All the power of GitHub pull requests and GitLab merge requests, and then some, right in your IDE. Leverage pull request comments Once the pull request has been approved and the code has been merged, that's usually the end of life for any comments in that pull request. Although there's often useful information in those comments that may have long-term value, they're rarely seen again. CodeStream gives those comments a second life by displaying them alongside the blocks of code that they refer to. To have pull request comments displayed as annotations in your codemarks, as well as in the Codemarks section of the CodeStream pane, click the gear icon in that section and check Show comments from pull requests. When you first check that box, if you haven’t already authenticated with your code-hosting service, you’ll be prompted to do so. Comments from merged PRs will appear next to the blocks of code they refer to. Comments from open PRs will also be included if you are on a relevant branch. For example, if the open PR is a request to merge the feature/some-name branch into main, you’ll see comments from that PR if you've checked out either feature/some-name or main, but not when you’re on any other branch. As the code evolves, the location of each comment is automatically updated so that it remains linked to the block of code it refers to. PR comments for a given file are updated roughly every 30 minutes, so new comments may not appear right away. You can force an update by restarting your IDE.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 347.51813,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage pull <em>requests</em> in <em>CodeStream</em>",
        "sections": "Manage pull <em>requests</em> in <em>CodeStream</em>",
        "body": "For most development teams, the final step in the development process is a pull <em>request</em>. Even if your team has decided to use New Relic <em>CodeStream</em>&#x27;s <em>feedback</em> requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based <em>code</em> reviews, <em>CodeStream</em> allows you to keep all"
      },
      "id": "61744006196a67ee542f0555"
    },
    {
      "image": "https://docs.newrelic.com/static/a6aec54accbdc1434b605775cb1bb6a6/f96db/FRSection1.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/feedback-requests-section/",
      "sections": [
        "CodeStream feedback requests"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream feedback requests",
      "updated_at": "2021-11-13T21:09:08Z",
      "type": "docs",
      "external_id": "def1d9cba862bb96307f99a80da7c7a634355582",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream's feedback requests section lists all of the open feedback requests that have been assigned to you or that you've requested, as well as any of your recent feedback requests that have been approved or where changes have been requested. Click a feedback request to jump in and start reviewing or to see your teammate’s comments on your work. When you hover over the Feedback Requests section heading, you'll see a + icon to create a new feedback request. If you're an admin you'll also see a gear icon to control how both feedback request assignments and approvals work for your organization. By default, the person requesting feedback can decide how approvals work, but you can also set a default behavior for all feedback requests for your organization. Any reviewer can approve: Anyone can approve the feedback request, regardless of how many reviewers have been assigned. All reviewers must approve individually: Each assigned reviewer must individually approve the feedback request before it’s considered approved. You can also decide if and how CodeStream suggests reviewers. Round-robin will cycle through all developers in the organization. Random will randomly assign the feedback request to any developer in the organization. The Authorship options suggest up to three reviewers based on the developers who wrote the lines of code impacted by the changes, as well as other developers who may have committed to the branch.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 267.2389,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> <em>feedback</em> <em>requests</em>",
        "sections": "<em>CodeStream</em> <em>feedback</em> <em>requests</em>",
        "body": "New Relic <em>CodeStream</em>&#x27;s <em>feedback</em> requests section lists all of the open <em>feedback</em> requests that have been assigned to you or that you&#x27;ve requested, as well as any of your recent <em>feedback</em> requests that have been approved or where changes have been requested. Click a <em>feedback</em> <em>request</em> to jump"
      },
      "id": "61743f4ae7b9d2636813d144"
    }
  ],
  "/docs/codestream/how-use-codestream/start-work": [
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 255.06314,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring <em>with</em> <em>CodeStream</em>",
        "sections": "Performance monitoring <em>with</em> <em>CodeStream</em>",
        "body": " connected, all of <em>your</em> collaboration <em>work</em> in <em>CodeStream</em> (such as the discussion, assignee, and error status) syncs with <em>New</em> Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on <em>code</em> in their IDEs, a DevOps engineer assigning errors"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.13235,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to <em>New</em> Relic <em>CodeStream</em>",
        "sections": "Intro to <em>New</em> Relic <em>CodeStream</em>",
        "body": "<em>New</em> Relic <em>CodeStream</em> is a developer collaboration platform that enables <em>your</em> development team to discuss and review <em>code</em> in a natural and contextual way. <em>CodeStream</em> not only makes <em>your</em> discussions easier, by allowing them to happen in context in <em>your</em> IDE, but it also preserves the institutional"
      },
      "id": "617440e3e7b9d2836c13c43c"
    },
    {
      "image": "https://docs.newrelic.com/static/30e00c292c5aa1c5d702d67be5021a45/f96db/CreateAnAccount6.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/sign-up-codestream/",
      "sections": [
        "Sign up for CodeStream",
        "Create an account",
        "CodeStream organizations",
        "Create or join an organization",
        "Invite your team"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Sign up for CodeStream",
      "updated_at": "2021-11-24T09:42:56Z",
      "type": "docs",
      "external_id": "2f8eda03703523f62844512a3b8ef005b624ebc2",
      "document_type": "page",
      "popularity": 1,
      "body": "To get the most out of New Relic CodeStream's collaboration tools, create an organization and then invite your team members to it. If you haven't already, sign up for a free New Relic account to jump from your application's stack trace errors in New Relic One directly to the line of code responsible in your IDE. Create an account If you already have the CodeStream extension installed in your IDE, you can start the sign up process from the CodeStream pane. There are two ways to sign up for CodeStream. You can sign up with a set of CodeStream credentials (such as, email address and password). Alternatively, you can sign up using your GitHub, GitLab, or Bitbucket (cloud versions only) account. Signing up via your code host also connects your repositories to CodeStream. If the email address you're using with your code host isn't your work email, you should create CodeStream-specific credentials instead. If you sign up via CodeStream, your next step will be to confirm your email address by entering a code sent to you via email. You can paste the code into any of the boxes rather than typing each number individually. CodeStream organizations A CodeStream organization is where you and your teammates will discuss code. Similar to a Slack workspace, all of the developers in your company are in the same CodeStream organization. CodeStream's activity feed keeps things relevant for you by showing activity related to the code you have open in your IDE. The discussions about code build up a knowledge base that is a company-wide resource, so the only reason to have multiple organizations on CodeStream is if you truly need separation. For example, you might have an organization for your day job and another for an open-source project you work on. Or maybe you're a consultant that's a member of different CodeStream organizations for each of your clients. Create or join an organization If you're invited to join an organization on CodeStream, sign up with the same email address the invitation was sent to. You'll automatically be added to that organization. If you're the first person from your company to sign up for CodeStream you can go ahead and create a new organization. Otherwise, there may be existing CodeStream organizations available for you to join based on your email domain. If you think your company is already on CodeStream, but don't see an organization to join, make sure that you've signed up with your work email. If you decide to create an organization you'll be asked to give it a name and, if you signed up with your work email address, anyone else with that email domain can join the organization. Invite your team Collaboration is a team sport so invite your teammates to join you on CodeStream. CodeStream will offer up some suggestions based on the commit history of the repositories you have open in your IDE. Now you're ready to start using CodeStream.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 185.89076,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Sign up for <em>CodeStream</em>",
        "sections": "Sign up for <em>CodeStream</em>",
        "body": "To get the most out of <em>New</em> Relic <em>CodeStream</em>&#x27;s collaboration tools, create an organization and then invite <em>your</em> team members to it. If you haven&#x27;t already, sign up for a free <em>New</em> Relic account to jump from <em>your</em> application&#x27;s stack trace errors in <em>New</em> Relic One directly to the line of <em>code</em>"
      },
      "id": "617440e3196a6782592f011c"
    }
  ],
  "/docs/codestream/start-here/codestream-new-relic": [
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1625.8469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Discover errors on <em>New</em> <em>Relic</em> <em>One</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, <em>New</em> <em>Relic</em> <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on <em>New</em> <em>Relic</em> <em>One</em> Once you’ve connected <em>CodeStream</em> to your <em>New</em> <em>Relic</em> <em>One</em> account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/bda6e10caadb978a13dfcbb98b4f79bb/f96db/ObservabilitySection-connect1.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/observability-section/",
      "sections": [
        "CodeStream observability",
        "Connect CodeStream and New Relic One",
        "See your errors in CodeStream",
        "Tip"
      ],
      "published_at": "2022-01-12T05:41:21Z",
      "title": "CodeStream observability",
      "updated_at": "2021-11-13T21:12:48Z",
      "type": "docs",
      "external_id": "15085b32c3ff3c5e4a469b6ef9151cdb8624b25a",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to get the most out of New Relic CodeStream, connect CodeStream to New Relic One via your New Relic user key. Once that's done, for entities you're monitoring with New Relic One, you'll see your errors directly in CodeStream. Connect CodeStream and New Relic One Before you can start seeing errors in your IDE and take advantage of other New Relic and CodeStream features, you'll need to enter your New Relic user key. Go here to get or create your New Relic user key. Once you have your user key, in Observability click Connect to New Relic One, then paste your user key and click Connect. See your errors in CodeStream Once you've connected New Relic to CodeStream, you'll see observed errors directly in CodeStream. These sections help you and your team manage and see your errors in different ways: Errors assigned to me: If an error has been assigned to you, you'll see it here. Recent errors in: Each repository you have open in your IDE will have its own grouping of errors. If your repository URL is mapped to more than one entity you're observing in New Relic, a dropdown lets you filter by entity. Select entity from New Relic: Use this to connect a repository in your IDE with an entity you're observing with New Relic. Tip If your project isn't monitored by New Relic, you can use CodeStream to get that started. In the CodeStream extension, in the Observability section, click the gear icon, and then click Instrument my App. Follow the instructions to instrument your code.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1447.8713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> observability",
        "sections": "Connect <em>CodeStream</em> <em>and</em> <em>New</em> <em>Relic</em> <em>One</em>",
        "body": "In order to get the most out of <em>New</em> <em>Relic</em> <em>CodeStream</em>, connect <em>CodeStream</em> to <em>New</em> <em>Relic</em> <em>One</em> via your <em>New</em> <em>Relic</em> user key. Once that&#x27;s done, for entities you&#x27;re monitoring with <em>New</em> <em>Relic</em> <em>One</em>, you&#x27;ll see your errors directly in <em>CodeStream</em>. Connect <em>CodeStream</em> and <em>New</em> <em>Relic</em> <em>One</em> Before you can start seeing"
      },
      "id": "61743f8be7b9d2b9d113c7cc"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1147.0834,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to <em>New</em> <em>Relic</em> <em>CodeStream</em>",
        "sections": "Intro to <em>New</em> <em>Relic</em> <em>CodeStream</em>",
        "body": ") If you haven&#x27;t already, sign up for a free <em>New</em> <em>Relic</em> account so that you can get the most out of <em>New</em> <em>Relic</em> <em>CodeStream</em>. Preview release <em>CodeStream</em>&#x27;s integration with <em>New</em> <em>Relic</em> <em>One</em> is a preview release limited to <em>New</em> <em>Relic</em> <em>One</em> accounts on our US data center, and your use is subject to the pre-release"
      },
      "id": "617440e3e7b9d2836c13c43c"
    }
  ],
  "/docs/codestream/start-here/codestream-user-guide": [
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.60477,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, <em>New</em> <em>Relic</em> <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on <em>New</em> <em>Relic</em> One Once you’ve connected <em>CodeStream</em> to your <em>New</em> <em>Relic</em> One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/89fc796cef01d95170eace2254590fbe/1efb2/connect-repo1.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-new-relic/",
      "sections": [
        "CodeStream and New Relic One",
        "Preview release",
        "Connect CodeStream and New Relic",
        "Caution",
        "See your errors and what's causing them",
        "Tip",
        "How to go from errors inbox to your IDE",
        "APM errors and CodeStream",
        "Associate repositories with errors",
        "Use environment variable with APM (recommended)",
        "Use the UI",
        "Use the NerdGraph API",
        "Associate build SHAs or release tags with errors",
        "Install APM agents with CodeStream",
        "Dynamic logging with Go and Pixie"
      ],
      "published_at": "2022-01-12T05:42:46Z",
      "title": "CodeStream and New Relic One",
      "updated_at": "2021-12-14T23:24:49Z",
      "type": "docs",
      "external_id": "a6c04d95011d9150cb0798580c15695bb9f3bbda",
      "document_type": "page",
      "popularity": 1,
      "body": "CodeStream and New Relic One work together to give you insight into your code's errors, as well as making it easier to get started instrumenting your code with our APM agents. With CodeStream connected to New Relic One, you can jump from a stack trace error directly to the offending line of code in your IDE. Once in your IDE, you can navigate the stack trace and collaborate with your teammates to resolve the issue. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Connect CodeStream and New Relic Before you can take advantage of New Relic's observability features in CodeStream, you'll need to connect them. Requirements for connecting CodeStream and New Relic: New Relic account (If you don't have a New Relic account, sign up at newrelic.com/signup. It's free, forever!) New Relic user key If you don't have a user key or want to learn more about how you can use and manage them, see our doc on the New Relic user key. Once you have your New Relic user key, in CodeStream's Observability section click Connect to New Relic One, then paste your API key and click Connect to New Relic One. Caution New Relic users can share stacktrace errors on CodeStream. Once you've connected CodeStream to New Relic, any new users you add to your CodeStream organization can see those errors. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Requirements for opening stack trace errors in your IDE: New Relic account (If you don't have a New Relic account, sign up at newrelic.com/signup. It's free, forever!) New Relic user key CodeStream and New Relic connection Data being reported to New Relic via APM monitoring A workload for errors inbox An error For APM errors, your repository's commit hash and release tag New Relic errors inbox is a single place to proactively detect, prioritize, and take action on your errors before they impact customers. With CodeStream, you can jump from an error directly to the offending code in your IDE. Tip Limited to APM errors. How to go from errors inbox to your IDE From one.newrelic.com/, go to Errors Inbox, click a stack trace error, then click Open in IDE. APM errors and CodeStream In order to view stack trace errors in your IDE, CodeStream needs to know what repository the error is associated with and, ideally, which version of the code generated the error. Associate repositories with errors Once you've started monitoring for APM, mobile, browser, or Lambda, you should create repository entities and associate them with entities for all of your services. In order to create a repository entity you'll need to provide the repository's remote URL. For example, the remote URL can be in either the SSH or HTTPS format: git@github.com:newrelic/beta-docs-site.git https://github.com/newrelic/beta-docs-site.git Caution It's possible to add the same GitHub repository more than once, if you're using different protocols to do so. The UI warns you about this, but won't prevent you from doing so. For example, https://github.com/tuna/repo and git@github.com:tuna/repo are the same repo, with different protocols. If you try to open an error in your IDE and there isn't an associated repository, CodeStream will prompt you to make an association and save that association for all errors from the given entity on New Relic. However, it would be preferably to use one of the following methods since they require less ongoing manual effort and eliminate the possibility of end-user mistakes, such as misconfigured remote URLs. Use environment variable with APM (recommended) Set the environment variable NEW_RELIC_METADATA_REPOSITORY_URL. New Relic APM agents create the repository entity and associate it to your application entity automatically. This requires the SSH or HTTPS URL format. We recommend that these be set as part of your build pipeline. Use the UI Once you've started sending data to New Relic, use the UI to connect your related repository. Go to the APM Summary page via one.newrelic.com > Explorer > Services - APM > (select an app), then look for the Repositories section at the bottom-right. Click Connect repository to find an existing repository or add a new one. Use the NerdGraph API Use New Relic's NerdGraph APIsto create a repository and associate it with your application entities. Step 1: Create a repository entity To create a repository entity, use the referenceEntityCreateOrUpdateRepository API and make sure to save the GUID that's produced. The API takes the following parameters: accountId - the integer account ID for the account you want to add the repository to url - example https://github.com/newrelic/beta-docs-site.git name - example: newrelic/beta-docs-site mutation { referenceEntityCreateOrUpdateRepository(repositories: [{accountId: [YOUR_ACCOUNT_ID], name: \"[REPO_NAME]\", url: \"[REPO_URL]\"}]) { created failures { guid message type } } } Copy In order to find the entity you create, you can use a query like the following. Note that the URL you provided to referenceEntityCreateOrUpdateRepository gets saved as an entity tag. { actor { entitySearch(query: \"name = 'a name' OR tags.url = 'a url'\") { count query results { entities { guid name tags { key values } } } } } } Copy Step 2: Associate the repository entity to your application entity First, find the GUID for the application you want to associate your repository to. Parameters: sourceEntityGuid - the entity GUID of the application targetEntityGuid - the entity GUID of your repository type - always BUILT_FROM mutation { entityRelationshipUserDefinedCreateOrReplace(sourceEntityGuid: \"\", targetEntityGuid: \"\", type: BUILT_FROM) { errors { message type } } } Copy To see all entities related to your repository you can do a query like this: { actor { entity(guid: \"[YOUR_REPOSITORY_GUID]]\") { relatedEntities(filter: {direction: BOTH, relationshipTypes: {include: BUILT_FROM}}) { results { target { entity { name guid type } } type } } name type tags { values key } } } } Copy Step 3: Cleanup (if needed) Delete a repository with the following query: mutation DeleteRepository { entityDelete(guids: \"[ENTITY_GUID_HERE]]\") { deletedEntities failures { message guid } } } Copy Associate build SHAs or release tags with errors To use CodeStream's Open in IDE with your APM stack trace errors, use environment variables to configure your APM agent with your application's commit sha and/or your release tag associated with the running version of your software. CodeStream only needs the first seven characters of your commit sha (for example, 734713b) to make this connection, but you can include the entire sha. Alternately, you can use a release tag (such as v0.1.209 or release-209) for CodeStream to find the correct version of your code. For New Relic APM, the commit and/or release tag (tags.commit and tags.releaseTag) are added as attributes on Transaction and TransactionError events. You can use APM environment variables to set these attributes. We recommend setting one or both of these variables as part of your build pipeline. NEW_RELIC_METADATA_COMMIT - The commit sha. You can include the whole thing or only the first seven characters. NEW_RELIC_METADATA_RELEASE_TAG - A release tag (such as v0.1.209 or release-209). This has the advantage of being human readable. For more on how to set these variables, here are specific configuration details for each language: Go Java .NET Node.js PHP Python Install APM agents with CodeStream Requirements for installing New Relic APM agents via CodeStream: New Relic account New Relic user key CodeStream and New Relic connection A supported language application codebase .NET Core Java Node.JS When you first connect CodeStream to New Relic, if you're working on an application's codebase that's not being monitored by New Relic, CodeStream will offer to instrument that application for you. Like New Relic's guided install, CodeStream will walk you through and automate all of the steps to installing the APM agent to start sending data to New Relic. This check only happens automatically when the initial connection is made. To do so later, in the CodeStream extension, click your username, then click New Relic Setup. Dynamic logging with Go and Pixie New Relic account New Relic user API key CodeStream and New Relic connection A Kubernetes cluster monitored by Pixie An application written in Go You can use CodeStream to enable dynamic logging for your Pixie-monitored Go applications. Just right-click on any method name and select Dynamic Logging Using Pixie.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.5964,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> and <em>New</em> <em>Relic</em> One",
        "sections": "<em>CodeStream</em> and <em>New</em> <em>Relic</em> One",
        "body": " have a <em>New</em> <em>Relic</em> account, sign up at newrelic.com&#x2F;signup. It&#x27;s free, forever!) <em>New</em> <em>Relic</em> <em>user</em> key If you don&#x27;t have a <em>user</em> key or want to learn more about how you can use and manage them, see our doc on the <em>New</em> <em>Relic</em> <em>user</em> key. Once you have your <em>New</em> <em>Relic</em> <em>user</em> key, in <em>CodeStream</em>&#x27;s Observability"
      },
      "id": "6171e652196a67e9c92f0156"
    },
    {
      "image": "https://docs.newrelic.com/static/5c1d085b14abf961ca66b96285f0c0fa/69902/QS-Integrations.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/install-codestream/",
      "sections": [
        "Install New Relic CodeStream",
        "Install CodeStream",
        "Instant Observability (I/O) quickstart",
        "Visual Studio Code",
        "Visual Studio",
        "JetBrains",
        "Connect your tools",
        "Tip",
        "Discuss any block of code, at any time",
        "Get feedback on your work in progress",
        "Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T06:15:45Z",
      "title": "Install New Relic CodeStream",
      "updated_at": "2021-11-24T09:39:10Z",
      "type": "docs",
      "external_id": "5d431c8f9a2690b64d26ac9fc173b18085153aac",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that makes it easy to discuss and review code in a more natural and contextual way. Once connected to New Relic, collaborate on your application errors directly in your IDE. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Install CodeStream You can install CodeStream for your specific IDE or install it through our Instant Observability (I/O) quickstart. Instant Observability (I/O) quickstart Install CodeStream with its Instant Observability (I/O) quickstart to connect CodeStream to your New Relic account via your user key. Visual Studio Code Download and install CodeStream for Visual Studio Code. You can also install it directly in Visual Studio Code via the extensions marketplace. Visual Studio Download and install CodeStream for Visual Studio. You can also install it directly in Visual Studio via the extensions marketplace. JetBrains Download and install CodeStream for JetBrains. You can also install it from the JetBrains plugins menu. Connect your tools Create and review pull requests on GitHub, GitLab, or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Investigate errors reported to New Relic One. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click your headshot at the top of the CodeStream pane, then click Integrations to connect all of your tools to CodeStream. Tip Once you've installed CodeStream, to connect to New Relic, you'll need your New Relic user key. Go here to learn more about finding or creating your user key. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, select the code and click the comment button to ask your question. Learn more about discussing code. Get feedback on your work in progress Click the + menu then click Request Feedback at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. Create or review a pull request In the CodeStream sidebar, look for the Pull Requests section to review an open pull request. Select a pull request (or load one from URL) to get a complete GitHub experience right in your IDE. You can create a pull request in GitHub, GitLab, or Bitbucket, but support for reviewing pull requests is currently only available for GitHub. Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.01093,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>New</em> <em>Relic</em> <em>CodeStream</em>",
        "sections": "Install <em>New</em> <em>Relic</em> <em>CodeStream</em>",
        "body": " to connect <em>CodeStream</em> to your <em>New</em> <em>Relic</em> account via your <em>user</em> key. Visual Studio <em>Code</em> Download and install <em>CodeStream</em> for Visual Studio <em>Code</em>. You can also install it directly in Visual Studio <em>Code</em> via the extensions marketplace. Visual Studio Download and install <em>CodeStream</em> for Visual Studio. You can also"
      },
      "id": "6174400564441ff1025fd832"
    }
  ],
  "/docs/codestream/start-here/install-codestream": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.6436,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Introduction to Android monitoring",
        "Install the Android agent",
        "Extend your instrumentation",
        "See your errors in CodeStream"
      ],
      "title": "Introduction to Android monitoring",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile Android",
        "Get started"
      ],
      "external_id": "ae1aceb4e03cd9acadc71fa9fedf674a3f8cc3cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-android/get-started/introduction-new-relic-mobile-android/",
      "published_at": "2022-01-12T02:08:38Z",
      "updated_at": "2021-10-23T01:47:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Mobile monitoring for Android monitors your mobile app, giving you a comprehensive view of your app's performance. It works for Android apps written using Java or Kotlin. Install the Android agent Before you install the Android agent, make sure your app follows the compatibility requirements. As part of the installation process, mobile monitoring automatically generates an application token. This is a 40-character hexadecimal string for authenticating each mobile app that you monitor. Follow the Android installation and configuration procedures for your environment as applicable. If you have problems with your Android installation, or if you do not see data in the mobile monitoring UI for your Android app, follow the troubleshooting procedures. Extend your instrumentation After you install the agent, extend the agent's instrumentation by using the mobile monitoring UI and following up on information in New Relic Insights. To access: In mobile monitoring: In NRQL and dashboards: Custom data Create and record custom events, interaction traces, and attributes to add details to your existing data and traces. Then, view the custom events that you created in NRQL or dashboards. Network requests Enable the MobileRequest event feature so you can perform a full network analysis. To further investigate network request error rates and response times, query MobileRequest and MobileRequestError events. Crash analysis Review detailed information using groups and filters to analyze trends that lead to crashes. To view more information about crashes, create NRQL queries to review Insights charts related to crash data. Android SDK API Use the Android SDK API for mobile monitoring to instrument parts of your code that are not instrumented by default. Then, view those custom events and attributes in New Relic Insights. Handled exceptions Report exceptions so you can identify factors creating a poor mobile app experience. To further improve performance, review MobileHandledException event records in New Relic Insights. Breadcrumbs Boost the level of detail in crash event trails by adding breadcrumbs. Then, query MobileBreadcrumbs events to see all breadcrumbs or just breadcrumbs related to crashes. See your errors in CodeStream You can also see your Android application's directly in your IDE using CodeStream and errors inbox. To do this, install CodeStream, connect CodeStream and New Relic and create Git tags that match your appVersion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.82962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Install</em> the Android agent",
        "tags": "<em>New</em> <em>Relic</em> Mobile Android",
        "body": " by adding breadcrumbs. Then, query MobileBreadcrumbs events to see all breadcrumbs or just breadcrumbs related to crashes. See your errors in <em>CodeStream</em> You can also see your Android application&#x27;s directly in your IDE using <em>CodeStream</em> and errors inbox. To do this, <em>install</em> <em>CodeStream</em>, connect <em>CodeStream</em> and <em>New</em> <em>Relic</em> and create Git tags that match your appVersion."
      },
      "id": "6043a48f196a6784e6960f6d"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2022-01-12T04:33:51Z",
      "updated_at": "2022-01-12T04:33:51Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the infrastructure agent and waited a few minutes, but no data appears in the infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Query builder, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.61545,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Verify <em>install</em> for apt (Debian or Ubuntu)",
        "body": " prompt or Powershell to verify that the agent directory exists: dir &quot;C:\\Program Files\\<em>New</em> <em>Relic</em>\\newrelic-infra&quot; Copy If you receive a File not found error, see <em>Install</em> for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    }
  ],
  "/docs/codestream/start-here/sign-up-codestream": [
    {
      "image": "https://docs.newrelic.com/static/490255cdad35ea9f73a5ec0877f086e6/f0991/MyOrgMenu.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/my-organization/",
      "sections": [
        "Manage your organization on CodeStream",
        "The organization menu",
        "Invite your teammates",
        "Blame map"
      ],
      "published_at": "2022-01-12T08:20:38Z",
      "title": "Manage your organization on CodeStream",
      "updated_at": "2021-11-13T21:12:13Z",
      "type": "docs",
      "external_id": "c781fd39a475318eb539553700ed1f1fe3bdf18c",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream provides a number of tools for managing your organization. The organization menu In the header, click the My Organization menu button to see who's in your New Relic CodeStream organization, invite new members, and create blame maps. Admins are identified in the teammates list. If you're an admin, there's a dropdown to assign or remove admin privileges to any member. Invite your teammates Click Invite Teammates to invite new members to your organization. The outstanding invitations section lists all open invitations. The right side of each row has links to remove the invitation or to resend the invite. Click reinvite to send another invitation via email. You can also hover over the reinvite link for the option to generate an email yourself. The suggested teammates section, only available for admins, is a list of possible teammates derived from the commit history of your open repositories. At the right side of each row are links to remove the suggestion from the list or to invite the person. Blame map Click Blame Map to add email addresses that you use for committing code that may be different from the email address you used to sign up for CodeStream. For example, your CodeStream email address might be dave@acme.com, but you might also commit code as dave@webmail.com. Click Add mapping, enter your Git email address, and then select your entry from the list of organization members. That way, when someone comments on code committed by dave@webmail.com, CodeStream will know to at-mention you (such as, dave@acme.com). While non-admins can only create blame maps for themselves, admins can create blame maps for any member of the organization. This is useful for reassigning code ownership when people leave the organization.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 902.49225,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage your organization on <em>CodeStream</em>",
        "sections": "Manage your organization on <em>CodeStream</em>",
        "body": " of your open repositories. At the right side of each row are links to remove the suggestion from the list or to invite the person. Blame map Click Blame Map to add email addresses that you use for committing <em>code</em> that may be different from the email address you used to <em>sign</em> <em>up</em> for <em>CodeStream</em>"
      },
      "id": "617f403c28ccbcd75f8003ea"
    },
    {
      "image": "https://docs.newrelic.com/static/8945e0a9c512b8638ebf8165d47aee04/69902/QS-SignUp3.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-user-guide/",
      "sections": [
        "New Relic CodeStream user guide",
        "Jump to a topic",
        "1. Install the CodeStream extension in your IDE and sign up.",
        "2. Connect your tools",
        "3. Discuss any block of code, at any time",
        "4. Get feedback on your work in progress",
        "5. Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "New Relic CodeStream user guide",
      "updated_at": "2021-11-24T04:44:31Z",
      "type": "docs",
      "external_id": "fa9af0118a8872fea89fda91482c44fb69913ea2",
      "document_type": "page",
      "popularity": 1,
      "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic CodeStream. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. 1. Install the CodeStream extension in your IDE and sign up. Install CodeStream for VS Code, Visual Studio or JetBrains. The CodeStream pane automatically appears in the sidebar for VS Code or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you're the first person from your team to join CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on GitHub, GitLab or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click on your headshot at the top of the CodeStream pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, just select the code and ask your question. Learn more about discussing code. 4. Get feedback on your work in progress Select Request Feedback from the + menu at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code!) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. 5. Create or review a pull request Look for the Pull Requests section of the CodeStream sidebar to review an open pull request. Just click on a pull request (or load one from URL) to get a complete GitHub experience right in your IDE! Note that you can create a pull request in GitHub, GitLab or Bitbucket, but support for reviewing pull requests is currently only available for GitHub (cloud or Enterprise). Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 866.13965,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic <em>CodeStream</em> user guide",
        "sections": "1. Install the <em>CodeStream</em> extension in your IDE and <em>sign</em> <em>up</em>.",
        "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic <em>CodeStream</em>. If you haven&#x27;t already, <em>sign</em> <em>up</em> for a free New Relic account so that you can get the most out of New Relic <em>CodeStream</em>. 1. Install the <em>CodeStream</em> extension"
      },
      "id": "61744137e7b9d2428b13c6a0"
    },
    {
      "image": "https://docs.newrelic.com/static/0d940de4d107acece21b6e518f1a2c53/d10fb/OrganizationSettings.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-settings/team-administration/",
      "sections": [
        "CodeStream organization settings and administration",
        "Manage people and roles",
        "Use blame map to reassign code",
        "Organization settings",
        "Onboarding settings",
        "Feedback request assignment and approval",
        "Change your organization's name",
        "Export your data"
      ],
      "published_at": "2022-01-12T07:46:11Z",
      "title": "CodeStream organization settings and administration",
      "updated_at": "2021-11-13T21:06:47Z",
      "type": "docs",
      "external_id": "643e62ff1fc920ca94e3bcd2e27e3a9be4317515",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream provides several tools for managing the teammates in your organization, whether they have accounts with New Relic One or not. Manage people and roles Click My Organization menu at the top of the CodeStream pane to invite people to your organization and assign or remove admin privileges. Use blame map to reassign code Click Blame Map to define code ownership with your organization. By default, when you comment on code, CodeStream mentions (or offers to email) the author or authors of the code you're commenting on. If that person has left the company it might not be the right thing to do. Non-admins are able to set up blame maps for themselves to handle situations where the email address they use to commit code is different than the one they used to sign up for CodeStream. Organization settings If you're an organization admin, look for the Organization Admin menu under the headshot menu at the top of the CodeStream pane. Onboarding settings Domain-based joining allows anyone with email addresses on the specified domains to join your CodeStream organization without being first invited. Not only does this make it very easy to get your teammates on board, but it ensures that they'll be part of your organization (as opposed to accidentally creating their own). Feedback request assignment and approval Admins can control how both feedback request assignments and approvals work for your organization. By default, the person requesting feedback can decide how approvals work, but you can, instead, set a default behavior for all feedback requests for the organization. Any reviewer can approve: Anyone can approve the feedback request, regardless of how many reviewers are assigned. All reviewers must approve individually: Each assigned reviewer must individually approve the feedback request before it’s considered approved. You can also decide if and how CodeStream suggests reviewers. Round-robin will cycle through all developers in the organization. Random will randomly assign the feedback request to any developer in the organization. The Authorship options will suggest up to three reviewers based on the developers who wrote the lines of code impacted by the changes, as well as other developers who may have committed to the branch. Change your organization's name Update the name of your CodeStream organization at any time. Export your data There's a lightweight export tool for getting your organization's discussions out of CodeStream. Click the icon to copy all of your data to the clipboard so that you can paste it elsewhere.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 833.3177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> organization settings and administration",
        "sections": "<em>CodeStream</em> organization settings and administration",
        "body": " not be the right thing to do. Non-admins are able to set <em>up</em> blame maps for themselves to handle situations where the email address they use to commit <em>code</em> is different than the one they used to <em>sign</em> <em>up</em> for <em>CodeStream</em>. Organization settings If you&#x27;re an organization admin, look for the Organization Admin menu"
      },
      "id": "61743ee264441ff1025fd5b5"
    }
  ],
  "/docs/codestream/start-here/what-is-codestream": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 372.34402,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.60464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, <em>New</em> <em>Relic</em> <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on <em>New</em> <em>Relic</em> One Once you’ve connected <em>CodeStream</em> to your <em>New</em> <em>Relic</em> One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/89fc796cef01d95170eace2254590fbe/1efb2/connect-repo1.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-new-relic/",
      "sections": [
        "CodeStream and New Relic One",
        "Preview release",
        "Connect CodeStream and New Relic",
        "Caution",
        "See your errors and what's causing them",
        "Tip",
        "How to go from errors inbox to your IDE",
        "APM errors and CodeStream",
        "Associate repositories with errors",
        "Use environment variable with APM (recommended)",
        "Use the UI",
        "Use the NerdGraph API",
        "Associate build SHAs or release tags with errors",
        "Install APM agents with CodeStream",
        "Dynamic logging with Go and Pixie"
      ],
      "published_at": "2022-01-12T05:42:46Z",
      "title": "CodeStream and New Relic One",
      "updated_at": "2021-12-14T23:24:49Z",
      "type": "docs",
      "external_id": "a6c04d95011d9150cb0798580c15695bb9f3bbda",
      "document_type": "page",
      "popularity": 1,
      "body": "CodeStream and New Relic One work together to give you insight into your code's errors, as well as making it easier to get started instrumenting your code with our APM agents. With CodeStream connected to New Relic One, you can jump from a stack trace error directly to the offending line of code in your IDE. Once in your IDE, you can navigate the stack trace and collaborate with your teammates to resolve the issue. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Connect CodeStream and New Relic Before you can take advantage of New Relic's observability features in CodeStream, you'll need to connect them. Requirements for connecting CodeStream and New Relic: New Relic account (If you don't have a New Relic account, sign up at newrelic.com/signup. It's free, forever!) New Relic user key If you don't have a user key or want to learn more about how you can use and manage them, see our doc on the New Relic user key. Once you have your New Relic user key, in CodeStream's Observability section click Connect to New Relic One, then paste your API key and click Connect to New Relic One. Caution New Relic users can share stacktrace errors on CodeStream. Once you've connected CodeStream to New Relic, any new users you add to your CodeStream organization can see those errors. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Requirements for opening stack trace errors in your IDE: New Relic account (If you don't have a New Relic account, sign up at newrelic.com/signup. It's free, forever!) New Relic user key CodeStream and New Relic connection Data being reported to New Relic via APM monitoring A workload for errors inbox An error For APM errors, your repository's commit hash and release tag New Relic errors inbox is a single place to proactively detect, prioritize, and take action on your errors before they impact customers. With CodeStream, you can jump from an error directly to the offending code in your IDE. Tip Limited to APM errors. How to go from errors inbox to your IDE From one.newrelic.com/, go to Errors Inbox, click a stack trace error, then click Open in IDE. APM errors and CodeStream In order to view stack trace errors in your IDE, CodeStream needs to know what repository the error is associated with and, ideally, which version of the code generated the error. Associate repositories with errors Once you've started monitoring for APM, mobile, browser, or Lambda, you should create repository entities and associate them with entities for all of your services. In order to create a repository entity you'll need to provide the repository's remote URL. For example, the remote URL can be in either the SSH or HTTPS format: git@github.com:newrelic/beta-docs-site.git https://github.com/newrelic/beta-docs-site.git Caution It's possible to add the same GitHub repository more than once, if you're using different protocols to do so. The UI warns you about this, but won't prevent you from doing so. For example, https://github.com/tuna/repo and git@github.com:tuna/repo are the same repo, with different protocols. If you try to open an error in your IDE and there isn't an associated repository, CodeStream will prompt you to make an association and save that association for all errors from the given entity on New Relic. However, it would be preferably to use one of the following methods since they require less ongoing manual effort and eliminate the possibility of end-user mistakes, such as misconfigured remote URLs. Use environment variable with APM (recommended) Set the environment variable NEW_RELIC_METADATA_REPOSITORY_URL. New Relic APM agents create the repository entity and associate it to your application entity automatically. This requires the SSH or HTTPS URL format. We recommend that these be set as part of your build pipeline. Use the UI Once you've started sending data to New Relic, use the UI to connect your related repository. Go to the APM Summary page via one.newrelic.com > Explorer > Services - APM > (select an app), then look for the Repositories section at the bottom-right. Click Connect repository to find an existing repository or add a new one. Use the NerdGraph API Use New Relic's NerdGraph APIsto create a repository and associate it with your application entities. Step 1: Create a repository entity To create a repository entity, use the referenceEntityCreateOrUpdateRepository API and make sure to save the GUID that's produced. The API takes the following parameters: accountId - the integer account ID for the account you want to add the repository to url - example https://github.com/newrelic/beta-docs-site.git name - example: newrelic/beta-docs-site mutation { referenceEntityCreateOrUpdateRepository(repositories: [{accountId: [YOUR_ACCOUNT_ID], name: \"[REPO_NAME]\", url: \"[REPO_URL]\"}]) { created failures { guid message type } } } Copy In order to find the entity you create, you can use a query like the following. Note that the URL you provided to referenceEntityCreateOrUpdateRepository gets saved as an entity tag. { actor { entitySearch(query: \"name = 'a name' OR tags.url = 'a url'\") { count query results { entities { guid name tags { key values } } } } } } Copy Step 2: Associate the repository entity to your application entity First, find the GUID for the application you want to associate your repository to. Parameters: sourceEntityGuid - the entity GUID of the application targetEntityGuid - the entity GUID of your repository type - always BUILT_FROM mutation { entityRelationshipUserDefinedCreateOrReplace(sourceEntityGuid: \"\", targetEntityGuid: \"\", type: BUILT_FROM) { errors { message type } } } Copy To see all entities related to your repository you can do a query like this: { actor { entity(guid: \"[YOUR_REPOSITORY_GUID]]\") { relatedEntities(filter: {direction: BOTH, relationshipTypes: {include: BUILT_FROM}}) { results { target { entity { name guid type } } type } } name type tags { values key } } } } Copy Step 3: Cleanup (if needed) Delete a repository with the following query: mutation DeleteRepository { entityDelete(guids: \"[ENTITY_GUID_HERE]]\") { deletedEntities failures { message guid } } } Copy Associate build SHAs or release tags with errors To use CodeStream's Open in IDE with your APM stack trace errors, use environment variables to configure your APM agent with your application's commit sha and/or your release tag associated with the running version of your software. CodeStream only needs the first seven characters of your commit sha (for example, 734713b) to make this connection, but you can include the entire sha. Alternately, you can use a release tag (such as v0.1.209 or release-209) for CodeStream to find the correct version of your code. For New Relic APM, the commit and/or release tag (tags.commit and tags.releaseTag) are added as attributes on Transaction and TransactionError events. You can use APM environment variables to set these attributes. We recommend setting one or both of these variables as part of your build pipeline. NEW_RELIC_METADATA_COMMIT - The commit sha. You can include the whole thing or only the first seven characters. NEW_RELIC_METADATA_RELEASE_TAG - A release tag (such as v0.1.209 or release-209). This has the advantage of being human readable. For more on how to set these variables, here are specific configuration details for each language: Go Java .NET Node.js PHP Python Install APM agents with CodeStream Requirements for installing New Relic APM agents via CodeStream: New Relic account New Relic user key CodeStream and New Relic connection A supported language application codebase .NET Core Java Node.JS When you first connect CodeStream to New Relic, if you're working on an application's codebase that's not being monitored by New Relic, CodeStream will offer to instrument that application for you. Like New Relic's guided install, CodeStream will walk you through and automate all of the steps to installing the APM agent to start sending data to New Relic. This check only happens automatically when the initial connection is made. To do so later, in the CodeStream extension, click your username, then click New Relic Setup. Dynamic logging with Go and Pixie New Relic account New Relic user API key CodeStream and New Relic connection A Kubernetes cluster monitored by Pixie An application written in Go You can use CodeStream to enable dynamic logging for your Pixie-monitored Go applications. Just right-click on any method name and select Dynamic Logging Using Pixie.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.86337,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> and <em>New</em> <em>Relic</em> One",
        "sections": "<em>CodeStream</em> and <em>New</em> <em>Relic</em> One",
        "body": "<em>CodeStream</em> and <em>New</em> <em>Relic</em> One work together to give you insight into your <em>code</em>&#x27;s errors, as well as making it easier to get started instrumenting your <em>code</em> with our APM agents. With <em>CodeStream</em> connected to <em>New</em> <em>Relic</em> One, you can jump from a stack trace error directly to the offending line of <em>code</em>"
      },
      "id": "6171e652196a67e9c92f0156"
    }
  ],
  "/docs/codestream/troubleshooting/client-logs": [
    {
      "image": "https://docs.newrelic.com/static/b437d8747d80db2b7cc2a4ab110a8c70/c1b63/errors-ui.png",
      "url": "https://docs.newrelic.com/docs/errors-inbox/errors-inbox/",
      "sections": [
        "Error tracking with errors inbox",
        "Why it matters",
        "Set up errors inbox",
        "Monitor errors",
        "Error groups",
        "Troubleshooting: similar looking events do not group together",
        "Occurrences",
        "Sort By Filter",
        "Triage errors",
        "Errors status",
        "Error details",
        "Attributes",
        "Activity",
        "Discussions",
        "Assign errors",
        "Important",
        "Connect an inbox to Slack",
        "Connect errors inbox to CodeStream",
        "Connect an inbox to Jira"
      ],
      "published_at": "2022-01-12T05:32:17Z",
      "title": "Error tracking with errors inbox",
      "updated_at": "2022-01-07T01:44:58Z",
      "type": "docs",
      "external_id": "3dbd9bf9bda2abf4f6af60c03dc1f2168dc18f9d",
      "document_type": "page",
      "popularity": 1,
      "body": "Errors inbox is a single place to proactively detect, triage, and take action on all the errors before they impact customers. Receive alerts whenever a critical, customer-impacting error arises via your preferred communication channel, like Slack. Resolve errors faster with errors from across your stack, including all APM, browser (RUM), mobile, and serverless (AWS Lambda) data, displayed on one screen. Errors are grouped to cut down on noise, and collaborating across teams is easy with shared visibility to the same error data. Why it matters Errors inbox provides a unified error tracking experience to detect and triage errors: View and triage issues across applications and services that your team cares about for faster error resolutions. Proactive notifications with detailed error information in Slack. Error profiles to show similarities between error events and surface the root cause by analyzing attributes. Analyze errors in context of the full stack and resolve errors with precision. APM, browser, mobile, and AWS Lambda Functions errors are all captured in the same inbox. Ready to get started? Make sure you have a New Relic account. It's free, forever! Set up errors inbox To enable errors inbox, follow these steps. Afterwards, errors groups should start to appear in your inbox. From one.newrelic.com select Errors inbox from the top nav. If this is your first time accessing errors inbox, you will be prompted to select a workload in the top left. If you have no workloads set up, you will be prompted to create one before you can use errors inbox. Once you select your workload, your inbox should populate with error groups. one.newrelic.com > More > Errors inbox Monitor errors Once you've set up your errors inbox, you can begin proactively monitoring all errors in your stack: Error groups Error groups are sets of events that make up a unique error. Error groups are stored long term and contain metrics, activity log, discussions, and basic information about the unique error. Error groups are tied to the entity, so making a change to the state of an error group in one errors inbox will impact all other inboxes that contain that entity. How error groups work Error events get grouped into an error group when they share the same fingerprint. As events are ingested by New Relic, we run the events through a set of managed rules that output a fingerprint. Every unique fingerprint has a single error group associated with it. The New Relic managed rules normalize the error data, identifying and ignoring unique values such as UUIDs, hex values, email addresses, etc. that would cause grouping “like” errors into unique groups. NR account ID, entity ID, error class, error message, stack trace and exception are all data that can impact a fingerprint. Troubleshooting: similar looking events do not group together If you see “like” error events grouped into different error groups incorrectly, try removing the unique identifier from the error class or message and store those as attributes instead. This will allow you to more easily facet on the attribute values and reduce the number of error groups. If you have a single application reporting as multiple entities in New Relic (i.e. running in different clusters, cells, etc), you might see duplicate error groups, since our grouping logic looks at account and entity IDs as part of the fingerprinting process. You can consider rolling up the multiple entities into a single entity and including only that rolled up entity as part of your errors inbox. You can also use the feedback tool in NR1 to share error groups that could use improved grouping. We’re continually updating our rules to improve the quality of error groups. Occurrences Your errors inbox displays the total number of occurrences of each error group within the selected timeframe. The corresponding sparkline chart displays the total number of occurrences per day over the selected timeframe as you hover over it. Sort By Filter Using the dropdown in the top right, you can sort the list of grouped errors by the number of occurrences or by the error that was last seen (latest first). Triage errors Errors status Errors inbox enables you to triage error groups directly from the main screen or from the error details page. Triaging helps remove the noise from your errors inbox, and lets you focus on the high impact errors that need attention. You can set one of three statuses, and filter your inbox by status. Unresolved: This is the default status of error groups. Resolved: Setting an error as resolved will hide it from the default inbox view unless filters are updated to include resolved errors. If events matching the error group fingerprint occur after marking an error group as resolved, it will automatically reset the status to Unresolved. This can be useful for identifying regressions. Ignored: Ignored will hide the error group from the inbox view unless filters are updated to include ignored errors, or until you stop ignoring the error group. Error details Clicking on a specific error group takes you to the error details page, where you will find full context of the issue. This context can assist in triaging the error and assigning it to the correct team or individual. Occurrences The Occurrences tab includes details like: Related account Stack trace Logs in context Error attributes Number and frequency of occurrences The detailed view also allows you to view specific errors. In the top right, you can navigate between the first instance of the error, the last, and any instance in-between. Attributes The Attributes tab enables you to quickly find commonalities between the related errors for faster resolution. Click on a specific attribute to open a sidebar with specific details. Activity The Activity tab displays a log of the status changes and user assignments of an error group. Discussions The Discussions tab provides room for detailed and organized collaboration. This is key to looping in collaborators and ensuring the entire team has the same context regardless of where they sit. Discussions includes: Threaded conversations: Reply directly to top level comments to tie replies to specific posts. Comment deletion: Delete comments. The content of the post will be removed unless it is the parent of a thread, in which case the box will remain with the message “Comment deleted by user.” Markdown support: Add styling and links to your comments in Markdown. Assign errors You can assign an error group to anyone. Simply select the user from the assign dropdown menu. You may also assign an error to any email address, even if they aren’t a New Relic user. You can update the filter in errors inbox to show only errors assigned to yourself, or a teammate. Important Currently assigning an error group to a user does not send a notification. Notifications of assignment and changes to error groups will be coming soon. Connect an inbox to Slack When connected to Slack, new and resurfaced error groups will be sent to a Slack channel within seconds of them occurring. This enables your team to quickly identify any new errors or regressions, and resolve them quickly with direct links to the stack trace. This short video shows how it works (1:24 minutes): To connect an inbox to Slack: If your Slack workspace does not have the New Relic app installed, do that first. From an inbox, select the Inbox Settings icon (looks like a gear) in the top right corner. Toggle the Slack button to on if it is off. If no workspaces are available, click the plus button to enable Slack with a one click Slack authentication. Once authenticated, you will be able to select a Workspace and specific Channel to send notifications to. Click Test to ensure messages are being sent to the right channel. Connect errors inbox to CodeStream To use CodeStream's Open in IDE integration with your APM stack trace errors, use environment variables to configure your APM agent with your application's commit sha and/or your release tag associated with the running version of your software. Once set up, you can jump from an error group directly to the offending code in your IDE by clicking the Open in IDE button. Learn more here. Connect an inbox to Jira Connect errors inbox to Jira to easily create tickets for your errors, allowing for faster collaboration and resolution. Jira templates allow you to quickly create a ticket containing error details and links directly to the stack trace and APM for quick access and resolution. We store a link to the the ticket alongside the associated error group for a period of time. If the error occurs again within that period, you can easily access associated tickets. Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. To connect an inbox to Jira: Click on the Jira integration icon on the far right side of the error group you want to connect to Jira. Clicking the Jira integration icon allows you to create a ticket based on a template, or create a template if you don't already have one. If you don’t already have a connection to Jira set up in your account, click Add JIRA Workspace from the dropdown. Fill in all the fields and click Test connection before saving to ensure that your details are correct. Next, set up a template. Templates determine what information will be sent to Jira. Find more information about specific fields here. Errors inbox does not currently support two-way communication with Jira, but you can select this option in case it is supported in the future. Once you have a template, click Send test notification to preview what the ticket looks like in Jira. If the preview looks good, click Update message to save the template. Note that a test notification will create a Jira ticket in your Jira workspace. Now your team can create Jira tickets by clicking the Jira integration icon on the far right side of the error group and selecting a template. Jira settings are associated with the account that owns the error group or entity. If you are using the cross-account errors inbox, you will need to set up a Jira connection multiple times.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.82011,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Error tracking with errors <em>inbox</em>",
        "sections": "Connect errors <em>inbox</em> to <em>CodeStream</em>",
        "body": " page, <em>where</em> you will <em>find</em> full context of the issue. This context <em>can</em> assist in triaging the error and assigning it to the correct team or individual. Occurrences The Occurrences tab includes details like: Related account Stack trace <em>Logs</em> in context Error attributes Number and frequency"
      },
      "id": "6174112928ccbc230ac6a4be"
    },
    {
      "image": "https://docs.newrelic.com/static/490255cdad35ea9f73a5ec0877f086e6/f0991/MyOrgMenu.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/my-organization/",
      "sections": [
        "Manage your organization on CodeStream",
        "The organization menu",
        "Invite your teammates",
        "Blame map"
      ],
      "published_at": "2022-01-12T08:20:38Z",
      "title": "Manage your organization on CodeStream",
      "updated_at": "2021-11-13T21:12:13Z",
      "type": "docs",
      "external_id": "c781fd39a475318eb539553700ed1f1fe3bdf18c",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream provides a number of tools for managing your organization. The organization menu In the header, click the My Organization menu button to see who's in your New Relic CodeStream organization, invite new members, and create blame maps. Admins are identified in the teammates list. If you're an admin, there's a dropdown to assign or remove admin privileges to any member. Invite your teammates Click Invite Teammates to invite new members to your organization. The outstanding invitations section lists all open invitations. The right side of each row has links to remove the invitation or to resend the invite. Click reinvite to send another invitation via email. You can also hover over the reinvite link for the option to generate an email yourself. The suggested teammates section, only available for admins, is a list of possible teammates derived from the commit history of your open repositories. At the right side of each row are links to remove the suggestion from the list or to invite the person. Blame map Click Blame Map to add email addresses that you use for committing code that may be different from the email address you used to sign up for CodeStream. For example, your CodeStream email address might be dave@acme.com, but you might also commit code as dave@webmail.com. Click Add mapping, enter your Git email address, and then select your entry from the list of organization members. That way, when someone comments on code committed by dave@webmail.com, CodeStream will know to at-mention you (such as, dave@acme.com). While non-admins can only create blame maps for themselves, admins can create blame maps for any member of the organization. This is useful for reassigning code ownership when people leave the organization.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.63225,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage your organization on <em>CodeStream</em>",
        "sections": "Manage your organization on <em>CodeStream</em>",
        "body": "New Relic <em>CodeStream</em> provides a number of tools for managing your organization. The organization menu In the header, click the <em>My</em> Organization menu button to see who&#x27;s in your New Relic <em>CodeStream</em> organization, invite new members, and create blame maps. Admins are identified in the teammates list"
      },
      "id": "617f403c28ccbcd75f8003ea"
    },
    {
      "sections": [
        "Python agent configuration",
        "Configuration methods and precedence",
        "Agent configuration file",
        "Tip",
        "Server-side configuration",
        "Important",
        "Environment variables",
        "Per-request configuration",
        "Example: Apache/mod_wsgi app name",
        "newrelic.set_background_task",
        "newrelic.ignore_transaction",
        "newrelic.suppress_apdex_metric",
        "newrelic.suppress_transaction_trace",
        "newrelic.disable_browser_autorum",
        "Multiple environment configuration",
        "General configuration settings",
        "license_key (REQUIRED)",
        "app_name (HIGHLY RECOMMENDED)",
        "monitor_mode",
        "developer_mode",
        "log_file",
        "log_level",
        "high_security",
        "proxy_scheme, proxy_host, proxy_port, proxy_user, proxy_pass",
        "audit_log_file",
        "Caution",
        "labels (tags)",
        "Two tags",
        "process_host.display_name",
        "api_key",
        "ca_bundle_path",
        "apdex_t",
        "Attributes",
        "attributes.enabled",
        "attributes.include",
        "attributes.exclude",
        "Transaction tracer configuration",
        "transaction_tracer.enabled",
        "transaction_tracer.transaction_threshold",
        "transaction_tracer.record_sql",
        "transaction_tracer.stack_trace_threshold",
        "transaction_tracer.explain_enabled",
        "transaction_tracer.explain_threshold",
        "transaction_tracer.attributes.enabled",
        "transaction_tracer.attributes.include",
        "transaction_tracer.attributes.exclude",
        "transaction_tracer.function_trace",
        "Transaction segment configuration",
        "transaction_segments.attributes.enabled",
        "transaction_segments.attributes.include",
        "transaction_segments.attributes.exclude",
        "Error collector configuration",
        "error_collector.enabled",
        "error_collector.ignore_classes",
        "error_collector.ignore_status_codes",
        "error_collector.expected_classes",
        "error_collector.expected_status_codes",
        "error_collector.attributes.enabled",
        "error_collector.attributes.include",
        "error_collector.attributes.exclude",
        "error_collector.capture_events",
        "Browser monitoring settings",
        "browser_monitoring.enabled",
        "browser_monitoring.auto_instrument",
        "browser_monitoring.content_type",
        "Instrument xhtml+xml page responses",
        "browser_monitoring.attributes.enabled",
        "browser_monitoring.attributes.include",
        "browser_monitoring.attributes.exclude",
        "Transaction events settings",
        "transaction_events.enabled",
        "transaction_events.attributes.enabled",
        "transaction_events.attributes.include",
        "transaction_events.attributes.exclude",
        "Custom events settings",
        "custom_insights_events.enabled",
        "Datastore tracer settings",
        "datastore_tracer.instance_reporting.enabled",
        "datastore_tracer.database_name_reporting.enabled",
        "Distributed tracing settings",
        "distributed_tracing.enabled",
        "Span event configuration",
        "span_events.enabled",
        "span_events.attributes.enabled",
        "span_events.attributes.include",
        "span_events.attributes.exclude",
        "Event harvest configuration",
        "Usage example",
        "event_harvest_config.harvest_limits.analytic_event_data",
        "event_harvest_config.harvest_limits.custom_event_data",
        "event_harvest_config.harvest_limits.span_event_data",
        "event_harvest_config.harvest_limits.error_event_data",
        "Event loop visibility settings",
        "event_loop_visibility.enabled",
        "event_loop_visibility.blocking_threshold",
        "Garbage collection runtime metrics settings",
        "gc_runtime_metrics.enabled",
        "gc_runtime_metrics.top_object_count_limit",
        "Other configuration settings",
        "utilization.detect_aws",
        "utilization.detect_azure",
        "utilization.detect_gcp",
        "utilization.detect_pcf",
        "utilization.detect_docker",
        "slow_sql.enabled",
        "thread_profiler.enabled",
        "cross_application_tracer.enabled",
        "strip_exception_messages.enabled",
        "strip_exception_messages.whitelist",
        "startup_timeout",
        "shutdown_timeout",
        "compressed_content_encoding",
        "Heroku",
        "heroku.use_dyno_names",
        "heroku.dyno_name_prefixes_to_shorten",
        "Built-in instrumentation",
        "Example: Disabling MySQLdb database query instrumentation"
      ],
      "title": "Python agent configuration",
      "type": "docs",
      "tags": [
        "Agents",
        "Python agent",
        "Configuration"
      ],
      "external_id": "923b7c4b9b48b55402bc7793ef3e12b0bdcfa8dc",
      "image": "https://docs.newrelic.com/static/0bbb623699fa652795cba242b01caa6b/8c557/diagram-python-config-precedence_0.png",
      "url": "https://docs.newrelic.com/docs/apm/agents/python-agent/configuration/python-agent-configuration/",
      "published_at": "2022-01-12T11:33:43Z",
      "updated_at": "2022-01-12T11:33:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Python agent lets you change the default agent behavior agent using configuration options. The only required Python agent configuration setting is the license key. The license key identifies the account where the agent reports application data. Depending on how you are hosting your application, the license key can be provided via a configuration file or an environment variable. Configuration methods and precedence The primary way to configure the Python agent is via the configuration file, which is generated as part of the standard install process. It is also possible to set a limited number of configuration options using server-side configuration in the UI or by using environment variables. You can also specify some settings on a per-request basis by passing settings with the WSGI request environ dictionary. The Python agent follows this order of precedence for configuration: With the Python agent, per-request options override server-side config. If enabled, server-side config overrides all corresponding values in the agent config file, even if the server-side values are left blank. The agent config file overrides environment variables. Environment variables override the agent defaults. Here are detailed descriptions of each configuration method: Agent configuration file Typically you configure your Python agent from a local configuration file on the agent's host system. Supply the path to the config file at startup using one of these methods: When you call newrelic.agent.initialize(), provide the path to the config file as the first argument. OR Set the NEW_RELIC_CONFIG_FILE environment variable. If you use the newrelic-admin wrapper script, you must use the environment variable because the wrapper script calls the agent automatically. The configuration file uses a structure similar to Microsoft Windows .ini files. For more information, see the Python ConfigParser module's file format documentation. Tip A sample configuration file is included with the Python agent as newrelic/newrelic.ini. You can also generate one from the newrelic-admin script using the generate-config command, or download a copy from our download repo. Server-side configuration Server-side configuration allows you to configure certain settings in the New Relic One UI. This applies your changes automatically to all agents even if they run across multiple hosts. Where available, this document includes the UI labels for server-side config under individual config options as the Server-side label. Important If server-side config is enabled, the agent ignores any value in the config file that could be set in the UI. Even if the UI value is empty, the agent treats this as an empty string and does not use the agent config file. Environment variables Environment variables allow you to override the defaults for certain core settings. If the equivalent setting is explicitly listed in the agent config file, the config file settings take precedence over the environment variable. Where available, environment variables are documented below under individual config options as the Environ variable. For simple configurations, you can use the environment variables in conjunction with server-side configuration and avoid the agent config file altogether. This is the default setup with Heroku, where installing the New Relic add-on automatically populates the necessary environment variables. If you're using New Relic APM and CodeStream, see how to associate repositories and how to associate build SHAs or release tags with errors inbox. Environment variable Configuration setting NEW_RELIC_LICENSE_KEY license_key NEW_RELIC_APP_NAME app_name NEW_RELIC_MONITOR_MODE monitor_mode NEW_RELIC_DEVELOPER_MODE developer_mode NEW_RELIC_LOG log_file NEW_RELIC_LOG_LEVEL log_level NEW_RELIC_HIGH_SECURITY high_security NEW_RELIC_PROXY_SCHEME proxy_scheme NEW_RELIC_PROXY_HOST proxy_host NEW_RELIC_PROXY_PORT proxy_port NEW_RELIC_PROXY_USER proxy_user NEW_RELIC_PROXY_PASS proxy_pass NEW_RELIC_AUDIT_LOG audit_log_file NEW_RELIC_STARTUP_TIMEOUT startup_timeout NEW_RELIC_SHUTDOWN_TIMEOUT shutdown_timeout NEW_RELIC_LABELS labels NEW_RELIC_PROCESS_HOST_DISPLAY_NAME process_host.display_name NEW_RELIC_API_KEY api_key NEW_RELIC_CA_BUNDLE_PATH ca_bundle_path NEW_RELIC_DISTRIBUTED_TRACING_ENABLED distributed_tracing.enabled NEW_RELIC_ANALYTICS_EVENTS_MAX_SAMPLES_STORED event_harvest_config.harvest_limits.analytic_event_data NEW_RELIC_CUSTOM_INSIGHTS_EVENTS_MAX_SAMPLES_STORED event_harvest_config.harvest_limits.custom_event_data NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED event_harvest_config.harvest_limits.span_event_data NEW_RELIC_ERROR_COLLECTOR_MAX_EVENT_SAMPLES_STORED event_harvest_config.harvest_limits.error_event_data NEW_RELIC_FEATURE_FLAG feature_flag Per-request configuration For certain WSGI servers, you can override the app name and capture attributes settings on a per-request basis. This is possible with WSGI servers where you can define additional key/value pairs that are passed into the per-request WSGI environ dictionary. Set these values with the strings on, off, true, false, 1 and 0. If set from a configuration mechanism implemented using Python code, Python objects evaluating to True or False will also be accepted. Example: Apache/mod_wsgi app name In the Apache/mod_wsgi server, you can use the SetEnv directive to override config settings (optionally inside a Location or Directory block). For example, you could override the app name for a complete virtual host, or for a subset of URLs handled by the WSGI application for that virtual host. In addition to being able to override certain agent configuration settings, you can set other per-request configuration settings with their WSGI environment key: newrelic.set_background_task If set to true, this web transaction will instead be reported as a non-web transaction. newrelic.ignore_transaction If set to true, this web transaction will not be reported. newrelic.suppress_apdex_metric If set to true, no Apdex metric will be generated for this web transaction. newrelic.suppress_transaction_trace If set to true, this web transaction cannot be recorded in a transaction trace. newrelic.disable_browser_autorum If set to true, this disables automatic insertion of the JavaScript header/footer for page load timing (sometimes referred to as real user monitoring or RUM). Only applicable if auto-insertion is available for your web framework. Important Using a WSGI middleware to set these values will not work where the Python agent's own WSGI application wrapper was applied at an outer scope. In these cases you must make calls to the agent API to achieve the same outcome. Multiple environment configuration The agent reads its primary configuration from an agent config section called newrelic. You can provide overrides for specific deployment environments (for example, Development, Staging, Production) in additional sections. Preface these sections with [newrelic:environment], where environment is replaced with the name of your environment. To specify that the agent should use an environment-based configuration, use one of these methods: When you call newrelic.agent.initialize(), provide the environment name as the second argument. OR Set the NEW_RELIC_ENVIRONMENT environment variable to the environment name. If no environment is specified, the agent will use the default settings as specified in the newrelic agent config section. The basic structure of the configuration file is: [newrelic] ... default settings [newrelic:development] ... override settings [newrelic:staging] ... override settings [newrelic:production] ... override settings Copy General configuration settings These settings are available in the agent configuration file. license_key (REQUIRED) Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_LICENSE_KEY Specifies the license key of your New Relic account. This key associates your app's metrics with your New Relic account. app_name (HIGHLY RECOMMENDED) Type String Default Python Application Set in Per-request option, config file, environment variable Per-request option newrelic.app_name Environ variable NEW_RELIC_APP_NAME The application name used to aggregate data in the New Relic One UI. To report data to multiple apps at the same time, specify a list of names separated with a semicolon ;. Do not put a space before the semicolon, which causes the Python config parser to interpret the name as an embedded comment. monitor_mode Type Boolean Default true Set in Config file, environment variable Environ variable NEW_RELIC_MONITOR_MODE When true, the agent collects performance data about your app and reports this data to our data collector. developer_mode Type Boolean Default false Set in Config file, environment variable Environ variable NEW_RELIC_DEVELOPER_MODE When true, the agent will instrument your web app, but will not send any actual data. In this offline mode, you will not be billed for an active agent. Use developer mode to test new versions of the agent, or test the agent against third-party packages in a developer environment. Offline mode is not a way of running the APM locally, because the metrics the agent collects are not reported anywhere. log_file Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_LOG Sets the name of a log file, which is useful for debugging issues with the agent. This is not set by default, since the agent does not know your web app process's parent user or what directories that process has permission to write to. For detailed information, see Python agent logging. Whatever you set this to, ensure the permissions for the containing directory and the file itself are correct, and that the user that your web application runs as can write to the file. Tip Use an absolute path unless you are sure what the working directory of your application will be at startup. If you can't write out a log file, you can also use stderr and output to standard error output. This would normally result in output appearing in your web server log. log_level Type String Default info Set in Config file, environment variable Environ variable NEW_RELIC_LOG_LEVEL Sets the level of detail of log messages, if you've set the log file location. This log_level will not affect the Python logging module log level. Possible values, in increasing order of detail, are critical, error, warning, info, and debug. To report agent issues, the most useful setting is debug. However, debug generates a lot of information very quickly, so do not keep the agent at this level for longer than it takes to reproduce your problem. high_security Type Boolean Default false Set in Config file, environment variable Environ variable NEW_RELIC_HIGH_SECURITY High security mode enforces certain security settings and prevents them from being overridden, so that no sensitive data is sent to us. Enabling high security mode means that request parameters are not collected, and you cannot send raw SQL. To activate high security mode, set it to true in the local .ini configuration file and activate it in the Account Settings in the UI. For more information, see High security. proxy_scheme, proxy_host, proxy_port, proxy_user, proxy_pass Type Strings Default (none) Set in Config file, environment variable Environ variables NEW_RELIC_PROXY_SCHEME NEW_RELIC_PROXY_HOST NEW_RELIC_PROXY_PORT NEW_RELIC_PROXY_USER NEW_RELIC_PROXY_PASS By default, the Python agent attempts to directly connect to our servers. If there is a firewall between your host and the our collector that requires you to use an HTTP proxy, set proxy_host and proxy_port to the required values for your HTTP proxy. If proxy authentication is implemented by the HTTP proxy, also set proxy_user and proxy_pass. The proxy_scheme setting dictates what protocol scheme is used to talk to the HTTP proxy. When set to http, the agent uses a SSL tunnel through the HTTP proxy for end-to-end encryption. Instead of setting the proxy_scheme, proxy_host and proxy_port settings, you can also set the proxy_host setting to a valid URI for the proxy. Include the scheme, host, and port; for example, http://proxy-host:8000. This also works if you set the details of the HTTP proxy with the NEW_RELIC_PROXY_HOST environment variable. Tip Python agent versions 2.0.0 or earlier do not provide the proxy_scheme setting, and the protocol scheme defaults to http or https depending whether ssl is disabled or enabled. If you are upgrading from an older agent version and your config file doesn't include proxy_scheme, ensure you add the setting and set it appropriately. If you don't, the agent will continue to base the protocol scheme on the ssl setting for backwards compatibility. As proxies are usually only configured to accept proxy requests via the http protocol scheme, not setting proxy_scheme may result in a failure. audit_log_file Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_AUDIT_LOG Sets the name of the audit log file. If set, the agent logs details of messages passed back and forth between the monitored process and the collector. This allows you to evaluate the security of the Python agent. Use an absolute path unless you are sure what your app's working directory will be at startup. Whatever you set this to, ensure the permissions for the containing directory and the file itself are correct. Also ensure your web app's parent user can write to the file. Caution Do not use audit logging on an ongoing basis, especially in a production environment. Because the agent does not truncate or rotate the log file, the log file can grow very quickly. labels (tags) Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_LABELS Adds tags. Specify name:value separated by a colon :, and separate additional tags with semicolons ;. Two tags Server:One;Data Center:Primary Copy process_host.display_name Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_PROCESS_HOST_DISPLAY_NAME Sets the hostname to be displayed in the APM UI. If set, this overrides the default hostname that the agent captures automatically. api_key Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_API_KEY Sets the api_key to be used with newrelic-admin record-deploy. ca_bundle_path Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_CA_BUNDLE_PATH Manual override for the path to your local CA bundle. This CA bundle will be used to validate the SSL certificate presented by our data collection service. Tip This configuration option is only available in Python agent versions 4.2.0 and newer. apdex_t Type Float Default 0.5 Set in Config file, environment variable Environ variable NEW_RELIC_APDEX_T We'll record transaction traces when they exceed this threshold. The format is a number of seconds (decimal points allowed). See our glossary entry for apdex_t Attributes Attributes are key-value pairs that provide information for transaction traces, traced errors, browser monitoring, and transaction events. In addition to configuring attributes for all four destinations with the general attribute settings below, they can also be configured on a per-destination basis. For more information, see Python agent attributes, Enabling and disabling attributes, and Attribute examples. attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes. attributes.include Type List of Strings Default (none) Set in Config file If attributes are enabled, attribute keys found in this list will be sent to us. Keys in the list should be space-separated as shown below: key1 key2 key3 Copy Rules for attributes can be found on the agent attributes page. attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent to us. Keys in the list should be space-separated as shown below: key1 key2 key3 Copy Rules for attributes can be found on the agent attributes page. Transaction tracer configuration Important Do not use brackets [suffix] at the end of your transaction name. The agent automatically strips brackets from the name. Instead, use parentheses (suffix) or other symbols if needed. For more information about transaction traces, see Transaction traces. transaction_tracer.enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable transaction tracing? If enabled, the transaction tracer captures deep information about slow transactions. transaction_tracer.transaction_threshold Type Positive float or string (apdex_f) Default apdex_f Set in Server-side config, config file Server-side label Threshold Threshold in seconds for when to collect a transaction trace. When the response time of a controller action exceeds this threshold, the agent records a transaction trace. Valid values are any positive float, or apdex_f (four times apdex_t). transaction_tracer.record_sql Type String Default obfuscated Set in Server-side config, config file Server-side label Record SQL? When the transaction tracer is enabled, the agent can record SQL statements. The recorder has three modes: off (sends no SQL), raw (sends the SQL statement in its original form), and obfuscated (strips out numeric and string literals). Most web frameworks (including Django) parameterize SQL queries so they do not actually contain the values used to fill out the query. If you use raw mode with one of these frameworks, the Python agent will only see the SQL prior to insertion of values. The parametrized SQL will look much like obfuscated mode. transaction_tracer.stack_trace_threshold Type Float Default 0.5 Set in Server-side config, config file Server-side label Stack trace threshold Threshold in seconds for when to collect stack traces from SQL calls. When SQL statements exceed this threshold, the agent captures the current stack trace. This is helpful for pinpointing where long SQL calls originate in an application. transaction_tracer.explain_enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable SQL query plans? Determines whether the Python agent will capture query plans for slow SQL queries. Only supported in MySQL and PostgreSQL. transaction_tracer.explain_threshold Type Float Default 0.5 Set in Server-side config, config file Server-side label Query plan threshold Queries in transaction traces that exceed this threshold will report slow query data and any available explain plans. Explain plan collection will not happen if transaction_tracer.explain_enabled is false. transaction_tracer.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes for transaction traces. If attributes.enabled at the root level is false, no attributes will be sent to transaction traces regardless on how this configuration setting (transaction_tracer.attributes.enabled) is set. transaction_tracer.attributes.include Type List of strings Default (none) Set in Config file If attributes are enabled for transaction traces, all attribute keys found in this list will be sent to us in transaction traces. For more information, see the agent attribute rules. transaction_tracer.attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent in transaction traces. For more information, see the agent attribute rules. transaction_tracer.function_trace Type String Default (none) Set in Config file For the specified functions or methods, the agent will capture additional function timing instrumentation. Specify these names in the form module:function or module:class.function. Wildcarding (globbing) for function and class names is possible using patterns supported by the fnmatch module. Module paths are not supported by wildcards. Specify the patterns in the form module:function* or module:class.*. For example, if you want to add function tracing to all validation functions in the below file: my-app/common/utils.py def validate_credentials(): … def validate_status(): … def format_message(): … Copy Add the following line to the agent config file to include function tracing to all validation functions in my-app/common/utils.py by using wildcarding. my-app/newrelic.ini [newrelic] ... transaction_tracer.function_trace = common.utils:validate* Copy Important Wilcarding requires Python agent version 6.4.4.161 or higher. Transaction segment configuration Here are Transaction segment settings available via the agent configuration file. transaction_segments.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes for segments of transaction traces. If attributes.enabled at the root level is false, no attributes will be sent to segments of transaction traces regardless on how this configuration setting (transaction_segments.attributes.enabled) is set. transaction_segments.attributes.include Type List of strings Default (none) Set in Config file If attributes are enabled for segments of transaction traces, all attribute keys found in this list will be sent in segments of transaction traces. For more information, see the agent attribute rules. transaction_segments.attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent in segments of transaction traces. For more information, see the agent attribute rules. Error collector configuration Here are error collector settings available via the agent configuration file. Tip For an overview of error configuration in APM, see Manage errors in APM. error_collector.enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable error collection? If enabled, the error collector captures information about uncaught exceptions. error_collector.ignore_classes Type String Default (none) Set in Server-side config, config file Server-side label Ignore these errors To stop collecting specific errors, set this to a space-separated list of the Python exception type names to ignore. Use the form module:class for the exception name. Tip Before version 6.4.0 of the agent, this setting was named error_collector.ignore_errors. If your configuration file still uses ignore_errors, update your agent to use ignore_classes. error_collector.ignore_status_codes Type String Default 100-102 200-208 226 300-308 404 Set in Server-side config, config file Server-side label Ignore these status codes Lists HTTP status codes which the agent should ignore rather than record as errors. List additional status codes as integers separated by spaces, and specify ranges with a hyphen - separator between the start and end values. To add one of the default codes to your allow list, preface the code with an exclamation point !. This setting is only compatible with some web frameworks, as some frameworks do not use exceptions to return HTTP responses. Tip This configuration option can only be set in server-side configuration in Python agent versions 6.4.0 and newer. error_collector.expected_classes Type String Default (none) Set in Server-side config, config file Server-side label Expect these error class names Prevents specified exception classes from affecting error rate or Apdex score while still reporting the errors to APM. Set this to a space-separated list of the Python exception type names to be expected. Use the form module:class for the exception name. Tip This configuration option is only available in Python agent versions 6.4.0 and newer. error_collector.expected_status_codes Type String Default (none) Set in Server-side config, config file Server-side label Expect these status codes Prevents specified HTTP status codes from affecting error rate or Apdex score while still reporting the errors to APM. List status codes as integers separated by spaces and specify ranges with a hyphen - separator between the start and end values. To negate one of the codes in your list, preface the code with an exclamation point !. This setting is only compatible with some web frameworks, as some frameworks do not use exceptions to return HTTP responses. Tip This configuration option is only available in Python Agent versions 6.4.0 and newer. error_collector.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes for traced errors. If attributes.enabled is false at the root level, then no attributes will be sent to traced errors regardless on how this configuration setting (error_collector.attributes.enabled) is set. error_collector.attributes.include Type List of strings Default (none) Set in Config file If attributes are enabled for traced errors, all attribute keys found in this list will be sent to in traced errors. For more information, see the agent attribute rules. error_collector.attributes.exclude Type List of strings Default (none) Set in Config file Attribute keys found in this list will not be sent to in traced errors. For more information, see the agent attribute rules. error_collector.capture_events Type Boolean Default true Set in Config file If enabled, the error collector captures event data for advanced analytics. For more information, see APM Errors. Browser monitoring settings Here are browser monitoring settings available via the agent configuration file. browser_monitoring.enabled Type Boolean Default true Set in Config file Enables browser monitoring. For more information, see Page load timing in Python. Important Before enabling browser monitoring in the config file, enable it in the application settings in the browser monitoring UI. browser_monitoring.auto_instrument Type Boolean Default true Set in Config file For supported Python web frameworks, this setting enables auto-insertion of the browser monitoring JavaScript fragments. browser_monitoring.content_type Type String Default text/html Set in Config file Specify the HTML Content-Type(s) that our browser monitoring agent should auto-instrument. Add additional entries in a space-separated list. Instrument xhtml+xml page responses If you are generating HTML page responses and using the Content-Type of application/xhtml+xml, you can override the allowed content types to list both this content type and the default text/html by using: browser_monitoring.content_type = text/html application/xhtml+xml Copy Important The browser monitoring JavaScript snippet prevents the page from validating as application/xhtml+xml, although the page should load and render in end-user browsers. browser_monitoring.attributes.enabled Type Boolean Default false Set in Config file This setting can be used to turn on or off all attributes for browser monitoring. This is the data which gets sent to page view events. If attributes.enabled is false at the root level, no attributes will be sent up in browser monitoring regardless on how the configuration setting (browser_monitoring.attributes.enabled) is set. browser_monitoring.attributes.include Type List of Strings Default (none) Set in Config file If attributes are enabled for browser_monitoring, all attribute keys found in this list will be sent in page views. For more information, see the agent attribute rules. browser_monitoring.attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent in page views. For more information, see the agent attribute rules. Transaction events settings Here are Transaction events settings available via the agent configuration file. Tip These configuration settings used to be called analytic_events. If your configuration file still uses analytic_events, update your agent to use transaction_events. transaction_events.enabled Type Boolean Default true Set in Config file Transaction event data allows the use of additional information such as histograms and percentiles. transaction_events.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes for transaction events. If attributes.enabled is false at the root level, then no attributes will be sent to transaction events regardless on how this configuration setting (transaction_events.attributes.enabled) is set. transaction_events.attributes.include Type List of Strings Default (none) Set in Config file If attributes are enabled for transaction events, all attribute keys found in this list will be sent in transaction events. For more information, see the agent attribute rules. transaction_events.attributes.exclude Type List of Strings Default (none) All attribute keys found in this list will not be sent to in transaction events. Note that excluding attributes from transaction events does not exclude from span events. For more information, see the agent attribute rules. Custom events settings Here are custom events settings available via the agent configuration file. custom_insights_events.enabled Type Boolean Default true Set in Config file Allow recording of events to the Insights custom events API via record_custom_event(). Datastore tracer settings These datastore tracer settings are available via the agent configuration file: datastore_tracer.instance_reporting.enabled Type Boolean Default true Set in Config file When enabled, the agent collects datastore instance metrics (such as host and port) for some database drivers. These are also reported on slow query traces and transaction traces. datastore_tracer.database_name_reporting.enabled Type Boolean Default true Set in Config file When enabled, the agent collects database name for some database drivers. The database name is reported on slow query traces and transaction traces. Distributed tracing settings Important Starting in Python agent version 7.0.0.166 or higher, distributed tracing is enabled by default. Enabling distributed tracing disables cross application tracing and has other effects on APM features. If migrating from cross application tracing, read the transition guide. Distributed tracing lets you see the path that a request takes as it travels through a distributed system. Settings include: distributed_tracing.enabled Type Boolean Default true Set in Config file, environment variable Environ variable NEW_RELIC_DISTRIBUTED_TRACING_ENABLED Enables Distributed Tracing Span event configuration Span events are collected for distributed tracing. Distributed tracing must be enabled to report span events. Configuration options include: span_events.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off whether the Python agent sends spans. span_events.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off for all attributes for span events. If attributes.enabled at the root level is false, no attributes will be sent to span events regardless on how this configuration setting (span_events.attributes.enabled) is set. For more information, see the agent attribute rules. span_events.attributes.include Type List of strings Default (none) Set in Config file If attributes are enabled for span events, all attribute keys found in this list will be sent in span events. For more information, see the agent attribute rules. span_events.attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent in span events. For more information, see the agent attribute rules. Event harvest configuration Event harvest settings limit the amount of event type data sent to New Relic. When you use these settings, consider these important points: Event harvest settings affect the limits for a single instance of the agent, and not across the entire application. See the usage example below for how to set limits across an entire application. Real time streaming sends data every five seconds (12 times per minute), but the event harvest settings still affect the rate in events per minute. Enabling or disabling real time streaming does not require changing these settings. With real time streaming (enabled by default), New Relic will display the event harvest limits for entities in five second intervals. This means, for example, when you set a limit value of 1200 in the config file, you'll see it as 100 in New Relic. Usage example Let's say an application is deployed across 10 hosts, each running four processes per host. To limit the number of span events to 10,000 events per minute for the entire application, divide that number by 10 hosts. Then divide again by four processes per host. 10000 / (10 * 4) = 250 Based on that calculation, the final setting is: event_harvest_config.harvest_limits.span_event_data = 250 Event harvest configuration settings include: event_harvest_config.harvest_limits.analytic_event_data Type Integer Default 1200 Set in Config file Environ variable NEW_RELIC_ANALYTICS_EVENTS_MAX_SAMPLES_STORED Limit for analytic events per minute sent by an instance of the Python agent to New Relic. event_harvest_config.harvest_limits.custom_event_data Type Integer Default 1200 Set in Config file Environ variable NEW_RELIC_CUSTOM_INSIGHTS_EVENTS_MAX_SAMPLES_STORED Limit for custom events per minute sent by an instance of the Python agent to New Relic. Custom events are created through the Python Telemetry SDK. event_harvest_config.harvest_limits.span_event_data Type Integer Default 1000 Set in Config file Environ variable NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED Limit for span events per minute sent by an instance of the Python agent to New Relic. event_harvest_config.harvest_limits.error_event_data Type Integer Default 100 Set in Config file Environ variable NEW_RELIC_ERROR_COLLECTOR_MAX_EVENT_SAMPLES_STORED Limit for error events per minute sent by an instance of the Python agent to New Relic. Event loop visibility settings Important Requires Python agent version 5.0.0.124 or higher. Event loop visibility surfaces information about transactions that block the event loop. The agent will generate information about transactions that have waited a significant amount of time to acquire control of the event loop. Settings include: event_loop_visibility.enabled Type Boolean Default true Set in Config file Set this to false to disable event loop information. event_loop_visibility.blocking_threshold Type Float Default 0.1 Set in Config file Threshold in seconds for how long a transaction must block the event loop before generating event loop information. Garbage collection runtime metrics settings Important Requires Python agent version 6.2.0.156 or higher. These garbage collection runtime metrics settings are available via the agent configuration file: gc_runtime_metrics.enabled Type Boolean Default false Set in Config file When enabled, the agent will generate and send garbage collection metrics. gc_runtime_metrics.top_object_count_limit Type Integer Default 5 Set in Config file The agent reports object count metrics for the most common object types being collected by the garbage collector. For each object type, this setting allows you to set the maximum number of individual metrics that will be sampled. Other configuration settings Here are assorted other settings available via the agent configuration file. utilization.detect_aws Type Boolean Default true If true, the agent automatically detects that it is running in an AWS environment. utilization.detect_azure Type Boolean Default true If true, the agent automatically detects that it is running in an Azure environment. utilization.detect_gcp Type Boolean Default true If true, the agent automatically detects that it is running in a Google Cloud Platform environment. utilization.detect_pcf Type Boolean Default true If true, the agent automatically detects that it is running in a Pivotal Cloud Foundry environment. utilization.detect_docker Type Boolean Default true If true, the agent automatically detects that it is running in Docker. slow_sql.enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable Slow SQL? If enabled, the agent captures details from long-running SQL database queries. thread_profiler.enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable thread profiler? Enables you to schedule thread profiling sessions. The thread profiler will periodically capture a snapshot of the call stack for each active thread in the application to construct a statistically representative call tree. cross_application_tracer.enabled Type Boolean Default false Set in Config file Enables cross application tracing, which connect your apps and services within your service-oriented architecture. strip_exception_messages.enabled Type Boolean Default false Set in Config file If enabled, exception messages will be stripped from error traces before they are sent to the collector, in order to prevent the inadvertent capture of sensitive information. This option is automatically enabled in high security mode. strip_exception_messages.whitelist Type String Default (none) Set in Config file Exceptions listed in your allow list will not have their messages stripped, even if strip_exception_messages.enabled is true. The allow list is a space-separated string of exception types, each in the form of module:exception_name. List built-in exceptions as exception_name; you do not need to prepend module: to them. Example: Built-in exception and user-defined exception KeyError my_module:MyException Copy startup_timeout Type Float Default 0.0 Set in Config file, environment variable Environ variable NEW_RELIC_STARTUP_TIMEOUT By default, the agent starts when it receives the first transaction (either web or non-web). The agent then starts in parallel, ensuring that this initial request is not delayed. However, the agent does not record the details of this initial request because the agent cannot collect data until registration is complete. To override this, you can set a startup timeout in seconds. The agent will then pause the initial transaction and wait for registration to complete. Important Since startup_timeout delays your app start, we recommend only setting a startup timeout for background task queuing systems, not web applications. shutdown_timeout Type Float Default 2.5 Set in Config file, environment variable Environ variable NEW_RELIC_SHUTDOWN_TIMEOUT On process shutdown, the agent attempts one final upload to the collector. To prevent the agent running indefinitely in case of an issue, the process shuts down normally if the shutdown_timeout threshold is reached. This shutdown can result in data loss, but the agent prioritizes key metric data during the upload process. For background task queuing systems, especially those which run a small number of tasks per process, you may want to increase the shutdown timeout to ensure the agent can upload all data on process shutdown. Tip The agent defaults to a 2.5 second timeout because Apache and many other web servers have a 3.0 second process termination timeout. The agent exits at 2.5 seconds to allow atexit cleanup code registered for the process to run. compressed_content_encoding Type String Default gzip Set in Config file If the data compression threshold is reached in the payload, the agent compresses data, using gzip compression by default. The config option compression_content_encoding can be set to deflate to use deflate compression. Heroku heroku.use_dyno_names Type Boolean Default true Environ variable NEW_RELIC_HEROKU_USE_DYNO_NAMES If true, the agent uses Heroku dyno names as the hostname. heroku.dyno_name_prefixes_to_shorten Type Array Default [\"scheduler\", \"run\"] Environ variable NEW_RELIC_HEROKU_DYNO_NAME_PREFIXES_TO_SHORTEN Ordinarily the agent reports dyno names with a trailing dot and process ID (for example, worker.3). You can remove this trailing data by specifying the prefixes you want to report without trailing data (for example, worker). Built-in instrumentation The Python agent instruments a range of Python packages/modules. This instrumentation only occurs when the target Python package/module is imported by an application. To disable default instrumentation, provide a special import-hook section corresponding to the name of the module that triggered instrumentation. Then set the enabled setting to false to disable instrumentation of that module. Example: Disabling MySQLdb database query instrumentation Add the following to the configuration file: [import-hook:MySQLdb] enabled = false Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.65994,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Example: Disabling <em>MySQLdb</em> database query <em>instrumentation</em>",
        "body": " with Heroku, <em>where</em> installing the New Relic add-on automatically populates the necessary environment variables. If you&#x27;re using New Relic APM and <em>CodeStream</em>, see how to associate repositories and how to associate build SHAs or release tags with errors inbox. Environment variable Configuration setting"
      },
      "id": "617dc153196a67cf3df7bf56"
    }
  ],
  "/docs/codestream/troubleshooting/git-issues": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.6436,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on <em>Git</em>Hub <em>CodeStream</em>&#x27;s third-party software notices on <em>Git</em>Hub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.79095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring <em>with</em> <em>CodeStream</em>",
        "sections": "Performance monitoring <em>with</em> <em>CodeStream</em>",
        "body": " status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on <em>CodeStream</em> or to a person suggested based on the repository’s <em>Git</em> commit history. You can update the error’s status from unresolved to resolved or ignored. Use"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/8945e0a9c512b8638ebf8165d47aee04/69902/QS-SignUp3.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-user-guide/",
      "sections": [
        "New Relic CodeStream user guide",
        "Jump to a topic",
        "1. Install the CodeStream extension in your IDE and sign up.",
        "2. Connect your tools",
        "3. Discuss any block of code, at any time",
        "4. Get feedback on your work in progress",
        "5. Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "New Relic CodeStream user guide",
      "updated_at": "2021-11-24T04:44:31Z",
      "type": "docs",
      "external_id": "fa9af0118a8872fea89fda91482c44fb69913ea2",
      "document_type": "page",
      "popularity": 1,
      "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic CodeStream. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. 1. Install the CodeStream extension in your IDE and sign up. Install CodeStream for VS Code, Visual Studio or JetBrains. The CodeStream pane automatically appears in the sidebar for VS Code or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you're the first person from your team to join CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on GitHub, GitLab or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click on your headshot at the top of the CodeStream pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, just select the code and ask your question. Learn more about discussing code. 4. Get feedback on your work in progress Select Request Feedback from the + menu at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code!) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. 5. Create or review a pull request Look for the Pull Requests section of the CodeStream sidebar to review an open pull request. Just click on a pull request (or load one from URL) to get a complete GitHub experience right in your IDE! Note that you can create a pull request in GitHub, GitLab or Bitbucket, but support for reviewing pull requests is currently only available for GitHub (cloud or Enterprise). Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.11987,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic <em>CodeStream</em> user guide",
        "sections": "1. Install the <em>CodeStream</em> extension in your IDE <em>and</em> sign up.",
        "body": " <em>CodeStream</em> or paste in your invitation <em>code</em> if you were invited to a team already on <em>CodeStream</em>. Learn more about how to use <em>CodeStream</em>. 2. Connect your tools Create and review pull requests on <em>Git</em>Hub, <em>Git</em>Lab or Bitbucket. Create <em>issues</em> on Jira, Trello, and other issue trackers. Share <em>code</em>"
      },
      "id": "61744137e7b9d2428b13c6a0"
    }
  ],
  "/docs/codestream/troubleshooting/github-org-repos": [
    {
      "image": "https://docs.newrelic.com/static/dd18b67123e9d4b7d40b56a8653a1f6b/f96db/OpenPullRequest1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/pull-requests/",
      "sections": [
        "Manage pull requests in CodeStream",
        "Pull request workflow",
        "Create a pull request",
        "Review a pull request",
        "Caution",
        "Leverage pull request comments"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Manage pull requests in CodeStream",
      "updated_at": "2021-11-13T21:14:26Z",
      "type": "docs",
      "external_id": "7e35f2ff4f06799fe492ffb6b2fedbb52e898b69",
      "document_type": "page",
      "popularity": 1,
      "body": "For most development teams, the final step in the development process is a pull request. Even if your team has decided to use New Relic CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, CodeStream allows you to keep all of that workflow inside your IDE. Pull request workflow There are four elements of CodeStream's pull request workflow. The following table outlines which code-hosting services are supported for each element. Feature Supported Services Create a pull request GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed, Bitbucket, Bitbucket Server Create a pull request across forks GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed Review and edit a pull request GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed Display pull request comments as code annotations GitHub, GitLab, Bitbucket Create a pull request To open a pull request at any time, click the + button at the top of the CodeStream pane or the + button in the header of the Pull Requests section. You can also use a keyboard shortcut (ctlr+shift+/ p, ctrl+/ p on a Mac, and m if you're a GitLab user). CodeStream provides you with tree view, list view, and diff view options for reviewing your changes before opening the pull request. With a single click you can name the pull request based on the last commit message, the branch name, or, if you started work by selecting a ticket, the ticket title. If you have a ticket selected, you can also explicitly tie the ticket to the pull request and CodeStream will include a link to the ticket in the pull request's description. Before submitting the pull request, you can review your changes by clicking on any of the files listed below the form. To create a pull request across forks, click the compare across forks link at the top of the page and the form will update to allow you to select both the base and head repositories. You can also create a pull request from within a CodeStream feedback request. Once the feedback request has been approved, you’ll see an option to open a pull request at the top. Before you can create a pull request, make sure that any changes included in the feedback request have been committed and pushed. Also, if the feature branch you’re working on doesn’t have a remote tracking branch you’ll be given the option to set that as part of creating the pull request. When you create a pull request from a feedback request, CodeStream connects the dots between the two by adding a link to the pull request in the feedback request. Add a link to the feedback request, along with information about who did the review and when, in the description of the pull request. Review a pull request Caution The ability to review pull requests is currently not available for Bitbucket. Regardless of where the pull request was created, you can edit, review, and even merge it from within CodeStream. CodeStream brings GitHub and GitLab into your IDE, so there's zero learning curve. If you know how to work with pull requests on GitHub or GitLab, you'll know how to do it in CodeStream as well. You can edit a GitHub pull request's details, such as reviewers, assignees and labels. For a GitLab merge request, you can use edit mode (via the dropdown at the top of the page) or use the sidebar. By default, you can only add a single reviewer and a single assignee to a GitLab merge request. If your organization supports multiple reviewers and assignees, click the gear menu in the heading of the Merge Requests section of the CodeStream pane to enable this. Review the conversation and add comments with the ability to @mention your collaborators. View the changes, add comments, and submit a review. CodeStream does improve upon the GitHub/GitLab experience in a couple of important ways. On GitHub and GitLab you can only view the changes as a series of diff hunks. CodeStream provides that view as well, but if you'd prefer to see the changes in the context of the full file you can use either list view or tree view. Select the code you want to comment on and then click the Comment button (or select Comment from the context menu). When commenting, you can either add a single comment or start a review. With CodeStream, you can comment on lines of code that haven't changed. You can select any lines of code in the diff and not just those that are part of the changeset. These comments are added as a single comment to the pull request and aren't part of any review you may have in progress. All the power of GitHub pull requests and GitLab merge requests, and then some, right in your IDE. Leverage pull request comments Once the pull request has been approved and the code has been merged, that's usually the end of life for any comments in that pull request. Although there's often useful information in those comments that may have long-term value, they're rarely seen again. CodeStream gives those comments a second life by displaying them alongside the blocks of code that they refer to. To have pull request comments displayed as annotations in your codemarks, as well as in the Codemarks section of the CodeStream pane, click the gear icon in that section and check Show comments from pull requests. When you first check that box, if you haven’t already authenticated with your code-hosting service, you’ll be prompted to do so. Comments from merged PRs will appear next to the blocks of code they refer to. Comments from open PRs will also be included if you are on a relevant branch. For example, if the open PR is a request to merge the feature/some-name branch into main, you’ll see comments from that PR if you've checked out either feature/some-name or main, but not when you’re on any other branch. As the code evolves, the location of each comment is automatically updated so that it remains linked to the block of code it refers to. PR comments for a given file are updated roughly every 30 minutes, so new comments may not appear right away. You can force an update by restarting your IDE.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 364.2016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage pull requests <em>in</em> <em>CodeStream</em>",
        "sections": "Manage pull requests <em>in</em> <em>CodeStream</em>",
        "body": " request. Review a pull request Caution The ability to review pull requests is currently not available for Bitbucket. Regardless of where the pull request was created, you can edit, review, and even merge it <em>from</em> within <em>CodeStream</em>. <em>CodeStream</em> brings <em>GitHub</em> and <em>Git</em>Lab into your IDE, so there&#x27;s zero"
      },
      "id": "61744006196a67ee542f0555"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/style-guide/writing-docs/writer-workflow/github-intro/",
      "sections": [
        "Get around GitHub",
        "Who is who in an issue/PR?",
        "Track issues in the board",
        "Deal with references in GitHub (and the style guide)",
        "Merge releases into main work (or, when do we publish?)",
        "GitHub labels"
      ],
      "published_at": "2022-01-12T18:25:08Z",
      "title": "Get around GitHub",
      "updated_at": "2021-12-20T04:59:44Z",
      "type": "docs",
      "external_id": "539ae5620ae9be8f8c3752fd3eda664186fbb5c4",
      "document_type": "page",
      "popularity": 1,
      "body": "As tech doc writers (TW) we edit docs, do peer edits, or use the Docs Team GitHub board to track the status of issues and pull requests (PR). Who is who in an issue/PR? GitHub keeps track of all activity concerning an issue or PR, including, of course, the people involved. When a new issue or PR is filed, check on the filer’s username and see if they're listed as a member of the New Relic organization. If they aren't, try to find them on Slack based on their username. If you're not sure about someone's affiliation, treat them as external until you know otherwise. People in an issue/PR include: Creator: The person who opened the issue or PR. This could be a writer, a Relic, or an external user. We'll label the issue or PR differently depending on who created it. If you're not sure if a user is a Relic, a good trick is to click on their profile and see if they're a member of the New Relic GitHub org. Assignee: The person taking responsibility for a PR or issue. This will usually be used by the Hero or Sidekick to assign non-TW PRs and issues to themselves. It can also be used to take a TW’s PR or issue over from them. Reviewer: The person who reviews or peer edits the code/document and approves the changes. Not necessarily the person responsible for that area or responsible for merging the commit. You can pre-assign up to 100 reviewers to a given issue. Track issues in the board The docs board has the following columns: Column Description Needs triage The Hero or Sidekick review and label issues and PRs in this column, then drag them to the appropriate column. If a PR or issue is labeled eng, the Hero/Sidekick can go ahead and click its ellipses icon to archive it. Hero: to do PRs that the Hero needs to review, publish, and follow up with SMEs as needed. Hero: Assign yourself as Assignee. In review (Hero or any TW) Drag PRs to this column when they are being reviewed. This shows who is reviewing and what is being reviewed, so two writers don’t mistakenly work on the same PR. Any TW: Writer needs PR review PRs from Tech Docs team members that need a light edit pass to make sure everything in GitHub is correct. This should be checked by other writers every few hours so PRs don’t get stale. If you have a PR that’s been lingering here too long, ask for a reviewer in #doc_sprint_talk. Whoever takes it: assign yourself as Reviewer. Any TW: needs peer edit Like our Needs Peer Edit column in Jira: A writer has requested a review of their PR. Review their PR in GitHub and leave comments. Whoever takes it: assign yourself as Reviewer. Waiting on SME/Blocked For PRs that are blocked by need for SME info or confirmation (for example, as Hero you are waiting on an answer from the person who sent in a Hero pull request). Waiting on TW to merge All reviews are complete. The TW who created the PR (or who is assigned the issue) needs to merge this work into develop. Drafts A draft is a way to open a PR while indicating that the work is still in progress and not necessarily ready to merge immediately. You can't merge a Draft PR directly. Instead, you must move it out of draft status first. When you see a draft PR (especially from outside the team!), treat it as though it's a working draft, and reach out to the creator to discuss. Read more on GitHub's drafts. As a Hero, make sure you attend to the following throughout your day: Check in with the previous Hero at the start of your day (especially on Monday at the start of the week). Don’t forget to sync with the BCN Hero if necessary. Watch for incoming PRs in #docs_deploys, and review everything in the Needs triage column. Drag cards from that column to the appropriate column. Work through the cards in the Hero: to do column. Everyone on the team helps keep things moving: All writers should keep an eye on both Any TW columns. There's one column for PRs that need a simpler review before merging (typo fixes, drive-by edits, etc), and another column for PRs that need a peer edit. There are also two blocked columns: One for PRs blocked on a SME, and another column where we're waiting on the TW who created the PR to review feedback and/or merge. After merging, remove your ticket from the board. Deal with references in GitHub (and the style guide) Don't link to anything non-public from a public place. You can reference Jira tickets, but reference tickets by issue key (DOC-1234 is ok) rather than a link (https://newrelic.atlassian.net/browse/DOC-1234 is not). Don't mention traffic or usage numbers publicly. Don't reference internal people by name. If they have a GH account, @mention their GH handle. If they don't, talk instead about teams (\"talk to a browser team engineer\" or \"Support Engineer\") rather than people. You can mention the #documentation channel and hero. Merge releases into main work (or, when do we publish?) The Hero currently merges three times a day: At 9 AM (morning), 12 PM (noon), and 3 PM (evening) Pacific. We merge release branches into main to avoid interuptions when someone merges into develop during a release. To learn more about this workflow, see the gitflow documentation in Atlassian. To start a release: Create a branch based off develop Github Desktop by clicking Current Branch in the top header, clicking New Branch in the dropdown, and then selecting Develop. Name the branch following this pattern: daily-release/mm-dd-yy-morning/noon/evening. Here's an example: daily-release/10-27-21-morning. Push your changes by clicking Push Origin in GitHub Desktop. Create a pull request into main from your new daily release branch by clicking Create Pull Request. This will open a pull request screen on github.com. Pull requests default to merging into develop, so select main as the base branch in the left side of the page and then click Submit Pull Request. Wait until all the checks complete, and then merge the pull request. All branches that follow the daily-release/mm-dd-yy-morning pattern are protected branches. This means the branches can't be deleted or pushed to by non-admins. GitHub labels Every issue needs labels to help us triage and track the health of our backlog: content: Always add, this indicates the issue is content-related rather than a design or engineering issue. pg_*: Always add to indicate the product group. For full definitions, see the \"Doc Jira and GitHub fields\" doc in the internal team Google Drive. Indicate who created the issue: from_internal: A Relic created it. from_external: A user opened it in the repo OR it came in through #customer-feedback process. from_tw: One of us created it (unless we were passing along #customer-feedback). Optionally: docs-issues-migrate: Issues that are too large in scope for the docs team to handle without product team expertise. This label alerts the docs issues team to migrate these issues into the customer feedback channel where they will be triaged and sent to product teams. Jira’d: Issues that have a corresponding Jira ticket. Make sure you leave the Jira number in the comments of the issue (for example, DOC-1234). Every pull request needs these labels so we can see where our contributions come from: content: Always add, this indicates the PR is content-related rather than design or engineering. Indicate who created the pull request: from_internal: A Relic created it. from_external: A user opened it in the repo OR it came in through #customer-feedback process. from_tw: One of us created it (unless we were passing along #customer-feedback). If the PR fixes an external issue, label it as from_tw since the work was done by a tech writer.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 336.68213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get around <em>GitHub</em>",
        "sections": "Deal with references <em>in</em> <em>GitHub</em> (<em>and</em> <em>the</em> style guide)",
        "body": " <em>GitHub</em> org. Assignee: The person taking responsibility for a PR or issue. This will usually be used by the Hero or Sidekick to assign non-TW <em>PRs</em> and <em>issues</em> to themselves. It can also be used to take a TW’s PR or issue over <em>from</em> them. Reviewer: The person who reviews or peer edits the <em>code</em>&#x2F;document"
      },
      "id": "61ab4782196a672667d0efa1"
    },
    {
      "image": "https://docs.newrelic.com/static/79d35d62b4c952d3f8b6131bbcfce9e7/f96db/Sidebar3.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/codestream-sidebar/",
      "sections": [
        "CodeStream sidebar overview",
        "Sidebar overview",
        "The username menu",
        "Header menu items"
      ],
      "published_at": "2022-01-12T05:40:36Z",
      "title": "CodeStream sidebar overview",
      "updated_at": "2021-11-13T21:09:54Z",
      "type": "docs",
      "external_id": "d3bb6106b4e568060453b597ac08e4a51471f3a3",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic CodeStream sidebar surfaces all the items you need to see, and do, in a customizable tree-based view that is always available. The main sidebar sections are: pull requests, feedback requests, codemarks, observability, and issues. Sidebar overview Here's a quick overview of the sidebar sections: Pull requests: If your team uses GitHub or GitHub Enterprise to host your code, you'll see all of your open pull requests listed here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you've been assigned a feedback request or you've requested feedback from someone else, find those listed here. Codemarks: Codemarks are the discussions that annotate your codebase. Codemarks are created pull requests, feedback requests, or through ad hoc code comments/issues. All of the codemarks in your current repository are listed for reference. Observability: Track errors assigned to you in New Relic One and discover recent errors in the repositories you have open in your IDE. Issues: See all of your open issues, across multiple services, in one place. Click a specific issue to update its status, create a feature branch to do your work, and update your status on Slack. The CodeStream sidebar is completely customizable: Drag and drop the sections to reorder them. Click and drag a section divider to resize the sections on either side. Expand or collapse each section as needed. Click the maximize button in each section to fill the whole screen. This is useful when you're looking at a longer list. Click it again to return to your previous view. The username menu The username menu gives you options for managing your account, your organization, how you receive notifications, and what sections are visible. Here are descriptions of each menu item: Account: View your profile. Includes various options for customizing the profile photo, email, username, and full name for your account. View: Uncheck sections you're not interested in seeing. Notifications: Manage how and what notifications you receive. Organization admin: Manage your organization settings. Also, export your data and delete your organization. Switch organization: Use this if you're a member of more than one organization. Integrations: Connect CodeStream to the code host, issue, and messaging providers you use. New Relic setup: Connect your New Relic account to CodeStream to get the most out of New Relic CodeStream's observability tools. Feedback: Got feedback for us? Write a GitHub issue. Help: Links to documentation, our video library, keybindings, the CodeStream workflow, what's new, and reporting an issue. Header menu items The header menu items provide different options for creating and discovering content in your organization. From left to right: Create, Activity feed, My organization, and Filter & search. + (Compose): Click to create a code comment/issue, request feedback on changes, or to create a pull request. Activity feed: The activity feed will let you know about new code comments/issues and feedback requests, as well as replies to existing ones. My organization: See who is in your CodeStream organization, invite new members, and create blame maps. Filter & search: The filter a search tools enable you to slice and dice your team’s collection of code comments, issues, and feature requests however you see fit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 300.3012,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> sidebar overview",
        "sections": "<em>CodeStream</em> sidebar overview",
        "body": " sections: Pull requests: If your team uses <em>GitHub</em> or <em>GitHub</em> Enterprise to host your <em>code</em>, you&#x27;ll see <em>all</em> of your open pull requests <em>listed</em> here. Click a specific pull request to start reviewing or editing it. Feedback requests: If you&#x27;ve been assigned a feedback request or you&#x27;ve requested feedback"
      },
      "id": "617cbd1b28ccbc18bf7fef35"
    }
  ],
  "/docs/codestream/troubleshooting/glsm-version": [
    {
      "image": "https://docs.newrelic.com/static/8fc91276c7cb53b35f896fcac177247d/f96db/IssuesSection.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-ui-overview/issues-section/",
      "sections": [
        "CodeStream issues",
        "Connect your service to CodeStream",
        "Manage your issues"
      ],
      "published_at": "2022-01-12T05:41:21Z",
      "title": "CodeStream issues",
      "updated_at": "2021-11-13T21:10:47Z",
      "type": "docs",
      "external_id": "4c463e5a5ce0017a2496b90dd3f410cd711cef62",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream issues are where you'll see the issues from your external code-hosting or issue-tracking service. Connect your service to CodeStream If you're not already connected to an external service, this section lists all of the available, supported services: Asana Azure DevOps Bitbucket (cloud) Clubhouse GitHub (cloud or Enterprise) GitLab (cloud or Self-Managed) Jira (cloud or Server) Linear Trello YouTrack (cloud) Manage your issues Once you’ve connected to your team’s issue-tracking or code-hosting services, all of the issues assigned to you are listed in the issues section. Your organization can use multiple external services at once. Select the one you want to use from the dropdown list. Click an issue to start working on it. You can create a feature branch, update the ticket status, and even update your status on Slack. Hover over an issue's row to see an option to view the issue on your issue-tracking service toward the end of the row. For many services you can also filter the list. For example, if you’re connected to Trello, you can filter to see a specific list or set of lists. For Jira, Jira Server, GitHub, GitHub Enterprise, GitLab, and GitLab Self-managed you can even create custom filters. There are some special guidelines when creating a custom query for GitHub and GitHub Enterprise or for GitLab and GitLab Self-Managed. Hover over the section's heading for more options. Click the refresh button to update the list with any recently added tickets. Click New issue (although it may be labelled differently based on the selected service) to create a issue in your issue-tracking service right from CodeStream. You can even associate that ticket with a block of code in your editor. If you need to work on something that doesn’t have an associated ticket, you can click Start Ad-hoc Work to get started without a ticket.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 481.2766,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> issues",
        "sections": "Connect <em>your</em> service to <em>CodeStream</em>",
        "body": " (cloud) Clubhouse <em>Git</em>Hub (cloud or Enterprise) <em>GitLab</em> (cloud or <em>Self</em>-<em>Managed</em>) Jira (cloud or Server) Linear Trello YouTrack (cloud) Manage <em>your</em> issues Once you’ve connected to <em>your</em> team’s issue-tracking or <em>code</em>-hosting services, all of the issues assigned to you are listed in the issues section"
      },
      "id": "61743f8b64441f60375fdaf5"
    },
    {
      "image": "https://docs.newrelic.com/static/dd18b67123e9d4b7d40b56a8653a1f6b/f96db/OpenPullRequest1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/pull-requests/",
      "sections": [
        "Manage pull requests in CodeStream",
        "Pull request workflow",
        "Create a pull request",
        "Review a pull request",
        "Caution",
        "Leverage pull request comments"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Manage pull requests in CodeStream",
      "updated_at": "2021-11-13T21:14:26Z",
      "type": "docs",
      "external_id": "7e35f2ff4f06799fe492ffb6b2fedbb52e898b69",
      "document_type": "page",
      "popularity": 1,
      "body": "For most development teams, the final step in the development process is a pull request. Even if your team has decided to use New Relic CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, CodeStream allows you to keep all of that workflow inside your IDE. Pull request workflow There are four elements of CodeStream's pull request workflow. The following table outlines which code-hosting services are supported for each element. Feature Supported Services Create a pull request GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed, Bitbucket, Bitbucket Server Create a pull request across forks GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed Review and edit a pull request GitHub, GitHub Enterprise, GitLab, GitLab Self-Managed Display pull request comments as code annotations GitHub, GitLab, Bitbucket Create a pull request To open a pull request at any time, click the + button at the top of the CodeStream pane or the + button in the header of the Pull Requests section. You can also use a keyboard shortcut (ctlr+shift+/ p, ctrl+/ p on a Mac, and m if you're a GitLab user). CodeStream provides you with tree view, list view, and diff view options for reviewing your changes before opening the pull request. With a single click you can name the pull request based on the last commit message, the branch name, or, if you started work by selecting a ticket, the ticket title. If you have a ticket selected, you can also explicitly tie the ticket to the pull request and CodeStream will include a link to the ticket in the pull request's description. Before submitting the pull request, you can review your changes by clicking on any of the files listed below the form. To create a pull request across forks, click the compare across forks link at the top of the page and the form will update to allow you to select both the base and head repositories. You can also create a pull request from within a CodeStream feedback request. Once the feedback request has been approved, you’ll see an option to open a pull request at the top. Before you can create a pull request, make sure that any changes included in the feedback request have been committed and pushed. Also, if the feature branch you’re working on doesn’t have a remote tracking branch you’ll be given the option to set that as part of creating the pull request. When you create a pull request from a feedback request, CodeStream connects the dots between the two by adding a link to the pull request in the feedback request. Add a link to the feedback request, along with information about who did the review and when, in the description of the pull request. Review a pull request Caution The ability to review pull requests is currently not available for Bitbucket. Regardless of where the pull request was created, you can edit, review, and even merge it from within CodeStream. CodeStream brings GitHub and GitLab into your IDE, so there's zero learning curve. If you know how to work with pull requests on GitHub or GitLab, you'll know how to do it in CodeStream as well. You can edit a GitHub pull request's details, such as reviewers, assignees and labels. For a GitLab merge request, you can use edit mode (via the dropdown at the top of the page) or use the sidebar. By default, you can only add a single reviewer and a single assignee to a GitLab merge request. If your organization supports multiple reviewers and assignees, click the gear menu in the heading of the Merge Requests section of the CodeStream pane to enable this. Review the conversation and add comments with the ability to @mention your collaborators. View the changes, add comments, and submit a review. CodeStream does improve upon the GitHub/GitLab experience in a couple of important ways. On GitHub and GitLab you can only view the changes as a series of diff hunks. CodeStream provides that view as well, but if you'd prefer to see the changes in the context of the full file you can use either list view or tree view. Select the code you want to comment on and then click the Comment button (or select Comment from the context menu). When commenting, you can either add a single comment or start a review. With CodeStream, you can comment on lines of code that haven't changed. You can select any lines of code in the diff and not just those that are part of the changeset. These comments are added as a single comment to the pull request and aren't part of any review you may have in progress. All the power of GitHub pull requests and GitLab merge requests, and then some, right in your IDE. Leverage pull request comments Once the pull request has been approved and the code has been merged, that's usually the end of life for any comments in that pull request. Although there's often useful information in those comments that may have long-term value, they're rarely seen again. CodeStream gives those comments a second life by displaying them alongside the blocks of code that they refer to. To have pull request comments displayed as annotations in your codemarks, as well as in the Codemarks section of the CodeStream pane, click the gear icon in that section and check Show comments from pull requests. When you first check that box, if you haven’t already authenticated with your code-hosting service, you’ll be prompted to do so. Comments from merged PRs will appear next to the blocks of code they refer to. Comments from open PRs will also be included if you are on a relevant branch. For example, if the open PR is a request to merge the feature/some-name branch into main, you’ll see comments from that PR if you've checked out either feature/some-name or main, but not when you’re on any other branch. As the code evolves, the location of each comment is automatically updated so that it remains linked to the block of code it refers to. PR comments for a given file are updated roughly every 30 minutes, so new comments may not appear right away. You can force an update by restarting your IDE.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 467.0641,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Manage</em> pull requests in <em>CodeStream</em>",
        "sections": "<em>Manage</em> pull requests in <em>CodeStream</em>",
        "body": " of that workflow inside <em>your</em> IDE. Pull request workflow There are four elements of <em>CodeStream</em>&#x27;s pull request workflow. The following table outlines which <em>code</em>-hosting services are supported for each element. Feature Supported Services Create a pull request <em>Git</em>Hub, <em>Git</em>Hub Enterprise, <em>GitLab</em>, <em>GitLab</em> <em>Self</em>"
      },
      "id": "61744006196a67ee542f0555"
    },
    {
      "image": "https://docs.newrelic.com/static/8945e0a9c512b8638ebf8165d47aee04/69902/QS-SignUp3.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-user-guide/",
      "sections": [
        "New Relic CodeStream user guide",
        "Jump to a topic",
        "1. Install the CodeStream extension in your IDE and sign up.",
        "2. Connect your tools",
        "3. Discuss any block of code, at any time",
        "4. Get feedback on your work in progress",
        "5. Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "New Relic CodeStream user guide",
      "updated_at": "2021-11-24T04:44:31Z",
      "type": "docs",
      "external_id": "fa9af0118a8872fea89fda91482c44fb69913ea2",
      "document_type": "page",
      "popularity": 1,
      "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic CodeStream. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. 1. Install the CodeStream extension in your IDE and sign up. Install CodeStream for VS Code, Visual Studio or JetBrains. The CodeStream pane automatically appears in the sidebar for VS Code or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you're the first person from your team to join CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on GitHub, GitLab or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click on your headshot at the top of the CodeStream pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, just select the code and ask your question. Learn more about discussing code. 4. Get feedback on your work in progress Select Request Feedback from the + menu at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code!) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. 5. Create or review a pull request Look for the Pull Requests section of the CodeStream sidebar to review an open pull request. Just click on a pull request (or load one from URL) to get a complete GitHub experience right in your IDE! Note that you can create a pull request in GitHub, GitLab or Bitbucket, but support for reviewing pull requests is currently only available for GitHub (cloud or Enterprise). Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 381.74615,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic <em>CodeStream</em> user guide",
        "sections": "1. Install <em>the</em> <em>CodeStream</em> extension in <em>your</em> IDE and sign up.",
        "body": " <em>CodeStream</em> or paste in <em>your</em> invitation <em>code</em> if you were invited to a team already on <em>CodeStream</em>. Learn more about how to use <em>CodeStream</em>. 2. Connect <em>your</em> tools Create and review pull requests on <em>Git</em>Hub, <em>GitLab</em> or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share <em>code</em>"
      },
      "id": "61744137e7b9d2428b13c6a0"
    }
  ],
  "/docs/codestream/troubleshooting/jira-server-integration": [
    {
      "image": "https://docs.newrelic.com/static/b437d8747d80db2b7cc2a4ab110a8c70/c1b63/errors-ui.png",
      "url": "https://docs.newrelic.com/docs/errors-inbox/errors-inbox/",
      "sections": [
        "Error tracking with errors inbox",
        "Why it matters",
        "Set up errors inbox",
        "Monitor errors",
        "Error groups",
        "Troubleshooting: similar looking events do not group together",
        "Occurrences",
        "Sort By Filter",
        "Triage errors",
        "Errors status",
        "Error details",
        "Attributes",
        "Activity",
        "Discussions",
        "Assign errors",
        "Important",
        "Connect an inbox to Slack",
        "Connect errors inbox to CodeStream",
        "Connect an inbox to Jira"
      ],
      "published_at": "2022-01-12T05:32:17Z",
      "title": "Error tracking with errors inbox",
      "updated_at": "2022-01-07T01:44:58Z",
      "type": "docs",
      "external_id": "3dbd9bf9bda2abf4f6af60c03dc1f2168dc18f9d",
      "document_type": "page",
      "popularity": 1,
      "body": "Errors inbox is a single place to proactively detect, triage, and take action on all the errors before they impact customers. Receive alerts whenever a critical, customer-impacting error arises via your preferred communication channel, like Slack. Resolve errors faster with errors from across your stack, including all APM, browser (RUM), mobile, and serverless (AWS Lambda) data, displayed on one screen. Errors are grouped to cut down on noise, and collaborating across teams is easy with shared visibility to the same error data. Why it matters Errors inbox provides a unified error tracking experience to detect and triage errors: View and triage issues across applications and services that your team cares about for faster error resolutions. Proactive notifications with detailed error information in Slack. Error profiles to show similarities between error events and surface the root cause by analyzing attributes. Analyze errors in context of the full stack and resolve errors with precision. APM, browser, mobile, and AWS Lambda Functions errors are all captured in the same inbox. Ready to get started? Make sure you have a New Relic account. It's free, forever! Set up errors inbox To enable errors inbox, follow these steps. Afterwards, errors groups should start to appear in your inbox. From one.newrelic.com select Errors inbox from the top nav. If this is your first time accessing errors inbox, you will be prompted to select a workload in the top left. If you have no workloads set up, you will be prompted to create one before you can use errors inbox. Once you select your workload, your inbox should populate with error groups. one.newrelic.com > More > Errors inbox Monitor errors Once you've set up your errors inbox, you can begin proactively monitoring all errors in your stack: Error groups Error groups are sets of events that make up a unique error. Error groups are stored long term and contain metrics, activity log, discussions, and basic information about the unique error. Error groups are tied to the entity, so making a change to the state of an error group in one errors inbox will impact all other inboxes that contain that entity. How error groups work Error events get grouped into an error group when they share the same fingerprint. As events are ingested by New Relic, we run the events through a set of managed rules that output a fingerprint. Every unique fingerprint has a single error group associated with it. The New Relic managed rules normalize the error data, identifying and ignoring unique values such as UUIDs, hex values, email addresses, etc. that would cause grouping “like” errors into unique groups. NR account ID, entity ID, error class, error message, stack trace and exception are all data that can impact a fingerprint. Troubleshooting: similar looking events do not group together If you see “like” error events grouped into different error groups incorrectly, try removing the unique identifier from the error class or message and store those as attributes instead. This will allow you to more easily facet on the attribute values and reduce the number of error groups. If you have a single application reporting as multiple entities in New Relic (i.e. running in different clusters, cells, etc), you might see duplicate error groups, since our grouping logic looks at account and entity IDs as part of the fingerprinting process. You can consider rolling up the multiple entities into a single entity and including only that rolled up entity as part of your errors inbox. You can also use the feedback tool in NR1 to share error groups that could use improved grouping. We’re continually updating our rules to improve the quality of error groups. Occurrences Your errors inbox displays the total number of occurrences of each error group within the selected timeframe. The corresponding sparkline chart displays the total number of occurrences per day over the selected timeframe as you hover over it. Sort By Filter Using the dropdown in the top right, you can sort the list of grouped errors by the number of occurrences or by the error that was last seen (latest first). Triage errors Errors status Errors inbox enables you to triage error groups directly from the main screen or from the error details page. Triaging helps remove the noise from your errors inbox, and lets you focus on the high impact errors that need attention. You can set one of three statuses, and filter your inbox by status. Unresolved: This is the default status of error groups. Resolved: Setting an error as resolved will hide it from the default inbox view unless filters are updated to include resolved errors. If events matching the error group fingerprint occur after marking an error group as resolved, it will automatically reset the status to Unresolved. This can be useful for identifying regressions. Ignored: Ignored will hide the error group from the inbox view unless filters are updated to include ignored errors, or until you stop ignoring the error group. Error details Clicking on a specific error group takes you to the error details page, where you will find full context of the issue. This context can assist in triaging the error and assigning it to the correct team or individual. Occurrences The Occurrences tab includes details like: Related account Stack trace Logs in context Error attributes Number and frequency of occurrences The detailed view also allows you to view specific errors. In the top right, you can navigate between the first instance of the error, the last, and any instance in-between. Attributes The Attributes tab enables you to quickly find commonalities between the related errors for faster resolution. Click on a specific attribute to open a sidebar with specific details. Activity The Activity tab displays a log of the status changes and user assignments of an error group. Discussions The Discussions tab provides room for detailed and organized collaboration. This is key to looping in collaborators and ensuring the entire team has the same context regardless of where they sit. Discussions includes: Threaded conversations: Reply directly to top level comments to tie replies to specific posts. Comment deletion: Delete comments. The content of the post will be removed unless it is the parent of a thread, in which case the box will remain with the message “Comment deleted by user.” Markdown support: Add styling and links to your comments in Markdown. Assign errors You can assign an error group to anyone. Simply select the user from the assign dropdown menu. You may also assign an error to any email address, even if they aren’t a New Relic user. You can update the filter in errors inbox to show only errors assigned to yourself, or a teammate. Important Currently assigning an error group to a user does not send a notification. Notifications of assignment and changes to error groups will be coming soon. Connect an inbox to Slack When connected to Slack, new and resurfaced error groups will be sent to a Slack channel within seconds of them occurring. This enables your team to quickly identify any new errors or regressions, and resolve them quickly with direct links to the stack trace. This short video shows how it works (1:24 minutes): To connect an inbox to Slack: If your Slack workspace does not have the New Relic app installed, do that first. From an inbox, select the Inbox Settings icon (looks like a gear) in the top right corner. Toggle the Slack button to on if it is off. If no workspaces are available, click the plus button to enable Slack with a one click Slack authentication. Once authenticated, you will be able to select a Workspace and specific Channel to send notifications to. Click Test to ensure messages are being sent to the right channel. Connect errors inbox to CodeStream To use CodeStream's Open in IDE integration with your APM stack trace errors, use environment variables to configure your APM agent with your application's commit sha and/or your release tag associated with the running version of your software. Once set up, you can jump from an error group directly to the offending code in your IDE by clicking the Open in IDE button. Learn more here. Connect an inbox to Jira Connect errors inbox to Jira to easily create tickets for your errors, allowing for faster collaboration and resolution. Jira templates allow you to quickly create a ticket containing error details and links directly to the stack trace and APM for quick access and resolution. We store a link to the the ticket alongside the associated error group for a period of time. If the error occurs again within that period, you can easily access associated tickets. Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. To connect an inbox to Jira: Click on the Jira integration icon on the far right side of the error group you want to connect to Jira. Clicking the Jira integration icon allows you to create a ticket based on a template, or create a template if you don't already have one. If you don’t already have a connection to Jira set up in your account, click Add JIRA Workspace from the dropdown. Fill in all the fields and click Test connection before saving to ensure that your details are correct. Next, set up a template. Templates determine what information will be sent to Jira. Find more information about specific fields here. Errors inbox does not currently support two-way communication with Jira, but you can select this option in case it is supported in the future. Once you have a template, click Send test notification to preview what the ticket looks like in Jira. If the preview looks good, click Update message to save the template. Note that a test notification will create a Jira ticket in your Jira workspace. Now your team can create Jira tickets by clicking the Jira integration icon on the far right side of the error group and selecting a template. Jira settings are associated with the account that owns the error group or entity. If you are using the cross-account errors inbox, you will need to set up a Jira connection multiple times.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.73288,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Connect</em> errors inbox to <em>CodeStream</em>",
        "body": " inbox to <em>CodeStream</em> To use <em>CodeStream</em>&#x27;s Open in IDE integration with your APM stack trace errors, use environment variables to <em>configure</em> your APM agent with your application&#x27;s commit sha and&#x2F;or your release tag associated with the running version of your software. Once set up, you can jump from"
      },
      "id": "6174112928ccbc230ac6a4be"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.08716,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": " that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that <em>CodeStream</em> can help make sure you&#x27;re looking at the right version of the <em>code</em>. To <em>configure</em> a git reference set the environment variables for your APM agent. Even"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://developer.newrelic.com/static/31a54fffa55465d7c2b36f21218a43d6/0086b/filters-pane.png",
      "url": "https://developer.newrelic.com/automate-workflows/error-inbox/manage-errors/",
      "sections": [
        "Manage your triaged errors",
        "lab",
        "View triaged errors",
        "Tip",
        "Optional: Integrate Errors Inbox with Slack, Jira, and CodeStream",
        "Summary",
        "Homework"
      ],
      "published_at": "2022-01-12T01:51:54Z",
      "title": "Manage your triaged errors",
      "updated_at": "2021-12-19T01:46:41Z",
      "type": "developer",
      "external_id": "fb7cac4eab154359e99dfdfb32af2536f217b82b",
      "document_type": "page",
      "popularity": 1,
      "info": "Managed your triaged errors in Errors Inbox",
      "body": "lab This procedure is part of a lab that teaches you how to manage errors using Errors Inbox. Each procedure in the lab builds upon the last, so make sure you've triaged your errors before starting this one. You're now observing Geek's Movie Shop's errors in Errors Inbox, and you're trying to debug your application before pushing your site live. With your errors triaged, you can track their progress, look at who's working on a bug, or even create tasks in Jira to resolve them. View triaged errors Change the filter in Errors Inbox to view your triaged errors Step 1 of 3 In Errors Inbox, find the filter pane below the top navigation bar. Step 2 of 3 Click Unresolved to change the filter value. Here, you see three options in the dropdown: Resolved Unresolved Ignored Step 3 of 3 Select Resolved. Errors Inbox now shows you all your resolved error groups. If you only resolved pika.exceptions:ChannelWrongStateError, you don't see any resolved errors here because Errors Inbox unresolved that one when it saw another occurrence. Tip If you want to observe your ignored error groups instead of resolved ones, filter by Ignored. Optional: Integrate Errors Inbox with Slack, Jira, and CodeStream Being able to view resolved and ignored errors is useful, but you're trying to squash the bugs in your application before you deploy it to production. To help you manage this, connect your inbox to Slack, Jira, and CodeStream. Summary In this lab, you set up Errors Inbox to proactively observe and catch errors from across your stack. You analyzed the errors in full context and triaged them before they could affect your customers. You also managed your errors in Errors Inbox and integrated your inbox with Jira, CodeStream, and Slack to help you collaborate and resolve errors faster. Once you resolve your high priority errors, you'll be more confident in your production release. But Errors Inbox is helpful even when you're in production, because you'll be able to see, triage, and manage errors that come from your customers as well. Homework Now that you know how to track and triage errors using Errors Inbox, here are some other resources you can use to familiarize yourself even more with Errors Inbox. Read our documentation on Errors Inbox Read our blog Collaborate and fix errors quickly with Errors Inbox and workloads Read our blog Error Tracking Across Your Entire Stack with New Relic Errors Inbox",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.20732,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Optional: Integrate Errors Inbox with Slack, <em>Jira</em>, and <em>CodeStream</em>",
        "body": " Being able to view resolved and ignored errors is useful, but you&#x27;re trying to squash the bugs in your application before you deploy it to production. To help you manage this, <em>connect</em> your inbox to Slack, <em>Jira</em>, and <em>CodeStream</em>. Summary In this lab, you set up Errors Inbox to proactively observe"
      },
      "id": "61be8f01196a67e048eef29c"
    }
  ],
  "/docs/codestream/troubleshooting/jira-server-version": [
    {
      "image": "https://docs.newrelic.com/static/13641ba13c10755096fdee8f67741a02/f96db/ConfigureJiraServer1.png",
      "url": "https://docs.newrelic.com/docs/codestream/troubleshooting/jira-server-integration/",
      "sections": [
        "Configure the Jira Server CodeStream connection",
        "Caution",
        "Generate a public/private key pair",
        "Create an application link",
        "Set up the integration in CodeStream"
      ],
      "published_at": "2022-01-12T06:16:25Z",
      "title": "Configure the Jira Server CodeStream connection",
      "updated_at": "2021-11-13T21:23:33Z",
      "type": "docs",
      "external_id": "fa12660611b9120e3aace8e1abdfb74ff89d82be",
      "document_type": "page",
      "popularity": 1,
      "body": "Caution Recent versions of Jira Server (8.14.0 or higher) support the use of API Tokens to access the Jira Server REST API. We recommend you use API Tokens if possible to avoid the more complicated setup described here. Check your Jira Server version. You must be a Jira administrator in order to configure this integration. To determine if you have the proper permissions in order to proceed, look for the Jira settings menu (cog icon, most likely at the top-right, next to your profile and settings menu icon) and make sure there's an Applications option there. If you don't have the Settings menu, or the Applications option, then you won't be able to configure the integration. This integration requires that your Jira Server instance be at a publicly accessible URL. New Relic CodeStream can integrate with Jira Server using Atlassian’s published REST API. To enable CodeStream to integrate with your Jira Server installation, set up a CodeStream application link. This application link serves as a conduit for users to authenticate against their Jira Server account without ever having to enter their credentials in CodeStream. Jira Server uses the OAuth standard (version 1.0a) for client authorization. For reference, See Atlassian's documentation. However, you don't need to follow the full instructions on that page. The relevant instructions are duplicated and simplified here for clarity. You'll need the openssl command-line tool to generate a public/private key pair for use with the application link. Generate a public/private key pair In a terminal, use openssl to generate your public/private key pair, using these steps: Generate a 1024-bit private key: openssl genrsa -out jira_privatekey.pem 1024 Copy Create an X509 certificate: openssl req -newkey rsa:1024 -x509 -key jira_privatekey.pem -out jira_publickey.cer -days 365 Copy Enter whatever information you see fit to accompany the certificate. Extract the private key (PKCS8 format) to the jira_privatekey.pcks8 file: openssl pkcs8 -topk8 -nocrypt -in jira_privatekey.pem -out jira_privatekey.pcks8 Copy Extract the public key from the certificate to the jira_publickey.pem file: openssl x509 -pubkey -noout -in jira_publickey.cer > jira_publickey.pem Copy Create an application link Follow these steps to create your application link within Jira Server. In Jira, navigate to Jira settings (gear icon in upper-right), click Applications. Enter your administrator password, if needed. Then select Application links under Integrations, in the left sidebar. Where it says Enter the URL of the application you want to link, enter any URL you want, for example, http://example.com/. Then click Create new link. You will likely see a warning starting with: No response was received from the URL you entered. You can ignore the warning; click Continue. Fill out the form as you see here, or as you like. None of the data entered here really matters, except to make sure that Create incoming link is checked. The Application Name can be whatever name works best for you to identify the link. Then click Continue. On the next dialog, enter any unique string you want for the Consumer Key. It does not need to be secure or encoded, just something fairly easy to remember. Make a note of what you enter here; it will be needed when you go to set up the integration with Jira Server from CodeStream. For Consumer Name, you can enter anything meaningful to you, like \"CodeStream app\". The important field to fill out correctly is Public Key. Copy the full text of the contents of the jira_publickey.pem file you created in Step #1. Paste this into the Public Key field, then click Continue. The application link you created should now show like this: Set up the integration in CodeStream Now you're ready to set up the integration from CodeStream to Jira Server for your team, using the application link you created. Assuming you have signed up for CodeStream and have the extension open in your IDE: In CodeStream, go to the Integrations panel by clicking the menu next to your username in the top-left. Then click Jira Server under Issue Providers. Since you won't be using API Tokens with your Jira Server integration, click at the top where it says Click here if your organization uses a version of Jira Server older than... to configure Jira Server using the OAuth method described herein. Fill out the form: For Jira Server Base URL, enter the URL used to access your Jira Server installation as known to your internal network, in the form http(s)://host:port. For Consumer Key, use the consumer key you entered when created the application link, from Step #2, above. Then copy the full contents of the private key, in PCKS8 format, that you created in Step #1 above. The file should be called jira_privatekey.pcks8. Paste those contents into the Private Key field, then click Submit. You'll then be taken to your Jira Server instance, where you'll approve access to your account using the application link. When you are finished, return to your IDE and you should see something similar to this: Now that the integration has been set up for your organization, other users will NOT have to go through the process described above. Other users in your organization will see the integration with your Jira Server (specified by host) alongside other available integrations. Initiating this integration will only require your other users to allow the CodeStream application link to access their account, as you did in the final step.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 255.33258,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>Jira</em> <em>Server</em> <em>CodeStream</em> connection",
        "sections": "Configure the <em>Jira</em> <em>Server</em> <em>CodeStream</em> connection",
        "body": " with <em>your</em> <em>Jira</em> <em>Server</em> installation, set up a <em>CodeStream</em> application link. This application link serves as a conduit for users to authenticate against their <em>Jira</em> <em>Server</em> account without ever having to enter their credentials in <em>CodeStream</em>. <em>Jira</em> <em>Server</em> uses the OAuth standard (<em>version</em> 1.0a) for client"
      },
      "id": "617441f964441fa1775fe265"
    },
    {
      "image": "https://docs.newrelic.com/static/b437d8747d80db2b7cc2a4ab110a8c70/c1b63/errors-ui.png",
      "url": "https://docs.newrelic.com/docs/errors-inbox/errors-inbox/",
      "sections": [
        "Error tracking with errors inbox",
        "Why it matters",
        "Set up errors inbox",
        "Monitor errors",
        "Error groups",
        "Troubleshooting: similar looking events do not group together",
        "Occurrences",
        "Sort By Filter",
        "Triage errors",
        "Errors status",
        "Error details",
        "Attributes",
        "Activity",
        "Discussions",
        "Assign errors",
        "Important",
        "Connect an inbox to Slack",
        "Connect errors inbox to CodeStream",
        "Connect an inbox to Jira"
      ],
      "published_at": "2022-01-12T05:32:17Z",
      "title": "Error tracking with errors inbox",
      "updated_at": "2022-01-07T01:44:58Z",
      "type": "docs",
      "external_id": "3dbd9bf9bda2abf4f6af60c03dc1f2168dc18f9d",
      "document_type": "page",
      "popularity": 1,
      "body": "Errors inbox is a single place to proactively detect, triage, and take action on all the errors before they impact customers. Receive alerts whenever a critical, customer-impacting error arises via your preferred communication channel, like Slack. Resolve errors faster with errors from across your stack, including all APM, browser (RUM), mobile, and serverless (AWS Lambda) data, displayed on one screen. Errors are grouped to cut down on noise, and collaborating across teams is easy with shared visibility to the same error data. Why it matters Errors inbox provides a unified error tracking experience to detect and triage errors: View and triage issues across applications and services that your team cares about for faster error resolutions. Proactive notifications with detailed error information in Slack. Error profiles to show similarities between error events and surface the root cause by analyzing attributes. Analyze errors in context of the full stack and resolve errors with precision. APM, browser, mobile, and AWS Lambda Functions errors are all captured in the same inbox. Ready to get started? Make sure you have a New Relic account. It's free, forever! Set up errors inbox To enable errors inbox, follow these steps. Afterwards, errors groups should start to appear in your inbox. From one.newrelic.com select Errors inbox from the top nav. If this is your first time accessing errors inbox, you will be prompted to select a workload in the top left. If you have no workloads set up, you will be prompted to create one before you can use errors inbox. Once you select your workload, your inbox should populate with error groups. one.newrelic.com > More > Errors inbox Monitor errors Once you've set up your errors inbox, you can begin proactively monitoring all errors in your stack: Error groups Error groups are sets of events that make up a unique error. Error groups are stored long term and contain metrics, activity log, discussions, and basic information about the unique error. Error groups are tied to the entity, so making a change to the state of an error group in one errors inbox will impact all other inboxes that contain that entity. How error groups work Error events get grouped into an error group when they share the same fingerprint. As events are ingested by New Relic, we run the events through a set of managed rules that output a fingerprint. Every unique fingerprint has a single error group associated with it. The New Relic managed rules normalize the error data, identifying and ignoring unique values such as UUIDs, hex values, email addresses, etc. that would cause grouping “like” errors into unique groups. NR account ID, entity ID, error class, error message, stack trace and exception are all data that can impact a fingerprint. Troubleshooting: similar looking events do not group together If you see “like” error events grouped into different error groups incorrectly, try removing the unique identifier from the error class or message and store those as attributes instead. This will allow you to more easily facet on the attribute values and reduce the number of error groups. If you have a single application reporting as multiple entities in New Relic (i.e. running in different clusters, cells, etc), you might see duplicate error groups, since our grouping logic looks at account and entity IDs as part of the fingerprinting process. You can consider rolling up the multiple entities into a single entity and including only that rolled up entity as part of your errors inbox. You can also use the feedback tool in NR1 to share error groups that could use improved grouping. We’re continually updating our rules to improve the quality of error groups. Occurrences Your errors inbox displays the total number of occurrences of each error group within the selected timeframe. The corresponding sparkline chart displays the total number of occurrences per day over the selected timeframe as you hover over it. Sort By Filter Using the dropdown in the top right, you can sort the list of grouped errors by the number of occurrences or by the error that was last seen (latest first). Triage errors Errors status Errors inbox enables you to triage error groups directly from the main screen or from the error details page. Triaging helps remove the noise from your errors inbox, and lets you focus on the high impact errors that need attention. You can set one of three statuses, and filter your inbox by status. Unresolved: This is the default status of error groups. Resolved: Setting an error as resolved will hide it from the default inbox view unless filters are updated to include resolved errors. If events matching the error group fingerprint occur after marking an error group as resolved, it will automatically reset the status to Unresolved. This can be useful for identifying regressions. Ignored: Ignored will hide the error group from the inbox view unless filters are updated to include ignored errors, or until you stop ignoring the error group. Error details Clicking on a specific error group takes you to the error details page, where you will find full context of the issue. This context can assist in triaging the error and assigning it to the correct team or individual. Occurrences The Occurrences tab includes details like: Related account Stack trace Logs in context Error attributes Number and frequency of occurrences The detailed view also allows you to view specific errors. In the top right, you can navigate between the first instance of the error, the last, and any instance in-between. Attributes The Attributes tab enables you to quickly find commonalities between the related errors for faster resolution. Click on a specific attribute to open a sidebar with specific details. Activity The Activity tab displays a log of the status changes and user assignments of an error group. Discussions The Discussions tab provides room for detailed and organized collaboration. This is key to looping in collaborators and ensuring the entire team has the same context regardless of where they sit. Discussions includes: Threaded conversations: Reply directly to top level comments to tie replies to specific posts. Comment deletion: Delete comments. The content of the post will be removed unless it is the parent of a thread, in which case the box will remain with the message “Comment deleted by user.” Markdown support: Add styling and links to your comments in Markdown. Assign errors You can assign an error group to anyone. Simply select the user from the assign dropdown menu. You may also assign an error to any email address, even if they aren’t a New Relic user. You can update the filter in errors inbox to show only errors assigned to yourself, or a teammate. Important Currently assigning an error group to a user does not send a notification. Notifications of assignment and changes to error groups will be coming soon. Connect an inbox to Slack When connected to Slack, new and resurfaced error groups will be sent to a Slack channel within seconds of them occurring. This enables your team to quickly identify any new errors or regressions, and resolve them quickly with direct links to the stack trace. This short video shows how it works (1:24 minutes): To connect an inbox to Slack: If your Slack workspace does not have the New Relic app installed, do that first. From an inbox, select the Inbox Settings icon (looks like a gear) in the top right corner. Toggle the Slack button to on if it is off. If no workspaces are available, click the plus button to enable Slack with a one click Slack authentication. Once authenticated, you will be able to select a Workspace and specific Channel to send notifications to. Click Test to ensure messages are being sent to the right channel. Connect errors inbox to CodeStream To use CodeStream's Open in IDE integration with your APM stack trace errors, use environment variables to configure your APM agent with your application's commit sha and/or your release tag associated with the running version of your software. Once set up, you can jump from an error group directly to the offending code in your IDE by clicking the Open in IDE button. Learn more here. Connect an inbox to Jira Connect errors inbox to Jira to easily create tickets for your errors, allowing for faster collaboration and resolution. Jira templates allow you to quickly create a ticket containing error details and links directly to the stack trace and APM for quick access and resolution. We store a link to the the ticket alongside the associated error group for a period of time. If the error occurs again within that period, you can easily access associated tickets. Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. To connect an inbox to Jira: Click on the Jira integration icon on the far right side of the error group you want to connect to Jira. Clicking the Jira integration icon allows you to create a ticket based on a template, or create a template if you don't already have one. If you don’t already have a connection to Jira set up in your account, click Add JIRA Workspace from the dropdown. Fill in all the fields and click Test connection before saving to ensure that your details are correct. Next, set up a template. Templates determine what information will be sent to Jira. Find more information about specific fields here. Errors inbox does not currently support two-way communication with Jira, but you can select this option in case it is supported in the future. Once you have a template, click Send test notification to preview what the ticket looks like in Jira. If the preview looks good, click Update message to save the template. Note that a test notification will create a Jira ticket in your Jira workspace. Now your team can create Jira tickets by clicking the Jira integration icon on the far right side of the error group and selecting a template. Jira settings are associated with the account that owns the error group or entity. If you are using the cross-account errors inbox, you will need to set up a Jira connection multiple times.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 253.86418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Connect errors inbox to <em>CodeStream</em>",
        "body": " inbox to <em>CodeStream</em> To use <em>CodeStream</em>&#x27;s Open in IDE integration with <em>your</em> APM stack trace errors, use environment variables to configure <em>your</em> APM agent with <em>your</em> application&#x27;s commit sha and&#x2F;or <em>your</em> release tag associated with the running <em>version</em> of <em>your</em> software. Once set up, you can jump from"
      },
      "id": "6174112928ccbc230ac6a4be"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.96078,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": " that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with <em>your</em> errors so that <em>CodeStream</em> can help make sure you&#x27;re looking at the right <em>version</em> of the <em>code</em>. To configure a git reference set the environment variables for <em>your</em> APM agent. Even"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    }
  ],
  "/docs/codestream/troubleshooting/keychain-issues": [
    {
      "image": "https://docs.newrelic.com/static/5c1d085b14abf961ca66b96285f0c0fa/69902/QS-Integrations.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/install-codestream/",
      "sections": [
        "Install New Relic CodeStream",
        "Install CodeStream",
        "Instant Observability (I/O) quickstart",
        "Visual Studio Code",
        "Visual Studio",
        "JetBrains",
        "Connect your tools",
        "Tip",
        "Discuss any block of code, at any time",
        "Get feedback on your work in progress",
        "Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T06:15:45Z",
      "title": "Install New Relic CodeStream",
      "updated_at": "2021-11-24T09:39:10Z",
      "type": "docs",
      "external_id": "5d431c8f9a2690b64d26ac9fc173b18085153aac",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that makes it easy to discuss and review code in a more natural and contextual way. Once connected to New Relic, collaborate on your application errors directly in your IDE. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Install CodeStream You can install CodeStream for your specific IDE or install it through our Instant Observability (I/O) quickstart. Instant Observability (I/O) quickstart Install CodeStream with its Instant Observability (I/O) quickstart to connect CodeStream to your New Relic account via your user key. Visual Studio Code Download and install CodeStream for Visual Studio Code. You can also install it directly in Visual Studio Code via the extensions marketplace. Visual Studio Download and install CodeStream for Visual Studio. You can also install it directly in Visual Studio via the extensions marketplace. JetBrains Download and install CodeStream for JetBrains. You can also install it from the JetBrains plugins menu. Connect your tools Create and review pull requests on GitHub, GitLab, or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Investigate errors reported to New Relic One. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click your headshot at the top of the CodeStream pane, then click Integrations to connect all of your tools to CodeStream. Tip Once you've installed CodeStream, to connect to New Relic, you'll need your New Relic user key. Go here to learn more about finding or creating your user key. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, select the code and click the comment button to ask your question. Learn more about discussing code. Get feedback on your work in progress Click the + menu then click Request Feedback at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. Create or review a pull request In the CodeStream sidebar, look for the Pull Requests section to review an open pull request. Select a pull request (or load one from URL) to get a complete GitHub experience right in your IDE. You can create a pull request in GitHub, GitLab, or Bitbucket, but support for reviewing pull requests is currently only available for GitHub. Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 516.3155,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> New Relic <em>CodeStream</em>",
        "sections": "<em>Install</em> New Relic <em>CodeStream</em>",
        "body": " install it directly in Visual Studio via the extensions marketplace. <em>JetBrains</em> Download and install <em>CodeStream</em> for <em>JetBrains</em>. You can also install it from the <em>JetBrains</em> plugins menu. Connect your tools Create and review pull requests on GitHub, GitLab, or Bitbucket. Create issues on Jira, Trello"
      },
      "id": "6174400564441ff1025fd832"
    },
    {
      "image": "https://docs.newrelic.com/static/8945e0a9c512b8638ebf8165d47aee04/69902/QS-SignUp3.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-user-guide/",
      "sections": [
        "New Relic CodeStream user guide",
        "Jump to a topic",
        "1. Install the CodeStream extension in your IDE and sign up.",
        "2. Connect your tools",
        "3. Discuss any block of code, at any time",
        "4. Get feedback on your work in progress",
        "5. Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "New Relic CodeStream user guide",
      "updated_at": "2021-11-24T04:44:31Z",
      "type": "docs",
      "external_id": "fa9af0118a8872fea89fda91482c44fb69913ea2",
      "document_type": "page",
      "popularity": 1,
      "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic CodeStream. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. 1. Install the CodeStream extension in your IDE and sign up. Install CodeStream for VS Code, Visual Studio or JetBrains. The CodeStream pane automatically appears in the sidebar for VS Code or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you're the first person from your team to join CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on GitHub, GitLab or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click on your headshot at the top of the CodeStream pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, just select the code and ask your question. Learn more about discussing code. 4. Get feedback on your work in progress Select Request Feedback from the + menu at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code!) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. 5. Create or review a pull request Look for the Pull Requests section of the CodeStream sidebar to review an open pull request. Just click on a pull request (or load one from URL) to get a complete GitHub experience right in your IDE! Note that you can create a pull request in GitHub, GitLab or Bitbucket, but support for reviewing pull requests is currently only available for GitHub (cloud or Enterprise). Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 505.86414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic <em>CodeStream</em> user guide",
        "sections": "1. <em>Install</em> the <em>CodeStream</em> extension <em>in</em> your <em>IDE</em> and <em>sign</em> up.",
        "body": " in your <em>IDE</em> and <em>sign</em> up. Install <em>CodeStream</em> for VS <em>Code</em>, Visual Studio or <em>JetBrains</em>. The <em>CodeStream</em> pane automatically appears in the sidebar for VS <em>Code</em> or in a tool window at the right side for <em>JetBrains</em> or Visual Studio. Click <em>Sign</em> Up and Create a team if you&#x27;re the first person from your team to join"
      },
      "id": "61744137e7b9d2428b13c6a0"
    },
    {
      "image": "https://docs.newrelic.com/static/ec73595e0bcde8b47ae3040cc556ddd1/f96db/NotificationSettings4.png",
      "url": "https://docs.newrelic.com/docs/codestream/codestream-integrations/notifications/",
      "sections": [
        "CodeStream notifications",
        "Notification settings",
        "Follow or unfollow",
        "Desktop notifications",
        "Other notifications"
      ],
      "published_at": "2022-01-12T08:54:27Z",
      "title": "CodeStream notifications",
      "updated_at": "2021-11-13T21:01:16Z",
      "type": "docs",
      "external_id": "0af3b7458032e1b89f8e3cc7225d7c7b0272354a",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream will notify you about comments, issues, and feedback requests that you follow, and you can choose whether you want to be notified via email, desktop (for the VS Code and JetBrains extensions only), or both. Look for the notifications option under your user profile menu at the top of the CodeStream pane. Notification settings By default, you're set to automatically follow any comment, issue, or feedback request that you created, where you’ve been mentioned (either in the original post or in a subsequent reply), or to which you’ve replied. You can always choose to follow or unfollow any individual comment, issue, or feedback request via its ellipses menu. Follow or unfollow Email notifications are sent immediately. You can participate in the discussion by replying to the email. Your reply will get added to CodeStream as a reply to the appropriate comment, issue, or feedback request. Be sure that when you reply you're doing so from the same email address where the notification was sent (that is, your email address listed in My Organization on CodeStream). You can unfollow a codemark or code review by clicking the link at the bottom of the email. Desktop notifications If you’re using CodeStream in VS Code or a JetBrains IDE you can also receive desktop notifications in the IDE for comment, issue, or feedback requests that you follow. Click the Open button to open the discussion so that you can participate. Other notifications CodeStream offers the following notifications that can be turned on and off via the checkboxes at the bottom of the page. Notify me about outstanding feedback requests: Get an email reminder about open feedback requests assigned to you that you haven't responded to in the last 24 hours. Notify me about new unreviewed commits from teammates when I pull: Any time you pull, if there are new commits from a teammate on the current branch you'll get a toast notification. Click the Review button to start reviewing your teammate's changes and provide feedback. Send me weekly emails summarizing my activity: Sent every Monday with information about you and your organization's activity for the previous week. If you've connected to GitHub or GitHub Enterprise to leverage CodeStream's pull request integration, you'll also be notified when a pull request is assigned to you or you are added as a reviewer. Click the Open button to open the pull request right in your IDE where you can review the changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 473.92456,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> notifications",
        "sections": "<em>CodeStream</em> notifications",
        "body": " notifications If you’re using <em>CodeStream</em> in VS <em>Code</em> or a <em>JetBrains</em> <em>IDE</em> you can also receive desktop notifications in the <em>IDE</em> for comment, issue, or feedback requests that you follow. Click the Open button to open the discussion so that you can participate. Other notifications <em>CodeStream</em> offers the following"
      },
      "id": "6174407564441f5c515fcf66"
    }
  ],
  "/docs/codestream/troubleshooting/proxy-support": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-12T07:33:52Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.6436,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CodeStream</em> licenses",
        "sections": "<em>CodeStream</em> licenses",
        "tags": "<em>CodeStream</em>",
        "body": "We love open-source software, and we use the following with <em>CodeStream</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we&#x27;ve chosen to use. <em>CodeStream</em> license on GitHub <em>CodeStream</em>&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-12T05:42:10Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.75592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Performance monitoring with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "image": "https://docs.newrelic.com/static/3c5d34598b67191429a2a95f8f7b1895/c1b63/error-ide.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/what-is-codestream/",
      "sections": [
        "Intro to New Relic CodeStream",
        "Preview release",
        "Discuss code just like commenting on a Google Doc",
        "Get feedback on work-in-progress with pre-PR code review",
        "Create and review pull requests",
        "Monitor your code’s performance in production",
        "See your errors and what's causing them"
      ],
      "published_at": "2022-01-12T08:22:01Z",
      "title": "Intro to New Relic CodeStream",
      "updated_at": "2021-12-15T01:41:52Z",
      "type": "docs",
      "external_id": "0b3f4199050df98161ce8c46259a8bad30269d72",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic CodeStream is a developer collaboration platform that enables your development team to discuss and review code in a natural and contextual way. CodeStream not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional knowledge that is currently being lost in Slack channels and emails. Not only that, our observability solutions take you from finding errors to fixing them, all within your IDE. A quick overview of how you can use New Relic CodeStream to discover, troubleshoot, and triage errors in your IDE. (2:27) If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. Preview release CodeStream's integration with New Relic One is a preview release limited to New Relic One accounts on our US data center, and your use is subject to the pre-release policy. (This does not apply to all other CodeStream functionality.) Discuss code just like commenting on a Google Doc Simply select a block of code and type your question or comment. Teammates can participate in the discussion right from their IDE and you can optionally share the discussion on Slack or Microsoft Teams so teammates can participate from their chat clients as well. Select some code and then click the add comment button. CodeStream turns conversation into documentation by capturing all of the discussion about your code and saving it with your code. And the real magic is that the discussions are automatically repositioned as your code changes, even across branches. All with zero effort on your part. Get feedback on work-in-progress with pre-PR code review CodeStream's lightweight feedback requests let you have someone look over your changes regardless of the current state of your repo, without the friction of committing, pushing, or issuing a pull request. Once you've made some changes to a file, in the Feedback requests section, click the + button to request feedback on that change. Your teammates can review your changes right in their IDE, with full file context, and with no need to set aside their current work to switch branches or pull the latest. Use code comments to respond to a feedback request on a change. CodeStream’s feedback requests are so easy that you can start doing them throughout the development process instead of waiting until the end. You’re a few days into a sprint and have some work stubbed out? Maybe some work that hasn’t even been committed? Request feedback on your work in progress so that you can identify and resolve issues early instead of saving those gotchas for when you need to get the code merged. Create and review pull requests For most development teams, the final step in the development process is a pull request. Even if your team has decided to use CodeStream's feedback requests as a replacement for, and not just a precursor to, your end-of-cycle PR-based code reviews, you can create and review pull requests right inside your IDE. CodeStream shows a diff view of all the files changed in a PR. Review and approve the PR as you would on GitHub. Monitor your code’s performance in production Your pursuit of software quality doesn’t end once the code has been merged. Connect CodeStream to your New Relic One account and you can either jump from an error on New Relic One into your IDE or you can discover errors in CodeStream's Observability section. Navigate the stack trace to find the offending code and collaborate with your teammates to resolve the issue. Once you've connected New Relic CodeStream to your repositories and are observing your code's performance, use the observability section to find errors and collaborate with your team on solving them. See your errors and what's causing them After you connect CodeStream and New Relic, use workloads and errors inbox to jump to the offending code in your IDE. Once you've connected CodeStream to your repositories and configured it to connect with New Relic One, you can use errors inbox to find and an error and then jump to that error in your IDE and the branch of your repository that's generating the error.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 145.09154,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Intro to New Relic <em>CodeStream</em>",
        "sections": "Intro to New Relic <em>CodeStream</em>",
        "body": "New Relic <em>CodeStream</em> is a developer collaboration platform that enables your development team to discuss and review <em>code</em> in a natural and contextual way. <em>CodeStream</em> not only makes your discussions easier, by allowing them to happen in context in your IDE, but it also preserves the institutional"
      },
      "id": "617440e3e7b9d2836c13c43c"
    }
  ],
  "/docs/data-apis/convert-to-metrics/analyze-monitor-data-trends-metrics": [
    {
      "sections": [
        "Create metrics from other data types",
        "Create a metrics rule",
        "Step 1. Create NRQL query rule",
        "Tip",
        "Step 2. Create API request",
        "Example NerdGraph API request",
        "Example NerdGraph API response",
        "Step 3. Create a metrics rule with API request",
        "Query and chart your metrics",
        "Summary metric example",
        "Count metric example",
        "Distribution metric example",
        "Troubleshooting"
      ],
      "title": "Create metrics from other data types",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Convert data to metrics"
      ],
      "external_id": "49b5860f405d53d1d9a630c2e031ab4331458005",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/convert-to-metrics/create-metrics-other-data-types/",
      "published_at": "2022-01-12T08:23:23Z",
      "updated_at": "2021-10-23T17:27:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic's metrics API service to define rules for creating metrics from your other types of data, such as events, logs, or spans. Recommendation: Before you begin, review our requirements and tips for creating rules. Create a metrics rule To create a rule for creating metrics from events, logs, or spans: Construct the metrics rule using NRQL. Construct a NerdGraph (GraphQL format) API request that contains your NRQL rule. Create the metric by making the API request. Once a metric is created, you can query and chart it using NRQL. Step 1. Create NRQL query rule The most important part of creating a metrics rule is constructing the NRQL query that defines the metric for your data from events, logs, or spans. You can create up to 10 metrics with a single NRQL query by following this procedure: Using New Relic's NRQL interface, construct a query for the metric you want to create. For example: FROM ProcessSample SELECT average(ioTotalReadBytes) WHERE nr.entityType = 'HOST' Copy Edit the query to use one of the three available metric types: summary: Use if the query's function is min, max, sum, count, or average. uniqueCount: Use if the query's function is uniqueCount. distribution: Use if the query's function is percentile or histogram. This example query uses average, so use summary: FROM ProcessSample SELECT summary (ioTotalReadBytes) WHERE nr.entityType = 'HOST' Copy This example query uses count on a non-numeric field: FROM ProcessSample SELECT count(hostname) WHERE hostname LIKE '%prod%' Copy For summary on a non-numeric field use summary(1): FROM ProcessSample SELECT summary(1) WHERE hostname LIKE '%prod%' Copy Tip For more detailed information on using these metric types in rules, see Creating metric rules: requirements and tips. Decide on the attributes you want to attach to the metric, following the limits on the cardinality of unique metric-name/attribute-value combinations. Recommendation: Run a separate query to ensure this count isn't over 50,000 for a 24-hour window. For example: FROM ProcessSample SELECT uniqueCount(awsRegion, awsAvailabilityZone, commandName) WHERE nr.entityType = 'HOST' SINCE 1 DAY AGO Copy To be able to aggregate and filter your metrics, add the attributes you want to attach to the metric using the FACET clause. For example: FROM ProcessSample SELECT summary(ioTotalReadBytes) WHERE nr.entityType = 'HOST' FACET awsRegion, awsAvailabilityZone, commandName Copy Set the name of the metric using the AS function. For example: FROM ProcessSample SELECT summary(ioTotalReadBytes) AS 'io.totalread.bytes' WHERE nr.entityType = 'HOST' FACET awsRegion, awsAvailabilityZone, commandName Copy Once your NRQL rule is complete, use it to create the API request. Step 2. Create API request After you build the NRQL rule to convert data from events, logs, or spans to metrics, continue with building the API request. You can use our NerdGraph API tool to explore the data structure and to construct and make your request. To check that the rule was created correctly, you can run a query to return that rule using its ID. For tips on querying the metrics you've created, see Query and chart your metrics. Example NerdGraph API request The following example NerdGraph API request uses the same NRQL rule from step 1. The IO Total Read Bytes Rule creates a metric named io.totalread.bytes. (The rule name can have spaces, which differs from the metric naming rules.) mutation { eventsToMetricsCreateRule(rules: { name: \"io.totalread.bytes for computeSample entities\", description:\"Created by Zach on March 27, 2019. Used by team Network.\", nrql:\"FROM ProcessSample SELECT summary(ioTotalReadBytes) AS 'io.totalread.bytes' WHERE nr.entityType = 'ComputeSample' FACET awsRegion, awsAvailabilityZone, commandName\", accountId: 123456 }) { successes { id name nrql enabled } failures { submitted { name nrql accountId } errors { reason description } } } } Copy In this request: Request elements Description mutation One of the basic API operation types. eventsToMetricsCreateRule The method being called to create a rule. rules Takes four parameters: name: The name of the rule. description: Optional. The description of the rule. We recommend you include information about who created the metric data and who will be using the data. accountId: The New Relic account ID where the events, logs, or spans live and the metrics will be created. nrql: The NRQL query that creates the rule. For more on this, see Create NRQL query. successes and submitted blocks Here you define the data returned by a successful or failed response. Available parameters for these blocks include: id (ruleId for submitted) name description nrql enabled (enabled/disabled status) accountId ruleId and accountId If a failure occurs, then the submitted ruleId and accountId will be returned along with the error reason and error description. Example NerdGraph API response Here's an example of a returned response: { \"data\": { \"eventsToMetricsCreateRule\": { \"failures\": [], \"successes\": [ { \"enabled\": true, \"id\": \"46\", \"name\": \"io.totalread.bytes for computeSample entities\", \"nrql\": \"FROM ProcessSample SELECT summary(ioTotalReadBytes) AS 'io.totalread.bytes' WHERE nr.entityType = 'ComputeSample' FACET awsRegion, awsAvailabilityZone, commandName\" } ] } } } Copy Step 3. Create a metrics rule with API request When your API request is ready, you can use the NerdGraph API to make the request, which will create the metrics. Query and chart your metrics After you create a metrics rule to convert data for your events, logs, or spans, you can view the new metric data in the New Relic UI. To view your data: Go to New Relic's NRQL query interface. Run the following query to see the name of all your metrics: SELECT uniques(metricName) FROM Metric Copy Pick the metric of interest, then run the following query to see the available attributes: SELECT * FROM Metric where metricName = 'yourMetric' Copy If you don't see expected data, follow the troubleshooting procedures. The available NRQL aggregator functions depend on the metric type you created. Here are some examples. Summary metric example If you created a summary metric type, you can use the count, sum, max, min, and average aggregator functions, as shown in the following query: SELECT count(appStartResponseTime), sum(appStartResponseTime), max(appStartResponseTime), min(appStartResponseTime), average(appStartResponseTime) FROM Metric Copy Count metric example If you created a uniqueCount metric type, you can only use the uniqueCount function, as shown in the following query: SELECT uniqueCount(playbackErrorStreamUniqueCount) * 100 / uniqueCount(streamUniqueCount) AS '% of Streams Impacted' FROM Metric Copy Distribution metric example If you created a distribution metric type, use the percentile or histogram functions, as shown in the following queries: SELECT percentile(service.responseTime, 95) FROM Metric Copy OR SELECT histogram(service.responseTime, 10, 20) FROM Metric Copy Troubleshooting If your NerdGraph call is not constructed correctly, you may receive a message like this: Cannot parse the unexpected character \"\\u201C” Copy Verify the quotes in the NerdGraph call are not smart quotes (curly quotes). Our NerdGraph API only accepts straight quotes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.20525,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>metrics</em> from other <em>data</em> types",
        "sections": "Create <em>metrics</em> from other <em>data</em> types",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " the NerdGraph API to make the request, which will create the <em>metrics</em>. Query and chart your <em>metrics</em> After you create a <em>metrics</em> rule to <em>convert</em> <em>data</em> for your events, logs, or spans, you can view the new <em>metric</em> <em>data</em> in the New Relic UI. To view your <em>data</em>: Go to New Relic&#x27;s NRQL query interface. Run the following"
      },
      "id": "603ebfc8196a67cab0a83d96"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.7626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report <em>metrics</em> via the <em>Metric</em> API",
        "sections": "Report <em>metrics</em> via the <em>Metric</em> API",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the <em>Metric</em> API to send custom <em>metrics</em> to the New Relic platform. This document includes a quick start to send your first custom <em>metric</em>, plus detailed information on how to format and send your <em>metric</em> <em>data</em>. Quick start: Send <em>metric</em> <em>data</em> We report the <em>metric</em> types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.56308,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    }
  ],
  "/docs/data-apis/convert-to-metrics/create-metrics-other-data-types": [
    {
      "sections": [
        "Analyze and monitor data trends with metrics",
        "Why create metrics from other data types?",
        "Available operations",
        "Mutations",
        "Create a rule",
        "Delete a rule",
        "Important",
        "Enable or disable a rule",
        "Queries",
        "List all rules for a New Relic account",
        "List rule by rule ID",
        "Use the NerdGraph GraphiQL API tool"
      ],
      "title": "Analyze and monitor data trends with metrics",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Convert data to metrics"
      ],
      "external_id": "06073bcfec2679ac1bc402dfe305426bbd9e2182",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/convert-to-metrics/analyze-monitor-data-trends-metrics/",
      "published_at": "2022-01-12T06:17:06Z",
      "updated_at": "2021-10-23T17:28:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can generate metric-type data from other types of data in New Relic, including events, logs, and spans. Metrics are aggregates of your data and are optimal for analyzing and monitoring trends over long time periods. This document explains: Reasons to use this feature Available operations How to use our NerdGraph API tool to perform operations Why create metrics from other data types? Using metrics allows for more efficient data storage. This in turn allows you to query your data and build charts more easily. The difference between metrics and other types of data in New Relic is based on time. For more information, see Understand data types. Events, logs, spans: These types of data represent a single record at a specific moment in time. For example, you may have an event for every request to the system. This data is ideal for in-depth troubleshooting and analysis. Metrics: These provide an aggregated view of your events, logs, or spans. Metrics are better for showing trends over longer time ranges. For example, you can aggregate the total number of requests per service to one metric and then examine this information month over month. Why use metrics? Comments Flexibility Metrics are dimensional. You can choose what metadata (like host name or app name) is attached to them. Common metric measurements, like average, sum, minimum, and maximum, are already calculated. Data aggregation and retention The data has already been pre-aggregated into longer-period time buckets. Data retention is 13 months. Query capabilities You can query using the Metric data type. When you create metrics, this does not delete your events or other types of data. However, metrics are better for longer-range querying and charting. To get started converting your data to metrics, create a rule. Available operations To show, create, and delete rules for generating metrics from events, logs, or spans, use NerdGraph, our GraphQL-format API. Before performing any operation, we recommend reading Intro to NerdGraph and exploring your data with the GraphiQL API tool. These operations fall under two basic request types: Mutations, which are operations that make changes to existing rules or settings (for example, creating a new metrics rule). Queries, for fetching existing data (for example, fetching existing metrics rules). All operations are role-based in NerdGraph as the currently logged-in New Relic user. Mutations Mutation operations for events to metrics, logs to metrics, or spans to metrics include: Create a rule See Create metrics. Delete a rule Important This operation modifies production settings, so we recommend thoroughly reviewing your changes before you run the operation. To delete a rule, you need the rule ID and the New Relic account ID. Example request: mutation { eventsToMetricsDeleteRule(deletes: {ruleId: \"12\", accountId: 123456}) { successes { id name nrql } failures { errors { description reason } submitted { ruleId accountId } } } } Copy In this request: Element Description mutation One of the basic API operation types. eventsToMetricsDeleteRule The method being called to delete a rule. deletes This takes two parameters: ruleId: The ID of the rule for events to metrics, logs to metrics, or spans to metrics. accountId: The New Relic account ID. successes and submitted blocks Here you define the data returned by a success or failure. Available parameters for these blocks: id (or ruleId for submitted) name description nrql enabled accountId Example response for the request: { \"data\": { \"eventsToMetricsDeleteRule\": { \"failures\": [], \"successes\": [ { \"id\": \"12\", \"name\": \"Test Rule\", \"nrql\": \"select summary(duration) as 'server.responseTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } Copy Enable or disable a rule Important This operation modifies production settings, so we recommend thoroughly reviewing your changes before you run the operation. To enable or disable an existing rule for events to metrics, logs to metrics, or spans to metrics, use the same eventsToMetricsUpdateRule operation. The only difference is whether enabled is set to true or false. Example request to enable an existing metrics rule: mutation { eventsToMetricsUpdateRule(updates: {ruleId: \"12\", accountId: 123456, enabled: true}) { successes { id name nrql } failures { errors { description reason } submitted { ruleId accountId } } } } Copy In this request: Element Description mutation One of the basic API operation types. eventsToMetricsUpdateRule The method being called to update an existing rule and either enable it or disable it. updates This takes three required parameters: ruleId: The ID of the rule for events to metrics, logs to metrics, or spans to metrics. accountId: The New Relic account ID. enabled: To enable a disabled rule, set this to true. To disable a rule, set this to false. successes and submitted blocks Here you define the data returned by a success or failure. Available parameters for these blocks: id (or ruleId for submitted) name description nrql enabled accountId Queries Query operations include: List all rules for a New Relic account You can list all rules in a New Relic account or return a specific rule. Example listing all rules for account 123456: query { actor { account(id:123456) { eventsToMetrics{ allRules{ rules{ id name enabled nrql description } } } } } } Copy In this request: Element Description query One of the basic API operation types. Used to query but not make changes. actor This specifies the current New Relic user. account(id: 123456) Specify the ID for the New Relic account where to retrieve data. eventsToMetrics Scope the data only for events-to-metrics, logs-to-metrics, or spans-to-metrics rules. allRules Returns all rules for that account. rules In the rules block, you can define what data you want returned. Available fields include: id name description nrql accountId enabled Example response: { \"data\": { \"actor\": { \"account\": { \"eventsToMetrics\": { \"allRules\": { \"rules\": [ { \"description\": \"Metric for total time\", \"enabled\": true, \"id\": \"1\", \"name\": \"Total Time Tx\", \"nrql\": \"select summary(totalTime) as 'server.totalTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" }, { \"description\": \"Metric for duration\", \"enabled\": true, \"id\": \"2\", \"name\": \"Duration Rule\", \"nrql\": \"select summary(duration) as 'server.responseTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } } } } Copy List rule by rule ID If you know the exact ID for a rule, then you can query for a specific rule. For example, you may have just created a rule and now you want to list its contents so you can review it. Example listing rule 36 for New Relic account 123456: query { actor { account(id: 123456) { eventsToMetrics { rulesById(ruleIds: \"36\") { rules { id name enabled nrql description accountId } } } } } } Copy For more details about the elements in this query, see List all rules. Example response: { \"data\": { \"actor\": { \"account\": { \"eventsToMetrics\": { \"rulesById\": { \"rules\": [ { \"accountId\": 123456, \"description\": \"Metric for total time\", \"enabled\": true, \"id\": \"36\", \"name\": \"Total Time Tx\", \"nrql\": \"select summary(totalTime) as 'server.totalTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } } } } Copy Use the NerdGraph GraphiQL API tool You can use our GraphiQL tool to explore the data structure. You can also use it to build and run the operations to convert events, logs, and spans to metrics. To use this tool: Create the metrics operation's request with the required parameters. Go to api.newrelic.com/graphiql, and paste your query into the box. To execute the operation, press Play. Or, to get the cURL format, select Copy as cURL.) Validate the response in the response box. Optional: To verify that your rule-creation operation was performed successfully, run a list query for that rule ID.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.20544,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Analyze <em>and</em> monitor <em>data</em> trends with <em>metrics</em>",
        "sections": "Analyze <em>and</em> monitor <em>data</em> trends with <em>metrics</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "You can generate <em>metric</em>-type <em>data</em> from other types of <em>data</em> in New Relic, including events, logs, and spans. <em>Metrics</em> are aggregates of your <em>data</em> and are optimal for analyzing and monitoring trends over long time periods. This document explains: Reasons to use this feature Available operations How"
      },
      "id": "603eb239e7b9d2b99d2a07bb"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.7626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report <em>metrics</em> via the <em>Metric</em> API",
        "sections": "Report <em>metrics</em> via the <em>Metric</em> API",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the <em>Metric</em> API to send custom <em>metrics</em> to the New Relic platform. This document includes a quick start to send your first custom <em>metric</em>, plus detailed information on how to format and send your <em>metric</em> <em>data</em>. Quick start: Send <em>metric</em> <em>data</em> We report the <em>metric</em> types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.56308,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    }
  ],
  "/docs/data-apis/convert-to-metrics/creating-metric-rules-requirements-tips": [
    {
      "sections": [
        "Analyze and monitor data trends with metrics",
        "Why create metrics from other data types?",
        "Available operations",
        "Mutations",
        "Create a rule",
        "Delete a rule",
        "Important",
        "Enable or disable a rule",
        "Queries",
        "List all rules for a New Relic account",
        "List rule by rule ID",
        "Use the NerdGraph GraphiQL API tool"
      ],
      "title": "Analyze and monitor data trends with metrics",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Convert data to metrics"
      ],
      "external_id": "06073bcfec2679ac1bc402dfe305426bbd9e2182",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/convert-to-metrics/analyze-monitor-data-trends-metrics/",
      "published_at": "2022-01-12T06:17:06Z",
      "updated_at": "2021-10-23T17:28:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can generate metric-type data from other types of data in New Relic, including events, logs, and spans. Metrics are aggregates of your data and are optimal for analyzing and monitoring trends over long time periods. This document explains: Reasons to use this feature Available operations How to use our NerdGraph API tool to perform operations Why create metrics from other data types? Using metrics allows for more efficient data storage. This in turn allows you to query your data and build charts more easily. The difference between metrics and other types of data in New Relic is based on time. For more information, see Understand data types. Events, logs, spans: These types of data represent a single record at a specific moment in time. For example, you may have an event for every request to the system. This data is ideal for in-depth troubleshooting and analysis. Metrics: These provide an aggregated view of your events, logs, or spans. Metrics are better for showing trends over longer time ranges. For example, you can aggregate the total number of requests per service to one metric and then examine this information month over month. Why use metrics? Comments Flexibility Metrics are dimensional. You can choose what metadata (like host name or app name) is attached to them. Common metric measurements, like average, sum, minimum, and maximum, are already calculated. Data aggregation and retention The data has already been pre-aggregated into longer-period time buckets. Data retention is 13 months. Query capabilities You can query using the Metric data type. When you create metrics, this does not delete your events or other types of data. However, metrics are better for longer-range querying and charting. To get started converting your data to metrics, create a rule. Available operations To show, create, and delete rules for generating metrics from events, logs, or spans, use NerdGraph, our GraphQL-format API. Before performing any operation, we recommend reading Intro to NerdGraph and exploring your data with the GraphiQL API tool. These operations fall under two basic request types: Mutations, which are operations that make changes to existing rules or settings (for example, creating a new metrics rule). Queries, for fetching existing data (for example, fetching existing metrics rules). All operations are role-based in NerdGraph as the currently logged-in New Relic user. Mutations Mutation operations for events to metrics, logs to metrics, or spans to metrics include: Create a rule See Create metrics. Delete a rule Important This operation modifies production settings, so we recommend thoroughly reviewing your changes before you run the operation. To delete a rule, you need the rule ID and the New Relic account ID. Example request: mutation { eventsToMetricsDeleteRule(deletes: {ruleId: \"12\", accountId: 123456}) { successes { id name nrql } failures { errors { description reason } submitted { ruleId accountId } } } } Copy In this request: Element Description mutation One of the basic API operation types. eventsToMetricsDeleteRule The method being called to delete a rule. deletes This takes two parameters: ruleId: The ID of the rule for events to metrics, logs to metrics, or spans to metrics. accountId: The New Relic account ID. successes and submitted blocks Here you define the data returned by a success or failure. Available parameters for these blocks: id (or ruleId for submitted) name description nrql enabled accountId Example response for the request: { \"data\": { \"eventsToMetricsDeleteRule\": { \"failures\": [], \"successes\": [ { \"id\": \"12\", \"name\": \"Test Rule\", \"nrql\": \"select summary(duration) as 'server.responseTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } Copy Enable or disable a rule Important This operation modifies production settings, so we recommend thoroughly reviewing your changes before you run the operation. To enable or disable an existing rule for events to metrics, logs to metrics, or spans to metrics, use the same eventsToMetricsUpdateRule operation. The only difference is whether enabled is set to true or false. Example request to enable an existing metrics rule: mutation { eventsToMetricsUpdateRule(updates: {ruleId: \"12\", accountId: 123456, enabled: true}) { successes { id name nrql } failures { errors { description reason } submitted { ruleId accountId } } } } Copy In this request: Element Description mutation One of the basic API operation types. eventsToMetricsUpdateRule The method being called to update an existing rule and either enable it or disable it. updates This takes three required parameters: ruleId: The ID of the rule for events to metrics, logs to metrics, or spans to metrics. accountId: The New Relic account ID. enabled: To enable a disabled rule, set this to true. To disable a rule, set this to false. successes and submitted blocks Here you define the data returned by a success or failure. Available parameters for these blocks: id (or ruleId for submitted) name description nrql enabled accountId Queries Query operations include: List all rules for a New Relic account You can list all rules in a New Relic account or return a specific rule. Example listing all rules for account 123456: query { actor { account(id:123456) { eventsToMetrics{ allRules{ rules{ id name enabled nrql description } } } } } } Copy In this request: Element Description query One of the basic API operation types. Used to query but not make changes. actor This specifies the current New Relic user. account(id: 123456) Specify the ID for the New Relic account where to retrieve data. eventsToMetrics Scope the data only for events-to-metrics, logs-to-metrics, or spans-to-metrics rules. allRules Returns all rules for that account. rules In the rules block, you can define what data you want returned. Available fields include: id name description nrql accountId enabled Example response: { \"data\": { \"actor\": { \"account\": { \"eventsToMetrics\": { \"allRules\": { \"rules\": [ { \"description\": \"Metric for total time\", \"enabled\": true, \"id\": \"1\", \"name\": \"Total Time Tx\", \"nrql\": \"select summary(totalTime) as 'server.totalTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" }, { \"description\": \"Metric for duration\", \"enabled\": true, \"id\": \"2\", \"name\": \"Duration Rule\", \"nrql\": \"select summary(duration) as 'server.responseTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } } } } Copy List rule by rule ID If you know the exact ID for a rule, then you can query for a specific rule. For example, you may have just created a rule and now you want to list its contents so you can review it. Example listing rule 36 for New Relic account 123456: query { actor { account(id: 123456) { eventsToMetrics { rulesById(ruleIds: \"36\") { rules { id name enabled nrql description accountId } } } } } } Copy For more details about the elements in this query, see List all rules. Example response: { \"data\": { \"actor\": { \"account\": { \"eventsToMetrics\": { \"rulesById\": { \"rules\": [ { \"accountId\": 123456, \"description\": \"Metric for total time\", \"enabled\": true, \"id\": \"36\", \"name\": \"Total Time Tx\", \"nrql\": \"select summary(totalTime) as 'server.totalTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } } } } Copy Use the NerdGraph GraphiQL API tool You can use our GraphiQL tool to explore the data structure. You can also use it to build and run the operations to convert events, logs, and spans to metrics. To use this tool: Create the metrics operation's request with the required parameters. Go to api.newrelic.com/graphiql, and paste your query into the box. To execute the operation, press Play. Or, to get the cURL format, select Copy as cURL.) Validate the response in the response box. Optional: To verify that your rule-creation operation was performed successfully, run a list query for that rule ID.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.20543,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Analyze <em>and</em> monitor <em>data</em> trends with <em>metrics</em>",
        "sections": "Analyze <em>and</em> monitor <em>data</em> trends with <em>metrics</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "You can generate <em>metric</em>-type <em>data</em> from other types of <em>data</em> in New Relic, including events, logs, and spans. <em>Metrics</em> are aggregates of your <em>data</em> and are optimal for analyzing and monitoring trends over long time periods. This document explains: Reasons to use this feature Available operations How"
      },
      "id": "603eb239e7b9d2b99d2a07bb"
    },
    {
      "sections": [
        "Create metrics from other data types",
        "Create a metrics rule",
        "Step 1. Create NRQL query rule",
        "Tip",
        "Step 2. Create API request",
        "Example NerdGraph API request",
        "Example NerdGraph API response",
        "Step 3. Create a metrics rule with API request",
        "Query and chart your metrics",
        "Summary metric example",
        "Count metric example",
        "Distribution metric example",
        "Troubleshooting"
      ],
      "title": "Create metrics from other data types",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Convert data to metrics"
      ],
      "external_id": "49b5860f405d53d1d9a630c2e031ab4331458005",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/convert-to-metrics/create-metrics-other-data-types/",
      "published_at": "2022-01-12T08:23:23Z",
      "updated_at": "2021-10-23T17:27:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic's metrics API service to define rules for creating metrics from your other types of data, such as events, logs, or spans. Recommendation: Before you begin, review our requirements and tips for creating rules. Create a metrics rule To create a rule for creating metrics from events, logs, or spans: Construct the metrics rule using NRQL. Construct a NerdGraph (GraphQL format) API request that contains your NRQL rule. Create the metric by making the API request. Once a metric is created, you can query and chart it using NRQL. Step 1. Create NRQL query rule The most important part of creating a metrics rule is constructing the NRQL query that defines the metric for your data from events, logs, or spans. You can create up to 10 metrics with a single NRQL query by following this procedure: Using New Relic's NRQL interface, construct a query for the metric you want to create. For example: FROM ProcessSample SELECT average(ioTotalReadBytes) WHERE nr.entityType = 'HOST' Copy Edit the query to use one of the three available metric types: summary: Use if the query's function is min, max, sum, count, or average. uniqueCount: Use if the query's function is uniqueCount. distribution: Use if the query's function is percentile or histogram. This example query uses average, so use summary: FROM ProcessSample SELECT summary (ioTotalReadBytes) WHERE nr.entityType = 'HOST' Copy This example query uses count on a non-numeric field: FROM ProcessSample SELECT count(hostname) WHERE hostname LIKE '%prod%' Copy For summary on a non-numeric field use summary(1): FROM ProcessSample SELECT summary(1) WHERE hostname LIKE '%prod%' Copy Tip For more detailed information on using these metric types in rules, see Creating metric rules: requirements and tips. Decide on the attributes you want to attach to the metric, following the limits on the cardinality of unique metric-name/attribute-value combinations. Recommendation: Run a separate query to ensure this count isn't over 50,000 for a 24-hour window. For example: FROM ProcessSample SELECT uniqueCount(awsRegion, awsAvailabilityZone, commandName) WHERE nr.entityType = 'HOST' SINCE 1 DAY AGO Copy To be able to aggregate and filter your metrics, add the attributes you want to attach to the metric using the FACET clause. For example: FROM ProcessSample SELECT summary(ioTotalReadBytes) WHERE nr.entityType = 'HOST' FACET awsRegion, awsAvailabilityZone, commandName Copy Set the name of the metric using the AS function. For example: FROM ProcessSample SELECT summary(ioTotalReadBytes) AS 'io.totalread.bytes' WHERE nr.entityType = 'HOST' FACET awsRegion, awsAvailabilityZone, commandName Copy Once your NRQL rule is complete, use it to create the API request. Step 2. Create API request After you build the NRQL rule to convert data from events, logs, or spans to metrics, continue with building the API request. You can use our NerdGraph API tool to explore the data structure and to construct and make your request. To check that the rule was created correctly, you can run a query to return that rule using its ID. For tips on querying the metrics you've created, see Query and chart your metrics. Example NerdGraph API request The following example NerdGraph API request uses the same NRQL rule from step 1. The IO Total Read Bytes Rule creates a metric named io.totalread.bytes. (The rule name can have spaces, which differs from the metric naming rules.) mutation { eventsToMetricsCreateRule(rules: { name: \"io.totalread.bytes for computeSample entities\", description:\"Created by Zach on March 27, 2019. Used by team Network.\", nrql:\"FROM ProcessSample SELECT summary(ioTotalReadBytes) AS 'io.totalread.bytes' WHERE nr.entityType = 'ComputeSample' FACET awsRegion, awsAvailabilityZone, commandName\", accountId: 123456 }) { successes { id name nrql enabled } failures { submitted { name nrql accountId } errors { reason description } } } } Copy In this request: Request elements Description mutation One of the basic API operation types. eventsToMetricsCreateRule The method being called to create a rule. rules Takes four parameters: name: The name of the rule. description: Optional. The description of the rule. We recommend you include information about who created the metric data and who will be using the data. accountId: The New Relic account ID where the events, logs, or spans live and the metrics will be created. nrql: The NRQL query that creates the rule. For more on this, see Create NRQL query. successes and submitted blocks Here you define the data returned by a successful or failed response. Available parameters for these blocks include: id (ruleId for submitted) name description nrql enabled (enabled/disabled status) accountId ruleId and accountId If a failure occurs, then the submitted ruleId and accountId will be returned along with the error reason and error description. Example NerdGraph API response Here's an example of a returned response: { \"data\": { \"eventsToMetricsCreateRule\": { \"failures\": [], \"successes\": [ { \"enabled\": true, \"id\": \"46\", \"name\": \"io.totalread.bytes for computeSample entities\", \"nrql\": \"FROM ProcessSample SELECT summary(ioTotalReadBytes) AS 'io.totalread.bytes' WHERE nr.entityType = 'ComputeSample' FACET awsRegion, awsAvailabilityZone, commandName\" } ] } } } Copy Step 3. Create a metrics rule with API request When your API request is ready, you can use the NerdGraph API to make the request, which will create the metrics. Query and chart your metrics After you create a metrics rule to convert data for your events, logs, or spans, you can view the new metric data in the New Relic UI. To view your data: Go to New Relic's NRQL query interface. Run the following query to see the name of all your metrics: SELECT uniques(metricName) FROM Metric Copy Pick the metric of interest, then run the following query to see the available attributes: SELECT * FROM Metric where metricName = 'yourMetric' Copy If you don't see expected data, follow the troubleshooting procedures. The available NRQL aggregator functions depend on the metric type you created. Here are some examples. Summary metric example If you created a summary metric type, you can use the count, sum, max, min, and average aggregator functions, as shown in the following query: SELECT count(appStartResponseTime), sum(appStartResponseTime), max(appStartResponseTime), min(appStartResponseTime), average(appStartResponseTime) FROM Metric Copy Count metric example If you created a uniqueCount metric type, you can only use the uniqueCount function, as shown in the following query: SELECT uniqueCount(playbackErrorStreamUniqueCount) * 100 / uniqueCount(streamUniqueCount) AS '% of Streams Impacted' FROM Metric Copy Distribution metric example If you created a distribution metric type, use the percentile or histogram functions, as shown in the following queries: SELECT percentile(service.responseTime, 95) FROM Metric Copy OR SELECT histogram(service.responseTime, 10, 20) FROM Metric Copy Troubleshooting If your NerdGraph call is not constructed correctly, you may receive a message like this: Cannot parse the unexpected character \"\\u201C” Copy Verify the quotes in the NerdGraph call are not smart quotes (curly quotes). Our NerdGraph API only accepts straight quotes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 226.20525,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>metrics</em> from other <em>data</em> types",
        "sections": "Create <em>metrics</em> from other <em>data</em> types",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " the NerdGraph API to make the request, which will create the <em>metrics</em>. Query and chart your <em>metrics</em> After you create a <em>metrics</em> rule to <em>convert</em> <em>data</em> for your events, logs, or spans, you can view the new <em>metric</em> <em>data</em> in the New Relic UI. To view your <em>data</em>: Go to New Relic&#x27;s NRQL query interface. Run the following"
      },
      "id": "603ebfc8196a67cab0a83d96"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.7625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report <em>metrics</em> via the <em>Metric</em> API",
        "sections": "Report <em>metrics</em> via the <em>Metric</em> API",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the <em>Metric</em> API to send custom <em>metrics</em> to the New Relic <em>platform</em>. This document includes a quick start to send your first custom <em>metric</em>, plus detailed information on how to format and send your <em>metric</em> <em>data</em>. Quick start: Send <em>metric</em> <em>data</em> We report the <em>metric</em> types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    }
  ],
  "/docs/data-apis/custom-data/custom-events/apm-report-custom-events-attributes": [
    {
      "sections": [
        "Report mobile monitoring custom events and attributes",
        "Create custom attributes and events",
        "Mobile event and attribute query examples",
        "Custom event example: Track purchases",
        "Tip",
        "Attribute example: Track a specific user",
        "Attribute example: Track a specific store id",
        "Custom attribute example: Track a specific action",
        "Important",
        "Size limits and restricted characters",
        "Set the time to send data",
        "Privacy considerations"
      ],
      "title": "Report mobile monitoring custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "65970eacbedb3360fd1c7394affc8cbc42f2ab0c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/report-mobile-monitoring-custom-events-attributes/",
      "published_at": "2022-01-12T02:25:05Z",
      "updated_at": "2021-10-23T21:59:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Mobile monitoring in New Relic sends some default event data from your mobile app to New Relic, such as data about interactions, sessions, crashes, and request errors. You can also create your own custom attributes and events for more detailed querying and analysis. Create custom attributes and events You can create custom session-level attributes for default mobile monitoring events using the mobile agent SDKs. For example, to record a username attribute for some part of your iOS or Android app, you would use the setAttribute API (Android | iOS). These attributes are session-related information and are shared by multiple mobile event types. You can also create entirely new custom event types and assign them their own custom attributes, using the recordCustomEvent API (Android | iOS). To help with crash analysis, you can use the SDK to create MobileBreadcrumb and MobileHandledException events. These events are available for querying and also displayed in the crash event trail UI. For more on creating custom attributes and custom events, see: Android SDK API guide iOS SDK API guide NRQL query examples MobileRequestError examples MobileRequest examples Limits and restricted characters Mobile event and attribute query examples Here are some examples of using NRQL to query your mobile app events and attributes: Custom event example: Track purchases To track purchases in your app, use recordCustomEvent to create an event type (such as \"UserAction\") and associate attributes such as \"name\" (with value \"Purchase\"), price, quantity, and SKU. Tip For performance reasons, you should limit the total number of event types to maybe one or two. The recordCustomEvent parameter eventType is meant to be used for high-level categories. For example, you might create an event typeGestures, and then create many different custom event names under the Gesture event type. Create an event on iOS: BOOL purchaseRecorded = [NewRelic recordCustomEvent:@\"UserAction\" attributes:@{@\"name\": @\"Purchase\", @\"sku\": @\"12345LPD\", @\"quantity\": @1, @\"unitPrice\": @99.99, @\"total\": @99.99}]; Copy Create an event on Android: Map<String, Object> userActionAttributes = new HashMap<String, Object>(); userActionAttributes.put(\"name\", \"Purchase\"); userActionAttributes.put(\"sku\", \"12345LPD\"); userActionAttributes.put(\"quantity\", 1); userActionAttributes.put(\"unitPrice\", 99.99); userActionAttributes.put(\"total\", 99.99); boolean userActionRecorded = NewRelic.recordCustomEvent(\"UserAction\", userActionAttributes); Copy New Relic reports a custom event of type UserAction and name Purchase, which allows you to query all purchases made in your app in the last day: SELECT * from UserAction where name = 'Purchase' since 1 day ago Copy Replace deprecated recordEvent method: As of Android agent version 5.12.0 and iOS agent version 5.12.0, use the recordCustomEvent method to create these custom events. If you have replaced the deprecated recordEvent method for your custom events, be sure to also replace its corresponding NRQL query with the new format. Look for queries used with recordEvent method, such as this: SELECT * from Mobile where category = 'Custom' and name = 'Purchase' since 1 day ago Copy Replace them with the query format used with recordCustomEvent: SELECT * from UserAction where name = 'Purchase' since 1 day ago Copy Attribute example: Track a specific user You can create a custom attribute to track a custom user identifier across the session, and then query for all that user's interactions. To add an attribute for the userId, call the setUserId method: Set the userId on iOS: BOOL userIdWasSet = [NewRelic setUserId:@\"jsmith\"]; Copy Set the userId on Android: boolean userIdWasSet = NewRelic.setUserId(\"jsmith\"); Copy With this attribute, you can use a WHERE clause to see all actions performed by that username in the last day: SELECT * from Mobile WHERE userId = 'jsmith' since 1 day ago Copy Attribute example: Track a specific store id You can create a custom attribute to track a store id across the session, and then query for all that store's interactions. To add an attribute for the storeId, call the setAttribute method: Set the storeId on iOS: BOOL attributeSet = [NewRelic setAttribute:@\"storeId\" value:@\"NY0531\"]; Copy Set the storeId on Android: boolean attributeSet = NewRelic.setAttribute(\"storeId\", \"NY0531\"); Copy With this attribute, you can use a WHERE clause to see all actions performed by that storeId in the last day: SELECT * from Mobile WHERE storeId = 'NY0531' since 1 day ago Copy Custom attribute example: Track a specific action You can use custom attributes to track the number of times that a specific action occurs in your application. For example, you can track the number of times a button was clicked or the number of times a level was completed in a game. To track completing a game level, call incrementAttribute with no value specified. This creates an attribute with a default value of 1: Create a counter on iOS: BOOL levelIncremented = [NewRelic incrementAttribute@\"level\"]; Copy Create a counter on Android: boolean levelIncremented = NewRelic.incrementAttribute(\"level\"); Copy Each subsequent call to incrementAttribute adds 1 to the attribute level: Increment a counter on iOS: levelIncremented = [NewRelic incrementAttribute@\"level\"]; Copy Increment a counter on Android: levelIncremented = NewRelic.incrementAttribute(\"level\"); Copy Important Be sure to reset the value to 0 when starting over. To reset the level back to 1 or 0, call setAttribute: Reset a counter on iOS: levelReset = [NewRelic setAttribute:@\"level\" value:@1]; Copy Reset a counter on Android: levelReset = NewRelic.setAttribute(\"level\", 1); Copy When querying, use this level attribute to filter your data. For example, if you have a username and level attribute, use the max() function to find the highest level the user had reached: SELECT max(level) from Mobile where username = 'jsmith' Copy Size limits and restricted characters Limits for custom attributes added to default mobile events: Attributes: 128 maximum String attributes: 4 KB maximum length (empty string values are not accepted) Limits for custom events: Attributes: 254 maximum per event (number includes default session attributes) String attributes: 4 KB maximum length (empty string values are not accepted) Naming syntax and rules: See Rules for custom data. Set the time to send data By default, New Relic transmits event data in any of these situations: A session has been ongoing for 600 seconds. The app session ends by backgrounding. The app crashes. If the app crashes, New Relic gathers the attributes and events for that session and sends them to Insights. (On iOS, this happens the next time the app is launched). You can then use Insights to query and analyze the event and attribute data. To set the maximum time (in seconds) that the agent will store events in memory, use the following SDK calls: iOS method: + (void) setMaxEventBufferTime:(unsigned int)seconds; Copy Android method: public static void setMaxEventBufferTime(int maxBufferTimeInSec); Copy Privacy considerations If you want to collect personal data via custom attributes, please consult with your privacy or legal teams. Be sure to follow your organization's obligations for notices and consent regulations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.87651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report mobile monitoring <em>custom</em> <em>events</em> and attributes",
        "sections": "Report mobile monitoring <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "Mobile monitoring in New Relic sends some default <em>event</em> <em>data</em> from your mobile app to New Relic, such as <em>data</em> about interactions, sessions, crashes, and request errors. You can also create your own <em>custom</em> attributes and <em>events</em> for more detailed querying and analysis. Create <em>custom</em> attributes"
      },
      "id": "609fa5cf28ccbc508d9832d3"
    },
    {
      "sections": [
        "Report browser monitoring custom events and attributes",
        "Page actions and views",
        "Prerequisites",
        "Create PageAction events",
        "Add custom attributes to PageView event",
        "Use setCustomAttribute browser API call",
        "Forward custom attributes from APM data",
        "PageAction and PageView attributes",
        "Troubleshooting"
      ],
      "title": "Report browser monitoring custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "17c97a462616f2b23ead796b62780a1ffeb3dfac",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/report-browser-monitoring-custom-events-attributes/",
      "published_at": "2022-01-12T02:21:28Z",
      "updated_at": "2021-10-23T21:59:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use browser monitoring in New Relic to add custom events and attributes. Page actions and views Use the browser API's addPageAction call to capture events, actions, route changes, or any end-user interactions with your application. The addPageAction call adds an event named PageAction that contains the action name and any custom attribute names and values you capture along with it. The PageAction event also contains any custom attributes you added to the PageView event. Add custom attributes to the PageView event so you can query or filter your data to answer more questions about your application. Prerequisites In order to report PageAction events, verify these prerequisites: Requirement Comments Agent version Your browser monitoring agent version must be 593 or higher. Client browser version To record PageAction events, the browser must support cross-domain XHRs. Max events per cycle PageAction events are reported every 30 seconds, with a maximum of 120 events per 30-second harvest cycle, per browser. After the 120-event limit is reached, additional events are not captured for that cycle. Event/attribute naming, data type, size Ensure you follow general requirements around event/attribute naming syntax, data types, and size. Create PageAction events To create a PageAction event: Ensure the browser agent is installed for your app. Call the newrelic.addPageAction function in the relevant part of your application's JavaScript. Wait a couple minutes for the application to run and report relevant PageAction events. Run a NRQL query of the PageAction event that includes the actionName attribute you used to capture the event (and any associated attributes you sent along with the action). Add custom attributes to PageView event The PageView event is a default browser-reported event. You can add custom attributes to the PageView event. Any custom attributes you add to the PageView event are also automatically added to the PageAction event. There are two ways to add custom attributes to the PageView event: Use setCustomAttribute browser API call To add a custom attribute to the PageView event via the browser agent, use the setCustomAttribute browser API call. This allows you to capture an attribute to be annotated on any PageAction event. Forward custom attributes from APM data If you added custom attributes to the APM Transaction event via an APM agent, you can forward those custom attributes to the PageView event automatically: Insert custom attributes by following the agent-specific instructions. Enable attribute forwarding in your agent configuration file: Agent Enable attribute forwarding C SDK Not supported. Go To enable attributes, add this to your config (disabled by default): cfg.BrowserMonitoring.Attributes.Enabled = true Copy Then add the attributes you want to include: cfg.BrowserMonitoring.Attributes.Include = []string{\"request.*\"} Copy Java Add the attributes.enabled option in the browser_monitoring stanza and set it to true. .NET Add the <attributes enabled=\"true\"> element as a child of the browserMonitoring element: <configuration xmlns=\"urn:newrelic-config\"> ... <browserMonitoring autoInstrument=\"true\"> ... <attributes enabled=\"true\"> ... </attributes> </browserMonitoring> ... </configuration> Copy If you are using manual browser instrumentation the attribute needs to be created before the GetBrowserTimingHeader() call. Node.js Add attributes: {enabled: true} to the browser_monitoring: { section of your app's newrelicjs configuration file. PHP Add the newrelic.browser_monitoring.attributes.enabled option and set it to true. Python Add the browser_monitoring.attributes.enabled option and set it to true. Ruby Add the browser_monitoring.attributes.enabled option and set it to true. PageAction and PageView attributes To see the default attributes of PageAction and PageView, see Browser events. Troubleshooting Here are some troubleshooting tips: Problem Comments Custom attributes missing If your custom attributes do not appear on PageView events, verify you are calling setCustomAttribute before the Load event on your page. If the custom attribute is called after the page load occurs, it will not be visible on PageView. PageAction events If your PageAction events do not appear when you query, check that your account is compatible. If your account is compatible, check that you are not using reserved attribute names or invalid values.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.87651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report browser monitoring <em>custom</em> <em>events</em> and attributes",
        "sections": "Report browser monitoring <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "You can use browser monitoring in New Relic to add <em>custom</em> <em>events</em> and attributes. Page actions and views Use the browser API&#x27;s addPageAction call to capture <em>events</em>, actions, route changes, or any end-user interactions with your application. The addPageAction call adds an <em>event</em> named PageAction"
      },
      "id": "609fa5cfe7b9d2c93dc3eb26"
    },
    {
      "sections": [
        "Data requirements and limits for custom event data",
        "General requirements",
        "Important",
        "Reserved words",
        "Event type limits"
      ],
      "title": "Data requirements and limits for custom event data",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "77720ef366038ba648a5fbf3cf34e8e48b38440a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/data-requirements-limits-custom-event-data/",
      "published_at": "2022-01-12T02:18:23Z",
      "updated_at": "2021-10-23T21:58:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains general requirements and rules for inserting and using custom events and their associated attributes. Additional requirements may apply based on the method you use. You can report custom events to New Relic in several ways, including: APM agent APIs Event API (There are additional requirements when using the Event API.) Browser monitoring agent APIs (There are additional requirements with the custom PageAction event.) Mobile monitoring SDK General requirements When reporting custom events and attributes, follow these general requirements for supported data types, naming syntax, and size: Requirement Description Payload Total maximum size or length: 1MB (10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. The Event API has additional HTTP rate limits. Attribute data types Attribute values can be either a string or a numeric integer or float. If your attribute values contain date information, define it as an unformatted Unix timestamp (in seconds or milliseconds) by using the Insights data formatter. Attribute size Maximum name size: 255 bytes. Maximum attribute value size: Custom attributes sent by the agent: 255 bytes Attributes attached to custom events sent using the Event API: 4096 characters Maximum total attributes per event: 254. Exception: If you use an APM agent API, the max is 64. Maximum total attributes per event type: 48,000. Important Charts may only display the first 255 characters of attribute values. For complete attribute values, use the JSON chart type or Query API. Naming syntax Attribute names can be a combination of alphanumeric characters, colons (:), periods (.), and underscores (_). Event types (using the eventType attribute) can be a combination of alphanumeric characters, colons (:), and underscores (_). If the name begins with anything other than an alphabetical character, enclose the name with backticks in your NRQL query. For example: FROM `0_hello` SELECT count(*) Copy Do not use words reserved for use by NRQL. Null values The database does not store any data with a null value. Reserved words Avoid using the following reserved words as names for events and attributes. Otherwise, unexpected results may occur. Important This is not a complete list. In general, avoid using MySQL-reserved words to avoid collision with future New Relic functionality. Keyword Description accountId This is a reserved attribute name. If it's included, it will be dropped during ingest. appId Value must be an integer. If it is not an integer, the attribute name and value will be dropped during ingest. eventType The event type as stored in New Relic. New Relic agents and scripts normally report this as eventType. Can be a combination of alphanumeric characters, colons (:), and underscores (_). Be sure to review the prohibited eventType values and eventType limits. Prohibited eventType values For your eventType value, avoid using: Metric, MetricRaw, and strings prefixed with Metric[0-9] (such as Metric2 or Metric1Minute). Public_ and strings prefixed with Public_. These event types are reserved for use by New Relic. Events passed in with these eventType values will be dropped. timestamp Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. It must be +/-1 day (24 hours) of the current time on the server. Log forwarding terms The following keys are reserved by the Infrastructure agent's log forwarding feature: entity.guid, log, hostname, plugin.type, fb.input. If used, they are dropped during ingest and a warning is added to the logs. NRQL syntax terms If you need to use NRQL syntax terms as attribute names, including dotted attributes, they must be enclosed in backticks; for example, `LIMIT` or `consumer.offset`. Otherwise, avoid using these reserved words: ago, and, as, auto, begin, begintime, compare, day, days, end, endtime, explain, facet, from, hour, hours, in, is, like, limit, minute, minutes, month, months, not, null, offset, or, raw, second, seconds, select, since, timeseries, until, week, weeks, where, with Event type limits The current limit for total number of eventType values is 250 per child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop data. Event types include: Default events from New Relic agents Custom events from New Relic agents Custom events from Insights custom event inserter",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.87636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Data</em> requirements and limits for <em>custom</em> <em>event</em> <em>data</em>",
        "sections": "<em>Data</em> requirements and limits for <em>custom</em> <em>event</em> <em>data</em>",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": " child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop <em>data</em>. <em>Event</em> types include: Default <em>events</em> from New Relic agents <em>Custom</em> <em>events</em> from New Relic agents <em>Custom</em> <em>events</em> from <em>Insights</em> <em>custom</em> <em>event</em> inserter"
      },
      "id": "609fa5cfe7b9d2bf16c3eb69"
    }
  ],
  "/docs/data-apis/custom-data/custom-events/collect-custom-attributes": [
    {
      "sections": [
        "Networks",
        "Tip",
        "TLS encryption",
        "User-facing domains",
        "APM agents",
        "Port 443 recommended",
        "Agent downloads",
        "Infrastructure agents",
        "Browser domains",
        "Mobile domains",
        "Synthetic monitor public locations",
        "Synthetic monitor private locations",
        "Alerts webhooks, api.newrelic.com, cloud integrations, and ticketing integrations",
        "Pixie integration",
        "OpenTelemetry"
      ],
      "title": "Networks",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "9f7555daaafae1753bf1e741a5d607e7f0f87b7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/networks/",
      "published_at": "2022-01-12T18:27:20Z",
      "updated_at": "2022-01-08T17:44:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This list is current. Networks, IPs, domains, ports, and endpoints last updated January 6, 2022. This is a list of the networks, IP addresses, domains, ports, and endpoints used by API clients or agents to communicate with New Relic. TLS is required for all domains. For information on our FedRAMP endpoints, see our FedRAMP endpoints documentation. Tip This doc describes how to ensure our agents and integrations can access New Relic's domains. To monitor the performance of your network, see Get started with Network Performance Monitoring. TLS encryption To ensure data security for our customers and to be in compliance with FedRAMP and other standards for data encryption, Transport Layer Security (TLS) is required for all domains. Our preferred protocol for all domains is TLS 1.2. For more information, see New Relic's Explorers Hub post about TLS 1.2. In addition, TLS 1.2 is required for most domains, except: APM agent connections Browser agent connections Event API For future updates to required and supported protocol versions, follow the Security Notifications tag in New Relic's Explorers Hub. User-facing domains Your browser must be able to communicate with a number of domains for New Relic One to work properly. Update your allow list to ensure New Relic can communicate with a number of integral domains that are listed in this section. Blocking domains can cause issues with individual product features or prevent pages from loading altogether. This list doesn't cover domains that New Relic connects to that can be blocked without affecting your usage of the product. It also doesn't cover Nerdpacks or other features that communicate with external services that have additional domain requirements. If your organization uses a firewall that restricts outbound traffic, follow the specific procedures for the operating system and the firewall you use to add the following domains to the allow list. Domain Description *.newrelic.com New Relic One and supporting services *.nr-assets.net Static New Relic assets *.nr-ext.net New Relic One Nerdpacks and assets *.amazonaws.com New Relic One catalog assets behind AWS S3 *.cloudfront.net Static New Relic assets behind AWS CloudFront CDN secure.gravatar.com Support for Gravatar avatars fonts.googleapis.com Support for Google Fonts fonts.gstatic.com Support for Google Fonts www.google.com Support for reCAPTCHA www.gstatic.com Support for reCAPTCHA *.nr-data.net OpenTelemetry and Pixie onenr.io New Relic One sharing permalinks APM agents To enhance network performance and data security, New Relic uses a CDN and DDoS prevention service with a large IP range. New Relic agents require your firewall to allow outgoing connections to the following networks and ports. To add the following IP connections to the allow list, follow the specific procedures for the operating system and the firewall you use. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: IP connections APM data Networks US region accounts: 162.247.240.0/22 EU region accounts: 185.221.84.0/22 Ports US region accounts: Default: TCP 443 (recommended) TCP 80 EU region accounts: Default: TCP 443 (recommended) TCP 80 Endpoints US region accounts: collector*.newrelic.com EU region accounts: collector*.eu01.nr-data.net:443 (recommended) Port 443 recommended Recommendation: Use port 443, a secured channel for encrypted HTTPS traffic. Some New Relic agents also offer port 80, an unsecured channel open to all HTTP traffic. While some agents can be configured to use both port 80 and port 443, we recommend that you choose the port 443 (default). If you have an existing configuration that uses port 80, you can update it to use port 443, the default New Relic connection. Agent downloads TLS is required for all domains. Service for download.newrelic.com is provided through Fastly and is subject to change without warning. For the most current list of public IP addresses for New Relic agent downloads, see api.fastly.com/public-ip-list. Infrastructure agents In order to report data to New Relic, our infrastructure monitoring needs outbound access to these domains, networks, and ports. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: IP connections Infrastructure data Domains infra-api.newrelic.com: Required to submit events, metrics, and inventory data. identity-api.newrelic.com: Required for entity registration (for example, a host entity). infrastructure-command-api.newrelic.com: Required to determine feature flags. Also used for gradual rollout of new capabilities. log-api.newrelic.com: Required to submit logs to a US datacenter. log-api.eu.newrelic.com: Required to submit logs to an EU datacenter. metric-api.newrelic.com: Required to submit dimensional metrics. Networks For US region accounts: 162.247.240.0/22 For EU region accounts: 185.221.84.0/22 Port 443 Domains + Port For US region accounts: infra-api.newrelic.com:443 identity-api.newrelic.com:443 infrastructure-command-api.newrelic.com:443 log-api.newrelic.com:443 metric-api.newrelic.com:443 For EU region accounts: infra-api.eu.newrelic.com:443 identity-api.eu.newrelic.com:443 infrastructure-command-api.eu.newrelic.com:443 log-api.eu.newrelic.com:443 metric-api.eu.newrelic.com:443 Proxy If your system needs a proxy to connect to this domain, use the Infrastructure proxy setting. Browser domains In addition to the IP addresses for APM agents, applications monitored by our browser agents use outgoing connections to the following domains. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: For US region accounts: bam.nr-data.net js-agent.newrelic.com For EU region accounts: eu01.nr-data.net bam.eu01.nr-data.net For more information about CDN access for the js-agent.newrelic.com file to the domain bam.nr-data.net or to one of the New Relic beacons, see Security for browser monitoring. Mobile domains In addition to the IP addresses for APM agents, applications monitored by our mobile agents use outgoing connections to the following domains. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: For US region accounts: mobile-collector.newrelic.com mobile-crash.newrelic.com mobile-symbol-upload.newrelic.com For EU region accounts: mobile-collector.eu01.nr-data.net mobile-crash.eu01.nr-data.net mobile-symbol-upload.eu01.nr-data.net Synthetic monitor public locations To configure your firewall to allow synthetic monitors to access your monitored URL, use Synthetic public minion IPs. TLS is required for all domains. Synthetic monitor private locations Synthetic private minions report to a specific endpoint based on region. To allow the private minion to access the endpoint or the static IP addresses associated with the endpoint, follow the specific procedures for the operating system and the firewall you use. These IP addresses may change in the future. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: IP connections Synthetics private location data Endpoint For US region accounts: https://synthetics-horde.nr-data.net/ For EU region accounts: https://synthetics-horde.eu01.nr-data.net/ IP addresses For US region accounts: 13.248.153.51 76.223.21.185 For EU region accounts: 185.221.86.57 185.221.86.25 Alerts webhooks, api.newrelic.com, cloud integrations, and ticketing integrations Endpoints that use api.newrelic.com (such as our GraphQL API for NerdGraph) and our New Relic-generated webhooks for alert policies use an IP address from designated network blocks for the US or European Union region. TLS is required for all addresses in these blocks. Network blocks for US region accounts: 162.247.240.0/22 Network blocks for EU region accounts: 158.177.65.64/29 159.122.103.184/29 161.156.125.32/28 These network blocks also apply to third-party ticketing integrations and New Relic cloud integrations. Pixie integration The Pixie integration runs in your Kubernetes cluster and pulls a set of curated observability data from Pixie to send it to New Relic using the OpenTelemetry line protocol. The Pixie integration requires outbound network access to the following: work.withpixie.ai:443 otlp.nr-data.net:4317 (US region accounts) otlp.eu01.nr-data.net:4317 (EU region accounts) OpenTelemetry New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. To export OTLP data to New Relic, configure the OTLP exporter to add a header ( api-key ) whose value is your account license key. And, based on your region, configure the endpoint where the exporter sends data to New Relic. See the OpenTelemetry quick start for more information. otlp.nr-data.net:4317 (US region accounts) otlp.eu01.nr-data.net:4317 (EU region accounts) Network blocks for US region accounts: 162.247.240.0/22 Network blocks for EU region accounts: 185.221.84.0/22",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 308.1178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>User</em>-facing domains",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em>",
        "body": " for <em>New</em> <em>Relic</em> One to work properly. Update your allow list to ensure <em>New</em> <em>Relic</em> can communicate with a number of integral domains that are listed in this section. Blocking domains can cause issues with individual <em>product</em> features or prevent pages from loading altogether. This list doesn&#x27;t cover domains"
      },
      "id": "603eb81364441f64a24e88b6"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-12T11:30:53Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.98447,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>New</em> <em>Relic</em> ",
        "sections": "<em>Install</em> <em>New</em> <em>Relic</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em>",
        "body": "After you sign up for a <em>New</em> <em>Relic</em> account (it&#x27;s free, forever!) and <em>install</em> any of our monitoring services, you can start working with your data. Get started quickly with our <em>New</em> <em>Relic</em> Instant Observability quickstarts. Alternatively, <em>use</em> our guided <em>install</em>. Here are links to instructions on how"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "Notification of changes to New Relic SaaS features and distributed software",
        "What we communicate",
        "Lead time for notifications",
        "EOL lead times: impact on distributed software",
        "Subject to EOL lead times:",
        "EOL lead times not applicable:",
        "Questions about EOL notifications?"
      ],
      "title": "Notification of changes to New Relic SaaS features and distributed software",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "9a6b696d32c2863811e2df52d736ed2428228a93",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/end-of-life/notification-changes-new-relic-saas-features-distributed-software/",
      "published_at": "2022-01-12T06:23:24Z",
      "updated_at": "2021-12-14T04:01:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we support our customers by continually improving our Software as a Service (SaaS) features and distributed software. Sometimes that means we introduce changes that can include moving away from existing software components and features, called end-of-life (EOL). As we make these decisions, we strive to balance the need to introduce improvements with our customer’s tolerance for change. We do our best to anticipate the impact of these changes for our customers. What we communicate We're committed to communicating about the changes that impact user experience, especially when these changes have a significant effect on workflow. In assessing impact we evaluate: Potential impact to customer workflow Feature usage information Customer feedback In communicating changes, we strive to: Give as much lead time as possible Minimize interruption Provide a documented alternative Lead time for notifications EOL changes are classified into the following categories: Transition impact Description Notification lead time Low Limited changes to customer processes or procedures are anticipated. None to 90 days Moderate Changes to functionality are limited in scope. Actions such as upgrading retired agents to supported versions may be required. Minimum of 90 days High Changes are highly impactful to customers and will require significant investment from customers to adapt. Minimum of 180 days Critical A third party dependency, security issue, or other critical risk causes an urgent need to change or remove functionality on an accelerated timeline. Notified as soon as reasonably practical EOL lead times: impact on distributed software Subject to EOL lead times: Our lead time notifications also apply to distributed software, which is software developed by New Relic that is installed on customer-owned systems. Distributed software is not cloud-based, and has a lifespan from the date of each released version. After that date, it may cease to function or stop reporting data to New Relic. We have fully open sourced many of our distributed software projects as community projects. We also participate in many community-led software development projects. If we have not open sourced our software projects, or if we have designated them as Community Plus projects, the following timeframes apply to them: Released on or after October 1, 2020: These projects will be compatible with our SaaS offering for at least two years after the date of publishing the release. Released prior to October 1, 2020: These projects will be compatible for at least three years after the date of publishing the release. All fixes and security updates are provided in the latest released version of distributed software, and we encourage our customers to keep installed software up to date. In the event a released version of software will cease normal function sooner than two (or three) years, New Relic will follow the EOL process. EOL lead times not applicable: Some of our distributed software have been open sourced, are designated as such, and are available at github.com/newrelic, including: Community projects New Relic One catalog Example code New Relic experimental Archived Our EOL policy does not apply to these projects, which is why we do not provide advance EOL notification for them. However, you can still find information and support for selected projects through: New Relic's documentation at docs.newrelic.com Project repos at github.com/newrelic Questions about EOL notifications? Share any questions or comments with us in New Relic’s Explorers Hub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 204.94855,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Notification of changes to <em>New</em> <em>Relic</em> SaaS features <em>and</em> distributed software",
        "sections": "Notification of changes to <em>New</em> <em>Relic</em> SaaS features <em>and</em> distributed software",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em>",
        "body": " that is installed on customer-owned systems. Distributed software is not cloud-based, and has a lifespan from the date of each released version. After that date, it may cease to <em>function</em> or stop reporting data to <em>New</em> <em>Relic</em>. We have fully open sourced many of our distributed software projects as community"
      },
      "id": "604454f4e7b9d261f3579a03"
    }
  ],
  "/docs/data-apis/custom-data/custom-events/data-requirements-limits-custom-event-data": [
    {
      "sections": [
        "Report mobile monitoring custom events and attributes",
        "Create custom attributes and events",
        "Mobile event and attribute query examples",
        "Custom event example: Track purchases",
        "Tip",
        "Attribute example: Track a specific user",
        "Attribute example: Track a specific store id",
        "Custom attribute example: Track a specific action",
        "Important",
        "Size limits and restricted characters",
        "Set the time to send data",
        "Privacy considerations"
      ],
      "title": "Report mobile monitoring custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "65970eacbedb3360fd1c7394affc8cbc42f2ab0c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/report-mobile-monitoring-custom-events-attributes/",
      "published_at": "2022-01-12T02:25:05Z",
      "updated_at": "2021-10-23T21:59:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Mobile monitoring in New Relic sends some default event data from your mobile app to New Relic, such as data about interactions, sessions, crashes, and request errors. You can also create your own custom attributes and events for more detailed querying and analysis. Create custom attributes and events You can create custom session-level attributes for default mobile monitoring events using the mobile agent SDKs. For example, to record a username attribute for some part of your iOS or Android app, you would use the setAttribute API (Android | iOS). These attributes are session-related information and are shared by multiple mobile event types. You can also create entirely new custom event types and assign them their own custom attributes, using the recordCustomEvent API (Android | iOS). To help with crash analysis, you can use the SDK to create MobileBreadcrumb and MobileHandledException events. These events are available for querying and also displayed in the crash event trail UI. For more on creating custom attributes and custom events, see: Android SDK API guide iOS SDK API guide NRQL query examples MobileRequestError examples MobileRequest examples Limits and restricted characters Mobile event and attribute query examples Here are some examples of using NRQL to query your mobile app events and attributes: Custom event example: Track purchases To track purchases in your app, use recordCustomEvent to create an event type (such as \"UserAction\") and associate attributes such as \"name\" (with value \"Purchase\"), price, quantity, and SKU. Tip For performance reasons, you should limit the total number of event types to maybe one or two. The recordCustomEvent parameter eventType is meant to be used for high-level categories. For example, you might create an event typeGestures, and then create many different custom event names under the Gesture event type. Create an event on iOS: BOOL purchaseRecorded = [NewRelic recordCustomEvent:@\"UserAction\" attributes:@{@\"name\": @\"Purchase\", @\"sku\": @\"12345LPD\", @\"quantity\": @1, @\"unitPrice\": @99.99, @\"total\": @99.99}]; Copy Create an event on Android: Map<String, Object> userActionAttributes = new HashMap<String, Object>(); userActionAttributes.put(\"name\", \"Purchase\"); userActionAttributes.put(\"sku\", \"12345LPD\"); userActionAttributes.put(\"quantity\", 1); userActionAttributes.put(\"unitPrice\", 99.99); userActionAttributes.put(\"total\", 99.99); boolean userActionRecorded = NewRelic.recordCustomEvent(\"UserAction\", userActionAttributes); Copy New Relic reports a custom event of type UserAction and name Purchase, which allows you to query all purchases made in your app in the last day: SELECT * from UserAction where name = 'Purchase' since 1 day ago Copy Replace deprecated recordEvent method: As of Android agent version 5.12.0 and iOS agent version 5.12.0, use the recordCustomEvent method to create these custom events. If you have replaced the deprecated recordEvent method for your custom events, be sure to also replace its corresponding NRQL query with the new format. Look for queries used with recordEvent method, such as this: SELECT * from Mobile where category = 'Custom' and name = 'Purchase' since 1 day ago Copy Replace them with the query format used with recordCustomEvent: SELECT * from UserAction where name = 'Purchase' since 1 day ago Copy Attribute example: Track a specific user You can create a custom attribute to track a custom user identifier across the session, and then query for all that user's interactions. To add an attribute for the userId, call the setUserId method: Set the userId on iOS: BOOL userIdWasSet = [NewRelic setUserId:@\"jsmith\"]; Copy Set the userId on Android: boolean userIdWasSet = NewRelic.setUserId(\"jsmith\"); Copy With this attribute, you can use a WHERE clause to see all actions performed by that username in the last day: SELECT * from Mobile WHERE userId = 'jsmith' since 1 day ago Copy Attribute example: Track a specific store id You can create a custom attribute to track a store id across the session, and then query for all that store's interactions. To add an attribute for the storeId, call the setAttribute method: Set the storeId on iOS: BOOL attributeSet = [NewRelic setAttribute:@\"storeId\" value:@\"NY0531\"]; Copy Set the storeId on Android: boolean attributeSet = NewRelic.setAttribute(\"storeId\", \"NY0531\"); Copy With this attribute, you can use a WHERE clause to see all actions performed by that storeId in the last day: SELECT * from Mobile WHERE storeId = 'NY0531' since 1 day ago Copy Custom attribute example: Track a specific action You can use custom attributes to track the number of times that a specific action occurs in your application. For example, you can track the number of times a button was clicked or the number of times a level was completed in a game. To track completing a game level, call incrementAttribute with no value specified. This creates an attribute with a default value of 1: Create a counter on iOS: BOOL levelIncremented = [NewRelic incrementAttribute@\"level\"]; Copy Create a counter on Android: boolean levelIncremented = NewRelic.incrementAttribute(\"level\"); Copy Each subsequent call to incrementAttribute adds 1 to the attribute level: Increment a counter on iOS: levelIncremented = [NewRelic incrementAttribute@\"level\"]; Copy Increment a counter on Android: levelIncremented = NewRelic.incrementAttribute(\"level\"); Copy Important Be sure to reset the value to 0 when starting over. To reset the level back to 1 or 0, call setAttribute: Reset a counter on iOS: levelReset = [NewRelic setAttribute:@\"level\" value:@1]; Copy Reset a counter on Android: levelReset = NewRelic.setAttribute(\"level\", 1); Copy When querying, use this level attribute to filter your data. For example, if you have a username and level attribute, use the max() function to find the highest level the user had reached: SELECT max(level) from Mobile where username = 'jsmith' Copy Size limits and restricted characters Limits for custom attributes added to default mobile events: Attributes: 128 maximum String attributes: 4 KB maximum length (empty string values are not accepted) Limits for custom events: Attributes: 254 maximum per event (number includes default session attributes) String attributes: 4 KB maximum length (empty string values are not accepted) Naming syntax and rules: See Rules for custom data. Set the time to send data By default, New Relic transmits event data in any of these situations: A session has been ongoing for 600 seconds. The app session ends by backgrounding. The app crashes. If the app crashes, New Relic gathers the attributes and events for that session and sends them to Insights. (On iOS, this happens the next time the app is launched). You can then use Insights to query and analyze the event and attribute data. To set the maximum time (in seconds) that the agent will store events in memory, use the following SDK calls: iOS method: + (void) setMaxEventBufferTime:(unsigned int)seconds; Copy Android method: public static void setMaxEventBufferTime(int maxBufferTimeInSec); Copy Privacy considerations If you want to collect personal data via custom attributes, please consult with your privacy or legal teams. Be sure to follow your organization's obligations for notices and consent regulations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.87651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report mobile monitoring <em>custom</em> <em>events</em> and attributes",
        "sections": "Report mobile monitoring <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "Mobile monitoring in New Relic sends some default <em>event</em> <em>data</em> from your mobile app to New Relic, such as <em>data</em> about interactions, sessions, crashes, and request errors. You can also create your own <em>custom</em> attributes and <em>events</em> for more detailed querying and analysis. Create <em>custom</em> attributes"
      },
      "id": "609fa5cf28ccbc508d9832d3"
    },
    {
      "sections": [
        "Report browser monitoring custom events and attributes",
        "Page actions and views",
        "Prerequisites",
        "Create PageAction events",
        "Add custom attributes to PageView event",
        "Use setCustomAttribute browser API call",
        "Forward custom attributes from APM data",
        "PageAction and PageView attributes",
        "Troubleshooting"
      ],
      "title": "Report browser monitoring custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "17c97a462616f2b23ead796b62780a1ffeb3dfac",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/report-browser-monitoring-custom-events-attributes/",
      "published_at": "2022-01-12T02:21:28Z",
      "updated_at": "2021-10-23T21:59:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use browser monitoring in New Relic to add custom events and attributes. Page actions and views Use the browser API's addPageAction call to capture events, actions, route changes, or any end-user interactions with your application. The addPageAction call adds an event named PageAction that contains the action name and any custom attribute names and values you capture along with it. The PageAction event also contains any custom attributes you added to the PageView event. Add custom attributes to the PageView event so you can query or filter your data to answer more questions about your application. Prerequisites In order to report PageAction events, verify these prerequisites: Requirement Comments Agent version Your browser monitoring agent version must be 593 or higher. Client browser version To record PageAction events, the browser must support cross-domain XHRs. Max events per cycle PageAction events are reported every 30 seconds, with a maximum of 120 events per 30-second harvest cycle, per browser. After the 120-event limit is reached, additional events are not captured for that cycle. Event/attribute naming, data type, size Ensure you follow general requirements around event/attribute naming syntax, data types, and size. Create PageAction events To create a PageAction event: Ensure the browser agent is installed for your app. Call the newrelic.addPageAction function in the relevant part of your application's JavaScript. Wait a couple minutes for the application to run and report relevant PageAction events. Run a NRQL query of the PageAction event that includes the actionName attribute you used to capture the event (and any associated attributes you sent along with the action). Add custom attributes to PageView event The PageView event is a default browser-reported event. You can add custom attributes to the PageView event. Any custom attributes you add to the PageView event are also automatically added to the PageAction event. There are two ways to add custom attributes to the PageView event: Use setCustomAttribute browser API call To add a custom attribute to the PageView event via the browser agent, use the setCustomAttribute browser API call. This allows you to capture an attribute to be annotated on any PageAction event. Forward custom attributes from APM data If you added custom attributes to the APM Transaction event via an APM agent, you can forward those custom attributes to the PageView event automatically: Insert custom attributes by following the agent-specific instructions. Enable attribute forwarding in your agent configuration file: Agent Enable attribute forwarding C SDK Not supported. Go To enable attributes, add this to your config (disabled by default): cfg.BrowserMonitoring.Attributes.Enabled = true Copy Then add the attributes you want to include: cfg.BrowserMonitoring.Attributes.Include = []string{\"request.*\"} Copy Java Add the attributes.enabled option in the browser_monitoring stanza and set it to true. .NET Add the <attributes enabled=\"true\"> element as a child of the browserMonitoring element: <configuration xmlns=\"urn:newrelic-config\"> ... <browserMonitoring autoInstrument=\"true\"> ... <attributes enabled=\"true\"> ... </attributes> </browserMonitoring> ... </configuration> Copy If you are using manual browser instrumentation the attribute needs to be created before the GetBrowserTimingHeader() call. Node.js Add attributes: {enabled: true} to the browser_monitoring: { section of your app's newrelicjs configuration file. PHP Add the newrelic.browser_monitoring.attributes.enabled option and set it to true. Python Add the browser_monitoring.attributes.enabled option and set it to true. Ruby Add the browser_monitoring.attributes.enabled option and set it to true. PageAction and PageView attributes To see the default attributes of PageAction and PageView, see Browser events. Troubleshooting Here are some troubleshooting tips: Problem Comments Custom attributes missing If your custom attributes do not appear on PageView events, verify you are calling setCustomAttribute before the Load event on your page. If the custom attribute is called after the page load occurs, it will not be visible on PageView. PageAction events If your PageAction events do not appear when you query, check that your account is compatible. If your account is compatible, check that you are not using reserved attribute names or invalid values.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.87651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report browser monitoring <em>custom</em> <em>events</em> and attributes",
        "sections": "Report browser monitoring <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "You can use browser monitoring in New Relic to add <em>custom</em> <em>events</em> and attributes. Page actions and views Use the browser API&#x27;s addPageAction call to capture <em>events</em>, actions, route changes, or any end-user interactions with your application. The addPageAction call adds an <em>event</em> named PageAction"
      },
      "id": "609fa5cfe7b9d2c93dc3eb26"
    },
    {
      "sections": [
        "APM: Report custom events and attributes",
        "Data considerations",
        "Tip",
        "Record custom events and attributes",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Timestamps",
        "Limits and restricted characters",
        "Reserved words"
      ],
      "title": "APM: Report custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "8731386e34fbced8d086795e273a1e2392b663ba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/apm-report-custom-events-attributes/",
      "published_at": "2022-01-12T02:14:31Z",
      "updated_at": "2021-10-23T19:43:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have APM, you can report custom event data. You can then query and visualize your data in New Relic. Data considerations New Relic agents send event data to New Relic as part of the normal harvest cycle every five seconds for agent versions supporting real time streaming. Sending a lot of events can increase the memory overhead of the agent. New Relic enforces an upper limit of 833 custom events every 5 seconds. Additionally, posts greater than 1MB (10^6 bytes) in size will not be recorded, regardless of the custom event limit. You can also send custom events using the Event API (without need for APM). However, be aware that custom events sent with the agent APIs are not compatible with high security mode. Tip For more information, check out New Relic University’s tutorial Adding custom data with the APM agent API. Or, go directly to the full online course Custom data with APM. Record custom events and attributes You can add your own custom APM events and attributes, which you can then use for querying and charting. This is one of several ways to report custom data. To record a custom event, follow the procedures for your New Relic language agent. To add custom attributes to APM events, you must first enable them for your APM agent, and then make an API call to record the attribute. Follow the agent-specific custom attribute procedures. When creating your own custom events and attributes, follow data requirements for: Size limits Attribute types Reserved words C SDK To add a custom event to apps monitored by the C SDK, start a transaction and use the newrelic_create_custom_event and newrelic_record_custom_event functions. For more information, see the Guide to using the C SDK API. You can then add custom attributes for your C SDK app. Go To add a custom event to apps monitored by the Go agent, use RecordCustomEvent. You can then add custom attributes for your Go app. Java Custom event collection is enabled by default in Java agent version 3.13.0 or higher. To send custom events, call recordCustomEvent. For example: Map<String, Object> eventAttributes = new HashMap<String, Object>(); NewRelic.getAgent().getInsights().recordCustomEvent(\"MyCustomEvent\", eventAttributes); Copy The first argument defines the name of your event type, and the second argument is a map with the attributes for your custom event. Event attributes must be strings or numbers. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can change the maximum number of events recorded by the Java agent via a configuration parameter in newrelic.yml. Specify the maximum number of events to record per minute as an integer. For example, if you want to send less than the default of 10000 events: custom_insights_events: max_samples_stored: 5000 Copy To disable custom events entirely, add the following to your newrelic.yml: custom_insights_events: enabled: false Copy You can then add custom attributes for your Java app. For Java agent versions prior to 4.1.0, use the following YAML configuration: custom_insights_events.enabled: true custom_insights_events.max_samples_stored: 5000 Copy .NET Custom event collection is enabled by default in .NET agent version 4.6.29.0 or higher. To send custom events, simply call RecordCustomEvent(). For example: var eventAttributes = new Dictionary<String, Object>(); NewRelic.Api.Agent.NewRelic.RecordCustomEvent('MyCustomEvent', eventAttributes); Copy The first argument defines the name of your event type, and the second argument is an IEnumerable with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can then add custom attributes for your .NET app. You can turn off custom events entirely by setting customEvents.enabled to false in newrelic.config. Node.js Custom event collection is enabled by default in Node.js agent version 1.15.0 or higher. To send custom events, simply call the relevant API. For example: recordCustomEvent(eventType, attributes) Copy Use recordCustomEvent to record an event-based metric, usually associated with a particular duration. The eventType must be an alphanumeric string less than 255 characters. The attributes must be an object of key and value pairs. The keys must be shorter than 255 characters, and the values must be string, number, or boolean. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can then add custom attributes for your Node.js app. To disable custom events entirely, add custom_insights_events.enabled: false to newrelic.js. PHP Custom event collection is enabled by default in PHP agent version 4.18 or higher. To send custom events, simply call the relevant API function. For example: newrelic_record_custom_event(\"WidgetSale\", array(\"color\"=>\"red\", \"weight\"=>12.5)); Copy The first argument defines the name of your event type, and the second argument is an array with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. By default, the maximum number of custom events recorded per minute is 10,000. This setting cannot be changed. You can then add custom attributes for your PHP app. To disable custom events entirely, add newrelic.custom_insights_events.enabled = false to your newrelic.ini and restart the agent. Python Custom event collection is enabled by default in Python agent version 2.60.0.46 or higher. To send custom events, simply call the relevant API. For example: newrelic.agent. record_custom_event (event_type, params, application=None) Copy The event_type defines the name (or type) of the custom event. Attributes of the custom event should be passed in as a dictionary via the params keyword argument. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For limits and restrictions on event_type and params, see our documentation about limits and restricted characters and reserved words If called outside of the context of a monitored web request or background task, the call will be ignored unless the application keyword argument is provided and an application object corresponding to the application against which the exception should be recorded is provided. A suitable application object can be obtained using the newrelic.agent.application() function. You can then add custom attributes for your Python app. To disable custom events entirely, set custom_insights_events.enabled to False in your newrelic.ini configuration file. Ruby Custom event collection is enabled by default in Ruby agent version 3.9.8.273 or higher. To send custom events, simply call the relevant API. For example: ::NewRelic::Agent.record_custom_event('WidgetSale', color: 'red', weight: 12.5) Copy The first argument defines the name of your event type, and the second argument is a hash with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can change the maximum number of events recorded by the Ruby agent via a configuration parameter in newrelic.yml: Add custom_insights_events.max_samples_stored: to your configuration file. Specify the maximum number of events to record per minute as an integer. For example, if you want to be able to send up to 5000 events per minute, add: custom_insights_events.max_samples_stored: 5000 Copy You can then add custom attributes for your Ruby app. To disable custom events entirely, add custom_insights_events.enabled: false to newrelic.yml. Timestamps You may not specify a timestamp on events that are collected and recorded via the agent. The agent will automatically assign a timestamp to events based on when they are recorded via the API. Limits and restricted characters See Custom event data requirements for size limits, data types, and naming syntax requirements. Reserved words Before creating custom attributes, review New Relic's list of reserved terms used by NRQL. Otherwise unexpected results may occur.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.85252,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM: Report <em>custom</em> <em>events</em> and attributes",
        "sections": "APM: Report <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": ". Record <em>custom</em> <em>events</em> and attributes You can add your own <em>custom</em> APM <em>events</em> and attributes, which you can then use for querying and charting. This is one of several ways to report <em>custom</em> <em>data</em>. To record a <em>custom</em> <em>event</em>, follow the procedures for your New Relic language agent. To add <em>custom</em> attributes"
      },
      "id": "609fa629e7b9d2fa8dc3eb04"
    }
  ],
  "/docs/data-apis/custom-data/custom-events/report-browser-monitoring-custom-events-attributes": [
    {
      "sections": [
        "Report mobile monitoring custom events and attributes",
        "Create custom attributes and events",
        "Mobile event and attribute query examples",
        "Custom event example: Track purchases",
        "Tip",
        "Attribute example: Track a specific user",
        "Attribute example: Track a specific store id",
        "Custom attribute example: Track a specific action",
        "Important",
        "Size limits and restricted characters",
        "Set the time to send data",
        "Privacy considerations"
      ],
      "title": "Report mobile monitoring custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "65970eacbedb3360fd1c7394affc8cbc42f2ab0c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/report-mobile-monitoring-custom-events-attributes/",
      "published_at": "2022-01-12T02:25:05Z",
      "updated_at": "2021-10-23T21:59:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Mobile monitoring in New Relic sends some default event data from your mobile app to New Relic, such as data about interactions, sessions, crashes, and request errors. You can also create your own custom attributes and events for more detailed querying and analysis. Create custom attributes and events You can create custom session-level attributes for default mobile monitoring events using the mobile agent SDKs. For example, to record a username attribute for some part of your iOS or Android app, you would use the setAttribute API (Android | iOS). These attributes are session-related information and are shared by multiple mobile event types. You can also create entirely new custom event types and assign them their own custom attributes, using the recordCustomEvent API (Android | iOS). To help with crash analysis, you can use the SDK to create MobileBreadcrumb and MobileHandledException events. These events are available for querying and also displayed in the crash event trail UI. For more on creating custom attributes and custom events, see: Android SDK API guide iOS SDK API guide NRQL query examples MobileRequestError examples MobileRequest examples Limits and restricted characters Mobile event and attribute query examples Here are some examples of using NRQL to query your mobile app events and attributes: Custom event example: Track purchases To track purchases in your app, use recordCustomEvent to create an event type (such as \"UserAction\") and associate attributes such as \"name\" (with value \"Purchase\"), price, quantity, and SKU. Tip For performance reasons, you should limit the total number of event types to maybe one or two. The recordCustomEvent parameter eventType is meant to be used for high-level categories. For example, you might create an event typeGestures, and then create many different custom event names under the Gesture event type. Create an event on iOS: BOOL purchaseRecorded = [NewRelic recordCustomEvent:@\"UserAction\" attributes:@{@\"name\": @\"Purchase\", @\"sku\": @\"12345LPD\", @\"quantity\": @1, @\"unitPrice\": @99.99, @\"total\": @99.99}]; Copy Create an event on Android: Map<String, Object> userActionAttributes = new HashMap<String, Object>(); userActionAttributes.put(\"name\", \"Purchase\"); userActionAttributes.put(\"sku\", \"12345LPD\"); userActionAttributes.put(\"quantity\", 1); userActionAttributes.put(\"unitPrice\", 99.99); userActionAttributes.put(\"total\", 99.99); boolean userActionRecorded = NewRelic.recordCustomEvent(\"UserAction\", userActionAttributes); Copy New Relic reports a custom event of type UserAction and name Purchase, which allows you to query all purchases made in your app in the last day: SELECT * from UserAction where name = 'Purchase' since 1 day ago Copy Replace deprecated recordEvent method: As of Android agent version 5.12.0 and iOS agent version 5.12.0, use the recordCustomEvent method to create these custom events. If you have replaced the deprecated recordEvent method for your custom events, be sure to also replace its corresponding NRQL query with the new format. Look for queries used with recordEvent method, such as this: SELECT * from Mobile where category = 'Custom' and name = 'Purchase' since 1 day ago Copy Replace them with the query format used with recordCustomEvent: SELECT * from UserAction where name = 'Purchase' since 1 day ago Copy Attribute example: Track a specific user You can create a custom attribute to track a custom user identifier across the session, and then query for all that user's interactions. To add an attribute for the userId, call the setUserId method: Set the userId on iOS: BOOL userIdWasSet = [NewRelic setUserId:@\"jsmith\"]; Copy Set the userId on Android: boolean userIdWasSet = NewRelic.setUserId(\"jsmith\"); Copy With this attribute, you can use a WHERE clause to see all actions performed by that username in the last day: SELECT * from Mobile WHERE userId = 'jsmith' since 1 day ago Copy Attribute example: Track a specific store id You can create a custom attribute to track a store id across the session, and then query for all that store's interactions. To add an attribute for the storeId, call the setAttribute method: Set the storeId on iOS: BOOL attributeSet = [NewRelic setAttribute:@\"storeId\" value:@\"NY0531\"]; Copy Set the storeId on Android: boolean attributeSet = NewRelic.setAttribute(\"storeId\", \"NY0531\"); Copy With this attribute, you can use a WHERE clause to see all actions performed by that storeId in the last day: SELECT * from Mobile WHERE storeId = 'NY0531' since 1 day ago Copy Custom attribute example: Track a specific action You can use custom attributes to track the number of times that a specific action occurs in your application. For example, you can track the number of times a button was clicked or the number of times a level was completed in a game. To track completing a game level, call incrementAttribute with no value specified. This creates an attribute with a default value of 1: Create a counter on iOS: BOOL levelIncremented = [NewRelic incrementAttribute@\"level\"]; Copy Create a counter on Android: boolean levelIncremented = NewRelic.incrementAttribute(\"level\"); Copy Each subsequent call to incrementAttribute adds 1 to the attribute level: Increment a counter on iOS: levelIncremented = [NewRelic incrementAttribute@\"level\"]; Copy Increment a counter on Android: levelIncremented = NewRelic.incrementAttribute(\"level\"); Copy Important Be sure to reset the value to 0 when starting over. To reset the level back to 1 or 0, call setAttribute: Reset a counter on iOS: levelReset = [NewRelic setAttribute:@\"level\" value:@1]; Copy Reset a counter on Android: levelReset = NewRelic.setAttribute(\"level\", 1); Copy When querying, use this level attribute to filter your data. For example, if you have a username and level attribute, use the max() function to find the highest level the user had reached: SELECT max(level) from Mobile where username = 'jsmith' Copy Size limits and restricted characters Limits for custom attributes added to default mobile events: Attributes: 128 maximum String attributes: 4 KB maximum length (empty string values are not accepted) Limits for custom events: Attributes: 254 maximum per event (number includes default session attributes) String attributes: 4 KB maximum length (empty string values are not accepted) Naming syntax and rules: See Rules for custom data. Set the time to send data By default, New Relic transmits event data in any of these situations: A session has been ongoing for 600 seconds. The app session ends by backgrounding. The app crashes. If the app crashes, New Relic gathers the attributes and events for that session and sends them to Insights. (On iOS, this happens the next time the app is launched). You can then use Insights to query and analyze the event and attribute data. To set the maximum time (in seconds) that the agent will store events in memory, use the following SDK calls: iOS method: + (void) setMaxEventBufferTime:(unsigned int)seconds; Copy Android method: public static void setMaxEventBufferTime(int maxBufferTimeInSec); Copy Privacy considerations If you want to collect personal data via custom attributes, please consult with your privacy or legal teams. Be sure to follow your organization's obligations for notices and consent regulations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.87651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report mobile monitoring <em>custom</em> <em>events</em> and attributes",
        "sections": "Report mobile monitoring <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "Mobile monitoring in New Relic sends some default <em>event</em> <em>data</em> from your mobile app to New Relic, such as <em>data</em> about interactions, sessions, crashes, and request errors. You can also create your own <em>custom</em> attributes and <em>events</em> for more detailed querying and analysis. Create <em>custom</em> attributes"
      },
      "id": "609fa5cf28ccbc508d9832d3"
    },
    {
      "sections": [
        "Data requirements and limits for custom event data",
        "General requirements",
        "Important",
        "Reserved words",
        "Event type limits"
      ],
      "title": "Data requirements and limits for custom event data",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "77720ef366038ba648a5fbf3cf34e8e48b38440a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/data-requirements-limits-custom-event-data/",
      "published_at": "2022-01-12T02:18:23Z",
      "updated_at": "2021-10-23T21:58:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains general requirements and rules for inserting and using custom events and their associated attributes. Additional requirements may apply based on the method you use. You can report custom events to New Relic in several ways, including: APM agent APIs Event API (There are additional requirements when using the Event API.) Browser monitoring agent APIs (There are additional requirements with the custom PageAction event.) Mobile monitoring SDK General requirements When reporting custom events and attributes, follow these general requirements for supported data types, naming syntax, and size: Requirement Description Payload Total maximum size or length: 1MB (10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. The Event API has additional HTTP rate limits. Attribute data types Attribute values can be either a string or a numeric integer or float. If your attribute values contain date information, define it as an unformatted Unix timestamp (in seconds or milliseconds) by using the Insights data formatter. Attribute size Maximum name size: 255 bytes. Maximum attribute value size: Custom attributes sent by the agent: 255 bytes Attributes attached to custom events sent using the Event API: 4096 characters Maximum total attributes per event: 254. Exception: If you use an APM agent API, the max is 64. Maximum total attributes per event type: 48,000. Important Charts may only display the first 255 characters of attribute values. For complete attribute values, use the JSON chart type or Query API. Naming syntax Attribute names can be a combination of alphanumeric characters, colons (:), periods (.), and underscores (_). Event types (using the eventType attribute) can be a combination of alphanumeric characters, colons (:), and underscores (_). If the name begins with anything other than an alphabetical character, enclose the name with backticks in your NRQL query. For example: FROM `0_hello` SELECT count(*) Copy Do not use words reserved for use by NRQL. Null values The database does not store any data with a null value. Reserved words Avoid using the following reserved words as names for events and attributes. Otherwise, unexpected results may occur. Important This is not a complete list. In general, avoid using MySQL-reserved words to avoid collision with future New Relic functionality. Keyword Description accountId This is a reserved attribute name. If it's included, it will be dropped during ingest. appId Value must be an integer. If it is not an integer, the attribute name and value will be dropped during ingest. eventType The event type as stored in New Relic. New Relic agents and scripts normally report this as eventType. Can be a combination of alphanumeric characters, colons (:), and underscores (_). Be sure to review the prohibited eventType values and eventType limits. Prohibited eventType values For your eventType value, avoid using: Metric, MetricRaw, and strings prefixed with Metric[0-9] (such as Metric2 or Metric1Minute). Public_ and strings prefixed with Public_. These event types are reserved for use by New Relic. Events passed in with these eventType values will be dropped. timestamp Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. It must be +/-1 day (24 hours) of the current time on the server. Log forwarding terms The following keys are reserved by the Infrastructure agent's log forwarding feature: entity.guid, log, hostname, plugin.type, fb.input. If used, they are dropped during ingest and a warning is added to the logs. NRQL syntax terms If you need to use NRQL syntax terms as attribute names, including dotted attributes, they must be enclosed in backticks; for example, `LIMIT` or `consumer.offset`. Otherwise, avoid using these reserved words: ago, and, as, auto, begin, begintime, compare, day, days, end, endtime, explain, facet, from, hour, hours, in, is, like, limit, minute, minutes, month, months, not, null, offset, or, raw, second, seconds, select, since, timeseries, until, week, weeks, where, with Event type limits The current limit for total number of eventType values is 250 per child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop data. Event types include: Default events from New Relic agents Custom events from New Relic agents Custom events from Insights custom event inserter",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.87634,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Data</em> requirements and limits for <em>custom</em> <em>event</em> <em>data</em>",
        "sections": "<em>Data</em> requirements and limits for <em>custom</em> <em>event</em> <em>data</em>",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": " child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop <em>data</em>. <em>Event</em> types include: Default <em>events</em> from New Relic agents <em>Custom</em> <em>events</em> from New Relic agents <em>Custom</em> <em>events</em> from <em>Insights</em> <em>custom</em> <em>event</em> inserter"
      },
      "id": "609fa5cfe7b9d2bf16c3eb69"
    },
    {
      "sections": [
        "APM: Report custom events and attributes",
        "Data considerations",
        "Tip",
        "Record custom events and attributes",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Timestamps",
        "Limits and restricted characters",
        "Reserved words"
      ],
      "title": "APM: Report custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "8731386e34fbced8d086795e273a1e2392b663ba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/apm-report-custom-events-attributes/",
      "published_at": "2022-01-12T02:14:31Z",
      "updated_at": "2021-10-23T19:43:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have APM, you can report custom event data. You can then query and visualize your data in New Relic. Data considerations New Relic agents send event data to New Relic as part of the normal harvest cycle every five seconds for agent versions supporting real time streaming. Sending a lot of events can increase the memory overhead of the agent. New Relic enforces an upper limit of 833 custom events every 5 seconds. Additionally, posts greater than 1MB (10^6 bytes) in size will not be recorded, regardless of the custom event limit. You can also send custom events using the Event API (without need for APM). However, be aware that custom events sent with the agent APIs are not compatible with high security mode. Tip For more information, check out New Relic University’s tutorial Adding custom data with the APM agent API. Or, go directly to the full online course Custom data with APM. Record custom events and attributes You can add your own custom APM events and attributes, which you can then use for querying and charting. This is one of several ways to report custom data. To record a custom event, follow the procedures for your New Relic language agent. To add custom attributes to APM events, you must first enable them for your APM agent, and then make an API call to record the attribute. Follow the agent-specific custom attribute procedures. When creating your own custom events and attributes, follow data requirements for: Size limits Attribute types Reserved words C SDK To add a custom event to apps monitored by the C SDK, start a transaction and use the newrelic_create_custom_event and newrelic_record_custom_event functions. For more information, see the Guide to using the C SDK API. You can then add custom attributes for your C SDK app. Go To add a custom event to apps monitored by the Go agent, use RecordCustomEvent. You can then add custom attributes for your Go app. Java Custom event collection is enabled by default in Java agent version 3.13.0 or higher. To send custom events, call recordCustomEvent. For example: Map<String, Object> eventAttributes = new HashMap<String, Object>(); NewRelic.getAgent().getInsights().recordCustomEvent(\"MyCustomEvent\", eventAttributes); Copy The first argument defines the name of your event type, and the second argument is a map with the attributes for your custom event. Event attributes must be strings or numbers. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can change the maximum number of events recorded by the Java agent via a configuration parameter in newrelic.yml. Specify the maximum number of events to record per minute as an integer. For example, if you want to send less than the default of 10000 events: custom_insights_events: max_samples_stored: 5000 Copy To disable custom events entirely, add the following to your newrelic.yml: custom_insights_events: enabled: false Copy You can then add custom attributes for your Java app. For Java agent versions prior to 4.1.0, use the following YAML configuration: custom_insights_events.enabled: true custom_insights_events.max_samples_stored: 5000 Copy .NET Custom event collection is enabled by default in .NET agent version 4.6.29.0 or higher. To send custom events, simply call RecordCustomEvent(). For example: var eventAttributes = new Dictionary<String, Object>(); NewRelic.Api.Agent.NewRelic.RecordCustomEvent('MyCustomEvent', eventAttributes); Copy The first argument defines the name of your event type, and the second argument is an IEnumerable with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can then add custom attributes for your .NET app. You can turn off custom events entirely by setting customEvents.enabled to false in newrelic.config. Node.js Custom event collection is enabled by default in Node.js agent version 1.15.0 or higher. To send custom events, simply call the relevant API. For example: recordCustomEvent(eventType, attributes) Copy Use recordCustomEvent to record an event-based metric, usually associated with a particular duration. The eventType must be an alphanumeric string less than 255 characters. The attributes must be an object of key and value pairs. The keys must be shorter than 255 characters, and the values must be string, number, or boolean. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can then add custom attributes for your Node.js app. To disable custom events entirely, add custom_insights_events.enabled: false to newrelic.js. PHP Custom event collection is enabled by default in PHP agent version 4.18 or higher. To send custom events, simply call the relevant API function. For example: newrelic_record_custom_event(\"WidgetSale\", array(\"color\"=>\"red\", \"weight\"=>12.5)); Copy The first argument defines the name of your event type, and the second argument is an array with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. By default, the maximum number of custom events recorded per minute is 10,000. This setting cannot be changed. You can then add custom attributes for your PHP app. To disable custom events entirely, add newrelic.custom_insights_events.enabled = false to your newrelic.ini and restart the agent. Python Custom event collection is enabled by default in Python agent version 2.60.0.46 or higher. To send custom events, simply call the relevant API. For example: newrelic.agent. record_custom_event (event_type, params, application=None) Copy The event_type defines the name (or type) of the custom event. Attributes of the custom event should be passed in as a dictionary via the params keyword argument. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For limits and restrictions on event_type and params, see our documentation about limits and restricted characters and reserved words If called outside of the context of a monitored web request or background task, the call will be ignored unless the application keyword argument is provided and an application object corresponding to the application against which the exception should be recorded is provided. A suitable application object can be obtained using the newrelic.agent.application() function. You can then add custom attributes for your Python app. To disable custom events entirely, set custom_insights_events.enabled to False in your newrelic.ini configuration file. Ruby Custom event collection is enabled by default in Ruby agent version 3.9.8.273 or higher. To send custom events, simply call the relevant API. For example: ::NewRelic::Agent.record_custom_event('WidgetSale', color: 'red', weight: 12.5) Copy The first argument defines the name of your event type, and the second argument is a hash with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can change the maximum number of events recorded by the Ruby agent via a configuration parameter in newrelic.yml: Add custom_insights_events.max_samples_stored: to your configuration file. Specify the maximum number of events to record per minute as an integer. For example, if you want to be able to send up to 5000 events per minute, add: custom_insights_events.max_samples_stored: 5000 Copy You can then add custom attributes for your Ruby app. To disable custom events entirely, add custom_insights_events.enabled: false to newrelic.yml. Timestamps You may not specify a timestamp on events that are collected and recorded via the agent. The agent will automatically assign a timestamp to events based on when they are recorded via the API. Limits and restricted characters See Custom event data requirements for size limits, data types, and naming syntax requirements. Reserved words Before creating custom attributes, review New Relic's list of reserved terms used by NRQL. Otherwise unexpected results may occur.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.85252,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM: Report <em>custom</em> <em>events</em> and attributes",
        "sections": "APM: Report <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": ". Record <em>custom</em> <em>events</em> and attributes You can add your own <em>custom</em> APM <em>events</em> and attributes, which you can then use for querying and charting. This is one of several ways to report <em>custom</em> <em>data</em>. To record a <em>custom</em> <em>event</em>, follow the procedures for your New Relic language agent. To add <em>custom</em> attributes"
      },
      "id": "609fa629e7b9d2fa8dc3eb04"
    }
  ],
  "/docs/data-apis/custom-data/custom-events/report-custom-event-data": [
    {
      "sections": [
        "APM: Report custom events and attributes",
        "Data considerations",
        "Tip",
        "Record custom events and attributes",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Timestamps",
        "Limits and restricted characters",
        "Reserved words"
      ],
      "title": "APM: Report custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "8731386e34fbced8d086795e273a1e2392b663ba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/apm-report-custom-events-attributes/",
      "published_at": "2022-01-12T02:14:31Z",
      "updated_at": "2021-10-23T19:43:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have APM, you can report custom event data. You can then query and visualize your data in New Relic. Data considerations New Relic agents send event data to New Relic as part of the normal harvest cycle every five seconds for agent versions supporting real time streaming. Sending a lot of events can increase the memory overhead of the agent. New Relic enforces an upper limit of 833 custom events every 5 seconds. Additionally, posts greater than 1MB (10^6 bytes) in size will not be recorded, regardless of the custom event limit. You can also send custom events using the Event API (without need for APM). However, be aware that custom events sent with the agent APIs are not compatible with high security mode. Tip For more information, check out New Relic University’s tutorial Adding custom data with the APM agent API. Or, go directly to the full online course Custom data with APM. Record custom events and attributes You can add your own custom APM events and attributes, which you can then use for querying and charting. This is one of several ways to report custom data. To record a custom event, follow the procedures for your New Relic language agent. To add custom attributes to APM events, you must first enable them for your APM agent, and then make an API call to record the attribute. Follow the agent-specific custom attribute procedures. When creating your own custom events and attributes, follow data requirements for: Size limits Attribute types Reserved words C SDK To add a custom event to apps monitored by the C SDK, start a transaction and use the newrelic_create_custom_event and newrelic_record_custom_event functions. For more information, see the Guide to using the C SDK API. You can then add custom attributes for your C SDK app. Go To add a custom event to apps monitored by the Go agent, use RecordCustomEvent. You can then add custom attributes for your Go app. Java Custom event collection is enabled by default in Java agent version 3.13.0 or higher. To send custom events, call recordCustomEvent. For example: Map<String, Object> eventAttributes = new HashMap<String, Object>(); NewRelic.getAgent().getInsights().recordCustomEvent(\"MyCustomEvent\", eventAttributes); Copy The first argument defines the name of your event type, and the second argument is a map with the attributes for your custom event. Event attributes must be strings or numbers. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can change the maximum number of events recorded by the Java agent via a configuration parameter in newrelic.yml. Specify the maximum number of events to record per minute as an integer. For example, if you want to send less than the default of 10000 events: custom_insights_events: max_samples_stored: 5000 Copy To disable custom events entirely, add the following to your newrelic.yml: custom_insights_events: enabled: false Copy You can then add custom attributes for your Java app. For Java agent versions prior to 4.1.0, use the following YAML configuration: custom_insights_events.enabled: true custom_insights_events.max_samples_stored: 5000 Copy .NET Custom event collection is enabled by default in .NET agent version 4.6.29.0 or higher. To send custom events, simply call RecordCustomEvent(). For example: var eventAttributes = new Dictionary<String, Object>(); NewRelic.Api.Agent.NewRelic.RecordCustomEvent('MyCustomEvent', eventAttributes); Copy The first argument defines the name of your event type, and the second argument is an IEnumerable with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can then add custom attributes for your .NET app. You can turn off custom events entirely by setting customEvents.enabled to false in newrelic.config. Node.js Custom event collection is enabled by default in Node.js agent version 1.15.0 or higher. To send custom events, simply call the relevant API. For example: recordCustomEvent(eventType, attributes) Copy Use recordCustomEvent to record an event-based metric, usually associated with a particular duration. The eventType must be an alphanumeric string less than 255 characters. The attributes must be an object of key and value pairs. The keys must be shorter than 255 characters, and the values must be string, number, or boolean. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can then add custom attributes for your Node.js app. To disable custom events entirely, add custom_insights_events.enabled: false to newrelic.js. PHP Custom event collection is enabled by default in PHP agent version 4.18 or higher. To send custom events, simply call the relevant API function. For example: newrelic_record_custom_event(\"WidgetSale\", array(\"color\"=>\"red\", \"weight\"=>12.5)); Copy The first argument defines the name of your event type, and the second argument is an array with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. By default, the maximum number of custom events recorded per minute is 10,000. This setting cannot be changed. You can then add custom attributes for your PHP app. To disable custom events entirely, add newrelic.custom_insights_events.enabled = false to your newrelic.ini and restart the agent. Python Custom event collection is enabled by default in Python agent version 2.60.0.46 or higher. To send custom events, simply call the relevant API. For example: newrelic.agent. record_custom_event (event_type, params, application=None) Copy The event_type defines the name (or type) of the custom event. Attributes of the custom event should be passed in as a dictionary via the params keyword argument. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For limits and restrictions on event_type and params, see our documentation about limits and restricted characters and reserved words If called outside of the context of a monitored web request or background task, the call will be ignored unless the application keyword argument is provided and an application object corresponding to the application against which the exception should be recorded is provided. A suitable application object can be obtained using the newrelic.agent.application() function. You can then add custom attributes for your Python app. To disable custom events entirely, set custom_insights_events.enabled to False in your newrelic.ini configuration file. Ruby Custom event collection is enabled by default in Ruby agent version 3.9.8.273 or higher. To send custom events, simply call the relevant API. For example: ::NewRelic::Agent.record_custom_event('WidgetSale', color: 'red', weight: 12.5) Copy The first argument defines the name of your event type, and the second argument is a hash with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can change the maximum number of events recorded by the Ruby agent via a configuration parameter in newrelic.yml: Add custom_insights_events.max_samples_stored: to your configuration file. Specify the maximum number of events to record per minute as an integer. For example, if you want to be able to send up to 5000 events per minute, add: custom_insights_events.max_samples_stored: 5000 Copy You can then add custom attributes for your Ruby app. To disable custom events entirely, add custom_insights_events.enabled: false to newrelic.yml. Timestamps You may not specify a timestamp on events that are collected and recorded via the agent. The agent will automatically assign a timestamp to events based on when they are recorded via the API. Limits and restricted characters See Custom event data requirements for size limits, data types, and naming syntax requirements. Reserved words Before creating custom attributes, review New Relic's list of reserved terms used by NRQL. Otherwise unexpected results may occur.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 822.9792,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM: <em>Report</em> <em>custom</em> <em>events</em> <em>and</em> <em>attributes</em>",
        "sections": "APM: <em>Report</em> <em>custom</em> <em>events</em> <em>and</em> <em>attributes</em>",
        "tags": "<em>Custom</em> <em>events</em>",
        "body": ". Record <em>custom</em> <em>events</em> and <em>attributes</em> You can add your own <em>custom</em> APM <em>events</em> and <em>attributes</em>, which you can then use for querying and charting. This is one of several ways to <em>report</em> <em>custom</em> data. To record a <em>custom</em> <em>event</em>, follow the procedures for your New Relic language agent. To add <em>custom</em> <em>attributes</em>"
      },
      "id": "609fa629e7b9d2fa8dc3eb04"
    },
    {
      "sections": [
        "iOS SDK API guide",
        "Caution",
        "Install the SDK",
        "Automatically instrumented classes and methods",
        "Instrument your Objective-C code",
        "Important",
        "Create and complete interactions",
        "Rename a default interaction",
        "Set a custom application version",
        "Set a custom build identifier",
        "Create custom metrics",
        "Objective-C: Report custom attributes and events",
        "Objective-C: Track custom network requests",
        "Instrument your Swift code",
        "Create and complete Swift interactions",
        "Rename a default Swift interaction",
        "Set a custom application version with Swift",
        "Set a custom build identifier with Swift",
        "Create custom metrics with Swift",
        "Swift: Report custom attributes and events",
        "Swift: Track custom network requests"
      ],
      "title": "iOS SDK API guide",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "API guides"
      ],
      "external_id": "211136f4a8e7f940d7f6ef753a1445eaed46bd92",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/ios-sdk-api/ios-sdk-api-guide/",
      "published_at": "2022-01-12T03:00:41Z",
      "updated_at": "2021-11-05T14:07:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the iOS SDK API to add custom data. For example: Instrument your own code. Start and stop interaction traces from events in your mobile app. Record custom metrics. Send custom attributes and events to Insights. Track networking from libraries not supported automatically. Set a custom identifier value with Objective-C or Swift to associate user sessions with analysis events and attributes (iOS SDK version 5.9.0 or higher). Caution Tracing is heavily optimized, but it does impose a performance overhead. Avoid instrumenting methods that are expected to be called hundreds of times. Install the SDK Ensure you have your app instrumented with the latest iOS SDK by going to one.newrelic.com > Add more data and following the instructions for iOS. This document contains the iOS SDK instrumentation requirements for: Objective C Swift For details about the available methods for custom attributes and events you can send to to New Relic Insights, see the iOS SDK API reference. You can also configure feature flags for: Objective-C Swift Automatically instrumented classes and methods The following methods (for the listed classes and their sub-classes) are already instrumented by New Relic. You do not need to add custom instrumentation to trace them. Classes Methods automatically instrumented by New Relic UIViewController viewDidLoad: viewWillAppear: viewDidAppear: viewWillDisappear: viewDidDisappear: viewWillLayoutSubviews: viewDidLayoutSubviews: UIImage imageNamed: imageWithContentsOfFile: imageWithData: imageWithData:scale: initWithContentsOfFile: initWithData: initWithData:scale: NSJSONSerialization JSONObjectWithData:options:error: JSONObjectWithStream:options:error: dataWithJSONObject:options:error: writeJSONObject:toStream:options:error: NSManagedObjectContext executeFetchRequest:error: processPendingChanges The agent aggregates performance for various methods into summary metrics that appear in the Interactions page. Summary categories include: View loading UI layout Database Images JSON Network Instrument your Objective-C code To have your own Objective-C code appear in interaction code breakdowns and timelines, add a _START call to the beginning of your method and a _STOP call to the end of it. Important Always include a _STOP for each _START, and only include one set of these commands in a given method. The trace system will automatically pick up the class and method name, and report performance metrics for your method to New Relic. - (void)myMethod { NR_TRACE_METHOD_START(0); // … existing code NR_TRACE_METHOD_STOP; } Copy If you are not using ARC, use this version of the _STOP macro to avoid memory leaks: NR_NONARC_TRACE_METHOD_STOP; Copy If you want your method’s performance to be included in the summary data on the APM Overview page, pass one of the NRTraceType enum values into the _START macro; for example: NR_TRACE_METHOD_START(NRTraceTypeDatabase); Copy Create and complete interactions By default, an interaction starts when a view controller is pushed. To manually start an interaction with Objective-C, use these API calls: NSString* uniqueIdentifier = NR_START_NAMED_INTERACTION(@\"name\"); Copy This macro will automatically begin tracking the name interaction trace from the current line. It will also complete any previously running interaction. It returns a unique identifier that can be used to complete that interaction by using this API call: NR_INTERACTION_STOP(uniqueIdentifier); Copy This macro will complete the interaction associated with the uniqueIdentifier if that interaction has not already completed automatically. You do not need to call this method. Rename a default interaction By default, the iOS agent will start an interaction trace when a new view controller is displayed. The interactions are named using the format Display <ViewController>. To change these default names with Objective-C, implement the - (NSString*) customNewRelicInteractionName instance method in your view controller, where the string returned becomes the interaction's name. Set a custom application version The New Relic iOS SDK allows you to set a custom application version string with Objective-C. Instead of using the string defined in CFBundleShortVersionString, call the +[NewRelic setApplicationVersion:] method and pass along the custom application version before calling +[NewRelic startWithApplicationToken:]; [NewRelic setApplicationVersion:(NSString*) appVersion]; Copy Set a custom build identifier As of version 5.1.0 of the New Relic iOS SDK, an API method allows you to set a custom build identifier that is displayed next to the application version in the Crash details page. Instead of using the CFBundleVersion string defined in Xcode with Objective-C, call the +[NewRelic setApplicationBuild:] method, and pass along the custom build identifier. [NewRelic setApplicationBuild:(NSString*) buildNumber]; Copy Create custom metrics Custom metrics can help track high level events specific to your application. With the recordMetric API, you can record arbitrary numerical data and named events with Objective-C and Swift. You can also use several API calls to record custom metrics that provide different levels of detail. Objective-C: Report custom attributes and events Use methods in the NewRelic object to report custom attributes and events. For details about the available methods for custom attributes and events with Objective-C, see the iOS SDK API reference. Methods that return BOOL results return YES if they succeed, or NO if the operation did not complete. These methods are available in versions 5.0.0 or higher of the New Relic iOS SDK. The SDK can store up to 128 user-defined custom attributes at a time. If you attempt to store more than 128 attributes, the SDK returns NO. Custom attributes names should use the simplest format needed, and New Relic recommends single word attributes, containing no spaces. Attribute phrases can be formatted in camel case, so My Custom Attribute is better specified as myCustomAttribute. As with custom metrics: Avoid using the characters / ] [ | * when naming things. Avoid multi-byte characters. Objective-C: Track custom network requests If you can express a transactional network request in terms similar to an HTTP request, you can track it. Use URLs that are well-formed and do not include highly variable paths or hostnames. For requests that complete, use this method: [NewRelic noticeNetworkRequestForURL:(NSURL*)url httpMethod:(NSString*)httpMethod withTimer:(NRTimer *)timer responseHeaders:(NSDictionary *)headers statusCode:(NSInteger)httpStatusCode bytesSent:(NSUInteger)bytesSent bytesReceived:(NSUInteger)bytesReceived responseData:(NSData *)responseData andParams:(NSDictionary *)params]; Copy Parameters include: Parameter Description url The URL of the request httpMethod The method type of the request; for example, POST, GET, etc. timer An NRTimer that timed the network request headers A dictionary containing the HTTP response headers, if available httpStatusCode The response status code If the httpStatusCode is greater than or equal to 400, the agent will record a server error and may capture the responseData body if provided. bytesSent The size of the request body bytesReceived The size of the responseBody responseData The response body data, captured if the agent records server error params params Additional parameters included in an HTTP error metric if the HTTP transaction is an error For requests that fail due to a socket or operating system error, use this method: [NewRelic noticeNetworkFailureForURL:(NSURL *)url httpMethod:(NSString*)httpMethod withTimer:(NRTimer *)timer andFailureCode:(NSInteger)iOSFailureCode]; Copy Parameters include: Parameter Description url The URL of the request httpMethod The method type of the request; for example, POST, GET, etc. timer An NRTimer that timed the network request iOSFailureCode The failure code Failure codes are interpreted as NSURLError* code. To view a complete list of supported codes, see NRConstants.h. Instrument your Swift code To have your own Swift code appear in interaction code breakdowns and timelines: Add a startTracingMethod() call to the beginning of your method. Add a endTracingMethodWithTimer() call to the end of it. Always include an endTracingMethodWithTimer() call for each startTracingMethod() reference. Include only one set of these commands in a given method. func myMethod(){ let timer = NRTimer(); NewRelic.startTracingMethod(#selector(MyClass.myMethod), object: self, timer: timer, category: NRTraceTypeNone) // … existing code NewRelic.endTracingMethodWithTimer(timer) } Copy If you want your method’s performance to be included in the summary data on the APM Overview page, pass one of the NRTraceType enum values into the startTracingMethod() macro; for example: NewRelic.startTracingMethod(#selector(MyClass.myMethod), object: self, timer: timer, category: NRTraceTypeDatabase) Copy Create and complete Swift interactions By default, an interaction starts when a view controller is pushed. To manually start an interaction, use these API calls: let uniqueIdentifier = NewRelic.startInteraction(withName: \"My Interaction\") Copy This call will automatically begin tracking an interaction trace named My Interaction from the current line. It will also complete any previously running interaction. It returns a unique identifier that can be used to complete that interaction by using this API call: NewRelic.stopCurrentInteraction(uniqueIdentifier) Copy This method will complete the interaction associated with the uniqueIdentifier if that interaction has not already completed automatically. You do not need to call this method. Rename a default Swift interaction By default, the iOS agent will start an interaction trace when a new view controller is displayed. The interactions are named using the format Display <ViewController>. To change these default names, implement the @objc func customNewRelicInteractionName() -> String method in your view controller, where the string returned becomes the interaction's name. Set a custom application version with Swift The New Relic iOS SDK allows you to set a custom application version string. Instead of using the string defined in CFBundleShortVersionString, call the NewRelic.setApplicationVersion() method, and pass along the custom application version before calling NewRelic.startWithApplicationToken();. NewRelic.setApplicationVersion(String appVersion) Copy Set a custom build identifier with Swift As of version 5.1.0 of the New Relic iOS SDK, an API method allows you to set a custom build identifier that is displayed next to the application version in the Crash details page. Instead of using the CFBundleVersion string defined in Xcode, call the NewRelic.setApplicationBuild() method, and pass along the custom build identifier. NewRelic.setApplicationBuild(buildNumber) Copy Create custom metrics with Swift Custom metrics can help track high level events specific to your application. With the recordMetric API, you can record arbitrary numerical data and named events with Objective-C and Swift. You can also use several API calls to record custom metrics that provide different levels of detail. Swift: Report custom attributes and events Use methods in the NewRelic object to report custom attributes and events. For details about the available methods for custom attributes and events with Swift, see the iOS SDK API reference. Methods that return BOOL results return YES if they succeed, or NO if the operation did not complete. These methods are available in versions 5.0.0 or higher of the New Relic iOS SDK. The SDK can store up to 128 user-defined custom attributes at a time. If you attempt to store more than 128 attributes, the SDK returns NO. Custom attributes names should use the simplest format needed, and New Relic recommends single word attributes, containing no spaces. Attribute phrases can be formatted in camel case, so My Custom Attribute is better specified as myCustomAttribute. As with custom metrics: Avoid using the characters / ] [ | * when naming things. Avoid multi-byte characters. Swift: Track custom network requests If you can express a transactional network request in terms similar to an HTTP request, you can track it. Use URLs that are well-formed and do not include highly variable paths or hostnames. For requests that complete, use this method: NewRelic.noticeNetworkRequestForURL(url: NSURL!, httpMethod: String!, withTimer: NRTimer!, responseHeaders:[NSObject : AnyObject]!, statusCode: Int, bytesSent: UInt, bytesReceived: UInt, responseData: NSData!, andParams: [NSObject : AnyObject]!) Copy Parameters include: Parameter Description url The URL of the request httpMethod The method type of the request; for example, POST, GET, etc. timer An NRTimer that timed the network request headers A dictionary containing the HTTP response headers, if available httpStatusCode The response status code If the httpStatusCode is greater than or equal to 400, the agent will record a server error and may capture the responseData body if provided. bytesSent The size of the request body bytesReceived The size of the responseBody responseData The response body data, captured if the agent records Server error params params Additional parameters included in an HTTP error metric if the HTTP transaction is an error For requests that fail due to a socket or operating system error, use this method: NewRelic.noticeNetworkFailureForURL(url: NSURL!, httpMethod: NSString!, withTimer: NRTimer!, andFailureCode: Int) Copy Parameters include: Parameter Description url The URL of the request httpMethod The method type of the request; for example, POST, GET, etc. timer An NRTimer that timed the network request iOSFailureCode The failure code Failure codes are interpreted as NSURLError* code. To view a complete list of supported codes, see NRConstants.h.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 336.6773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Objective-C: <em>Report</em> <em>custom</em> <em>attributes</em> <em>and</em> <em>events</em>",
        "body": ". You can also use several API calls to record <em>custom</em> metrics that provide different levels of detail. Objective-C: <em>Report</em> <em>custom</em> <em>attributes</em> and <em>events</em> Use methods in the NewRelic object to <em>report</em> <em>custom</em> <em>attributes</em> and <em>events</em>. For details about the available methods for <em>custom</em> <em>attributes</em> and <em>events</em>"
      },
      "id": "619e9c2128ccbcbf64b9abeb"
    },
    {
      "sections": [
        "Report mobile monitoring custom events and attributes",
        "Create custom attributes and events",
        "Mobile event and attribute query examples",
        "Custom event example: Track purchases",
        "Tip",
        "Attribute example: Track a specific user",
        "Attribute example: Track a specific store id",
        "Custom attribute example: Track a specific action",
        "Important",
        "Size limits and restricted characters",
        "Set the time to send data",
        "Privacy considerations"
      ],
      "title": "Report mobile monitoring custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "65970eacbedb3360fd1c7394affc8cbc42f2ab0c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/report-mobile-monitoring-custom-events-attributes/",
      "published_at": "2022-01-12T02:25:05Z",
      "updated_at": "2021-10-23T21:59:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Mobile monitoring in New Relic sends some default event data from your mobile app to New Relic, such as data about interactions, sessions, crashes, and request errors. You can also create your own custom attributes and events for more detailed querying and analysis. Create custom attributes and events You can create custom session-level attributes for default mobile monitoring events using the mobile agent SDKs. For example, to record a username attribute for some part of your iOS or Android app, you would use the setAttribute API (Android | iOS). These attributes are session-related information and are shared by multiple mobile event types. You can also create entirely new custom event types and assign them their own custom attributes, using the recordCustomEvent API (Android | iOS). To help with crash analysis, you can use the SDK to create MobileBreadcrumb and MobileHandledException events. These events are available for querying and also displayed in the crash event trail UI. For more on creating custom attributes and custom events, see: Android SDK API guide iOS SDK API guide NRQL query examples MobileRequestError examples MobileRequest examples Limits and restricted characters Mobile event and attribute query examples Here are some examples of using NRQL to query your mobile app events and attributes: Custom event example: Track purchases To track purchases in your app, use recordCustomEvent to create an event type (such as \"UserAction\") and associate attributes such as \"name\" (with value \"Purchase\"), price, quantity, and SKU. Tip For performance reasons, you should limit the total number of event types to maybe one or two. The recordCustomEvent parameter eventType is meant to be used for high-level categories. For example, you might create an event typeGestures, and then create many different custom event names under the Gesture event type. Create an event on iOS: BOOL purchaseRecorded = [NewRelic recordCustomEvent:@\"UserAction\" attributes:@{@\"name\": @\"Purchase\", @\"sku\": @\"12345LPD\", @\"quantity\": @1, @\"unitPrice\": @99.99, @\"total\": @99.99}]; Copy Create an event on Android: Map<String, Object> userActionAttributes = new HashMap<String, Object>(); userActionAttributes.put(\"name\", \"Purchase\"); userActionAttributes.put(\"sku\", \"12345LPD\"); userActionAttributes.put(\"quantity\", 1); userActionAttributes.put(\"unitPrice\", 99.99); userActionAttributes.put(\"total\", 99.99); boolean userActionRecorded = NewRelic.recordCustomEvent(\"UserAction\", userActionAttributes); Copy New Relic reports a custom event of type UserAction and name Purchase, which allows you to query all purchases made in your app in the last day: SELECT * from UserAction where name = 'Purchase' since 1 day ago Copy Replace deprecated recordEvent method: As of Android agent version 5.12.0 and iOS agent version 5.12.0, use the recordCustomEvent method to create these custom events. If you have replaced the deprecated recordEvent method for your custom events, be sure to also replace its corresponding NRQL query with the new format. Look for queries used with recordEvent method, such as this: SELECT * from Mobile where category = 'Custom' and name = 'Purchase' since 1 day ago Copy Replace them with the query format used with recordCustomEvent: SELECT * from UserAction where name = 'Purchase' since 1 day ago Copy Attribute example: Track a specific user You can create a custom attribute to track a custom user identifier across the session, and then query for all that user's interactions. To add an attribute for the userId, call the setUserId method: Set the userId on iOS: BOOL userIdWasSet = [NewRelic setUserId:@\"jsmith\"]; Copy Set the userId on Android: boolean userIdWasSet = NewRelic.setUserId(\"jsmith\"); Copy With this attribute, you can use a WHERE clause to see all actions performed by that username in the last day: SELECT * from Mobile WHERE userId = 'jsmith' since 1 day ago Copy Attribute example: Track a specific store id You can create a custom attribute to track a store id across the session, and then query for all that store's interactions. To add an attribute for the storeId, call the setAttribute method: Set the storeId on iOS: BOOL attributeSet = [NewRelic setAttribute:@\"storeId\" value:@\"NY0531\"]; Copy Set the storeId on Android: boolean attributeSet = NewRelic.setAttribute(\"storeId\", \"NY0531\"); Copy With this attribute, you can use a WHERE clause to see all actions performed by that storeId in the last day: SELECT * from Mobile WHERE storeId = 'NY0531' since 1 day ago Copy Custom attribute example: Track a specific action You can use custom attributes to track the number of times that a specific action occurs in your application. For example, you can track the number of times a button was clicked or the number of times a level was completed in a game. To track completing a game level, call incrementAttribute with no value specified. This creates an attribute with a default value of 1: Create a counter on iOS: BOOL levelIncremented = [NewRelic incrementAttribute@\"level\"]; Copy Create a counter on Android: boolean levelIncremented = NewRelic.incrementAttribute(\"level\"); Copy Each subsequent call to incrementAttribute adds 1 to the attribute level: Increment a counter on iOS: levelIncremented = [NewRelic incrementAttribute@\"level\"]; Copy Increment a counter on Android: levelIncremented = NewRelic.incrementAttribute(\"level\"); Copy Important Be sure to reset the value to 0 when starting over. To reset the level back to 1 or 0, call setAttribute: Reset a counter on iOS: levelReset = [NewRelic setAttribute:@\"level\" value:@1]; Copy Reset a counter on Android: levelReset = NewRelic.setAttribute(\"level\", 1); Copy When querying, use this level attribute to filter your data. For example, if you have a username and level attribute, use the max() function to find the highest level the user had reached: SELECT max(level) from Mobile where username = 'jsmith' Copy Size limits and restricted characters Limits for custom attributes added to default mobile events: Attributes: 128 maximum String attributes: 4 KB maximum length (empty string values are not accepted) Limits for custom events: Attributes: 254 maximum per event (number includes default session attributes) String attributes: 4 KB maximum length (empty string values are not accepted) Naming syntax and rules: See Rules for custom data. Set the time to send data By default, New Relic transmits event data in any of these situations: A session has been ongoing for 600 seconds. The app session ends by backgrounding. The app crashes. If the app crashes, New Relic gathers the attributes and events for that session and sends them to Insights. (On iOS, this happens the next time the app is launched). You can then use Insights to query and analyze the event and attribute data. To set the maximum time (in seconds) that the agent will store events in memory, use the following SDK calls: iOS method: + (void) setMaxEventBufferTime:(unsigned int)seconds; Copy Android method: public static void setMaxEventBufferTime(int maxBufferTimeInSec); Copy Privacy considerations If you want to collect personal data via custom attributes, please consult with your privacy or legal teams. Be sure to follow your organization's obligations for notices and consent regulations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.87027,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Report</em> mobile monitoring <em>custom</em> <em>events</em> <em>and</em> <em>attributes</em>",
        "sections": "<em>Report</em> mobile monitoring <em>custom</em> <em>events</em> <em>and</em> <em>attributes</em>",
        "tags": "<em>Custom</em> <em>events</em>",
        "body": "Mobile monitoring in New Relic sends some default <em>event</em> data from your mobile app to New Relic, such as data about interactions, sessions, crashes, and request errors. You can also create your own <em>custom</em> <em>attributes</em> and <em>events</em> for more detailed querying and analysis. Create <em>custom</em> <em>attributes</em>"
      },
      "id": "609fa5cf28ccbc508d9832d3"
    }
  ],
  "/docs/data-apis/custom-data/custom-events/report-mobile-monitoring-custom-events-attributes": [
    {
      "sections": [
        "Report browser monitoring custom events and attributes",
        "Page actions and views",
        "Prerequisites",
        "Create PageAction events",
        "Add custom attributes to PageView event",
        "Use setCustomAttribute browser API call",
        "Forward custom attributes from APM data",
        "PageAction and PageView attributes",
        "Troubleshooting"
      ],
      "title": "Report browser monitoring custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "17c97a462616f2b23ead796b62780a1ffeb3dfac",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/report-browser-monitoring-custom-events-attributes/",
      "published_at": "2022-01-12T02:21:28Z",
      "updated_at": "2021-10-23T21:59:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use browser monitoring in New Relic to add custom events and attributes. Page actions and views Use the browser API's addPageAction call to capture events, actions, route changes, or any end-user interactions with your application. The addPageAction call adds an event named PageAction that contains the action name and any custom attribute names and values you capture along with it. The PageAction event also contains any custom attributes you added to the PageView event. Add custom attributes to the PageView event so you can query or filter your data to answer more questions about your application. Prerequisites In order to report PageAction events, verify these prerequisites: Requirement Comments Agent version Your browser monitoring agent version must be 593 or higher. Client browser version To record PageAction events, the browser must support cross-domain XHRs. Max events per cycle PageAction events are reported every 30 seconds, with a maximum of 120 events per 30-second harvest cycle, per browser. After the 120-event limit is reached, additional events are not captured for that cycle. Event/attribute naming, data type, size Ensure you follow general requirements around event/attribute naming syntax, data types, and size. Create PageAction events To create a PageAction event: Ensure the browser agent is installed for your app. Call the newrelic.addPageAction function in the relevant part of your application's JavaScript. Wait a couple minutes for the application to run and report relevant PageAction events. Run a NRQL query of the PageAction event that includes the actionName attribute you used to capture the event (and any associated attributes you sent along with the action). Add custom attributes to PageView event The PageView event is a default browser-reported event. You can add custom attributes to the PageView event. Any custom attributes you add to the PageView event are also automatically added to the PageAction event. There are two ways to add custom attributes to the PageView event: Use setCustomAttribute browser API call To add a custom attribute to the PageView event via the browser agent, use the setCustomAttribute browser API call. This allows you to capture an attribute to be annotated on any PageAction event. Forward custom attributes from APM data If you added custom attributes to the APM Transaction event via an APM agent, you can forward those custom attributes to the PageView event automatically: Insert custom attributes by following the agent-specific instructions. Enable attribute forwarding in your agent configuration file: Agent Enable attribute forwarding C SDK Not supported. Go To enable attributes, add this to your config (disabled by default): cfg.BrowserMonitoring.Attributes.Enabled = true Copy Then add the attributes you want to include: cfg.BrowserMonitoring.Attributes.Include = []string{\"request.*\"} Copy Java Add the attributes.enabled option in the browser_monitoring stanza and set it to true. .NET Add the <attributes enabled=\"true\"> element as a child of the browserMonitoring element: <configuration xmlns=\"urn:newrelic-config\"> ... <browserMonitoring autoInstrument=\"true\"> ... <attributes enabled=\"true\"> ... </attributes> </browserMonitoring> ... </configuration> Copy If you are using manual browser instrumentation the attribute needs to be created before the GetBrowserTimingHeader() call. Node.js Add attributes: {enabled: true} to the browser_monitoring: { section of your app's newrelicjs configuration file. PHP Add the newrelic.browser_monitoring.attributes.enabled option and set it to true. Python Add the browser_monitoring.attributes.enabled option and set it to true. Ruby Add the browser_monitoring.attributes.enabled option and set it to true. PageAction and PageView attributes To see the default attributes of PageAction and PageView, see Browser events. Troubleshooting Here are some troubleshooting tips: Problem Comments Custom attributes missing If your custom attributes do not appear on PageView events, verify you are calling setCustomAttribute before the Load event on your page. If the custom attribute is called after the page load occurs, it will not be visible on PageView. PageAction events If your PageAction events do not appear when you query, check that your account is compatible. If your account is compatible, check that you are not using reserved attribute names or invalid values.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.8765,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report browser monitoring <em>custom</em> <em>events</em> and attributes",
        "sections": "Report browser monitoring <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "You can use browser monitoring in New Relic to add <em>custom</em> <em>events</em> and attributes. Page actions and views Use the browser API&#x27;s addPageAction call to capture <em>events</em>, actions, route changes, or any end-user interactions with your application. The addPageAction call adds an <em>event</em> named PageAction"
      },
      "id": "609fa5cfe7b9d2c93dc3eb26"
    },
    {
      "sections": [
        "Data requirements and limits for custom event data",
        "General requirements",
        "Important",
        "Reserved words",
        "Event type limits"
      ],
      "title": "Data requirements and limits for custom event data",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "77720ef366038ba648a5fbf3cf34e8e48b38440a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/data-requirements-limits-custom-event-data/",
      "published_at": "2022-01-12T02:18:23Z",
      "updated_at": "2021-10-23T21:58:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains general requirements and rules for inserting and using custom events and their associated attributes. Additional requirements may apply based on the method you use. You can report custom events to New Relic in several ways, including: APM agent APIs Event API (There are additional requirements when using the Event API.) Browser monitoring agent APIs (There are additional requirements with the custom PageAction event.) Mobile monitoring SDK General requirements When reporting custom events and attributes, follow these general requirements for supported data types, naming syntax, and size: Requirement Description Payload Total maximum size or length: 1MB (10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. The Event API has additional HTTP rate limits. Attribute data types Attribute values can be either a string or a numeric integer or float. If your attribute values contain date information, define it as an unformatted Unix timestamp (in seconds or milliseconds) by using the Insights data formatter. Attribute size Maximum name size: 255 bytes. Maximum attribute value size: Custom attributes sent by the agent: 255 bytes Attributes attached to custom events sent using the Event API: 4096 characters Maximum total attributes per event: 254. Exception: If you use an APM agent API, the max is 64. Maximum total attributes per event type: 48,000. Important Charts may only display the first 255 characters of attribute values. For complete attribute values, use the JSON chart type or Query API. Naming syntax Attribute names can be a combination of alphanumeric characters, colons (:), periods (.), and underscores (_). Event types (using the eventType attribute) can be a combination of alphanumeric characters, colons (:), and underscores (_). If the name begins with anything other than an alphabetical character, enclose the name with backticks in your NRQL query. For example: FROM `0_hello` SELECT count(*) Copy Do not use words reserved for use by NRQL. Null values The database does not store any data with a null value. Reserved words Avoid using the following reserved words as names for events and attributes. Otherwise, unexpected results may occur. Important This is not a complete list. In general, avoid using MySQL-reserved words to avoid collision with future New Relic functionality. Keyword Description accountId This is a reserved attribute name. If it's included, it will be dropped during ingest. appId Value must be an integer. If it is not an integer, the attribute name and value will be dropped during ingest. eventType The event type as stored in New Relic. New Relic agents and scripts normally report this as eventType. Can be a combination of alphanumeric characters, colons (:), and underscores (_). Be sure to review the prohibited eventType values and eventType limits. Prohibited eventType values For your eventType value, avoid using: Metric, MetricRaw, and strings prefixed with Metric[0-9] (such as Metric2 or Metric1Minute). Public_ and strings prefixed with Public_. These event types are reserved for use by New Relic. Events passed in with these eventType values will be dropped. timestamp Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. It must be +/-1 day (24 hours) of the current time on the server. Log forwarding terms The following keys are reserved by the Infrastructure agent's log forwarding feature: entity.guid, log, hostname, plugin.type, fb.input. If used, they are dropped during ingest and a warning is added to the logs. NRQL syntax terms If you need to use NRQL syntax terms as attribute names, including dotted attributes, they must be enclosed in backticks; for example, `LIMIT` or `consumer.offset`. Otherwise, avoid using these reserved words: ago, and, as, auto, begin, begintime, compare, day, days, end, endtime, explain, facet, from, hour, hours, in, is, like, limit, minute, minutes, month, months, not, null, offset, or, raw, second, seconds, select, since, timeseries, until, week, weeks, where, with Event type limits The current limit for total number of eventType values is 250 per child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop data. Event types include: Default events from New Relic agents Custom events from New Relic agents Custom events from Insights custom event inserter",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.87634,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Data</em> requirements and limits for <em>custom</em> <em>event</em> <em>data</em>",
        "sections": "<em>Data</em> requirements and limits for <em>custom</em> <em>event</em> <em>data</em>",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": " child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop <em>data</em>. <em>Event</em> types include: Default <em>events</em> from New Relic agents <em>Custom</em> <em>events</em> from New Relic agents <em>Custom</em> <em>events</em> from <em>Insights</em> <em>custom</em> <em>event</em> inserter"
      },
      "id": "609fa5cfe7b9d2bf16c3eb69"
    },
    {
      "sections": [
        "APM: Report custom events and attributes",
        "Data considerations",
        "Tip",
        "Record custom events and attributes",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Timestamps",
        "Limits and restricted characters",
        "Reserved words"
      ],
      "title": "APM: Report custom events and attributes",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "8731386e34fbced8d086795e273a1e2392b663ba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/apm-report-custom-events-attributes/",
      "published_at": "2022-01-12T02:14:31Z",
      "updated_at": "2021-10-23T19:43:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have APM, you can report custom event data. You can then query and visualize your data in New Relic. Data considerations New Relic agents send event data to New Relic as part of the normal harvest cycle every five seconds for agent versions supporting real time streaming. Sending a lot of events can increase the memory overhead of the agent. New Relic enforces an upper limit of 833 custom events every 5 seconds. Additionally, posts greater than 1MB (10^6 bytes) in size will not be recorded, regardless of the custom event limit. You can also send custom events using the Event API (without need for APM). However, be aware that custom events sent with the agent APIs are not compatible with high security mode. Tip For more information, check out New Relic University’s tutorial Adding custom data with the APM agent API. Or, go directly to the full online course Custom data with APM. Record custom events and attributes You can add your own custom APM events and attributes, which you can then use for querying and charting. This is one of several ways to report custom data. To record a custom event, follow the procedures for your New Relic language agent. To add custom attributes to APM events, you must first enable them for your APM agent, and then make an API call to record the attribute. Follow the agent-specific custom attribute procedures. When creating your own custom events and attributes, follow data requirements for: Size limits Attribute types Reserved words C SDK To add a custom event to apps monitored by the C SDK, start a transaction and use the newrelic_create_custom_event and newrelic_record_custom_event functions. For more information, see the Guide to using the C SDK API. You can then add custom attributes for your C SDK app. Go To add a custom event to apps monitored by the Go agent, use RecordCustomEvent. You can then add custom attributes for your Go app. Java Custom event collection is enabled by default in Java agent version 3.13.0 or higher. To send custom events, call recordCustomEvent. For example: Map<String, Object> eventAttributes = new HashMap<String, Object>(); NewRelic.getAgent().getInsights().recordCustomEvent(\"MyCustomEvent\", eventAttributes); Copy The first argument defines the name of your event type, and the second argument is a map with the attributes for your custom event. Event attributes must be strings or numbers. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can change the maximum number of events recorded by the Java agent via a configuration parameter in newrelic.yml. Specify the maximum number of events to record per minute as an integer. For example, if you want to send less than the default of 10000 events: custom_insights_events: max_samples_stored: 5000 Copy To disable custom events entirely, add the following to your newrelic.yml: custom_insights_events: enabled: false Copy You can then add custom attributes for your Java app. For Java agent versions prior to 4.1.0, use the following YAML configuration: custom_insights_events.enabled: true custom_insights_events.max_samples_stored: 5000 Copy .NET Custom event collection is enabled by default in .NET agent version 4.6.29.0 or higher. To send custom events, simply call RecordCustomEvent(). For example: var eventAttributes = new Dictionary<String, Object>(); NewRelic.Api.Agent.NewRelic.RecordCustomEvent('MyCustomEvent', eventAttributes); Copy The first argument defines the name of your event type, and the second argument is an IEnumerable with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can then add custom attributes for your .NET app. You can turn off custom events entirely by setting customEvents.enabled to false in newrelic.config. Node.js Custom event collection is enabled by default in Node.js agent version 1.15.0 or higher. To send custom events, simply call the relevant API. For example: recordCustomEvent(eventType, attributes) Copy Use recordCustomEvent to record an event-based metric, usually associated with a particular duration. The eventType must be an alphanumeric string less than 255 characters. The attributes must be an object of key and value pairs. The keys must be shorter than 255 characters, and the values must be string, number, or boolean. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can then add custom attributes for your Node.js app. To disable custom events entirely, add custom_insights_events.enabled: false to newrelic.js. PHP Custom event collection is enabled by default in PHP agent version 4.18 or higher. To send custom events, simply call the relevant API function. For example: newrelic_record_custom_event(\"WidgetSale\", array(\"color\"=>\"red\", \"weight\"=>12.5)); Copy The first argument defines the name of your event type, and the second argument is an array with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. By default, the maximum number of custom events recorded per minute is 10,000. This setting cannot be changed. You can then add custom attributes for your PHP app. To disable custom events entirely, add newrelic.custom_insights_events.enabled = false to your newrelic.ini and restart the agent. Python Custom event collection is enabled by default in Python agent version 2.60.0.46 or higher. To send custom events, simply call the relevant API. For example: newrelic.agent. record_custom_event (event_type, params, application=None) Copy The event_type defines the name (or type) of the custom event. Attributes of the custom event should be passed in as a dictionary via the params keyword argument. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For limits and restrictions on event_type and params, see our documentation about limits and restricted characters and reserved words If called outside of the context of a monitored web request or background task, the call will be ignored unless the application keyword argument is provided and an application object corresponding to the application against which the exception should be recorded is provided. A suitable application object can be obtained using the newrelic.agent.application() function. You can then add custom attributes for your Python app. To disable custom events entirely, set custom_insights_events.enabled to False in your newrelic.ini configuration file. Ruby Custom event collection is enabled by default in Ruby agent version 3.9.8.273 or higher. To send custom events, simply call the relevant API. For example: ::NewRelic::Agent.record_custom_event('WidgetSale', color: 'red', weight: 12.5) Copy The first argument defines the name of your event type, and the second argument is a hash with the attributes for your custom event. Ensure you limit the number of unique event type names that you create, and do not generate these names dynamically. For restrictions on event type names, see our documentation about limits and restricted characters and NRQL reserved words. You can change the maximum number of events recorded by the Ruby agent via a configuration parameter in newrelic.yml: Add custom_insights_events.max_samples_stored: to your configuration file. Specify the maximum number of events to record per minute as an integer. For example, if you want to be able to send up to 5000 events per minute, add: custom_insights_events.max_samples_stored: 5000 Copy You can then add custom attributes for your Ruby app. To disable custom events entirely, add custom_insights_events.enabled: false to newrelic.yml. Timestamps You may not specify a timestamp on events that are collected and recorded via the agent. The agent will automatically assign a timestamp to events based on when they are recorded via the API. Limits and restricted characters See Custom event data requirements for size limits, data types, and naming syntax requirements. Reserved words Before creating custom attributes, review New Relic's list of reserved terms used by NRQL. Otherwise unexpected results may occur.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.85252,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM: Report <em>custom</em> <em>events</em> and attributes",
        "sections": "APM: Report <em>custom</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": ". Record <em>custom</em> <em>events</em> and attributes You can add your own <em>custom</em> APM <em>events</em> and attributes, which you can then use for querying and charting. This is one of several ways to report <em>custom</em> <em>data</em>. To record a <em>custom</em> <em>event</em>, follow the procedures for your New Relic language agent. To add <em>custom</em> attributes"
      },
      "id": "609fa629e7b9d2fa8dc3eb04"
    }
  ],
  "/docs/data-apis/custom-data/intro-custom-data": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/report-custom-event-data/",
      "sections": [
        "Report custom events and attributes",
        "Requirements",
        "Avoid rate limits",
        "Example use cases",
        "Using custom attributes",
        "Using custom events",
        "Send custom events and attributes",
        "Extend data retention"
      ],
      "published_at": "2022-01-12T03:39:09Z",
      "title": "Report custom events and attributes",
      "updated_at": "2021-10-23T21:59:19Z",
      "type": "docs",
      "external_id": "ff7b6544c9a15b49f77c4d86f69c66949c45cb87",
      "document_type": "page",
      "popularity": 1,
      "body": "One of the ways to report custom data to New Relic is with custom events and attributes. Have questions about why you'd use custom data? See Introduction to custom data. Requirements For event and attribute formatting requirements and best practices, see Limits and requirements. Avoid rate limits Reporting a large number of custom events and/or attributes can cause degraded query performance. It may also result in approaching or passing data collection rate limits. For optimal performance, first think about what data you want to analyze, and then create only the events and/or attributes necessary to meet these specific goals. Be aware of the following data and subscription requirements for inserting and accessing custom data: Ensure you follow limits and requirements around event/attribute data types, naming syntax, and size. The amount of data you have access to over time depends on your data retention policy. Example use cases Two popular custom data solutions are custom events and custom attributes. There are several ways to accomplish this (more on that later in this doc), depending on your New Relic implementation and tools. Here are some common use cases for implementing custom events and attributes. Using custom attributes Custom attributes are often used to add important business and operational context to existing events. Business context might include: Customer token Customer market segment Customer value classification Workflow control values not obvious in the URIStem User/product/account privilege context Operational context might include: Which feature flags were used What datastore was accessed What cache was accessed What errors were detected and ignored (fault partitioning) Using custom events Event data is one of New Relic's four core data types. We recommend reading that definition to understand what we mean by \"event\" and why that data type is most used for reporting specific types of activity. The use cases for custom events varies widely: basically they are used for any type of activity that an organization deems important and that is not already being monitored. A couple examples: An event might represent an activity involving multiple actions, like a customer purchasing a certain combination of products. An event might record backup activity. For example, they might set up reporting of events that represent production backups of their SOLR instances into an event table, with a timestamp of when it occurred, which cluster, and the duration. Send custom events and attributes Methods for sending custom events and attributes include: Source How to send custom data APM agent Use APM agent APIs to report custom events and custom attributes. Browser monitoring agent Add custom attributes to the PageView event via the browser API call addCustomAttribute. Send PageAction event and attributes via browser API. Forward APM agent custom attributes to PageView event. Event API To report custom events not associated with other New Relic products, use the Event API. Infrastructure monitoring agent Add custom attributes to default Infrastructure events. Use the Flex integration tool to report your own custom event data. Mobile monitoring agent Use the mobile agent API to send custom events and attributes. Synthetic monitoring Add custom attributes to the SyntheticCheck event via the $util.insights tools. For ways to report other types of custom data, see: Metric API Logs Trace API Extend data retention To learn how to extend how long events are retained in your account, see our documentation about event data retention.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 545.5213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report <em>custom</em> events and attributes",
        "sections": "Report <em>custom</em> events and attributes",
        "body": "One of the ways to report <em>custom</em> <em>data</em> to New Relic is with <em>custom</em> events and attributes. Have questions about why you&#x27;d use <em>custom</em> <em>data</em>? See <em>Introduction</em> to <em>custom</em> <em>data</em>. Requirements For event and attribute formatting requirements and best practices, see Limits and requirements. Avoid rate limits"
      },
      "id": "609fa5fb64441f9ebfd2a1db"
    },
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "View logs for your APM and infrastructure data",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2022-01-12T04:06:37Z",
      "updated_at": "2022-01-12T04:06:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on infrastructure monitoring UI pages Filter hosts by application data Switch between infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. View logs for your APM and infrastructure data You can also bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 82.526726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in infrastructure monitoring",
        "sections": "How <em>to</em> integrate APM and infrastructure <em>data</em>",
        "tags": "Manage your <em>data</em>",
        "body": "The integration of APM and infrastructure <em>data</em> lets you see your APM <em>data</em> and infrastructure <em>data</em> side by side so you can find the root cause of problems more quickly. The main ways to find and use APM <em>data</em> in infrastructure monitoring are: View APM charts on infrastructure monitoring UI pages"
      },
      "id": "603e88b2e7b9d246932a07f6"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 73.25903,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand and manage <em>data</em> ingest",
        "sections": "Understand and manage <em>data</em> ingest",
        "tags": "Ingest and manage <em>data</em>",
        "body": " to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_&lt;value&gt; <em>custom</em> <em>data</em> partition created. Usage metric group: LoggingBytes. Log records are stored on the Log <em>data</em> type by default. Additional <em>custom</em>"
      },
      "id": "603e978228ccbc8984eba79e"
    }
  ],
  "/docs/data-apis/get-started/nrdb-horsepower-under-hood": [
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.17471,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick <em>start</em>: Send metric <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick <em>start</em> to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick <em>start</em>: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.98381,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " the chart, click View query. From the account dropdown, select <em>Manage</em> your <em>data</em>, and then select <em>Data</em> ingestion. For how to <em>get</em> more details about ingested <em>data</em>, see <em>Get</em> <em>ingest</em> details. <em>Data</em> ingestion sources The <em>data</em> ingestion UI chart shows you a high level breakdown of your billable <em>data</em> usage"
      },
      "id": "603e978228ccbc8984eba79e"
    },
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.42805,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "sections": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " in: they let you decide what <em>data</em> you send to New Relic and how long it should be stored. <em>Data</em> management hub: from the account dropdown in the top right of the UI, select <em>Manage</em> your <em>data</em>. Coupled with user management tools, <em>data</em> management helps you <em>get</em> maximum value from your investment in New Relic, all"
      },
      "id": "603e96ff28ccbcf8bceba796"
    }
  ],
  "/docs/data-apis/ingest-apis/introduction-event-api": [
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 316.9215,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report metrics via the Metric <em>API</em>",
        "sections": "Report metrics via the Metric <em>API</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric <em>API</em> to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Telemetry SDKs: Report custom telemetry data",
        "Requirements and compatibility",
        "Tip",
        "Available libraries",
        "Write your own Telemetry SDK or contribute to an existing one"
      ],
      "title": "Telemetry SDKs: Report custom telemetry data",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "d883a07b7ede4c3beaba4077c507b95f9a228435",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/telemetry-sdks-report-custom-telemetry-data/",
      "published_at": "2022-01-12T06:17:44Z",
      "updated_at": "2021-12-19T14:27:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Telemetry SDKs are an open source set of API client libraries that send data to the New Relic platform. Under the hood, these SDKs rely on our primary data ingest APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don't meet your needs, our Telemetry SDKs are one way to create a custom telemetry solution (see other solutions for reporting custom data). Requirements and compatibility The Telemetry SDKs use our Metric API, Event API, Log API, and Trace API, which all require a license key, so you'll need a license key for the account you wish to send data to. Tip New Relic has contributed the Telemetry SDK to the open source community under an Apache 2.0 license. Available libraries The Telemetry SDKs are open source software on GitHub. Use the language-specific GitHub links below to get library details, coding examples, and procedures for how to use the SDKs. We currently support the following libraries, with more to be created in the future: Language Library Supported New Relic data types Java Java library on GitHub Metrics Events Logs Traces Node/TypeScript NodeJS library on GitHub Metrics Traces Python Python library on GitHub Metrics Events Logs Traces Go Go library on Github Metrics Traces .NET .NET library on GitHub .NET package in NuGet Metrics Traces C C library on Github Traces Rust Rust library on Github Traces Ruby Ruby library on Github Gem on Rubygems Traces For more on the supported data types, see: An overview of New Relic data types Metrics: see the Metric API Logs: see the Log API Traces: see the Trace API Events: see the Event API Write your own Telemetry SDK or contribute to an existing one If you need a Telemetry SDK in a language that does not currently exist or want to contribute to an existing library, please see the Telemetry SDK specifications.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.95622,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "sections": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Our Telemetry SDKs are an open source set of <em>API</em> client libraries that send <em>data</em> to the New Relic platform. Under the hood, these SDKs rely on our primary <em>data</em> <em>ingest</em> <em>APIs</em>: the Metric <em>API</em>, Trace <em>API</em>, Log <em>API</em>, and Event <em>API</em>. If our pre-built solutions don&#x27;t meet your needs, our Telemetry SDKs"
      },
      "id": "603ea196196a670192a83d83"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.98381,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    }
  ],
  "/docs/data-apis/ingest-apis/metric-api/NRQL-high-cardinality-metrics": [
    {
      "sections": [
        "NRQL syntax, clauses, and functions",
        "Syntax",
        "Query components",
        "Required clauses",
        "Required: SELECT statement",
        "Avg response time since last week",
        "Required: FROM clause",
        "Query one data type",
        "Query multiple data types",
        "Optional clauses",
        "AS clause",
        "Query using math function and AS",
        "Query using funnel and AS",
        "COMPARE WITH clause",
        "EXTRAPOLATE clause",
        "Important",
        "Example of extrapolating throughput",
        "Example of extrapolating throughput as a time series",
        "FACET clause",
        "Faceted query using count()",
        "Faceted query using uniqueCount()",
        "Grouping results across time",
        "FACET ... AS clause",
        "FACET CASES clause",
        "Basic usage with WHERE",
        "Group based on multiple attributes",
        "Label groups with AS",
        "Facet non-matching data with OR",
        "FACET ... ORDER BY clause",
        "Tip",
        "LIMIT clause",
        "Query using LIMIT",
        "OFFSET clause",
        "ORDER BY clause",
        "SHOW EVENT TYPES clause",
        "Data types in the last day",
        "SINCE clause",
        "SLIDE BY clause",
        "Use SLIDE BY with MAX or AUTO interval",
        "TIMESERIES clause",
        "Use a set interval",
        "Use an automatically set interval",
        "Use MAX interval",
        "UNTIL clause",
        "WHERE clause",
        "Example query with three conditions",
        "WITH METRIC_FORMAT clause",
        "WITH TIMEZONE clause",
        "Query metric data",
        "Functions",
        "Aggregator functions",
        "aggregationendtime()",
        "apdex(attribute, t: )",
        "Get Apdex for specific customers",
        "Get Apdex for specific transaction",
        "Get overall Apdex for your app",
        "average(attribute)",
        "buckets(attribute, ceiling [,number of buckets])",
        "bucketPercentile(attribute)",
        "cardinality(attribute)",
        "count(*)",
        "derivative(attribute [,time interval])",
        "dimensions(include: {attributes}, exclude: {attributes})",
        "latestrate(attribute, time interval)",
        "Get the most recent rate of change of PageView Duration",
        "max(attribute)",
        "median(attribute)",
        "Median query",
        "min(attribute)",
        "minuteOf(attribute)",
        "mod(attribute, divisor)",
        "mod() within a WHERE clause condition",
        "mod() within a FACET clause",
        "percentage(function(attribute), WHERE condition)",
        "percentile(attribute [, percentile [, ...]])",
        "Basic percentile query",
        "predictLinear(attribute, [,time interval])",
        "rate(function(attribute) [,time interval])",
        "Basic rate query",
        "round(attribute)",
        "stddev(attribute)",
        "stdvar(attribute)",
        "sum(attribute)",
        "uniqueCount(attribute)",
        "uniques(attribute [,limit])",
        "Using tuple",
        "capture(attribute, regular expression)",
        "capture() within a SELECT clause condition",
        "capture() within a FACET clause condition",
        "capture() within a WHERE clause condition",
        "capture() with a numeric cast",
        "Non-aggregator functions",
        "earliest(attribute)",
        "Get earliest country per user agent from PageView",
        "eventType()",
        "Use eventType() in filter() function",
        "Use eventType() with FACET",
        "filter(function(attribute), WHERE condition)",
        "Analyze purchases that used offer codes",
        "funnel(attribute, steps)",
        "getField(attribute, field)",
        "histogram(attribute, ceiling [,number of buckets])",
        "Histogram of response times from PageView events",
        "Prometheus histogram buckets",
        "New Relic distribution metric",
        "Histogram with a FACET clause",
        "keyset()",
        "See all attributes for a data type",
        "latest(attribute)",
        "Get most recent country per user agent from PageView",
        "Type conversion"
      ],
      "title": "NRQL syntax, clauses, and functions",
      "type": "docs",
      "tags": [
        "Query your data",
        "NRQL: New Relic Query Language",
        "Get started"
      ],
      "external_id": "97c38ce7950d354d9f1d9efa5f432326f9bb4b00",
      "image": "https://docs.newrelic.com/static/507a44dd5750a7c536bee652e105179f/8c557/screen-apdex-function.png",
      "url": "https://docs.newrelic.com/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions/",
      "published_at": "2022-01-12T11:33:08Z",
      "updated_at": "2021-12-30T17:52:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NRQL is a query language you can use to query the New Relic database. This document explains NRQL syntax, clauses, components, and functions. Syntax This document is a reference for the functions and clauses used in a NRQL query. Other resources for understanding NRQL: Intro to NRQL: explains what NRQL is used for, what data you can query with it, and basic NRQL syntax Examine NRQL queries used to build New Relic charts Learn how to query the Metric data type Use funnels to evaluate a series of related data Format NRQL for querying with the Event API Query components Every NRQL query will begin with a SELECT statement or a FROM clause. All other clauses are optional. The clause definitions below also contain example NRQL queries. Required clauses Required: SELECT statement SELECT attribute ... Copy SELECT function(attribute) ... Copy The SELECT specifies what portion of a data type you want to query by specifying an attribute or a function. It's followed by one or more arguments separated by commas. In each argument you can: Get the values of all available attributes by using * as a wildcard. For example: SELECT * from Transaction. Get values associated with a specified attribute or multiple attributes specified in a comma separated list. Get aggregated values from specified attributes by selecting an aggregator function. Label the results returned in each argument with the AS clause. You can also use SELECT with basic math functions. Avg response time since last week This query returns the average response time since last week. SELECT average(duration) FROM PageView SINCE 1 week ago Copy Required: FROM clause SELECT ... FROM data type ... Copy Use the FROM clause to specify the data type you wish to query. You can start your query with FROM or with SELECT. You can merge values for the same attributes across multiple data types in a comma separated list. Query one data type This query returns the count of all APM transactions over the last three days: SELECT count(*) FROM Transaction SINCE 3 days ago Copy Query multiple data types This query returns the count of all APM transactions and browser events over the last three days: SELECT count(*) FROM Transaction, PageView SINCE 3 days ago Copy Optional clauses AS clause SELECT ... AS 'label' ... Copy Use the AS clause to label an attribute, aggregator, step in a funnel, or the result of a math function with a string delimited by single quotes. The label is used in the resulting chart. Query using math function and AS This query returns the number of page views per session: SELECT count(*)/uniqueCount(session) AS 'Pageviews per Session' FROM PageView Copy Query using funnel and AS This query returns a count of people who have visited both the main page and the careers page of a site over the past week: SELECT funnel(SESSION, WHERE name='Controller/about/main' AS 'Step 1', WHERE name = 'Controller/about/careers' AS 'Step 2') FROM PageView SINCE 1 week ago Copy COMPARE WITH clause SELECT ... (SINCE or UNTIL) (integer units) AGO COMPARE WITH (integer units) AGO ... Copy Use the COMPARE WITH clause to compare the values for two different time ranges. COMPARE WITH requires a SINCE or UNTIL statement. The time specified by COMPARE WITH is relative to the time specified by SINCE or UNTIL. For example, SINCE 1 day ago COMPARE WITH 1 day ago compares yesterday with the day before. The time range for theCOMPARE WITH value is always the same as that specified by SINCE or UNTIL. For example, SINCE 2 hours ago COMPARE WITH 4 hours ago might compare 3:00pm through 5:00pm against 11:00am through 1:00pm. COMPARE WITH can be formatted as either a line chart or a billboard: With TIMESERIES, COMPARE WITH creates a line chart with the comparison mapped over time. Without TIMESERIES, COMPARE WITH generates a billboard with the current value and the percent change from the COMPARE WITH value. Example: This query returns data as a line chart showing the 95th percentile for the past hour compared to the same range one week ago. First as a single value, then as a line chart. SELECT percentile(duration) FROM PageView SINCE 1 week ago COMPARE WITH 1 week AGO SELECT percentile(duration) FROM PageView SINCE 1 week ago COMPARE WITH 1 week AGO TIMESERIES AUTO Copy EXTRAPOLATE clause You can use this clause with these data types: Transaction TransactionError Custom events reported via APM agent APIs The purpose of EXTRAPOLATE is to mathematically compensate for the effects of APM agent sampling of event data so that query results more closely represent the total activity in your system. This clause will be useful when a APM agent reports so many events that it often passes its harvest cycle reporting limits. When that occurs, the agent begins to sample events. When EXTRAPOLATE is used in a NRQL query that supports its use, the ratio between the reported events and the total events is used to extrapolate a close approximation of the total unsampled data. When it is used in a NRQL query that doesn’t support its use or that hasn’t used sampled data, it has no effect. Important Note that EXTRAPOLATE is most useful for homogenous data (like throughput or error rate). It's not effective when attempting to extrapolate a count of distinct things (like uniqueCount() or uniques()). This clause works only with NRQL queries that use one of the following aggregator functions: apdex average count histogram sum percentage (if function it takes as an argument supports EXTRAPOLATE) rate (if function it takes as an argument supports EXTRAPOLATE) stddev Example of extrapolating throughput A query that will show the extrapolated throughput of a service named interestingApplication. SELECT count(*) FROM Transaction WHERE appName='interestingApplication' SINCE 60 minutes ago EXTRAPOLATE Copy Example of extrapolating throughput as a time series A query that will show the extrapolated throughput of a service named interestingApplication by transaction name, displayed as a time series. SELECT count(*) FROM Transaction WHERE appName='interestingApplication' SINCE 60 minutes ago FACET name TIMESERIES 1 minute EXTRAPOLATE Copy FACET clause SELECT ... FACET attribute ... Copy Use FACET to separate and group your results by attribute values. For example, you could FACET your PageView data by deviceType to figure out what percentage of your traffic comes from mobile, tablet, and desktop devices. Use the LIMIT clause to specify how many facets appear (default is 10). For more complex grouping, use FACET CASES. FACET clauses support up to five attributes, separated by commas. The facets are sorted in descending order by the first field you provide in the SELECT clause. If you are faceting on attributes with more than 2,000 unique values, a subset of facet values is selected and sorted according to the query type. When selecting min(), max(), percentile(), average() or count(), FACET uses those functions to determine how facets are picked and sorted. When selecting any other function, FACET uses the frequency of the attribute you are faceting on to determine how facets are picked and sorted. Faceted query using count() This query shows cities with the highest pageview counts. This query uses the total number of pageviews per city to determine how facets are picked and ordered. SELECT count(*) FROM PageView FACET city Copy Faceted query using uniqueCount() This query shows the cities that access the highest number of unique URLs. This query uses the total number of times a particular city appears in the results to determine how facets are picked and ordered. SELECT uniqueCount(pageUrl) FROM PageView FACET city Copy Grouping results across time Advanced segmentation and cohort analysis allow you to facet on bucket functions to more effectively break out your data. Cohort analysis is a way to group results together based on timestamps. You can separate them into buckets that cover a specified range of dates and times. FACET ... AS clause Use FACET ... AS to name facets using the AS keyword in queries. This clause is helpful for adding clearer or simplified names for facets in your results. It can also be used to rename facets in nested aggregation queries. FACET ... AS queries will change the facet names in results (when they appear as headers in tables, for example), but not the actual facet names themselves. FROM Transaction SELECT count(*) FACET response.headers.contentType AS 'content type' Copy FACET CASES clause SELECT ... FACET CASES ( WHERE attribute operator value, WHERE attribute operator value, ... ) ... Copy Use FACET CASES to break out your data by more complex conditions than possible with FACET. Separate multiple conditions with a comma ,. For example, you could query your PageView data and FACET CASES into categories like less than 1 second, from 1 to 10 seconds, and greater than 10 seconds. You can combine multiple attributes within your cases, and label the cases with the AS selector. Data points will be added to at most one facet case, the first facet case that they match. You may also use a time function with your attribute, and you can use the OR operator to facet results that don't match any of your specified cases. Basic usage with WHERE SELECT count(*) FROM PageView FACET CASES (WHERE duration < 1, WHERE duration > 1 and duration < 10, WHERE duration > 10) Copy Group based on multiple attributes This example groups results into one bucket where the transaction name contains login, and another where the URL contains login and a custom attribute indicates that the user was a paid user: SELECT count(*) FROM Transaction FACET CASES (WHERE name LIKE '%login%', WHERE name LIKE '%feature%' AND customer_type='Paid') Copy Label groups with AS This example uses the AS selector to give your results a human-readable name: SELECT count(*) FROM Transaction FACET CASES (WHERE name LIKE '%login%' AS 'Total Logins', WHERE name LIKE '%feature%' AND customer_type='Paid' AS 'Feature Visits from Paid Users') Copy Facet non-matching data with OR This example uses the OR operator to facet results that didn't match any of your cases: SELECT count(*) FROM Transaction FACET CASES (WHERE name LIKE '%login%', WHERE name LIKE '%feature%' AND customer_type='Paid') OR name Copy FACET ... ORDER BY clause In NRQL, the default is for the first aggregation in the SELECT clause to guide the selection of facets in a query. FACET ... ORDER BY allows you to override this default behavior by adding an aggregate function with the ORDER BY modifier to specify how facets are selected. Specifically, the clause will override the priority by which facets are chosen to be in the final result before being limited by the LIMIT clause. This clause can be used in querying but not for alerts or streaming. This example shows how to use FACET ... ORDER BY to find the average durations of app transactions, showing the top 10 (default limit) highest durations by apps which have the highest response size. In this case, if FACET ... ORDER BY is not used, the query results will instead show the top 10 by highest durations, with response size being irrelevant to the app selection. FROM Transaction SELECT average(duration) TIMESERIES FACET appName ORDER BY max(responseSize) Copy Tip Because the operations are performed before the LIMIT clause is applied, FACET ... ORDER BY does not impact the sort of the final query results, which will be particularly noticeable in the results for non-timeseries queries. Important The ORDER BY modifier in this case works differently than the ORDER BY clause. When parsing queries that follow the format FACET attribute1 ORDER BY attribute2, New Relic will read these as FACET ... ORDER BY queries, but only if ORDER BY appears immediately after FACET. Otherwise ORDER BY will be interpreted by New Relic as a clause. LIMIT clause SELECT ... LIMIT count ... Copy Use the LIMIT clause to control the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. This clause takes a single integer value as an argument. If the LIMIT clause is not specified, or no value is provided, the limit defaults to 10 for FACET queries and 100 in the case of SELECT * queries. The maximum allowed value for the LIMIT clause is 2,000. Query using LIMIT This query shows the top 20 countries by session count and provides 95th percentile of response time for each country for Windows users only. SELECT uniqueCount(session), percentile(duration, 95) FROM PageView WHERE userAgentOS = 'Windows' FACET countryCode LIMIT 20 SINCE YESTERDAY Copy OFFSET clause SELECT ... LIMIT count OFFSET count ... Copy Use the OFFSET clause with LIMIT to control the portion of rows returned by SELECT * or SELECT column queries. Like the LIMIT clause, OFFSET takes a single integer value as an argument. OFFSET sets the number of rows to be skipped before the selected rows of your query are returned. This is constrained by LIMIT. OFFSET rows are skipped starting from the most recent record. For example, the query SELECT interestingValue FROM Minute_Report LIMIT 5 OFFSET 1 returns the last 5 values from Minute_Report except for the most recent one. ORDER BY clause The ORDER BY clause allows you to specify how you want to sort your query results in queries that select event attributes by row. This query orders transactions by duration. FROM Transaction SELECT appName, duration ORDER BY duration Copy The default sort order is ascending, but this can be changed by adding the ASC or DESC modifiers. SHOW EVENT TYPES clause SHOW EVENT TYPES... Copy SHOW EVENT TYPES will return a list of all the data types present in your account for a specific time range. It is used as the first clause in a query instead of SELECT. Important In this context, \"event types\" refers to the data types you can access with a NRQL query. Data types in the last day This query will return all the data types present over the past day: SHOW EVENT TYPES SINCE 1 day ago Copy SINCE clause SELECT ... SINCE [numerical units AGO | phrase] ... Copy The default value is 1 hour ago. Use the SINCE clause to define the beginning of a time range for the returned data. You can specify a timezone for the query but not for the results. NRQL results are based on your system time. When using NRQL, you can set a UTC timestamp or a relative time range: Timestamps use the format YYYY-MM-DD HH:MM:SS ZZZZ. For instance, FROM Transaction SELECT count(*) SINCE '2021-12-25 00:00:00 +0000' UNTIL '2021-12-25 23:59:59 +0000'. We support the following relative time ranges: YESTERDAY, TODAY, SUNDAY, MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY. For example, SINCE YESTERDAY UNTIL NOW. We also support YEAR, QUARTER, MONTH, WEEK, DAY, HOUR, MINUTE, SECOND. For these cases, you can combine SINCE with THIS or LAST. For instance, SINCE LAST MONTH UNTIL THIS WEEK. You can also include AGO, as in SINCE 3 WEEKS AGO UNTIL 10 MINUTES AGO. SLIDE BY clause The SLIDE BY clause supports a feature known as sliding windows. With sliding windows,SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. To use SLIDE BY, place it in a query after the TIMESERIES clause. For example, this query pulls data in 5-minute windows with a 1-minute SLIDE BY interval, meaning that each window lasts 5 minutes, but window 1 starts at 0 minutes, window 2 starts at 1 minute, window 3 starts at 2 minutes, and so on. SELECT average(duration) FROM Transaction TIMESERIES 5 minutes SLIDE BY 1 minute Copy To learn more about how and when you can use SLIDE BY, see Create smoother charts with sliding windows. Or, watch this short video (approx. 3:20 minutes). Use SLIDE BY with MAX or AUTO interval You can use sliding windows in combination with MAX or AUTO. However, MAX or AUTO may not be placed between TIMESERIES and SLIDE BY. This query will automatically decide a SLIDE BY window interval. SELECT average(duration) FROM Transaction TIMESERIES 5 minutes SLIDE BY AUTO Copy This query will set the SLIDE BY window to the maximum interval granularity. SELECT average(duration) FROM Transaction TIMESERIES 5 minutes SLIDE BY MAX Copy Important The SLIDE BY value as determined by AUTO or MAX can produce a step interval greater than the window size, which can cause gaps and unexpected results. TIMESERIES clause SELECT ... TIMESERIES integer units ... Copy Use the TIMESERIES clause to return data as a time series broken out by a specified period of time. Since TIMESERIES is used to trigger certain charts, there is no default value. To indicate the time range, use integer units. For example: TIMESERIES 1 minute TIMESERIES 30 minutes TIMESERIES 1 hour TIMESERIES 30 seconds TIMESERIES can be combined with arguments such as MAX, AUTO, and SLIDE BY to further tailor query results, as shown in the examples below. Important For functions such as average( ) or percentile( ), a large aggregation window can have a significant smoothing effect on outliers. This is true whether or not the query makes use of sliding windows. Use a set interval The value provided indicates the units used to break out the graph. For example, to present a one-day graph showing 30 minute increments: SELECT ... SINCE 1 day AGO TIMESERIES 30 minutes Copy Use an automatically set interval TIMESERIES can also be set to AUTO, which will divide your graph into a reasonable number of divisions. For example, a daily chart will be divided into 30 minute intervals and a weekly chart will be divided into 6 hour intervals. This query returns data as a line chart showing the 50th and 90th percentile of client-side transaction time for one week with a data point every 6 hours. SELECT average(duration), percentile(duration, 50, 90) FROM PageView SINCE 1 week AGO TIMESERIES AUTO Copy Use MAX interval You can set TIMESERIES to MAX, which will automatically adjust your time window to the maximum number of intervals allowed for a given time period. This allows you to update your time windows without having to manually update your TIMESERIES buckets and ensures your time window is being split into the peak number of intervals allowed. The maximum number of TIMESERIES buckets that will be returned is 366. For example, the following query creates 4-minute intervals, which is the ceiling for a daily chart. SELECT average(duration) FROM Transaction since 1 day ago TIMESERIES MAX Copy UNTIL clause SELECT ... UNTIL integer units AGO ... Copy The default value is NOW. Only use UNTIL to specify an end point other than the default. Use the UNTIL clause to define the end of a time range across which to return data. Once a time range has been specified, the data will be preserved and can be reviewed after the time range has ended. See Use the time picker to adjust time settings for detailed information and examples. WHERE clause Use the WHERE clause to filter results. NRQL returns the results that fulfill the condition(s) you specify in the clause. SELECT function(attribute) ... WHERE attribute [operator 'value' | IN ('value' [, 'value]) | IS [NOT] NULL ] [AND|OR ...] ... Copy If you specify more than one condition, separate the conditions by the operators AND or OR. If you want to simulate a SQL join, use custom attributes in a WHERE or FACET clause. Operators that the WHERE clause accepts Description =, !=, <, <=, >, >= NRQL accepts standard comparison operators. Example: state = 'WA' AND Used to define an intersection of two conditions. OR Used to define a union of two conditions. IS NULL Determines if an attribute has a null value. IS NOT NULL Determines if an attribute does not have a null value. IN Determines if the string value of an attribute is in a specified set. Using this method yields better performance than stringing together multiple WHERE clauses. Example: animalType IN ('cat', 'dog', 'fish') NOT IN Determines if the string value of an attribute is not in a specified set. Using this method yields better performance than stringing together multiple WHERE clauses. Values must be in parentheses, separated by commas. For example: SELECT * FROM PageView WHERE countryCode NOT IN ('CA', 'WA') Copy LIKE Determines if an attribute contains a specified sub-string. The string argument for the LIKE operator accepts the percent sign (%) as a wildcard anywhere in the string. If the substring does not begin or end the string you are matching against, the wildcard must begin or end the string. Examples: userAgentName LIKE 'IE%' IE IE Mobile userAgentName LIKE 'o%a%' Opera Opera Mini userAgentName LIKE 'o%a' Opera userAgentName LIKE '%o%a%' Opera Opera Mini Mozilla Gecko NOT LIKE Determines if an attribute does not contain a specified sub-string. RLIKE Determines if an attribute contains a specified Regex sub-string. Uses RE2 syntax. Examples: appName RLIKE r'z.*|q.*'' hostname RLIKE r'ip-10-351-[0-2]?[0-9]-.*' z-app q-app ip-10-351-19-237 ip-10-351-2-41 ip-10-351-24-238 ip-10-351-14-15 Important Regex defaults to full-string matching, therefore ^ and $ are implicit and you do not need to add them. NOT RLIKE Determines if an attribute does not contain a specified Regex sub-string. Uses RE2 syntax. Example query with three conditions This query returns the browser response time for pages with checkout in the URL for Safari users in the United States and Canada over the past 24 hours. SELECT histogram(duration, 50, 20) FROM PageView WHERE countryCode IN ('CA', 'US') AND userAgentName='Safari' AND pageUrl LIKE '%checkout%' SINCE 1 day ago Copy WITH METRIC_FORMAT clause For information on querying metric data, see Query metrics. WITH TIMEZONE clause SELECT ... WITH TIMEZONE (selected zone) ... Copy By default, query results are displayed in the timezone of the browser you're using. Use the WITH TIMEZONE clause to select a time zone for a date or time in the query that hasn't already had a time zone specified for it. For example, the query clause SINCE Monday UNTIL Tuesday WITH TIMEZONE 'America/New_York' will return data recorded from Monday at midnight, Eastern Standard Time, until midnight Tuesday, Eastern Standard Time. Available Time Zone Selections Africa/Abidjan Africa/Addis_Ababa Africa/Algiers Africa/Blantyre Africa/Cairo Africa/Windhoek America/Adak America/Anchorage America/Araguaina America/Argentina/Buenos_Aires America/Belize America/Bogota America/Campo_Grande America/Cancun America/Caracas America/Chicago America/Chihuahua America/Dawson_Creek America/Denver America/Ensenada America/Glace_Bay America/Godthab America/Goose_Bay America/Havana America/La_Paz America/Los_Angeles America/Miquelon America/Montevideo America/New_York America/Noronha America/Santiago America/Sao_Paulo America/St_Johns Asia/Anadyr Asia/Bangkok Asia/Beirut Asia/Damascus Asia/Dhaka Asia/Dubai Asia/Gaza Asia/Hong_Kong Asia/Irkutsk Asia/Jerusalem Asia/Kabul Asia/Katmandu Asia/Kolkata Asia/Krasnoyarsk Asia/Magadan Asia/Novosibirsk Asia/Rangoon Asia/Seoul Asia/Tashkent Asia/Tehran Asia/Tokyo Asia/Vladivostok Asia/Yakutsk Asia/Yekaterinburg Asia/Yerevan Atlantic/Azores Atlantic/Cape_Verde Atlantic/Stanley Australia/Adelaide Australia/Brisbane Australia/Darwin Australia/Eucla Australia/Hobart Australia/Lord_Howe Australia/Perth Chile/EasterIsland Etc/GMT+10 Etc/GMT+8 Etc/GMT-11 Etc/GMT-12 Europe/Amsterdam Europe/Belfast Europe/Belgrade Europe/Brussels Europe/Dublin Europe/Lisbon Europe/London Europe/Minsk Europe/Moscow Pacific/Auckland Pacific/Chatham Pacific/Gambier Pacific/Kiritimati Pacific/Marquesas Pacific/Midway Pacific/Norfolk Pacific/Tongatapu UTC See Set time range on dashboards and charts for detailed information and examples. Query metric data Metric data is more complex than other types of data. There are specific tips for querying it well. We have two types of metric data, each with their own query guidelines: Query dimensional metrics, which are reported by our Metric API and by some of our solutions that use that API (for example, our Dropwizard integration or Micrometer integration). Query metric timeslice data, which is our original metric data type reported by our APM, mobile monitoring, and browser monitoring. For more details about how we report metric data, see Metric data types. Functions In this section we explain NRQL functions, both aggregator functions and non-aggregator functions. Aggregator functions You can use aggregator functions to filter and aggregate data. Some tips for using these: See New Relic University tutorials for Filter queries, Apdex queries, and Percentile queries. Or, go to the full online course Writing NRQL queries. If you're using an aggregator function multiple times in the same query (for example, SELECT median(one_metric), median(another_metric)), it can cause problems in displaying results. To solve this, use the AS function. For example: `SELECT median(one_metric) as 'med-a', median(another_metric) as 'med-b'` Copy Data type \"coercion\" is not supported. Read about available type conversion functions. For how to display results over time, see Group results over time. Examples: SELECT histogram(duration, 10, 20) FROM PageView SINCE 1 week ago Copy aggregationendtime() Use the aggregationendtime() function to return the time of the relevant aggregation. More specifically, for a given aggregate, the aggregationendtime() function provides the timestamp of the end of the time period of that aggregation. For example, in a timeseries query, for a data point that encompasses an hour’s worth of data, the function would return the timestamp of the end of that hour period. apdex(attribute, t: ) Use the apdex function to return an Apdex score for a single transaction or for all your transactions. The attribute can be any attribute based on response time, such as duration or backendDuration. The t: argument defines an Apdex T threshold in the same unit of time as the chosen attribute. For instance, if the attribute is measured in seconds, t will be a threshold in seconds. The Apdex score returned by the apdex( ) function is based only on execution time. It does not account for APM errors. If a transaction includes an error but completes in Apdex T or less, that transaction will be rated satisfying by the apdex ( ) function. Get Apdex for specific customers If you have defined custom attributes, you can filter based on those attributes. For example, you could monitor the Apdex for a particularly important customer: SELECT apdex(duration, t: 0.4) FROM Transaction WHERE customerName='ReallyImportantCustomer' SINCE 1 day ago Copy Get Apdex for specific transaction Use the name attribute to return a score for a specific transaction, or return an overall Apdex by omitting name. This query returns an Apdex score for the Controller/notes/index transaction over the last hour: The apdex function returns an Apdex score that measures user satisfaction with your site. Arguments are a response time attribute and an Apdex T threshold in seconds. SELECT apdex(duration, t: 0.5) from Transaction WHERE name='Controller/notes/index' SINCE 1 hour ago Copy Get overall Apdex for your app This example query returns an overall Apdex for the application over the last three weeks: SELECT apdex(duration, t: 0.08) FROM Transaction SINCE 3 week ago Copy average(attribute) Use the average( ) function to return the average value for an attribute. It takes a single attribute name as an argument. If a value of the attribute is not numeric, it will be ignored when aggregating. If data matching the query's conditions is not found, or there are no numeric values returned by the query, it will return a value of null. buckets(attribute, ceiling [,number of buckets]) Use the buckets() function to aggregate data split up by a FACET clause into buckets based on ranges. You can bucket by any attribute that is stored as a numerical value in the New Relic database. It takes three arguments: Attribute name Maximum value of the sample range. Any outliers will appear in the final bucket. Total number of buckets For more information and examples, see Split your data into buckets. bucketPercentile(attribute) The bucketPercentile( ) function is the NRQL equivalent of the histogram_quantile function in Prometheus. It is intended to be used with dimensional metric data. Instead of the quantile, New Relic returns the percentile, which is the quantile * 100. Use the bucketPercentile( ) function to calculate the quantile from the histogram data in a Prometheus format. It takes the bucket name as an argument and reports percentiles along the bucket's boundaries: SELECT bucketPercentile(duration_bucket) FROM Metric SINCE 1 day ago Copy Optionally, you can add percentile specifications as an argument: SELECT bucketPercentile(duration_bucket, 50, 75, 90) FROM Metric SINCE 1 day ago Copy Because multiple metrics are used to make up Prometheus histogram data, you must query for specific Prometheus metrics in terms of the associated <basename>. For example, to compute percentiles from a Prometheus histogram, with the <basename> prometheus_http_request_duration_seconds using NRQL, use bucketPercentile(prometheus_http_request_duration_seconds_bucket, 50). Note how _ bucket is added to the end of the <basename> as a suffix. See the Prometheus.io documentation for more information. cardinality(attribute) Use the cardinality( ) function to obtain the number of combinations of all the dimensions (attributes) on a metric. It takes three arguments, all optional: Metric name: if present, cardinality( ) only computes the metric specified. Include: if present, the include list restricts the cardinality computation to those attributes. Exclude: if present, the exclude list causes those attributes to be ignored in the cardinality computation. SELECT cardinality(metric_name, include:{attribute_list}, exclude:{attribute_list}) Copy count(*) Use the count( ) function to return a count of available records. It takes a single argument; either *, an attribute, or a constant value. Currently, it follows typical SQL behavior and counts all records that have values for its argument. Since count(*) does not name a specific attribute, the results will be formatted in the default \"humanize\" format. derivative(attribute [,time interval]) derivative() finds the rate of change for a given dataset. The rate of change is calculated using a linear least-squares regression to approximate the derivative. Since this calculation requires comparing more than one datapoint, if only one datapoint is included in the evaluation range, the calculation is indeterminate and won't work, resulting in a null value. The time interval is the period for which the rate of change is calculated. For example, derivative(attributeName, 1 minute) will return the rate of change per minute. dimensions(include: {attributes}, exclude: {attributes}) Use the dimensions( ) function to return all the dimensional values on a data type. You can explicitly include or exclude specific attributes using the optional arguments: Include: if present, the include list limits dimensions( ) to those attributes. Exclude: if present, the dimensions( ) calculation ignores those attributes. FROM Metric SELECT count(node_filesystem_size) TIMESERIES FACET dimensions() Copy When used with a FACET clause, dimensions( ) produces a unique timeseries for all facets available on the event type, similar to how Prometheus behaves with non-aggregated queries. latestrate(attribute, time interval) Use the latestrate( ) function to return the rate of change of a value based on the last 2 data points. It takes the attribute in question as the first argument and the unit of time for the resulting rate as the second argument. The function returns a result in units of change in attribute/time interval. This function can be useful to provide the most recent rate of change for an attribute in order to see leading-edge trends. Get the most recent rate of change of PageView Duration This query returns the rate of change of duration based on the last 2 data points. It will be returned in units of duration/second because of the 1 SECOND argument. SELECT latestrate(duration, 1 SECOND) FROM PageView Copy max(attribute) Use the max( ) function to return the maximum recorded value of a numeric attribute over the time range specified. It takes a single attribute name as an argument. If a value of the attribute is not numeric, it will be ignored when aggregating. If data matching the query's conditions is not found, or there are no numeric values returned by the query, it will return a value of null. median(attribute) Use the median( ) function to return an attribute's median, or 50th percentile. For more information about percentile queries, see percentile(). Tip The median( ) query is only available when using the query builder. Median query This query will generate a line chart for the median value. SELECT median(duration) FROM PageView TIMESERIES AUTO Copy min(attribute) Use the min( ) function to return the minimum recorded value of a numeric attribute over the time range specified. It takes a single attribute name as an argument. If a value of the attribute is not numeric, it will be ignored when aggregating. If data matching the query's conditions is not found, or there are no numeric values returned by the query, it will return a value of null. minuteOf(attribute) Use the minuteOf() function to extract only the minute portion (that is, seconds 0 to 59) of an attribute holding a valid timestamp value. mod(attribute, divisor) Use the mod( ) function to return the floor modulus after dividing the value of the provided numeric attribute (the first argument, or dividend) by a numeric value (the second argument, or divisor). This modulo operation can be used within a WHERE clause condition to filter to an arbitrary subset of results or within a FACET clause as a way to subdivide the result set. mod() within a WHERE clause condition FROM Transaction SELECT * WHERE mod(port, 2) = 1 Copy mod() within a FACET clause FROM NrDailyUsage SELECT uniques(hostId, 10000) SINCE 1 day AGO FACET mod(hostId, 10) Copy percentage(function(attribute), WHERE condition) Use the percentage( ) function to return the percentage of a target data set that matches some condition. The first argument requires an aggregator function against the desired attribute. Use exactly two arguments (arguments after the first two will be ignored). If the attribute is not numeric, this function returns a value of 100%. percentile(attribute [, percentile [, ...]]) Use the percentile( ) function to return an attribute's approximate value at a given percentile. It requires an attribute and can take any number of arguments representing percentile points. The percentile() function enables percentiles to displays with up to three digits after the decimal point, providing greater precision. Percentile thresholds may be specified as decimal values, but be aware that for most data sets, percentiles closer than 0.1 from each other will not be resolved. Percentile display examples Use TIMESERIES to generate a line chart with percentiles mapped over time. Omit TIMESERIES to generate a billboard and attribute sheet showing aggregate values for the percentiles. If no percentiles are listed, the default is the 95th percentile. To return only the 50th percentile value, the median, you can also use median(). Basic percentile query This query will generate a line chart with lines for the 5th, 50th, and 95th percentile. SELECT percentile(duration, 5, 50, 95) FROM PageView TIMESERIES AUTO Copy predictLinear(attribute, [,time interval]) predictLinear() is an extension of the derivative() function. It uses a similar method of least-squares linear regression to predict the future values for a dataset. The time interval is how far the query will look into the future. For example, predictLinear(attributeName, 1 hour) is a linear prediction 1 hour into the future of the query time window. Generally, predictLinear() is helpful for continuously growing values like disk space, or predictions on large trends. Since predictLinear() is a linear regression, familiarity with the dataset being queried helps to ensure accurate long-term predictions. Any dataset which grows exponentially, logarithmically, or by other nonlinear means will likely only be successful in very short-term predictions. New Relic recommends against using predictLinear in TIMESERIES queries. This is because each bucket will be making an individual prediction based on its relative timeframe within the query, meaning that such queries will not show predictions from the end of the timeseries forward. rate(function(attribute) [,time interval]) Use the rate( ) function to visualize the frequency or rate of a given query per time interval. For example, you might want to know the number of pageviews per minute over an hour-long period or the count of unique sessions on your site per hour over a day-long period. Use TIMESERIES to generate a line chart with rates mapped over time. Omit TIMESERIES to generate a billboard showing a single rate value averaged over time. Basic rate query This query will generate a line chart showing the rate of throughput for APM transactions per 10 minutes over the past 6 hours. SELECT rate(count(*), 10 minute) FROM Transaction SINCE 6 hours ago TIMESERIES Copy round(attribute) Use the round( ) function to return the rounded value of an attribute. Optionally round( ) can take a second argument, to_nearest, to round the first argument to the closest multiple of the second one. to_nearest can be fractional. SELECT round(n [, to_nearest]) Copy stddev(attribute) Use the stddev( ) function to return one standard deviation for a numeric attribute over the time range specified. It takes a single argument. If the attribute is not numeric, it will return a value of zero. stdvar(attribute) Use the stdvar( ) function to return the standard variance for a numeric attribute over the time range specified. It takes a single argument. If the attribute is not numeric, it will return a value of zero. sum(attribute) Use the sum( ) function to return the sum recorded values of a numeric attribute over the time range specified. It takes a single argument. Arguments after the first will be ignored. If the attribute is not numeric, it will return a value of zero. uniqueCount(attribute) Use the uniqueCount( ) function to return the number of unique values recorded for an attribute over the time range specified. Tip To optimize query performance, this function returns approximate results for queries that inspect more than 256 unique values. uniques(attribute [,limit]) Use the uniques( ) function to return a list of unique values recorded for an attribute over the time range specified. When used along with the facet clause, a list of unique attribute values will be returned per each facet value. The limit parameter is optional. When it is not provided, the default limit of 1,000 unique attribute values per facet is applied. You may specify a different limit value, up to a maximum of 10,000. The uniques( ) function will return the first set of unique attribute values discovered, until the limit is reached. Therefore, if you have 5,000 unique attribute values in your data set, and the limit is set to 1,000, the operator will return the first 1,000 unique values that it discovers, regardless of their frequency. The maximum number of values that can be returned in a query result is the product of the uniques( ) limit times the facet limit. In the following query, the theoretical maximum number of values that can be returned is 5 million (5,000 x 1,000). Depending on the data set being queried, and the complexity of the query, memory protection limits may prevent a very large query from being executed. From Transaction SELECT uniques(host,5000) FACET appName LIMIT 1000 Copy Using tuple If you'd like to know the unique combinations of a handful of attributes, you can structure a query in the format SELECT uniques(tuple(x, y, ... z)) ...` to get all the unique tuples of values, to maintain their relationship. In the following query, tuple is used on index and cellName together to find uniques where those two values occur in combination. FROM NodeStatus SELECT uniques(tuple(index, cellName), 5) Copy capture(attribute, regular expression) Use the capture() to extract values from an attribute using a regular expression. Uses RE2 syntax. It takes two arguments: Attribute name Regular expression with capture syntax. Regex expressions in NRQL use Python-like syntax, r'...'. When capturing, use the RE2 named-capture syntax ...(?P<name> pattern )... to capture the contained pattern, given the specified name. Currently, only 1 capture group is supported. Please see the examples below. capture() within a SELECT clause condition The following will select the domain name of the website, removing https:// and any paths following the .com SELECT capture(pageUrl, r'https://(?P<baseUrl>.*.com)/.+') FROM PageView SINCE 1 day ago Copy The following will capture only the first word of the error message. SELECT capture(errorMessage, r'(?P<firstWord>\\S+)\\s.+') FROM Transaction SINCE 1 hour ago where errorMessage is not null Copy capture() within a FACET clause condition The following will facet by the captured HTTP method. SELECT count(*) FROM Log WHERE message like '%HTTP%' FACET capture(message, r'.* \"(?P<httpMethod>[A-Z]+) .*') Copy capture() within a WHERE clause condition The following will filter the results based on Log events with message attribute that matches the regular expression where the captured job name is ExampleJob. SELECT message FROM Log WHERE capture(message, r'.*Job Failed: (?P<jobName>[A-Za-z]+),.*') = 'ExampleJob' SINCE 10 minutes ago Copy capture() with a numeric cast The following will capture sum of CPU Time from log lines. You must explicitly cast to numeric to do mathematical operations. SELECT sum(numeric(capture(message, r'.*CpuTime:\\s(?P<cpuTime>\\d+)'))) FROM Log WHERE message like '%CpuTime:%' SINCE 1 hour ago Copy Non-aggregator functions Use non-aggregator functions for non-numerical data in NRQL queries. earliest(attribute) Use the earliest( ) function to return the earliest value for an attribute over the specified time range. It takes a single argument. Arguments after the first will be ignored. If used in conjunction with a FACET it will return the most recent value for an attribute for each of the resulting facets. Get earliest country per user agent from PageView This query returns the earliest country code per each user agent from the PageView event. SELECT earliest(countryCode) FROM PageView FACET userAgentName Copy eventType() ...WHERE eventType() = 'EventNameHere'... ...FACET eventType()... Copy Use the eventType() function in a FACET clause to break out results by the selected data type or in a WHERE clause to filter results to a specific data type. This is particularly useful for targeting specific data types with the filter() and percentage() functions. Important In this context, \"event type\" refers to the types of data you can access with a NRQL query. Use eventType() in filter() function This query returns the percentage of total TransactionError results out of the total Transaction results. You can use the eventType() function to target specific types of data with the filter() function. SELECT 100 * filter(count(*), where eventType() = 'TransactionError') / filter(count(*), where eventType() = 'Transaction') FROM Transaction, TransactionError WHERE appName = 'App.Prod' TIMESERIES 2 Minutes SINCE 6 hours ago Copy Use eventType() with FACET This query displays a count of how many records each data type (Transaction and TransactionError) returns. SELECT count(*) FROM Transaction, TransactionError FACET eventType() TIMESERIES Copy filter(function(attribute), WHERE condition) Use the filter() function to limit the results for one of the aggregator functions in your SELECT statement. You can use filter() in conjunction with FACET or TIMESERIES. Filter is only useful when selecting multiple different aggregations such as SELECT filter(sum(x), WHERE attribute='a') AS 'A', filter(sum(x), WHERE attribute='b') AS 'B' .... Otherwise, it's better to just use the standard WHERE clause. Analyze purchases that used offer codes You could use filter() to compare the items bought in a set of transactions for those using an offer code versus those who aren't: Use the filter( ) function to limit the results for one of the aggregator functions in your SELECT statement. funnel(attribute, steps) Use the funnel() function to generate a funnel chart. It takes an attribute as its first argument. You then specify steps as WHERE clauses (with optional AS clauses for labels) separated by commas. For details and examples, see the funnels documentation. getField(attribute, field) Use the getField() function to extract a field from compound data types, such as metric data. It takes the following arguments: Metric type Supported fields summary count, total, max, min, type gauge count, total, max, min, latest, type distribution count, total, max, min, type counter count, type timeslice count, total, totalExclusive, min, and max Examples: SELECT max(getField(mySummary, count)) from Metric Copy SELECT sum(mySummary) from Metric where getField(mySummary, count) > 10 Copy histogram(attribute, ceiling [,number of buckets]) Use the histogram( ) function to generate histograms. It takes three arguments: Attribute name Maximum value of the sample range Total number of buckets (between 1 and 500, inclusive) Histogram of response times from PageView events This query results in a histogram of response times ranging up to 10 seconds over 20 buckets. SELECT histogram(duration, 10, 20) FROM PageView SINCE 1 week ago Copy Prometheus histogram buckets histogram( ) accepts Prometheus histogram buckets: SELECT histogram(duration_bucket, 10, 20) FROM Metric SINCE 1 week ago Copy New Relic distribution metric histogram( ) accepts Distribution metric as an input: SELECT histogram(myDistributionMetric, 10, 20) FROM Metric SINCE 1 week ago Copy Histogram with a FACET clause Use histogram( ) with a FACET clause to generate a heatmap chart: SELECT histogram(duration) FROM PageView FACET appName SINCE 1 week ago Copy keyset() Using keyset() will allow you to see all of the attributes for a given data type over a given time range. It takes no arguments. It returns a JSON structure containing groups of string-typed keys, numeric-typed keys, boolean-typed keys, and all keys. See all attributes for a data type This query returns the attributes found for PageView events from the last day: SELECT keyset() FROM PageView SINCE 1 day ago Copy latest(attribute) Use the latest( ) function to return the most recent value for an attribute over a specified time range. It takes a single argument. Arguments after the first will be ignored. If used in conjunction with a FACET it will return the most recent value for an attribute for each of the resulting facets. Get most recent country per user agent from PageView This query returns the most recent country code per each user agent from the PageView event. SELECT latest(countryCode) FROM PageView FACET userAgentName Copy Type conversion NRQL does not support \"coercion.\" This means that a float stored as a string is treated as a string and cannot be operated on by functions expecting float values. You can convert a string with a numeric value or a boolean with a string value to their numeric and boolean types with these functions: Use the numeric() function to convert a number with a string format to a numeric function. The function can be built into a query that uses math functions on query results or NRQL aggregator functions, such as average(). Use the boolean() function to convert a string value of \"true\" or \"false\" to the corresponding boolean value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 236.32787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>NRQL</em> syntax, clauses, and functions",
        "sections": "<em>Query</em> one <em>data</em> type",
        "tags": "<em>NRQL</em>: <em>New</em> <em>Relic</em> <em>Query</em> <em>Language</em>",
        "body": "<em>NRQL</em> is a <em>query</em> <em>language</em> you can use to <em>query</em> the <em>New</em> <em>Relic</em> database. This document explains <em>NRQL</em> syntax, clauses, components, and functions. Syntax This document is a reference for the functions and clauses used in a <em>NRQL</em> <em>query</em>. Other resources for understanding <em>NRQL</em>: Intro to <em>NRQL</em>: explains what"
      },
      "id": "604456c1196a678db8960f41"
    },
    {
      "sections": [
        "Query infrastructure dimensional metrics with NRQL",
        "BETA FEATURE",
        "Why it matters",
        "Get started",
        "Where and how to query dimensional metrics",
        "Naming conventions for metrics and attributes",
        "Examples",
        "AWS EBS query example",
        "Azure Service bus query example",
        "Azure functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Known limitations"
      ],
      "title": "Query infrastructure dimensional metrics with NRQL",
      "type": "docs",
      "tags": [
        "Query your data",
        "NRQL: New Relic Query Language",
        "NRQL query tutorials"
      ],
      "external_id": "a131af1f3655ef8b78acfccf8be619c43cb2c51e",
      "image": "https://docs.newrelic.com/static/916ce526afc3e8c7d9ea1325f1fdb980/1b853/naming-convention.png",
      "url": "https://docs.newrelic.com/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/query-infrastructure-dimensional-metrics-nrql/",
      "published_at": "2022-01-12T10:30:52Z",
      "updated_at": "2021-09-14T20:46:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Dimensional metrics are an industry standard for storing and querying metric data. All infrastructure metrics are stored as event data in New Relic, but you can also query them through dimensional metrics. In this page you can learn: The benefits of dimensional metrics. A few examples on how and where to use them. Known issues. Why it matters At New Relic we report metrics in several ways, including dimensional metrics, which are used by our metric API, Telemetry SDK, some open-source integrations, and our infrastructure services. This type of metric enables you to: Enjoy an improved query experience for Infrastructure data. Discover all your metrics in one place. Tap into more metric sources, such as Prometheus. For example, the query to get the maximum duration of your Lambda functions is simplified: Query with samples FROM ServerlessSample SELECT max(provider.duration.Maximum) WHERE provider = 'LambdaFunction' Copy Query with metrics FROM Metric SELECT max(aws.lambda.function.duration) Copy Get started No agent or integration updates are required to use these metrics. NRQL alerting based on dimensional metrics is also supported, except for data coming from cloud integrations (that is metrics from AWS polling integrations, GCP, and Azure). AWS CloudWatch Metric Streams metrics are ingested as dimensional metrics and NRQL alerts are recommended. Where and how to query dimensional metrics All current NRQL query features are supported. Queries can use WHERE, FACET, and time selection functions such as SINCE, UNTIL, and COMPARE WITH. The query builder in New Relic One supports metrics in both simple and advanced (NRQL) mode. Naming conventions for metrics and attributes All metric names and attributes for dimensional metrics follow the same naming convention in order to make them easy to find and use. Metric and attribute names are namespaced with dots: for example, the host. prefix is used for host metrics, the k8s. prefix is used for Kubernetes metrics, and aws. is used for AWS metrics. The graphic below shows how a ProcessSample that contains three metrics (cpuPercent, ioTotalReadBytes, and ioTotalWriteBytes) is split into three separate metrics. Note the updated naming of the metrics and the attributes. Dimensional metrics naming convention Examples Here are some examples of NQRL queries with and without dimensional metrics: AWS EBS query example Get the total write time by EBS Volume. Query with samples FROM BlockDeviceSample SELECT sum('provider.volumeTotalWriteTime.Sum') WHERE provider = 'EbsVolume' FACET entityName Copy Query with metrics FROM Metric SELECT sum(aws.ebs.volume.TotalWriteTime) FACET entity.name Copy Azure Service bus query example Maximum number of messages in an Azure Service Bus topic by resource group. Query with samples FROM AzureServiceBusTopicSample SELECT max(activeMessages.Maximum) FACET resourceGroupName Copy Query with metrics FROM Metric SELECT max(azure.servicebus.topic.activeMessages) FACET azure.resourceGroup Copy Azure functions query example Number of function executions Azure Functions over the past 6 hours by region over time. Query with samples FROM AzureFunctionsAppSample SELECT sum(functionExecutionCount.Total) FACET regionName TIMESERIES SINCE 6 hours ago Copy Query with metrics FROM Metric SELECT sum(azure.functions.app.functionExecutionCount) FACET azure.region TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Compare the number of Azure VMs over the past thirty minutes with the same time a week ago. Query with samples FROM AzureVirtualMachineScaleSetSample SELECT uniqueCount(vMName) FACET name SINCE 30 MINUTES AGO COMPARE WITH 1 WEEK AGO Copy Query with metrics FROM Metric SELECT uniqueCount(azure.vms.vmName) FACET azure.resourceName WHERE azure.resourceType='Microsoft.Compute/virtualMachineScaleSets' SINCE 30 MINUTES AGO COMPARE WITH 1 WEEK AGO Copy NGINX query example The average number of NGINX requests per second over time. Query with samples FROM NginxSample SELECT average(net.requestsPerSecond) TIMESERIES Copy Query with metrics FROM Metric SELECT average(nginx.server.net.requestsPerSecond) TIMESERIES Copy MySQL query example The maximum number of used MySQL connections. Query with samples FROM MysqlSample SELECT max(net.maxUsedConnections) Copy Query with metrics FROM Metric SELECT max(mysql.node.net.maxUsedConnections) Copy Known limitations Metric queries with * do not return Infrastructure sample data (for example, SELECT * FROM Metric). In order to select attributes starting with tags. a metric name has to be provided. For example, SELECT uniques(tags.environment) FROM Metric WHERE metricName='aws.lambda.function.duration' does not work without the WHERE clause. Results may not be complete if the selection criteria matches too many samples. For example, SELECT uniqueCount(entity.guid) FROM Metric maps to all Infrastructure samples, and may return incomplete results. Initially there is no support for the newly introduced metric wildcarding feature (for example, SELECT average(host.swap%Bytes) FROM Metric).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.28893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Query</em> infrastructure dimensional metrics with <em>NRQL</em>",
        "sections": "<em>Query</em> infrastructure dimensional metrics with <em>NRQL</em>",
        "tags": "<em>NRQL</em>: <em>New</em> <em>Relic</em> <em>Query</em> <em>Language</em>",
        "body": "BETA FEATURE This feature is currently in beta. Dimensional metrics are an industry standard for storing and querying metric <em>data</em>. All infrastructure metrics are stored as event <em>data</em> in <em>New</em> <em>Relic</em>, but you can also <em>query</em> them through dimensional metrics. In this page you can learn: The benefits"
      },
      "id": "603e95e8e7b9d286642a07fa"
    },
    {
      "sections": [
        "NRQL query examples for mobile monitoring",
        "Mobile, MobileSession, and MobileCrash event query examples",
        "Interactions: Which interactions are most popular among my users?",
        "Location: Which regions of China have the most users?",
        "Device profile: How many users use the latest OS versions?",
        "App version: Have we seen an increase in session duration since yesterday's release?",
        "Performance: How much memory does my app use for sessions longer than 5 seconds?",
        "Crashes: What are my app's most common crashes?",
        "Crash rate: What is the crash rate for different versions of my app?",
        "MobileRequest event query examples",
        "Error rate by request domain",
        "Error rate for business-critical API",
        "Response time percentiles of important APIs",
        "Volume of network requests",
        "Slow response user impact",
        "Response time distribution by domain, carrier, ASN owner, country, etc.",
        "Percentile response time",
        "Requests per session",
        "MobileRequestError event query examples",
        "HTTP errors",
        "Network failures",
        "Error rate: Percentage of users impacted",
        "Errors by version",
        "Unique devices (by UUID)",
        "Historical HTTP error counts",
        "MobileHandledException event query examples",
        "App exceptions",
        "Top exception locations",
        "Most common interaction generating exceptions",
        "Most common exception message",
        "Most common method reporting exceptions",
        "Handled exception rate"
      ],
      "title": "NRQL query examples for mobile monitoring",
      "type": "docs",
      "tags": [
        "Query your data",
        "NRQL: New Relic Query Language",
        "NRQL query tutorials"
      ],
      "external_id": "2844422852b86681e69a3ef9333f2268deacbecb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/nrql-query-examples-mobile-monitoring/",
      "published_at": "2022-01-12T10:30:12Z",
      "updated_at": "2021-08-03T00:46:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There are several ways to query your data. This document will show you some example NRQL queries from mobile monitoring data. To see descriptions of the mobile-reported events and attributes available, see Mobile events. Mobile, MobileSession, and MobileCrash event query examples Mobile queries allow you to understand and compare a wide variety of mobile data, including interactions, location, device profile, app version, crashes, and performance. These examples use queries made on the Mobile, MobileSession, and MobileCrash event types: Interactions: Which interactions are most popular among my users? SELECT uniqueCount(uuid) FROM Mobile SINCE 1 day ago FACET name Copy Location: Which regions of China have the most users? SELECT uniqueCount(uuid) FROM MobileSession WHERE countryCode='CN' FACET regionCode SINCE 7 days ago Copy Device profile: How many users use the latest OS versions? SELECT uniqueCount(uuid) FROM MobileSession FACET osVersion SINCE 7 days ago Copy App version: Have we seen an increase in session duration since yesterday's release? SELECT percentile(sessionDuration, 90) FROM MobileSession since 1 day ago compare with 2 days ago Copy Performance: How much memory does my app use for sessions longer than 5 seconds? SELECT histogram(memUsageMb) FROM MobileSession WHERE sessionDuration > 5 Copy Crashes: What are my app's most common crashes? SELECT count(*) FROM MobileCrash FACET crashException Copy Crash rate: What is the crash rate for different versions of my app? SELECT percentage(uniqueCount(sessionId), WHERE category = 'Crash') as `Crash rate` FROM MobileSession, MobileCrash facet appVersion since 90 days ago Copy MobileRequest event query examples This feature requires mobile monitoring agent version 5.14.0 or higher. MobileRequest data is enabled by default for: Android version 5.15.2 or higher iOS version 6.0.0 or higher For earlier versions, starting with Android version 5.14.0 or iOS version 5.14.0, you must enable the feature. Upgrade to the latest Android or iOS version, or add the required feature flag to your app. Below are some NRQL queries that address common use cases. Use the MobileRequest attributes to make your own NRQL queries. The last two examples use MobileRequestError events in addition to MobileRequest to get an error rate. Error rate by request domain Which domains are prone to failure and error? SELECT percentage(count(*), where errorType = 'NetworkFailure' OR errorType = 'HTTPError') as 'Error Rate %', count(*) as '# of Requests', filter(count(*), where errorType='NetworkFailure' OR errorType='HTTPError' as '# of Errors') FROM MobileRequestError, MobileRequest facet requestDomain Copy Error rate for business-critical API What is the error rate seen by our mobile apps for the most business-critical API? SELECT percentage(count(*), where errorType = 'NetworkFailure' OR errorType = 'HTTPError') as 'Error Rate %', count(*) as '# of Requests', filter(count(*), where errorType='NetworkFailure' OR errorType='HTTPError' as '# of Errors') FROM MobileRequestError, MobileRequest facet requestPath where requestPath = '{MY_API_PATH}' Copy Response time percentiles of important APIs For important requests in the 90th percentile, what is the response time by URL? SELECT percentile(responseTime, 90), latest(requestUrl) as 'Latest URL' from MobileRequest facet cases(where requestUrl like '%{YOUR_CORE_API}%' as 'Core API', where requestUrl like '%{YOUR_FEATURE_API}%' as 'New Feature API') Copy Volume of network requests How much network traffic from the apps are backend services receiving? SELECT count(*) FROM MobileRequest FACET requestDomain since 3 days ago Copy Slow response user impact What % of users are impacted by http response times greater than 3 seconds? SELECT filter(uniqueCount(MobileRequest.uuid), WHERE responseTime > 3) / uniqueCount(MobileSession.uuid) * 100 as '% Users Impacted' FROM MobileRequest, MobileSession since 1 day ago timeseries compare with 2 days ago Copy Response time distribution by domain, carrier, ASN owner, country, etc. What is the distribution of response time and request count across domain, country, carrier, or ASN owner? SELECT histogram(responseTime, 20, 20) FROM MobileRequest since 3 days ago facet asnOwner Copy Percentile response time What is the breakdown of response time by different percentiles? SELECT percentile(responseTime, 98) as '98 percentile (sec)', percentile(responseTime, 90) as '90 percentile (sec)', percentile(responseTime, 50) as '50 percentile (sec)' from MobileRequest since 3 days ago Copy Requests per session How do requests per session compare across different apps or subsequent builds of those apps? SELECT count(*)/uniqueCount(sessionId) from MobileRequest, MobileSession facet appName timeseries Copy MobileRequestError event query examples Below are some NRQL queries that address common use cases. Use the MobileRequestError attributes to make your own NRQL queries. HTTP errors Which queries are causing the most errors? SELECT count(*) FROM MobileRequestError where errorType = 'HTTPError' FACET requestUrl Copy Network failures What network failures are most common for my application? SELECT count(*) FROM MobileRequestError where errorType = 'NetworkFailure' facet networkError Copy Error rate by request domain Which domains are prone to failure and error? SELECT percentage(count(*), where errorType = 'NetworkFailure' OR errorType = 'HTTPError') as 'Error Rate %', count(*) as '# of Requests', filter(count(*), where errorType='NetworkFailure' OR errorType='HTTPError' as '# of Errors') FROM MobileRequestError, MobileRequest facet requestDomain Copy Error rate for business-critical API What is the error rate in our mobile apps for the most business-critical API? SELECT percentage(count(*), where errorType = 'NetworkFailure' OR errorType = 'HTTPError') as 'Error Rate %', count(*) as '# of Requests', filter(count(*), where errorType='NetworkFailure' OR errorType='HTTPError' as '# of Errors') FROM MobileRequestError, MobileRequest facet requestPath where requestPath = '{MY_API_PATH}' Copy Error rate: Percentage of users impacted How many users are experiencing errors as compared to my total user count? SELECT filter(uniqueCount(MobileRequestError.uuid), WHERE errorType = 'HTTPError') / uniqueCount(MobileSession.uuid) * 100 as '% Users Impacted by Errors' FROM MobileRequestError, MobileSession COMPARE WITH 7 days AGO Copy Errors by version Which version(s) of my app are causing the most errors? SELECT count(*) FROM MobileRequestError FACET appVersion Copy Unique devices (by UUID) Which unique devices (by UUID) are having the most issues with my application? SELECT count(*), latest(device), latest(carrier), latest(asnOwner), latest(countryCode) FROM MobileRequestError FACET deviceUuid limit 100 SINCE 1 days ago Copy Historical HTTP error counts What does my historical HTTP Error count look like (by domain)? SELECT count(*) FROM MobileRequestError where errorType = 'HTTPError' FACET requestDomain timeseries Copy MobileHandledException event query examples Below are some NRQL queries for common handled exception use cases. Use the MobileHandledException attributes to make your own NRQL queries. App exceptions Which apps have reported the most number of handled exceptions? SELECT count(*) FROM MobileHandledException FACET appName SINCE 3 days ago Copy Top exception locations What are most common exception locations for my application? How many exceptions do we have, and where do they occur? SELECT count(*) FROM MobileHandledException FACET exceptionLocation SINCE 3 days ago Copy Most common interaction generating exceptions Which interaction produces the most exceptions? SELECT count(*) FROM MobileHandledException FACET lastInteraction SINCE 3 days ago Copy Most common exception message What are the most common reported exception messages? SELECT count(*) FROM MobileHandledException FACET exceptionMessage SINCE 3 days ago Copy Most common method reporting exceptions What are the most common methods reporting exceptions? SELECT count(*) FROM MobileHandledException FACET exceptionLocationMethod SINCE 3 days ago Copy Handled exception rate How often are handled exceptions encountered by our users? SELECT percentage(uniqueCount(sessionId), WHERE exceptionLocation IS NOT NULL) FROM MobileSession,MobileHandledException SINCE 3 days ago Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.37073,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>NRQL</em> <em>query</em> examples for mobile monitoring",
        "sections": "<em>NRQL</em> <em>query</em> examples for mobile monitoring",
        "tags": "<em>NRQL</em>: <em>New</em> <em>Relic</em> <em>Query</em> <em>Language</em>",
        "body": "There are several ways to <em>query</em> <em>your</em> <em>data</em>. This document will show you some example <em>NRQL</em> queries from mobile monitoring <em>data</em>. To see descriptions of the mobile-reported events and attributes available, see Mobile events. Mobile, MobileSession, and MobileCrash event <em>query</em> examples Mobile queries"
      },
      "id": "60445a6128ccbc6b6a2c60ca"
    }
  ],
  "/docs/data-apis/ingest-apis/metric-api/introduction-metric-api": [
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 316.9214,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report metrics via the Metric <em>API</em>",
        "sections": "Report metrics via the Metric <em>API</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric <em>API</em> to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Telemetry SDKs: Report custom telemetry data",
        "Requirements and compatibility",
        "Tip",
        "Available libraries",
        "Write your own Telemetry SDK or contribute to an existing one"
      ],
      "title": "Telemetry SDKs: Report custom telemetry data",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "d883a07b7ede4c3beaba4077c507b95f9a228435",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/telemetry-sdks-report-custom-telemetry-data/",
      "published_at": "2022-01-12T06:17:44Z",
      "updated_at": "2021-12-19T14:27:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Telemetry SDKs are an open source set of API client libraries that send data to the New Relic platform. Under the hood, these SDKs rely on our primary data ingest APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don't meet your needs, our Telemetry SDKs are one way to create a custom telemetry solution (see other solutions for reporting custom data). Requirements and compatibility The Telemetry SDKs use our Metric API, Event API, Log API, and Trace API, which all require a license key, so you'll need a license key for the account you wish to send data to. Tip New Relic has contributed the Telemetry SDK to the open source community under an Apache 2.0 license. Available libraries The Telemetry SDKs are open source software on GitHub. Use the language-specific GitHub links below to get library details, coding examples, and procedures for how to use the SDKs. We currently support the following libraries, with more to be created in the future: Language Library Supported New Relic data types Java Java library on GitHub Metrics Events Logs Traces Node/TypeScript NodeJS library on GitHub Metrics Traces Python Python library on GitHub Metrics Events Logs Traces Go Go library on Github Metrics Traces .NET .NET library on GitHub .NET package in NuGet Metrics Traces C C library on Github Traces Rust Rust library on Github Traces Ruby Ruby library on Github Gem on Rubygems Traces For more on the supported data types, see: An overview of New Relic data types Metrics: see the Metric API Logs: see the Log API Traces: see the Trace API Events: see the Event API Write your own Telemetry SDK or contribute to an existing one If you need a Telemetry SDK in a language that does not currently exist or want to contribute to an existing library, please see the Telemetry SDK specifications.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.9562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "sections": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Our Telemetry SDKs are an open source set of <em>API</em> client libraries that send <em>data</em> to the New Relic platform. Under the hood, these SDKs rely on our primary <em>data</em> <em>ingest</em> <em>APIs</em>: the Metric <em>API</em>, Trace <em>API</em>, Log <em>API</em>, and Event <em>API</em>. If our pre-built solutions don&#x27;t meet your needs, our Telemetry SDKs"
      },
      "id": "603ea196196a670192a83d83"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.9837,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    }
  ],
  "/docs/data-apis/ingest-apis/metric-api/metric-api-limits-restricted-attributes": [
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 316.9212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report metrics via the Metric <em>API</em>",
        "sections": "Report metrics via the Metric <em>API</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric <em>API</em> to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Telemetry SDKs: Report custom telemetry data",
        "Requirements and compatibility",
        "Tip",
        "Available libraries",
        "Write your own Telemetry SDK or contribute to an existing one"
      ],
      "title": "Telemetry SDKs: Report custom telemetry data",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "d883a07b7ede4c3beaba4077c507b95f9a228435",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/telemetry-sdks-report-custom-telemetry-data/",
      "published_at": "2022-01-12T06:17:44Z",
      "updated_at": "2021-12-19T14:27:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Telemetry SDKs are an open source set of API client libraries that send data to the New Relic platform. Under the hood, these SDKs rely on our primary data ingest APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don't meet your needs, our Telemetry SDKs are one way to create a custom telemetry solution (see other solutions for reporting custom data). Requirements and compatibility The Telemetry SDKs use our Metric API, Event API, Log API, and Trace API, which all require a license key, so you'll need a license key for the account you wish to send data to. Tip New Relic has contributed the Telemetry SDK to the open source community under an Apache 2.0 license. Available libraries The Telemetry SDKs are open source software on GitHub. Use the language-specific GitHub links below to get library details, coding examples, and procedures for how to use the SDKs. We currently support the following libraries, with more to be created in the future: Language Library Supported New Relic data types Java Java library on GitHub Metrics Events Logs Traces Node/TypeScript NodeJS library on GitHub Metrics Traces Python Python library on GitHub Metrics Events Logs Traces Go Go library on Github Metrics Traces .NET .NET library on GitHub .NET package in NuGet Metrics Traces C C library on Github Traces Rust Rust library on Github Traces Ruby Ruby library on Github Gem on Rubygems Traces For more on the supported data types, see: An overview of New Relic data types Metrics: see the Metric API Logs: see the Log API Traces: see the Trace API Events: see the Event API Write your own Telemetry SDK or contribute to an existing one If you need a Telemetry SDK in a language that does not currently exist or want to contribute to an existing library, please see the Telemetry SDK specifications.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.95618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "sections": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Our Telemetry SDKs are an open source set of <em>API</em> client libraries that send <em>data</em> to the New Relic platform. Under the hood, these SDKs rely on our primary <em>data</em> <em>ingest</em> <em>APIs</em>: the Metric <em>API</em>, Trace <em>API</em>, Log <em>API</em>, and Event <em>API</em>. If our pre-built solutions don&#x27;t meet your needs, our Telemetry SDKs"
      },
      "id": "603ea196196a670192a83d83"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.98361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    }
  ],
  "/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api": [
    {
      "sections": [
        "Telemetry SDKs: Report custom telemetry data",
        "Requirements and compatibility",
        "Tip",
        "Available libraries",
        "Write your own Telemetry SDK or contribute to an existing one"
      ],
      "title": "Telemetry SDKs: Report custom telemetry data",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "d883a07b7ede4c3beaba4077c507b95f9a228435",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/telemetry-sdks-report-custom-telemetry-data/",
      "published_at": "2022-01-12T06:17:44Z",
      "updated_at": "2021-12-19T14:27:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Telemetry SDKs are an open source set of API client libraries that send data to the New Relic platform. Under the hood, these SDKs rely on our primary data ingest APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don't meet your needs, our Telemetry SDKs are one way to create a custom telemetry solution (see other solutions for reporting custom data). Requirements and compatibility The Telemetry SDKs use our Metric API, Event API, Log API, and Trace API, which all require a license key, so you'll need a license key for the account you wish to send data to. Tip New Relic has contributed the Telemetry SDK to the open source community under an Apache 2.0 license. Available libraries The Telemetry SDKs are open source software on GitHub. Use the language-specific GitHub links below to get library details, coding examples, and procedures for how to use the SDKs. We currently support the following libraries, with more to be created in the future: Language Library Supported New Relic data types Java Java library on GitHub Metrics Events Logs Traces Node/TypeScript NodeJS library on GitHub Metrics Traces Python Python library on GitHub Metrics Events Logs Traces Go Go library on Github Metrics Traces .NET .NET library on GitHub .NET package in NuGet Metrics Traces C C library on Github Traces Rust Rust library on Github Traces Ruby Ruby library on Github Gem on Rubygems Traces For more on the supported data types, see: An overview of New Relic data types Metrics: see the Metric API Logs: see the Log API Traces: see the Trace API Events: see the Event API Write your own Telemetry SDK or contribute to an existing one If you need a Telemetry SDK in a language that does not currently exist or want to contribute to an existing library, please see the Telemetry SDK specifications.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.95618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "sections": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Our Telemetry SDKs are an open source set of <em>API</em> client libraries that send <em>data</em> to the New Relic platform. Under the hood, these SDKs rely on our primary <em>data</em> <em>ingest</em> <em>APIs</em>: the Metric <em>API</em>, Trace <em>API</em>, Log <em>API</em>, and Event <em>API</em>. If our pre-built solutions don&#x27;t meet your needs, our Telemetry SDKs"
      },
      "id": "603ea196196a670192a83d83"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.98361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    },
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.42786,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "sections": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " in: they let you decide what <em>data</em> you send to New Relic and how long it should be stored. <em>Data</em> management hub: from the account dropdown in the top right of the UI, select <em>Manage</em> your <em>data</em>. Coupled with user management tools, <em>data</em> management helps you get maximum value from your investment in New Relic, all"
      },
      "id": "603e96ff28ccbcf8bceba796"
    }
  ],
  "/docs/data-apis/ingest-apis/metric-api/troubleshoot-nrintegrationerror-events": [
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 316.92108,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report metrics via the Metric <em>API</em>",
        "sections": "Report metrics via the Metric <em>API</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric <em>API</em> to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Telemetry SDKs: Report custom telemetry data",
        "Requirements and compatibility",
        "Tip",
        "Available libraries",
        "Write your own Telemetry SDK or contribute to an existing one"
      ],
      "title": "Telemetry SDKs: Report custom telemetry data",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "d883a07b7ede4c3beaba4077c507b95f9a228435",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/telemetry-sdks-report-custom-telemetry-data/",
      "published_at": "2022-01-12T06:17:44Z",
      "updated_at": "2021-12-19T14:27:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Telemetry SDKs are an open source set of API client libraries that send data to the New Relic platform. Under the hood, these SDKs rely on our primary data ingest APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don't meet your needs, our Telemetry SDKs are one way to create a custom telemetry solution (see other solutions for reporting custom data). Requirements and compatibility The Telemetry SDKs use our Metric API, Event API, Log API, and Trace API, which all require a license key, so you'll need a license key for the account you wish to send data to. Tip New Relic has contributed the Telemetry SDK to the open source community under an Apache 2.0 license. Available libraries The Telemetry SDKs are open source software on GitHub. Use the language-specific GitHub links below to get library details, coding examples, and procedures for how to use the SDKs. We currently support the following libraries, with more to be created in the future: Language Library Supported New Relic data types Java Java library on GitHub Metrics Events Logs Traces Node/TypeScript NodeJS library on GitHub Metrics Traces Python Python library on GitHub Metrics Events Logs Traces Go Go library on Github Metrics Traces .NET .NET library on GitHub .NET package in NuGet Metrics Traces C C library on Github Traces Rust Rust library on Github Traces Ruby Ruby library on Github Gem on Rubygems Traces For more on the supported data types, see: An overview of New Relic data types Metrics: see the Metric API Logs: see the Log API Traces: see the Trace API Events: see the Event API Write your own Telemetry SDK or contribute to an existing one If you need a Telemetry SDK in a language that does not currently exist or want to contribute to an existing library, please see the Telemetry SDK specifications.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.95616,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "sections": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Our Telemetry SDKs are an open source set of <em>API</em> client libraries that send <em>data</em> to the New Relic platform. Under the hood, these SDKs rely on our primary <em>data</em> <em>ingest</em> <em>APIs</em>: the Metric <em>API</em>, Trace <em>API</em>, Log <em>API</em>, and Event <em>API</em>. If our pre-built solutions don&#x27;t meet your needs, our Telemetry SDKs"
      },
      "id": "603ea196196a670192a83d83"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.98352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    }
  ],
  "/docs/data-apis/ingest-apis/telemetry-sdks-report-custom-telemetry-data": [
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 316.92108,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report metrics via the Metric <em>API</em>",
        "sections": "Report metrics via the Metric <em>API</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric <em>API</em> to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.98352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    },
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.42776,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "sections": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " in: they let you decide what <em>data</em> you send to New Relic and how long it should be stored. <em>Data</em> management hub: from the account dropdown in the top right of the UI, select <em>Manage</em> your <em>data</em>. Coupled with user management tools, <em>data</em> management helps you get maximum value from your investment in New Relic, all"
      },
      "id": "603e96ff28ccbcf8bceba796"
    }
  ],
  "/docs/data-apis/manage-data/calculate-data-ingest": [
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 41.15678,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> management hub",
        "sections": "New Relic&#x27;s <em>data</em> management hub",
        "tags": "<em>Ingest</em> and manage <em>data</em>",
        "body": " <em>ingest</em> and <em>cost</em>? See <em>Calculate</em> <em>data</em> <em>ingest</em>. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the <em>data</em> we store"
      },
      "id": "603e96ff28ccbcf8bceba796"
    },
    {
      "sections": [
        "Overview of data retention (original pricing model)",
        "Important",
        "Limits on editing and deleting data",
        "Product and API data retention policies",
        "APM",
        "APM data retention policies",
        "Legacy APM data retention policies",
        "Browser",
        "Infrastructure",
        "Tip",
        "Infrastructure downsampling and data retention",
        "Integration compute unit event limits",
        "Insights",
        "Logs",
        "Logs in context data retention",
        "Mobile",
        "Mobile data retention policies",
        "Standard Mobile (legacy) data retention policies",
        "Synthetics",
        "Synthetics data retention policies",
        "Metric API",
        "Trace API",
        "Data components",
        "Event data: reported by most products",
        "Metric timeslice data: reported by APM, Browser, and Mobile",
        "Aggregate metric timeslice data: reported by APM, Browser, and Mobile",
        "Key metrics: reported by APM, Browser, and Mobile",
        "Trace data: reported by APM, Browser, and Mobile"
      ],
      "title": "Overview of data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "22d5ab9f4d623ead28ee7bb82c118d91804dee22",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/overview-data-retention-components/",
      "published_at": "2022-01-12T07:48:35Z",
      "updated_at": "2022-01-12T07:48:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original Product-based pricing. If you're on our New Relic One pricing model, see Manage your data. Not sure which you're on? See Overview of pricing models. If you're on the original product-based pricing model, you retain your existing subscriptions and data retention values. You manage these existing retention settings from the Data management hub in New Relic One. To manage your retention settings, go to the Data management UI. You'll see your existing retention settings. Adjust retention values by clicking Edit retention. New Relic stores different types of data for different periods of time. The retention period for a type of data will vary depending on the product, the subscription level, and the feature. Limits on editing and deleting data Once telemetry data (events, metrics, logs, traces) is reported to New Relic and available for querying, that data cannot be edited or deleted. This is a purposeful design decision that optimizes New Relic's speed and performance. Data will expire after its data retention ends. If you sent unwanted data or sensitive data to New Relic that must be removed, contact your account representative at New Relic, or get support at support.newrelic.com. Product and API data retention policies Select a New Relic product to see details about its data retention: APM Specific retention policies apply to each APM subscription level, including Lite, Essentials, Pro, and Enterprise. This includes metric timeslice data, key metrics, trace data, and event data. In addition to retention limits, your data is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate metric data description. APM data retention policies For accounts on our original product-based pricing, APM data retention policies depend on your APM product subscription level. Component Lite Essentials Pro Metric timeslice data 24 hours 3 days 90 days Key metrics none forever forever Distributed tracing and logs in context none none 8 days Other trace data * 1 day 3 days 7 days Event data * * 1 day 3 days 8 days * If you view a transaction trace in the New Relic UI, New Relic stores that trace data for up to one year. * * Learn about how to extend the retention of event data. Legacy APM data retention policies Component Standard Startup Small Business Enterprise Metric timeslice data 7 days 14 days 30 days 90 days Key metrics none none none forever Trace data 7 days 7 days 7 days 7 days Event data none 8 days 8 days 8 days Browser For accounts on our original product-based pricing, Browser data is stored depending on your subscription level: Component Lite Pro Metric data 24 hours 90 days Key metrics 24 hours forever Trace data 7 days 7 days Event data * 1 day 8 days SPA data Unavailable 8 days * Learn about how to extend the retention of event data. Infrastructure For accounts on our original product-based pricing, Infrastructure data retention policies depend on your Infrastructure subscription level and your infrastructure compute units pricing model. Data retention rules apply the same whether that data is displayed in the UI or queried. Tip Infrastructure data retention is not governed by your Insights subscription. Infrastructure downsampling and data retention Types of data are stored depending on your subscription level: Component Essentials Pro Infrastructure data 3 months 13 months Host count Host count will stop reflecting a host whose agent is no longer reporting after three minutes. However, host data will be available, subject to other retention criteria. 3 minutes 3 minutes Inventory attributes removed Inventory attributes for a host are retained for 24 hours after the agent stops reporting. 24 hours 24 hours Integration data Not available with Essentials Limit of 2275 integration events per compute unit per month In addition, Infrastructure downsamples your data on the fly, as it's generated. All Infrastructure metric data types (including On-Host Integrations metrics) will display different granularity depending on the age of the data and the size of the time window. The following table illustrates when different downsampled buckets will be used, both in the Infrastructure UI and for queries: Bucket Size Used For Time Windows Covering... Data Retained For... Raw (5, 10 or 15 second) 0 to 59 minutes 7 days 1 minute 60 minutes to 6 hours 30 days 10 minutes 6 hours to 3 days Full account retention period 1 hour 3 days to 14 days Full account retention period 3 hours 14+ days Full account retention period Integration compute unit event limits The 2275 limit on integration events per compute unit per month is a limit on total Infrastructure integration events. It's not a limit of 2275 for each integration's events. Additional details and clarifications about this limit: This limit applies to all events from all integrations (cloud integrations and on-host integrations). The events are all handled the same. Default data received by the Infrastructure agent does not count against the 2275 event per compute unit limit. If you exceed your limit, we do not enforce this limit. If you exceed your limit, we'll review pricing options with you to ensure you get the most cost-effective pricing for your organization's integrations. Insights For accounts on our original product-based pricing, an Insights subscription extends your event data retention. An Insights Pro subscription allows you to customize the length of your event data retention. Logs For accounts on our original product-based pricing, log data can be retained for up to 30 days by New Relic. Shorter retention periods of 8 or 15 days are also available. Logs in context data retention New Relic Logs logs in context data retention policy is based on your current APM product subscription level. For more information, see APM data retention. Mobile For accounts on our original product-based pricing, Mobile data retention policies depend on your New Relic Mobile product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and event data. For information about your subscription-related data usage, see Mobile subscription usage. Mobile data retention policies Component Lite Enterprise Overview page data 24 hours 90 days Crash data and stack traces 24 hours 90 days HTTP requests (except errors) as metric data Unavailable 90 days HTTP request errors as metric data Unavailable 3 days Interaction traces Unavailable 90 days Custom events * Unavailable 1 day Mobile events * 1 day 8 days MobileBreadcrumb events * Unavailable 3 days MobileCrash events * 1 day 90 days MobileHandledException events * Unavailable 3 days MobileJSError events (React Native beta) * Unavailable 3 days MobileRequest events * Unavailable 3 days MobileRequestError events * Unavailable 3 days MobileSession events * 1 day 90 days * Learn how to extend retention of event data. Standard Mobile (legacy) data retention policies Unless otherwise noted, Insights event data is unavailable for Standard subscriptions. Component Standard Overview page data 7 days Crash data and stack traces 7 days HTTP requests (except errors) as metric data 7 days HTTP request errors as metric data 3 days Interaction traces Unavailable MobileCrash events 8 days MobileSession events 8 days Synthetics For accounts on our original product-based pricing, Synthetics data retention policies depend on your Synthetics product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and Insights events. Tip Synthetics events do not count against an Insights Pro subscription. Your current subscription level appears in the right column of the Account summary page: Go to synthetics.newrelic.com > (account dropdown) > Account settings > Account > Summary. Synthetics data retention policies Component Lite Pro Synthetics monitor data 2 weeks 13 months Event data 2 weeks 13 months Also see the data retention details for APIs, including: Metric API All raw metric data points will be retained for 30 days. All additional aggregated data derived from the raw metric data points (for example, one-minute rollups) will be retained for 13 months. Any change to the retention period beyond such periods may result in a charge to you. Trace API See Trace API general requirements and limits. Data components For accounts on our original product-based pricing, the section below provides an explanation of some types of data components governed by the data retention rules of specific products: Event data: reported by most products See Event data retention for information on the event data type. Metric timeslice data: reported by APM, Browser, and Mobile Metric timeslice data is a specific type of data that is used for most metric charts and reports in APM, mobile, and browser. Important Note that metric timeslice data differs from other metric data types. All metric timeslice data is aggregated, but New Relic deals with fresh data and old data in different ways. Fresh data has specific policies applied to the data to keep granular views of performance as aggregate metrics. As data ages and becomes less useful in a granular state, we summarize that data and only keep key metrics. Aggregate metric timeslice data: reported by APM, Browser, and Mobile Aggregate metric timeslice data summarizes calls to specific methods in your application: how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on your subscription level. Subscription Level Aggregate retention Enterprise Aggregates (averages) to 1-hour periods after 8 days. After 90 days, the permanent metric data continues to be stored in 1-hour periods. This means you cannot obtain data granularity of less than 1 hour after 8 days, and only a subset of metrics are available after 90 days. Pro After 8 days Essentials After 3 days. Legacy Small Business, Startup, and Standard After 3 days. Lite After 2 hours. We retain your most recent data in one-minute increments. We also aggregate permanent metric data to day-size increments after 90 days. When looking at older data in small time windows, you may notice that charts show less detail. As data ages, it is aggregated into larger segments. Key metrics: reported by APM, Browser, and Mobile New Relic retains certain \"key\" metric timeslice data aggregations forever for Enterprise and Pro customers, for any number of applications. New Relic aggregates permanent key metric data to hour-size increments after 90 days. Product Key metrics APM Apdex, for app server Throughput and page views Response time, plus breakdown into tiers or categories on your app's main Overview chart Page load time, plus breakdown into segments Error rate CPU usage Memory usage Browser Apdex, for browser Browser page load time Throughput total. After 90 days, there is no breakout by browser type, and only the combined average value is available for the time range requested. Mobile Crash rate: For iOS and for Android Network throughput Network response time Network error rates Interaction traces Trace data: reported by APM, Browser, and Mobile Depending on the product, New Relic retains different types of trace data: Product Trace data APM Types of trace data: Transaction traces Distributed tracing Slow query samples Error details See APM data retention details. Browser Types of trace data: Session traces Browser traces JavaScript errors* See Browser data retention details. Mobile Types of trace data: App crash stack traces Interaction traces See Mobile data retention details. * JavaScript errors in the stack trace UI are saved as trace data. JS errors are also saved as events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 21.576492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of <em>data</em> retention (original pricing model)",
        "sections": "Overview of <em>data</em> retention (original pricing model)",
        "tags": "Original <em>data</em> retention",
        "body": ", Infrastructure downsamples your <em>data</em> on the fly, as it&#x27;s generated. All Infrastructure metric <em>data</em> types (including On-Host Integrations metrics) will display different granularity depending on the age of the <em>data</em> and the <em>size</em> of the time window. The following table illustrates when different downsampled"
      },
      "id": "6043f75364441f6967378ec6"
    },
    {
      "sections": [
        "REST API for Applied Intelligence",
        "Authentication",
        "Batching data",
        "Default data size limits",
        "Tip",
        "Make API calls",
        "API specifications",
        "Available methods",
        "Available fields",
        "De-duplication and identifications",
        "Significant incident attributes (hints)"
      ],
      "title": "REST API for Applied Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "12adbe321b0b8ee72f50e8ce0cc9caaf5da6299f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/rest-api-applied-intelligence/",
      "published_at": "2022-01-12T07:28:38Z",
      "updated_at": "2021-03-16T08:06:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The incident events API allows you to report any related activities from your own proprietary incident management systems for advanced correlation and reasoning. The API is designed in a generic way with minimal required attributes and flexibility to keep native keys and values of your incident events. The more information your system can provide, the better our decision engine works to surface more relevant information to you. Use the attributes field to push your specific information in addition to the required fields. Authentication The REST API supports secure token-based authentication and accepts JSON content as input. To get your secure token, go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Sources, then click REST API. Your user account must have permissions to manage Sources to get a secure token. Batching data We currently support up to 10 events in the same API call. In order to batch, just append new event data to the body. A batch must be POSTed as {\"events\": [{\"event_source\": \"Snap\", ...}]} with each event part in a list that is the value of \"events\", as the following sample shows. Sample JSON { \"events\": [{ \"application\": \"Name of my application\", \"attributes\": { \"alert/description\": \"Add a description about the alert itself\", \"state\": \"alarm\", \"application/state\": \"MAJOR\", \"environment\": \"prod\" }, \"event_description\": \"Add a description about the incident\", \"event_source\": \"List the application that created the incident\", \"host\": \"host-name\", \"value\": \"medium\" }] } Copy Our collector will first check to see if events is the key in the root object and if so, extract and iterate. Default data size limits Batch – up to 10 messages inside the body. We support up to 20 attributes per metric or event. Each string value field size is limited to 1024 characters. Each attribute name field size is limited to 128 characters. Tip If you have different requirements for data size/restrictions please contact us as those parameters can be tuned for you. Make API calls On the Incident Intelligence Sources page, click REST API. Choose the correct account and click on the clipboard icon to copy the collector URL. The security token should be used in the Authorization: Bearer HTTP header. Here is an example curl command using this interface: curl -L -X POST 'https://collectors.signifai.io/v1/incidents' -H 'Authorization: Bearer XXXXXXXXX' -H 'Content-Type: application/json' --data-raw '{\"application\": \"A Unique App Name\",\"attributes\": {\"alert/description\": \"The health check end-point is failing for A Unique App Name\",\"annotations/description\": \"Health check failure\",\"cluster/name\": \"auan_001\",\"datacenter/name\": \"US-EAST-1\",\"health_check/entity_name\": \"a-unique-app-name\",\"service/name\": \"a-unique-app-name\",\"service/status\": \"down\",\"state\": \"alarm\",\"label/namespace\": \"use labels to control how events aggregate into incidents\"},\"event_description\": \"Something in the health check failed for A Unique App Name\",\"event_source\": \"my-rest-api\",\"value\": \"critical\"}' Copy API specifications Available methods Method API Endpoint Description POST https://collectors.signifai.io/v1/incidents Sends incident events to Applied Intelligence for processing. Available fields Field Description event_source String REQUIRED. Your own representation for the system or application that is generating the event. Example: {\"event_source\": \"sensu\"} host|service|application String REQUIRED. What generated the event. Can be the associated host or, if a host isn't relevant, a service or an application. Note: only one is required. Example: {\"host\": \"payments001\"} value String, Boolean (only true is supported) REQUIRED. Incident priority. String must be one of: critical, high, medium, low Use a boolean to indicate an event happened Example: {\"value\": \"high\"} timestamp Long Epoch time in seconds (UTC) that the event occurred. Negative timestamps are not supported. Default: the time our server receives the event Example: {\"timestamp\": 1591302334} event_description String REQUIRED. Free text information that describes this event. We recommend describing what happened on which entity. This is used in various places in the UI. This attributes is required. Example: {\"event_description\": \"Response time is > 2 seconds for the last 10min on [Production] Storefront\"} attributes JSON Object This is a flat key/value mapping of additional attributes about the incident event. We recommend putting as much information and meta data as you can here. Any attributes added here will be used to perform better correlations. Labels are defined within the attributes object. Labels allow you to control how the de-duplication step works for your events. See De-duplication and identification below. Example: {\"attributes\": { \"affected_cluster\": \"cluster foo\", \"team\": \"SRE Infra Team\", \"environment\": \"production\" }} De-duplication and identifications To better support our system’s advanced de-duplication and correlation capabilities, we recommend supplying a set of labels or an alert ID, as part of the incident’s attributes. Any attribute prefixed with label/ is considered as a label. The combination of the labels, event_source, host, application, and server are used to create unique incidents. If labels are not provided, provide an alert/id attribute to be used as a de-duplication key. Significant incident attributes (hints) Some attributes have enhanced capabilities. Sending these allows us to process the data more intelligently. Attribute name Description alert/description Additional details that describe what happened. alert/id Alert identification - used to deduplicate incidents. Every incident that is received with the same alert/id will be aggregated to the same open incident. Important: Specifying alert/id manually will deactivate New Relic's automatic creation of an alert ID using the defined labels. alert/metric_name Metric name that triggered the incident. alert/policy_id Policy id of the alert. alert/policy_name Policy name of the alert. annotations/description The incident description that will be used for representation in the UI. application/id Instrumented application identifier. application/name Instrumented application name assigned from the application field in the payload. availability_zone Instrumented incident availability zone. cloud/region Region hosting the service (for example: us-east-1). cluster/name Identify the cluster name impacted. datacenter/name Identify the datacenter name impacted. environment Environment kind (dev, prod). health_check/entity_name The name of the health check originally triggered the incident if available. health_check/id Health check identification. health_check/name Health check name. health_check/probe_id Identification of the probe id used for health check. health_check/probe_name Name of the probe name used for health check. health_check/type Health check type. host/name Instrumented host's name assigned from the host field in the payload. instance/name Instrumented instance name. instance/type Instrumented instance type. label/* Any attribute prefixed with label/ will be considered as part of the incident de-duplication key and will be used to identify the incident. organization/name Organization name (in cases there are more than one). runbook_url In case there is a link for a runbook. service/name Service name that reported the incident. Assigned from the service field in the payload. service/status Instrumented service status. Can be either up or down. state Indicates what state the event is in. Acceptable values are: alarm and ok. The default value is alarm if state is not provided which will create an incident or update an existing incident with the same labels or alert/id. If the value is ok this will close the open incident with the same labels or alert/id.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 20.781551,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Default <em>data</em> <em>size</em> limits",
        "body": " the incident&quot;, &quot;host&quot;: &quot;host-name&quot;, &quot;value&quot;: &quot;medium&quot; }] } Copy Our collector will first check to see if events is the key in the root object and if so, extract and iterate. Default <em>data</em> <em>size</em> limits Batch – up to 10 messages inside the body. We support up to 20 attributes per metric or event. Each string"
      },
      "id": "603ea67c28ccbcdc10eba775"
    }
  ],
  "/docs/data-apis/manage-data/drop-data-using-nerdgraph": [
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "b3a08d5b6e4c4c04f4046167eb836e6b45523376",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2022-01-12T01:55:42Z",
      "updated_at": "2021-10-24T02:42:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1795.1913,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Drop</em> <em>data</em> <em>using</em> Prometheus remote write",
        "sections": "<em>Drop</em> <em>data</em> <em>using</em> Prometheus remote write",
        "body": "You can <em>drop</em> <em>data</em> you don&#x27;t want to keep by changing the remote_write section of the YAML config file. Tip You can also <em>drop</em> remote write <em>data</em> <em>using</em> <em>NerdGraph</em>. For more information, see <em>Drop</em> <em>data</em> <em>using</em> <em>NerdGraph</em>. <em>Drop</em> entire metric <em>data</em> points from remote write integration If a target is sending"
      },
      "id": "617dae7a196a6740e2f7e23d"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2022-01-12T02:35:08Z",
      "updated_at": "2021-12-09T15:28:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 TISAX The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1367.2489,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Data</em> Control, Facilities, and Encryption",
        "body": " that customer&#x27;s manage and&#x2F;or control. New Relic&#x27;s customers can <em>use</em> New Relic&#x27;s Services such as <em>NerdGraph</em> to filter out and <em>drop</em> <em>data</em>. See https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;telemetry-<em>data</em>-platform&#x2F;manage-<em>data</em>&#x2F;<em>drop</em>-<em>data</em>-<em>using</em>-<em>nerdgraph</em>&#x2F;. New Relic&#x27;s customers can adjust their <em>data</em> retention periods as appropriate"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Data privacy with New Relic",
        "Tip",
        "Personal data transfer (Privacy Shield and SCC)",
        "Compliance with legal requirements",
        "Privacy by design and by default",
        "Personal data requests (GDPR, CCPA, etc.)",
        "Events and attributes",
        "Dropping data at ingest",
        "Technical security controls",
        "Organizational security controls",
        "Account security",
        "Retention of your data",
        "New Relic account emails",
        "Account changes (NrAuditEvent)",
        "Account usage",
        "Security for products and services",
        "Alerts and Applied Intelligence",
        "APIs",
        "APM",
        "Browser monitoring",
        "Diagnostics",
        "Infrastructure monitoring",
        "Integrations and serverless monitoring",
        "Logs management",
        "Mobile monitoring",
        "Pixie auto-telemetry data",
        "Synthetic monitoring"
      ],
      "title": "Data privacy with New Relic",
      "type": "docs",
      "tags": [
        "Security",
        "Security and Privacy",
        "Data privacy"
      ],
      "external_id": "d46953520476285467540433180d483815efecc6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/",
      "published_at": "2022-01-12T17:55:49Z",
      "updated_at": "2022-01-04T18:06:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic takes your data privacy seriously. Our principles-based approach aims to go beyond the legal requirements for consent. We understand your concerns when you entrust us with your data, and we always strive to embrace your expectations and preferences. This document provides links to detailed information about the privacy and security measures we take to protect you and your customers' data privacy. Our monitoring tools are data-agnostic; they don't require sensitive materials, and many of them don't require any personal data. You are responsible for ensuring that your systems are appropriately set up and configured so that they don't send inappropriate personal data or sensitive materials to New Relic monitoring tools. For additional information about policies, credentials, audits, and other resources, see our New Relic security website. Tip New Relic now offers the option of HIPPA-enabled accounts for customers meeting certain requirements. To learn more, see HIPAA readiness at New Relic. Personal data transfer (Privacy Shield and SCC) The Schrems case ruling invalidates Privacy Shield. However, it explicitly reaffirms the validity of Standard Contractual Clauses (SCC) as an appropriate legal mechanism to transfer personal data outside of the European Union. You can find more information in How the Demise of Privacy Shield Affects Your New Relic Account. If you want to send personal data from the EU, we offer an appropriate data processing agreement (DPA) with SCC to govern the transfer of that data in accordance with the Schrems decision. For more information, consult our Data Processing Addendum FAQ, or download our pre-signed DPA (PDF|697 KB). Compliance with legal requirements We always strive to comply with all applicable laws as they take effect. This includes the European Union's General Data Protection Regulation (GDPR) and all relevant US State laws, such as the California Consumer Privacy Act (CCPA). Our disk-based encryption provides additional security while your data is at rest (FIPS 140-2 compliant). In addition, we are authorized for Moderate Impact SaaS Services (FedRAMP Authorized Moderate) for accounts that meet specific criteria. For privacy-related details about New Relic's contractual and regulatory commitments for services, see: Terms of Service or Master Subscription Agreement Data Protection Agreement Services Privacy Notice For more information about annual audits, see Regulatory audits for New Relic services. Privacy by design and by default New Relic follows \"privacy by design\" principles as part of our overarching security program. For example, when New Relic agents capture a webpage or referrer URL, all query parameters are stripped by default. Here are examples of how we incorporate privacy considerations into our data and security practices. Personal data requests (GDPR, CCPA, etc.) New Relic strives to comply with all applicable laws as they take effect. This includes the European Union's GDPR and ePrivacy Directive and all applicable privacy laws, such as the California Consumer Privacy Act (CCPA) in the US. For more information about our process when responding to requests to access or delete personal data, see New Relic personal data requests. Events and attributes You can query events and attributes, as well as create charts and alert conditions about this data. For a complete list of all events and attributes tracked by New Relic agents, see our data dictionary. Events and attributes example: If you use the Infrastructure ProcessSample event's commandLine attribute, by default we strip options and arguments from the full command line to prevent accidental leakage of sensitive information. Dropping data at ingest Dropping data gives you control over the data that you send to New Relic, including any personal data that you configured to be collected. By dropping specific events or attributes from events, you determine what data New Relic ultimately stores so that you can query, alert on, and analyze it. For more information, see Drop data using NerdGraph. When our agents refer to data obfuscation, the agent actually removes the data before sending it to New Relic. The data cannot be recovered. For example, with APM queries, the Record SQL? value defaults to obfuscated. This strips the string literals and numeric sequences and then replaces them with the ? character. You can mask sensitive information in HTTP or HTTPS requests. For example, queries about distributed traces and transaction traces are obfuscated by default, in which case they cannot be recovered. For more information, see the documentation for specific New Relic services, including: APM transaction traces Distributed tracing Technical security controls We use a comprehensive set of technical controls to support general security needs as well as security for data we receive. For more information, see our documentation about data security, data encryption, and high security mode for APM agents. Organizational security controls New Relic maintains a number of internal policies and procedures to guide employees in privacy-related subjects such as data classification and handling, data retention, handling of personal data, fulfilling personal data requests, incident response, etc. All employees must complete the security and privacy training upon hiring and renew this training annually. Account security Our role-based account structure gives you direct control over who can access or change your account settings. For more information, see Users and roles. Retention of your data The New Relic One platform gives you a single source of truth for all your operational data, empowering you to ask and answer any question in milliseconds. This platform stores different types of data for different periods of time. The Data retention page in our UI provides information on how long your data will be stored in the New Relic database (NRDB). For more information, see Manage data retention. New Relic account emails By default, we communicate with you for a variety of purposes related to your status as New Relic subscribers. This includes product engagement, support, alert notifications, updates, billings, etc. Individual users can unsubscribe from certain communications. General email preferences are managed through the account user interface. For more information, see Account email settings. Alert notification emails are managed through the alerting UI. Account changes (NrAuditEvent) To view changes made to your account's users or to record configuration changes, query NrAuditEvent events. To be notified about account changes, create NRQL alert conditions. For more about available NrAuditEvent attributes, see our data dictionary. Account usage For more about usage, see Manage data. Security for products and services We publish security bulletins with detailed information about vulnerabilities, remediation strategies, and applicable updates for affected software. To receive notifications for future advisories, use either of these options: Subscribe to our security bulletins RSS feed. Select the Watching option in our Explorers Hub's Security notifications community channel to receive email alerts. The following summarizes how individual New Relic products and components ensure security, with links to additional details. Alerts and Applied Intelligence By default, our alerting services do not record any personal data. In addition, they automatically set default permissions for individual account users and access levels within account structures. For more information, see our documentation about Applied Intelligence, as well as our rules and limits for alerts. APIs APIs simply are interfaces for data exchange automation. APIs have no knowledge of the content being transferred. We require authorized users to provide their API keys to monitor subscription usage, manage account user permissions, query data, and perform other automated tasks. For more information, see Introduction to New Relic APIs. APM APM agents monitor your applications' performance. By default, APM agents do not record any personal data. For more information, see our APM security documentation. Browser monitoring Our browser monitoring agent allows you to monitor the performance of their websites. For more information, see: Browser security documentation Visitor's IP address New Relic cookies used by browser Enabling or disabling cookie collection for session tracking Diagnostics The New Relic Diagnostics service inspects relevant system information and any other necessary information (such as logs and config files) to perform diagnostic checks that assess configuration and operability. By default, this data is not transmitted to New Relic. You do have the option to upload this information to a support ticket over HTTPS. For more information, see the Diagnostics security documentation. Infrastructure monitoring The Infrastructure agent allows you to monitor the performance of components in your ecosystem, such as servers, platforms, operating systems, databases, etc. Infrastructure may record the userID and username of users connecting to Infrastructure resources. For more information, see the security documentation for infrastructure monitoring. Integrations and serverless monitoring Our integrations services allow you to retrieve and load data into the New Relic database from a variety of sources, including: Cloud-based integrations On-host integrations in containerized environments, such as Kubernetes On-host integrations built by New Relic On-host integrations built by the open-source community On-host integrations built by you Depending on the integration, different types of data may be recorded so that you can monitor the integrations in New Relic. The integration services are data agnostic. They will have no knowledge of whether the imported data contains any personal information. For more information, see the documentation for the specific integration, including: Amazon Web Services (AWS) Google Cloud Platform (GCP) Kubernetes Microsoft Azure On-host integrations Serverless function monitoring Logs management Due to the nature of our logs management service, you have direct control over what data is reported to New Relic. To ensure data privacy and to limit the types of information New Relic receives, no customer data is captured except what you supply in your API calls or log forwarder configuration. All data for the logs service is then reported to New Relic over HTTPS. The logs service does mask number patterns that appear to be for items such as credit cards or Social Security numbers. For more information, see the Logs security documentation. Mobile monitoring By default, our mobile monitoring service collects two pieces of personal data: The IP address is used to derive high-level geographical data, and then is discarded. A device ID is generated by New Relic and is used for billing purposes. For more information, see our security documentation for mobile monitoring. Pixie auto-telemetry data Auto-telemetry with Pixie is New Relic One's integration of Community Cloud for Pixie, a managed version of Pixie open source software. The data that Pixie collects is stored entirely within your Kubernetes cluster. This data does not persist outside of your environment, and it will never be stored by Community Cloud for Pixie. This means that your sensitive data remains within your environment and control. For example, you can: Control who has access to your Pixie data. Manage auto-update and two-way communication. For more information, see our security documentation for auto-telemetry with Pixie data. Synthetic monitoring The synthetic monitoring service uses monitors distributed throughout data centers around the world. It captures what is essentially performance data of simulated traffic. By default, it does not capture any personal data. For more information, see the data privacy and security documentation for synthetic monitoring. If you configure the synthetic service to monitor areas of websites that are located behind a login page, take care to create a non-personal login dedicated to this purpose. This will reduce the risk of unintended personal data exposure. For example, to securely store sensitive information, such as passwords, API keys, and user names, you can use secured credentials for scripted browsers and API tests. The synthetic monitoring service also supports a variety of authentication mechanisms. Depending on the type of monitor you choose, this includes Basic, Digest, NTLM, and NTLMv2. You can also control which of your users can access your monitors and private locations. For more information, see our documentation about user role-based permissions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 963.2611,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Data</em> privacy with New Relic",
        "sections": "<em>Dropping</em> <em>data</em> at ingest",
        "tags": "<em>Data</em> privacy",
        "body": " or attributes from events, you determine what <em>data</em> New Relic ultimately stores so that you can query, alert on, and analyze it. For more information, see <em>Drop</em> <em>data</em> <em>using</em> <em>NerdGraph</em>. When our agents refer to <em>data</em> obfuscation, the agent actually removes the <em>data</em> before sending it to New Relic. The <em>data</em>"
      },
      "id": "603ec2d4e7b9d22fba2a07c6"
    }
  ],
  "/docs/data-apis/manage-data/manage-data-coming-new-relic": [
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.24002,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "sections": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " in: they let you decide what <em>data</em> you send to New Relic and how long it should be stored. <em>Data</em> management hub: from the account dropdown in the top right of the UI, select <em>Manage</em> your <em>data</em>. Coupled with user management tools, <em>data</em> management helps you get maximum value from your investment in New Relic, all"
      },
      "id": "603e96ff28ccbcf8bceba796"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 303.0943,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick start: Send metric <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Telemetry SDKs: Report custom telemetry data",
        "Requirements and compatibility",
        "Tip",
        "Available libraries",
        "Write your own Telemetry SDK or contribute to an existing one"
      ],
      "title": "Telemetry SDKs: Report custom telemetry data",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "d883a07b7ede4c3beaba4077c507b95f9a228435",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/telemetry-sdks-report-custom-telemetry-data/",
      "published_at": "2022-01-12T06:17:44Z",
      "updated_at": "2021-12-19T14:27:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Telemetry SDKs are an open source set of API client libraries that send data to the New Relic platform. Under the hood, these SDKs rely on our primary data ingest APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don't meet your needs, our Telemetry SDKs are one way to create a custom telemetry solution (see other solutions for reporting custom data). Requirements and compatibility The Telemetry SDKs use our Metric API, Event API, Log API, and Trace API, which all require a license key, so you'll need a license key for the account you wish to send data to. Tip New Relic has contributed the Telemetry SDK to the open source community under an Apache 2.0 license. Available libraries The Telemetry SDKs are open source software on GitHub. Use the language-specific GitHub links below to get library details, coding examples, and procedures for how to use the SDKs. We currently support the following libraries, with more to be created in the future: Language Library Supported New Relic data types Java Java library on GitHub Metrics Events Logs Traces Node/TypeScript NodeJS library on GitHub Metrics Traces Python Python library on GitHub Metrics Events Logs Traces Go Go library on Github Metrics Traces .NET .NET library on GitHub .NET package in NuGet Metrics Traces C C library on Github Traces Rust Rust library on Github Traces Ruby Ruby library on Github Gem on Rubygems Traces For more on the supported data types, see: An overview of New Relic data types Metrics: see the Metric API Logs: see the Log API Traces: see the Trace API Events: see the Event API Write your own Telemetry SDK or contribute to an existing one If you need a Telemetry SDK in a language that does not currently exist or want to contribute to an existing library, please see the Telemetry SDK specifications.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.31627,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "sections": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Our Telemetry SDKs are an open source set of API client libraries that send <em>data</em> to the New Relic platform. Under the hood, these SDKs rely on our primary <em>data</em> <em>ingest</em> APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don&#x27;t meet your needs, our Telemetry SDKs"
      },
      "id": "603ea196196a670192a83d83"
    }
  ],
  "/docs/data-apis/manage-data/manage-data-retention": [
    {
      "sections": [
        "Introduction to New Relic NerdGraph, our GraphQL API",
        "What is NerdGraph?",
        "Important",
        "Use the GraphiQL explorer",
        "Requirements and endpoints",
        "What can you do with NerdGraph?",
        "NerdGraph terminology",
        "Tips on using the GraphiQL explorer",
        "Query accounts a New Relic user can access",
        "Query user, account, and NRQL in one request"
      ],
      "title": "Introduction to New Relic NerdGraph, our GraphQL API",
      "type": "docs",
      "tags": [
        "APIs",
        "NerdGraph",
        "Get started"
      ],
      "external_id": "e8e96c16cd75f494ebfacb3bc53b4ee9ccf1c727",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/nerdgraph/get-started/introduction-new-relic-nerdgraph/",
      "published_at": "2022-01-12T08:59:39Z",
      "updated_at": "2022-01-08T03:28:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. After you sign up for a free New Relic account and install any of our monitoring services, you can get started with NerdGraph. What is NerdGraph? New Relic has several APIs. NerdGraph is the API we recommend for querying New Relic data and for performing some specific configurations (learn more about features). NerdGraph provides a single API interface for returning data from New Relic’s various APIs and microservices. Over time, other configuration capabilities will be added to NerdGraph. Important NerdGraph isn’t used for data ingest. For that, you'd use our data ingest APIs. NerdGraph is built using GraphQL, which is an open source API format that allows you to request exactly the data needed, with no over-fetching or under-fetching. For a lesson in how to use NerdGraph, watch this 7-minute video: Want to watch more video tutorials? Go to the New Relic University’s Intro to NerdGraph. Or see the online course on New Relic APIs. Use the GraphiQL explorer To get started using GraphQL, we recommend playing around with our GraphiQL explorer (GraphiQL is an open source graphical interface for using GraphQL). You can use it to explore our data schema, to read built-in object definitions, and to build and execute queries. To use GraphQL, you’ll need a user-specific New Relic API key called a user key. You can generate one or find an existing one from the GraphiQL explorer’s API key dropdown. To find the GraphiQL explorer: If your New Relic account uses an EU data center, go to api.eu.newrelic.com/graphiql. Otherwise use api.newrelic.com/graphiql. For tips on how to build queries, see Build queries. Requirements and endpoints To use NerdGraph, you need a New Relic user key, which can be generated and accessed from the GraphiQL explorer. The endpoints are: Main endpoint: https://api.newrelic.com/graphql Endpoint for accounts using EU data center: https://api.eu.newrelic.com/graphql To access the endpoint, use the following cURL command: curl -X POST https://api.newrelic.com/graphql \\ -H 'Content-Type: application/json' \\ -H 'API-Key: YOUR_NEW_RELIC_USER_KEY' \\ -d '{ \"query\": \"{ requestContext { userId apiKey } }\" } ' Copy What can you do with NerdGraph? NerdGraph functionality can be broken down into two main categories: Querying New Relic data. You can fetch data for a variety of purposes, including using it in a programmatic workflow, or building a New Relic One app for custom data visualizations. Configuring New Relic features. There are a variety of configurations available and more will be added over time. You can do things like add tags, configure workloads, or customize \"golden metrics.\" You can use NerdGraph to return a wide range of New Relic data but we’ve created some tutorials for common use cases: Topic Tutorials Your monitored entities Get data about entities Understand entity relationships and dependencies (used to build service maps) Query and configure \"golden metrics\" (important entity metrics) Querying data Query using NRQL (our query language) Tags Add and manage tags APM agents APM agent configuration Dashboards Create dashboards Export dashboards to other accounts Export dashboards as files Migrate from Insights Dashboard API to NerdGraph Alerts See all alert-related tutorials Applied Intelligence View and configure topology Workloads View and configure workloads Service levels Configure and manage service levels Manage keys Create and manage keys (license keys used for data ingest, and user keys) Manage data Convert event data to metric data Drop data Distributed tracing Query distributed tracing data Configure Infinite Tracing New Relic One apps Build a New Relic One app Cloud integrations (AWS, Azure, GCP) Configure cloud integrations Partners and resellers Manage subscriptions (only for partners using original pricing model) Data partitions Manage data partitions Date retention Manage data retention NerdGraph terminology The following are terms that originate with GraphQL (the API format NerdGraph uses). Term Definition Queries and mutations There are two classes of GraphQL operations: Queries are basic requests used only to fetch data. These queries are not static, meaning that you can ask for more data or less data, depending on your needs. For each query, you can specify exactly what data you want to retrieve, as long as it is supported by the schema. Mutations are requests that perform an action, such as creating a resource or changing configuration. Mutations require the keyword mutation, as well as the name of the mutation. Type Data in GraphQL is organized into types. Types can be scalars (like strings, numbers, or booleans) or object types. An object type is a custom type made up of a collection of fields. For example, an object type called User may represent a user in a system. Field A field represents a piece of information on an object type that can be queried. Fields can be scalars, lists, or objects. For example, a User object type could have a string field called name. Interface An interface is an abstract type that represents a collection of common fields that other object types can implement. Tips on using the GraphiQL explorer You can make queries with the NerdGraph GraphiQL explorer. The explorer provides built-in schema definitions and features, including auto-complete and query validation. Query accounts a New Relic user can access You can query for the name of an account that an actor (a New Relic authorized user) has access to: query { actor { account(id: YOUR_ACCOUNT_ID) { name } } } Copy The response will mirror the query structure you defined in the request, making it easy to ask for the specific data that you want. { \"data\": { \"actor\": { \"account\": { \"name\": \"Data Nerd\" } } } } Copy Query user, account, and NRQL in one request The graph structure shows its capabilities when queries become more complex. For example, you can query for user information, account information, and make a NRQL query with one request. With REST API, this would take three different requests to three different endpoints. query { actor { account(id: YOUR_ACCOUNT_ID) { name nrql(query: \"SELECT * FROM Transaction\") { results } } user { name id } } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 721.5552,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " using original pricing model) <em>Data</em> partitions <em>Manage</em> <em>data</em> partitions Date <em>retention</em> <em>Manage</em> <em>data</em> <em>retention</em> NerdGraph terminology The following are terms that originate with GraphQL (the API format NerdGraph uses). Term Definition Queries and mutations There are two classes of GraphQL operations: Queries"
      },
      "id": "6043ff97196a67d0a0960f55"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2022-01-12T02:35:08Z",
      "updated_at": "2021-12-09T15:28:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 TISAX The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 550.6176,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Data</em> Control, Facilities, and Encryption",
        "body": " that customer&#x27;s <em>manage</em> and&#x2F;or control. New Relic&#x27;s customers can use New Relic&#x27;s Services such as NerdGraph to filter out and drop <em>data</em>. See https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;telemetry-<em>data</em>-platform&#x2F;<em>manage</em>-<em>data</em>&#x2F;drop-<em>data</em>-using-nerdgraph&#x2F;. New Relic&#x27;s customers can adjust their <em>data</em> <em>retention</em> periods as appropriate"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Event data retention (original pricing model)",
        "Important",
        "Data retention UI",
        "Overview of event data retention",
        "Extend your event retention",
        "Insights Pro",
        "How number of events stored is calculated",
        "Insights Pro event overage example",
        "Disable/enable Transaction and Pageview event reporting",
        "Tip",
        "Flexible data retention",
        "How it works",
        "Manage retention via UI",
        "Glossary"
      ],
      "title": "Event data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "76d1289aad7de08b355bb8c313f9e7a42a5779d8",
      "image": "https://docs.newrelic.com/static/e53a1e416eb6116545627d3ec880d08e/e9c9b/flex-2.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-data-retention/event-data-retention-original-pricing-plan/",
      "published_at": "2022-01-12T08:37:37Z",
      "updated_at": "2021-11-14T09:17:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original pricing model, not our New Relic One pricing model. Not sure which you're on? See Overview of pricing models. For organizations on New Relic One pricing, our various New Relic products report a wide range of event data. Different products have different data retention periods, and different ways to extend event data retention. You can customize the length of your event data retention through flexible event retention. Data retention UI For how to find the data retention UI, see Manage data. Overview of event data retention All New Relic product subscriptions come with a certain level of data retention that governs how long different types of data are retained. One type of data governed by data retention rules is event data. Event data is available in some UI charts and tables, and also available for querying via NRQL, our querying language. There are events reported from products by default, and there are custom events: each have their own retention rules, depending on the product and subscription level. Here are some examples of how different product subscriptions can affect event data retention: Free/Lite APM subscription: default-reported events available for 1 day. No custom events available. Pro APM subscription: default-reported events available for 8 days. Custom events available for 1 day (and able to be extended with Insight Pro). To see your subscriptions, go to the Account summary page. Extend your event retention Product Method APM, Browser, and Mobile Event data retention can be extended with a paid subscription to these products (see product data retention). To extend retention of both default-reported events and custom events further, you need an Insights Pro subscription. Infrastructure Event data retention can be extended with a paid Infrastructure subscription. See Infrastructure data retention rules. Synthetics Event data retention can be extended with a paid Synthetics subscription. See Synthetics data retention rules. Custom events Custom events reported by agent APIs or the Event API: Extension requires an Insights Pro subscription. Insights Pro Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. A paid Insights subscription is what governs the extension of event data retention for: Our APM, Browser, Mobile, and Serverless products Custom events that come from an agent API or from the Event API Important Note that having an Insights Pro subscription doesn't require use of the Insights UI (insights.newrelic.com) to query your data: there are other querying options available. To see the data retention governed by your Insights subscription: go to the usage UI and select Insights usage. With an Insights Pro subscription, you can use flexible retention to customize how your event data is retained. This lets you keep only the data you need, for as long as you need it. How number of events stored is calculated This is an explanation of how the number of stored events are calculated by default for an Insights Pro subscription. (Note that with flexible retention, you have more fine-grained control over the retention period.) The events stored is calculated based on 1) total events stored over time (calculated based on the events generated per week) and 2) the weeks of data retention available. This equation can be represented like this: events stored = (events generated per week) * (weeks of retention) Copy An Insights Pro subscription provides a given number of weeks of data retention as well as a given number of events over that retention period. For example: (200M transactions per week) * (4 weeks of retention) = 800M events stored in Insights (16M transactions per week) * (50 weeks of retention) = 800M events stored in Insights For Insights Pro subscriptions, data is purged based on retention window, not volume. It is deleted from the system once it's past the retention window. For example: If your Insights license is for 800 million events with a 4 week retention period, your data would start being purged after it is older than four weeks. Temporary spikes in data exceeding your subscription level will still be recorded, but consistent overage should be solved by upgrading your subscription level or decreasing data collected. For customers without an Insights Pro subscription, New Relic may throttle or downsample events to a limit of not more than than 4,000 events per host per minute. Insights Pro event overage example In this example, you have an Insights Pro subscription with a license for 800 million events over 4 weeks, a rate of 200 million events per week. You have APM Pro, Browser Pro, and Mobile Enterprise. A fifth week of data is added via your subscriptions, bumping you to a total of 1 billion events stored within your plan: If you are using 975 million events, you are not over your retention. If you are using 1.25 billion events, you are over your retention. Disable/enable Transaction and Pageview event reporting Tip Owners or Admins The Insights Data summary UI page is used to see the types of events being reported. You can also use this page to enable and disable the reporting of PageView and Transaction events. To view Data summary: Go to insights.newrelic.com > Manage data. Select the Summary tab. Note: if you disable PageView or Transaction event reporting, this can affect some New Relic UI elements. You may see some empty charts on some UI pages that rely on this data. Go to insights.newrelic.com > Manage data > Summary. From the Summary tab, select Configure data sources. Toggle the appropriate switch on or off, then save. Toggling Transaction on or off will cause reporting agents to restart themselves. For more about configuring event reporting, see Event data retention. Flexible data retention With an Insights Pro subscription, you get access to flexible retention, which lets you define how some types of event data are retained. This lets you keep only the event data you need, for as long as you need it. You can manage your flexible retention through the UI or through our GraphQL API. Requirements to use this feature: An Insights Pro subscription or equivalent trial. Applies only for events governed by an Insights Pro subscription. To use this feature, you must be an account Owner or data retention add-on manager for your account. How it works To understand how standard event data retention works, first read Event data retention. With flexible retention, you specify the data retention for applicable event namespaces across your accounts. This gives you per-event namespace control of your data. The retention that you specify for an event namespace will be shared by all the event types under that namespace. If some namespaces are not relevant to you, you can avoid collecting their event data entirely. Your retention value can’t be lower than the included retention or higher than the default retention. You can control data retention either in our UI or by API. Manage retention via UI You can control data retention either using our GraphQL API or in the UI. To do this with the UI, go to the data retention UI. Your retention changes take effect within 24 hours after updating. Glossary To understand the terms used with flexible retention, see the following: Term Description Event namespace An event's namespace corresponds to one or more event types that share a single data retention value. For more information, see Event namespaces (types). You can also use NerdGraph to get the list of customizable event namespaces. Retention value The number (in days) that specifies how long your event data is stored. Retention rule The event namespace and retention value pair that you specify to override the current retention. Licensed retention Retention period that’s determined in weeks by your Insights Pro subscription contract. Included retention Retention period for which your data is stored but not charged under the Insights Pro subscription. For details, see the data retention details for a specific product. Paid retention Retention period for which your data is stored and is charged under the Insights Pro subscription. By default, your licensed retention determines this value but Flexible retention lets you override it. Default retention Retention period that comes out of the box. This is based on the total of included retention plus licensed retention. For information on managing retention settings with APIs, see the Manage data retention documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 436.58804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Event <em>data</em> <em>retention</em> (original pricing model)",
        "sections": "<em>Manage</em> <em>retention</em> via UI",
        "tags": "Original <em>data</em> <em>retention</em>",
        "body": " different <em>data</em> <em>retention</em> periods, and different ways to extend event <em>data</em> <em>retention</em>. You can customize the length of your event <em>data</em> <em>retention</em> through flexible event <em>retention</em>. <em>Data</em> <em>retention</em> UI For how to find the <em>data</em> <em>retention</em> UI, see <em>Manage</em> <em>data</em>. Overview of event <em>data</em> <em>retention</em> All New Relic"
      },
      "id": "6043f713e7b9d2ccee579a1d"
    }
  ],
  "/docs/data-apis/manage-data/manage-your-data": [
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 329.0906,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 303.09418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick start: Send metric <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Telemetry SDKs: Report custom telemetry data",
        "Requirements and compatibility",
        "Tip",
        "Available libraries",
        "Write your own Telemetry SDK or contribute to an existing one"
      ],
      "title": "Telemetry SDKs: Report custom telemetry data",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "d883a07b7ede4c3beaba4077c507b95f9a228435",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/telemetry-sdks-report-custom-telemetry-data/",
      "published_at": "2022-01-12T06:17:44Z",
      "updated_at": "2021-12-19T14:27:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Telemetry SDKs are an open source set of API client libraries that send data to the New Relic platform. Under the hood, these SDKs rely on our primary data ingest APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don't meet your needs, our Telemetry SDKs are one way to create a custom telemetry solution (see other solutions for reporting custom data). Requirements and compatibility The Telemetry SDKs use our Metric API, Event API, Log API, and Trace API, which all require a license key, so you'll need a license key for the account you wish to send data to. Tip New Relic has contributed the Telemetry SDK to the open source community under an Apache 2.0 license. Available libraries The Telemetry SDKs are open source software on GitHub. Use the language-specific GitHub links below to get library details, coding examples, and procedures for how to use the SDKs. We currently support the following libraries, with more to be created in the future: Language Library Supported New Relic data types Java Java library on GitHub Metrics Events Logs Traces Node/TypeScript NodeJS library on GitHub Metrics Traces Python Python library on GitHub Metrics Events Logs Traces Go Go library on Github Metrics Traces .NET .NET library on GitHub .NET package in NuGet Metrics Traces C C library on Github Traces Rust Rust library on Github Traces Ruby Ruby library on Github Gem on Rubygems Traces For more on the supported data types, see: An overview of New Relic data types Metrics: see the Metric API Logs: see the Log API Traces: see the Trace API Events: see the Event API Write your own Telemetry SDK or contribute to an existing one If you need a Telemetry SDK in a language that does not currently exist or want to contribute to an existing library, please see the Telemetry SDK specifications.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.31625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "sections": "Telemetry SDKs: Report custom telemetry <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Our Telemetry SDKs are an open source set of API client libraries that send <em>data</em> to the New Relic platform. Under the hood, these SDKs rely on our primary <em>data</em> <em>ingest</em> APIs: the Metric API, Trace API, Log API, and Event API. If our pre-built solutions don&#x27;t meet your needs, our Telemetry SDKs"
      },
      "id": "603ea196196a670192a83d83"
    }
  ],
  "/docs/data-apis/manage-data/nrintegrationerror": [
    {
      "sections": [
        "Troubleshooting OpenTelemetry with New Relic",
        "OpenTelemetry data sent via OTLP is not queryable",
        "Problem",
        "Solution",
        "Important",
        "OpenTelemetry entities or relationships are missing",
        "Tip"
      ],
      "title": "Troubleshooting OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "9478cc98ba9216af5ad8c74883abdf14565a21a4",
      "image": "https://docs.newrelic.com/static/93271ff8121b09ca17395fdf3f27e700/c1b63/otlp-troubleshooting-facet-query.png",
      "url": "https://docs.newrelic.com/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-troubleshooting/",
      "published_at": "2022-01-12T03:03:45Z",
      "updated_at": "2022-01-08T10:01:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Troubleshooting OpenTelemetry with New Relic may just be a matter of making sure you are following best practices, but sometimes you may need to take additional steps to diagnose your issues. Here are some examples of specific problems you might encounter, along with steps and tools to resolve them. OpenTelemetry data sent via OTLP is not queryable Problem You sent OpenTelemetry metrics, logs, or traces using OTLP and are unable to view the data. Before digging deeper, make sure you've checked the following: The OTLP endpoint configured matches one of our documented endpoints, is properly formatted, and includes the official default port, 4317. Sending OTLP data via port 443 is not supported at this time. Please note the specific endpoint for FedRAMP compliance, if applicable. The outbound traffic is not restricted by a firewall. Our Networks document explains domains and network blocks that you may need to explicitly allow. The client is configured to use TLS 1.2 or higher and the request includes the api-key header with a valid New Relic account (ingest) license key. Requests include valid protobuf payloads and use gRPC and HTTP/2 transport, preferably with gzip compression enabled. Sending protobuf or JSON-encoded payloads over HTTP/1.1 is not supported at this time. Client output and logs do not indicate 4xx or 5xx response codes are being returned. Solution There are number of tools you can use to validate the successful delivery of telemetry data to our platform. A good first step is to check the data management hub to facet data ingest and determine how much data is arriving from various sources. You can also use the data explorer or query builder to look for data faceted by instrumentation.provider or newrelic.source attributes: FROM Log, Metric, Span SELECT datapointcount() WHERE instrumentation.provider = 'opentelemetry' FACET instrumentation.provider, newrelic.source Copy This query should tell you whether data is arriving via OTLP. If the data you expect is not present, try this alternate query: FROM Log, Metric, Span SELECT count(*) where newrelic.source LIKE 'api.%.otlp' Copy You can also check for integration errors by querying NrIntegrationError events. This can help you determine whether you have configuration or format issues or if you've run into our platform limits. Important The ingest limits for metrics, logs, and traces via OTLP are the same as our other data ingest API limits. Various parts of the New Relic UI rely on the presence of specific attributes to function properly. You can use the NRQL console feature in many places to check the WHERE or FACET clauses of the query for required attributes. You can also edit those clauses and re-run the query to determine whether there is data present with those attributes missing. Examples of required attributes include service.name and service.instance.id. For a more complete list of examples, see resources. OpenTelemetry entities or relationships are missing Problem You sent OpenTelemetry data from a service or infrastructure component and either the entity or its relationships are missing or incorrect. Solution OpenTelemetry entities will be synthesized based on the public rules described for the EXT-SERVICE entity type. The standard rule to match relies on the presence of the service.name dimension which follows the OpenTelemetry semantic conventions. To set the service.name with the OpenTelemetry Java SDK, include it in your resource: var resource = Resource.getDefault() .merge(Resource.builder().put(SERVICE_NAME, serviceName).build()); Copy Depending on the SDK, you may also set the service.name by declaring it in the OTEL_RESOURCE_ATTRIBUTES or OTEL_SERVICE_NAME environment variables. For Logs, you can use a structured log template to inject the service.name. Here are some log examples: Setting the service name Logs in context with Log4j2 Tip For more OpenTelemetry examples with New Relic, visit the newrelic-opentelemetry-examples repository on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.32135,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "OpenTelemetry <em>data</em> sent via OTLP is not queryable",
        "tags": "<em>Integrations</em>",
        "body": " newrelic.source LIKE &#x27;api.%.otlp&#x27; Copy You can also check for <em>integration</em> errors by querying <em>NrIntegrationError</em> events. This can help you determine whether you have configuration or format issues or if you&#x27;ve run into our platform limits. Important The <em>ingest</em> limits for metrics, logs, and traces via OTLP"
      },
      "id": "618e863f196a67bd4ce723da"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 302.86557,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick start: Send metric <em>data</em>",
        "tags": "<em>Ingest</em> and manage <em>data</em>",
        "body": " that New Relic found an <em>error</em> during this asynchronous validation. You can find these errors by querying <em>NrIntegrationError</em> events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the <em>NrIntegrationError</em> <em>event</em>. For more information, see"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "Best practices for OpenTelemetry with New Relic",
        "Tip",
        "Resources",
        "Batching",
        "Caution",
        "Compression",
        "Traces",
        "Required fields",
        "Sampling",
        "OpenTelemetry built-in samplers",
        "OpenTelemetry tail-based samplers",
        "New Relic tail-based sampling with Infinite Tracing",
        "Metrics",
        "Sum metrics",
        "Delta sums",
        "Cumulative sums",
        "Sum configuration examples",
        "Gauge metrics",
        "Histogram metrics",
        "Summary metrics",
        "Start time",
        "Array values for attributes",
        "Exemplars",
        "How to query metrics",
        "Query cumulative sums stored as gauges",
        "Example: Raw gauge value for cumulative sums",
        "Example: Rate of change with cumulative sums as gauges",
        "Query gauge metrics",
        "Query histogram metrics",
        "Example: Normal distribution",
        "Example: Heat map",
        "Important",
        "Logs",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs",
        "The time field"
      ],
      "title": "Best practices for OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "b26055149cf57bdca8f27ae00d3ac93f75e4cc86",
      "image": "https://docs.newrelic.com/static/38ea1bb698187129fb2d8d38ea2dcde8/c1b63/sum-derivative-function.png",
      "url": "https://docs.newrelic.com/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/",
      "published_at": "2022-01-12T03:02:57Z",
      "updated_at": "2022-01-08T09:08:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some best practices based on how OpenTelemetry works with New Relic: Resources Batching Compression Traces Metrics Logs Tip For information about resolving specific issues, see our troubleshooting guide. Resources A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The OpenTelemetry Resource SDK specification defines the functionality implemented by all language SDKs for defining a resource. The following suites of attributes are defined by the OpenTelemetry resource semantic conventions. These attributes are usually set by creating a resource using the OpenTelemetry SDK. service.* attributes service.name attribute is required to associate your resource with an entity in the UI service.instance.id is required for certain panes to light up telemetry.sdk.language=java is required to see data in the JVM section Batching Caution Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section. By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited. All OpenTelemetry SDKs and Collectors provide a BatchProcessor, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point. Component Batch Processor Collector Batch Processor Go SDK BatchSpanProcessor JS SDK BatchSpanProcessor Python SDK BatchExportSpanProcessor Compression New Relic supports gzip compression for OTLP payloads exported over gRPC. To maximize the amount of data you can send per request, we recommend enabling compression in all OTLP exporters. If there are other compression formats you'd like to see us support, please let us know in the CNCF Slack channel. Traces Familiarize yourself with these trace topics to ensure your traces and spans appear in New Relic. Required fields The startTimeUnixNano and endTimeUnixNano fields on spans are required according to the OpenTelemetry protocol for trace data. When startTimeUnixNano is not present, the span is dropped and a NrIntegrationError is created. When endTimeUnixNano is not present, the duration of your span is large and negative. The timeUnixNano field on span events is required. When timeUnixNano is not present, the span event is dropped and a NrIntegrationError is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace data. When traceId or spanId are not present, the span is dropped and a NrIntegrationError is created. Sampling Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy. You can configure sampling in a number of places: Service: Use the OpenTelemetry SDK for your language. Collector: If you're running your own instance of the OpenTelemetry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling (see below). Check out this documentation about how to configure different types of sampling: OpenTelemetry built-in samplers Built-in samplers implemented by the OpenTelemetry SDK for each language. OpenTelemetry tail-based samplers The OpenTelemetry collector has a tail-based sampling processor. We have an example demonstrating the use of the tail-based sampling processor. New Relic tail-based sampling with Infinite Tracing Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler. Metrics OpenTelemetry metrics are largely compatible with New Relic dimensional metrics. We support OpenTelemetry metrics v0.10. All of the supported metric types include an independent set of associated attributes (name-value pairs) which map directly to dimensions you can use to facet or filter metric data at query time. OpenTelemetry metrics are accompanied by a set of resource attributes that identify the originating entity that produced them and map to dimensions for faceting and filtering. The OpenTelemetry data model for metrics defines a number of different metric types: sum, gauge, histogram, and summary. Sum metrics OpenTelemetry sums are a scalar metric that is the sum of all data points over a given time window. Sums have a notion of temporality indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality). In addition, sums can either be monotonic (only go up or only go down) or non-monotonic (go up and down). Delta sums In New Relic, delta metrics are handled differently depending on whether they are monotonic or non-monotonic: Monotonic delta sums are mapped to the count metric type. Non-monotonic delta sums are mapped to the gauge metric type. Cumulative sums Monotonic and non-monotonic cumulative sums are mapped to the New Relic gauge metric type. Sum configuration examples To understand how to configure aggregation temporality, see these examples using the Java and Go OpenTelemetry SDKs. Gauge metrics OpenTelemetry gauge metric data points represent a sampled value at a given time. These values are converted to the New Relic gauge metric type. OpenTelemetry gauges do not have an aggregation temporality, but the sampled values can be aggregated at query time. Histogram metrics OpenTelemetry histograms compactly represent a population of recorded values along with a total count and sum. Optionally, histograms may include a series of buckets with explicit bounds and a count value for that bucket’s population. OpenTelemetry histograms are converted to New Relic’s distribution metric type, which is backed by a scaled exponential base 2 histogram (see NrSketch for a more thorough explanation). Counts from OpenTelemetry histogram buckets are assigned to New Relic’s distribution metric buckets using linear interpolation. Also, OpenTelemetry has negative and positive infinity bound buckets which we represent in New Relic as zero-width buckets. We do this because we do not have a representation for negative and positive infinity. For example, an OpenTelemetry bucket with bounds [-∞, 10) will be represented by a [10,10) zero width New Relic bucket. You may see exaggerated bucket counts at the endpoints of your distribution due to this translation. Summary metrics OpenTelemetry summary metric data points are used to represent quantile summaries (for example, P99 latency). These map directly to the New Relic summary metric type. Summary metric data points include count, sum, and quantile values, with 0.0 as min and 1.0 as max. OpenTelemetry provides summary metrics for compatibility with other formats. Start time The startTimeUnixNano field is optional according to the OpenTelemetry specification. When this field is provided, it is used for the timestamp on the resulting NewRelic metric, and the duration is calculated as timeUnixNano - startTimeUnixNano. The duration field is used to calculate the queryable endTimeStamp attribute on the New Relic metric, but it serves no other semantic purpose. If startTimeUnixNano is not provided, then timeUnixNano is used for the timestamp field on the resulting NewRelic metric, and the duration field is set to zero. Array values for attributes OpenTelemetry metrics and other signals may include attributes that consist of a homogenous array of primitive types. These attributes are not supported by New Relic. Exemplars OpenTelemetry defines exemplar values that allow other signals, like traces, to be connected to a metric event and provide context. Exemplars are not supported by New Relic. How to query metrics Consider these tips for building metric NRQL queries in New Relic. Query cumulative sums stored as gauges Since cumulative sums are converted to gauges, here are some ways to query your data: Example: Raw gauge value for cumulative sums To view the raw gauge value for cumulative sums, you can use the latest() NRQL function: SELECT latest(totalApiBytesSent) FROM Metric TIMESERIES FACET description, statusCode Copy Example: Rate of change with cumulative sums as gauges To see the rate of change over a given time interval for a cumulative sum stored as a gauge, you can use the derivative() NRQL function: SELECT derivative(totalApiBytesSent, 1 second) FROM Metric TIMESERIES 5 MINUTES SLIDE BY 1 MINUTE FACET description, statusCode Copy New Relic does not currently support either reporting on resets and gaps or accounting for them with cumulative counters. Query gauge metrics When New Relic converts cumulative sums to gauges, you can query them using either the latest() or derivative() NRQL functions. The function you choose depends on whether you want to see the raw value or compute the rate of change. Query histogram metrics New Relic histograms translated from OpenTelemetry metrics have the same query semantics as other New Relic histograms. Namely, the histogram() NRQL function can be used to represent the histogram with a configurable number of buckets and bucket width. Note that you may see larger bucket counts at the endpoint buckets. This is because we are adding negative and positive infinity bound OpenTelemetry buckets into a zero width New Relic bucket. Example: Normal distribution FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) WHERE distributionType = 'Normal Distribution' SINCE 1 day ago Copy Example: Heat map The FACET keyword is also available to create heat map charts. FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) FACET distributionType SINCE 1 day ago Copy Important The TIMESERIES keyword is not supported for New Relic histograms. Logs Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a log data model for representing log data. You can send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic. Send logs to New Relic The OpenTelemetry Collector and OpenTelemetry Collector Contrib repositories contain a number of components for consuming log data. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs to New Relic via the OTLP exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application. The time field The timeUnixNano field is optional according to the OpenTelemetry specification for log data. When timeUnixNano is not present New Relic will use the time that the data was received for the New Relic log timestamp.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 298.26642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "How <em>to</em> query metrics",
        "tags": "<em>Integrations</em>",
        "body": " events is required. When timeUnixNano is not present, the span <em>event</em> is dropped and a <em>NrIntegrationError</em> is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace <em>data</em>. When traceId or spanId are not present, the span is dropped"
      },
      "id": "617d77ec28ccbc08677ff6b5"
    }
  ],
  "/docs/data-apis/manage-data/query-limits": [
    {
      "sections": [
        "Know your data limits",
        "Responses to limit violations",
        "System limits UI",
        "Troubleshooting system limits",
        "Account-level limits",
        "Data ingest API limits",
        "Finding other agent and integration limits"
      ],
      "title": "Know your data limits",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "7c540d94a8b5e4f024d175ad53cab9fab343187c",
      "image": "https://docs.newrelic.com/static/8ee61e3091f6e044202cff92026afada/8c557/limits-graph.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/view-system-limits/",
      "published_at": "2022-01-12T06:19:45Z",
      "updated_at": "2021-10-31T06:37:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To ensure our systems are always up and ready to support you, and to keep you from unintended use, we place limits on the amount of data you can send, query, and store. Responses to limit violations Limits are enforced per child account, and across our APIs. You might reach a limit if you start monitoring a new high-traffic application, or have a sudden data spike. When you do reach a limit, New Relic responds according to the type of data and the limit that’s reached. For example: We place a limit on the number of ingested requests per minute (RPM) per data type. When this limit is reached, we stop accepting data and return a 429 status code for the duration of the minute. For queries, we place limits on the number of queries per minute and the number of records inspected (see query limits). When the number of queries per minute limit is reached, New Relic will begin rejecting queries until the number of queries is below the limit. When the records inspected limit is reached, New Relic will reject traffic from the source scanning the largest number of records and attempt to allow traffic from other sources. For metrics, we place a limit on the number of unique timeseries (cardinality) per account and per metric. When this limit is reached, aggregated data is turned off for the rest of the UTC day. For every major limit violation, New Relic creates an NrIntegrationError event for that account, which has these limit-related attributes: Attribute Description category RateLimit or ApiLimit. The RateLimit category is used for limits based on a unit of time such as the number of requests ingested per minute. The ApiLimit is used for constant limits, such as the number of attributes on a record. limitName The name of the limit. message Describes the limit and the impact. limitValue The limit reached. System limits UI The system Limits page (from the account dropdown, click Manage your data and click Limits on the left) displays when your account has encountered a rate limit in the specified time period. The page displays a default period of 24 hours; you can set a custom range from the top-right of the page. Non-limit-related NrIntegrationError events are not displayed here. In addition, this page does not display information about limits you have not hit, or how close you are to reaching a limit. For more on creating queries and alerts for data ingest and billing metrics, see Query billing/usage data. one.newrelic.com > account dropdown > Manage your data > Limits: An example of a chart on the Limits UI page displaying a cardinality violation limit issue. To add more detail, or build a dashboard, click the View NRQL button on the chart to see the NRQL powering this view. The graph displays each unique limit type that was reached during the selected time-period. This can help you find any trends based on time. The Limits page also provides a table where you can find the limit name, the limit event message associated with it, and last occurrence time and date. If you click a limit in the table, you see more about what happened, and when. one.newrelic.com > account dropdown > Manage your data > Limits: An example of a limit events table on the Limits UI page. Troubleshooting system limits To troubleshoot limits when you reach them, click the limit info in the table, and then follow the docs link that's provided. Different limits have different solutions. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest API limits Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Finding other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our quickstarts here. Some default reporting limits are located in these tools' configuration docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.9804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Know your <em>data</em> <em>limits</em>",
        "sections": "<em>Data</em> <em>ingest</em> API <em>limits</em>",
        "tags": "<em>Ingest</em> and <em>manage</em> <em>data</em>",
        "body": ". <em>limit</em>Value The <em>limit</em> reached. <em>System</em> <em>limits</em> UI The <em>system</em> <em>Limits</em> page (from the account dropdown, click <em>Manage</em> your <em>data</em> and click <em>Limits</em> on the left) displays when your account has encountered a rate <em>limit</em> in the specified time period. The page displays a default period of 24 hours; you can set"
      },
      "id": "60446a7c64441f48d7378f2b"
    },
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.7971,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand and <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand and <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> and <em>manage</em> <em>data</em>",
        "body": " for network, storage, and <em>system</em> events. Disable process <em>metrics</em>. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. <em>Manage</em> Kubernetes events integration. Adjust log <em>data</em>"
      },
      "id": "603e978228ccbc8984eba79e"
    },
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.50551,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "sections": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "tags": "<em>Ingest</em> and <em>manage</em> <em>data</em>",
        "body": " the account dropdown, and the click <em>Manage</em> your <em>data</em>. Better cost, performance, and compliance Collecting and storing <em>data</em> in New Relic allows you to analyze, visualize, and alert on all your <em>metrics</em>, events, logs, and traces from across all of your sources. However, it’s important to <em>manage</em> that <em>data</em>"
      },
      "id": "603e96ff28ccbcf8bceba796"
    }
  ],
  "/docs/data-apis/manage-data/view-system-limits": [
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 329.0903,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "Understand <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    },
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.23956,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "sections": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " in: they let you decide what <em>data</em> you send to New Relic and how long it should be stored. <em>Data</em> management hub: from the account dropdown in the top right of the UI, select <em>Manage</em> your <em>data</em>. Coupled with user management tools, <em>data</em> management helps you get maximum value from your investment in New Relic, all"
      },
      "id": "603e96ff28ccbcf8bceba796"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 303.09387,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick start: Send metric <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    }
  ],
  "/docs/data-apis/understand-data/event-data/customized-security-settings-insights": [
    {
      "sections": [
        "Default events reported by New Relic products"
      ],
      "title": "Default events reported by New Relic products",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Default events"
      ],
      "external_id": "598dfde069ba5a8bbbd5834c44b9740d6b338cdc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/understand-data/event-data/default-events-reported-new-relic-products/",
      "published_at": "2022-01-12T03:45:20Z",
      "updated_at": "2021-10-23T17:29:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic products report different types of data. One type of data reported is event data. Events are displayed in UI charts and tables, and also made available for querying. To understand the types of data available, see Data available via NRQL. Learn more about the events reported by New Relic products: APM default events Browser default events Infrastructure default events Mobile default events Synthetics default events NrAuditEvent events for understanding changes to your account",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.07796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Default</em> <em>events</em> reported by New Relic products",
        "sections": "<em>Default</em> <em>events</em> reported by New Relic products",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "New Relic products report different types of <em>data</em>. One type of <em>data</em> reported is <em>event</em> <em>data</em>. <em>Events</em> are displayed in UI charts and tables, and also made available for querying. To understand the types of <em>data</em> available, see <em>Data</em> available via NRQL. Learn more about the <em>events</em> reported by New Relic"
      },
      "id": "609f8faf64441f8af9d2a1f0"
    },
    {
      "sections": [
        "Events reported by synthetic monitoring"
      ],
      "title": "Events reported by synthetic monitoring",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Default events"
      ],
      "external_id": "908d6d1bc27321c7d0c330318504e4681c25a400",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/understand-data/event-data/events-reported-synthetic-monitoring/",
      "published_at": "2022-01-12T02:05:55Z",
      "updated_at": "2021-10-23T17:25:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Synthetic monitoring in New Relic reports event data that is displayed in some UI displays and is also available for querying and charting. Select an event name in the following table to see its attributes. Event Description SyntheticCheck SyntheticCheck returns metrics from one run of a specific monitor. These metrics include duration information for the monitor, location of the monitor check, size of the request and response headers, the type of monitor, and a timestamp. Each time a synthetic monitor runs a check, details about the check are captured in theSyntheticCheck event type. SyntheticCheck events contain details specific to the check to provide visibility such as the status, type of monitor, and size of request and response headers. SyntheticRequest SyntheticRequest returns results from individual HTTP requests made during a check. The data gathered include job information, location, type of content for request, duration information, request size, and page load information. With each simple or scripted monitor check, we capture each individual HTTP request made during the check. The HTTP details are captured at a more granular level than the SyntheticCheck event type. SyntheticPrivateLocationStatus Every monitor check running on a private location triggers capacity details for that private location. These details are captured in a SyntheticPrivateLocationStatus event. This provides visibility into the capacity of a private location and whether additional minions are required to support the workload. SyntheticPrivateMinion If you have private locations, such as those inside your firewall, you can view information regarding those locations with the SyntheticPrivateMinion event. Each private minion running sends health details to SyntheticPrivateMinion every 30 seconds. This allows you to understand the health of the private minion running at the location. Related documentation: Report custom events Extend data retention See example NRQL queries",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.07721,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Events</em> reported by synthetic monitoring",
        "sections": "<em>Events</em> reported by synthetic monitoring",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "Synthetic monitoring in New Relic reports <em>event</em> <em>data</em> that is displayed in some UI displays and is also available for querying and charting. Select an <em>event</em> name in the following table to see its attributes. <em>Event</em> Description SyntheticCheck SyntheticCheck returns metrics from one run of a specific"
      },
      "id": "609f8faf64441f8e99d2a1d5"
    },
    {
      "sections": [
        "Data requirements and limits for custom event data",
        "General requirements",
        "Important",
        "Reserved words",
        "Event type limits"
      ],
      "title": "Data requirements and limits for custom event data",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "77720ef366038ba648a5fbf3cf34e8e48b38440a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/data-requirements-limits-custom-event-data/",
      "published_at": "2022-01-12T02:18:23Z",
      "updated_at": "2021-10-23T21:58:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains general requirements and rules for inserting and using custom events and their associated attributes. Additional requirements may apply based on the method you use. You can report custom events to New Relic in several ways, including: APM agent APIs Event API (There are additional requirements when using the Event API.) Browser monitoring agent APIs (There are additional requirements with the custom PageAction event.) Mobile monitoring SDK General requirements When reporting custom events and attributes, follow these general requirements for supported data types, naming syntax, and size: Requirement Description Payload Total maximum size or length: 1MB (10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. The Event API has additional HTTP rate limits. Attribute data types Attribute values can be either a string or a numeric integer or float. If your attribute values contain date information, define it as an unformatted Unix timestamp (in seconds or milliseconds) by using the Insights data formatter. Attribute size Maximum name size: 255 bytes. Maximum attribute value size: Custom attributes sent by the agent: 255 bytes Attributes attached to custom events sent using the Event API: 4096 characters Maximum total attributes per event: 254. Exception: If you use an APM agent API, the max is 64. Maximum total attributes per event type: 48,000. Important Charts may only display the first 255 characters of attribute values. For complete attribute values, use the JSON chart type or Query API. Naming syntax Attribute names can be a combination of alphanumeric characters, colons (:), periods (.), and underscores (_). Event types (using the eventType attribute) can be a combination of alphanumeric characters, colons (:), and underscores (_). If the name begins with anything other than an alphabetical character, enclose the name with backticks in your NRQL query. For example: FROM `0_hello` SELECT count(*) Copy Do not use words reserved for use by NRQL. Null values The database does not store any data with a null value. Reserved words Avoid using the following reserved words as names for events and attributes. Otherwise, unexpected results may occur. Important This is not a complete list. In general, avoid using MySQL-reserved words to avoid collision with future New Relic functionality. Keyword Description accountId This is a reserved attribute name. If it's included, it will be dropped during ingest. appId Value must be an integer. If it is not an integer, the attribute name and value will be dropped during ingest. eventType The event type as stored in New Relic. New Relic agents and scripts normally report this as eventType. Can be a combination of alphanumeric characters, colons (:), and underscores (_). Be sure to review the prohibited eventType values and eventType limits. Prohibited eventType values For your eventType value, avoid using: Metric, MetricRaw, and strings prefixed with Metric[0-9] (such as Metric2 or Metric1Minute). Public_ and strings prefixed with Public_. These event types are reserved for use by New Relic. Events passed in with these eventType values will be dropped. timestamp Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. It must be +/-1 day (24 hours) of the current time on the server. Log forwarding terms The following keys are reserved by the Infrastructure agent's log forwarding feature: entity.guid, log, hostname, plugin.type, fb.input. If used, they are dropped during ingest and a warning is added to the logs. NRQL syntax terms If you need to use NRQL syntax terms as attribute names, including dotted attributes, they must be enclosed in backticks; for example, `LIMIT` or `consumer.offset`. Otherwise, avoid using these reserved words: ago, and, as, auto, begin, begintime, compare, day, days, end, endtime, explain, facet, from, hour, hours, in, is, like, limit, minute, minutes, month, months, not, null, offset, or, raw, second, seconds, select, since, timeseries, until, week, weeks, where, with Event type limits The current limit for total number of eventType values is 250 per child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop data. Event types include: Default events from New Relic agents Custom events from New Relic agents Custom events from Insights custom event inserter",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.30388,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Data</em> requirements and limits for custom <em>event</em> <em>data</em>",
        "sections": "<em>Data</em> requirements and limits for custom <em>event</em> <em>data</em>",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": " child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop <em>data</em>. <em>Event</em> types include: <em>Default</em> <em>events</em> from New Relic agents Custom <em>events</em> from New Relic agents Custom <em>events</em> from <em>Insights</em> custom <em>event</em> inserter"
      },
      "id": "609fa5cfe7b9d2bf16c3eb69"
    }
  ],
  "/docs/data-apis/understand-data/event-data/default-events-reported-new-relic-products": [
    {
      "sections": [
        "Security for New Relic-reported events and attributes",
        "Default events and attributes",
        "Adjust the data reported"
      ],
      "title": "Security for New Relic-reported events and attributes ",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Default events"
      ],
      "external_id": "446305a7d17c6dfb44e9e87520a1c08b79f5bcf9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/understand-data/event-data/customized-security-settings-insights/",
      "published_at": "2022-01-12T03:45:19Z",
      "updated_at": "2021-10-23T17:29:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "By default, New Relic products report a variety of data used in our UI charts and that is available for querying. Our products will not transmit sensitive information without being explicitly instrumented to do so. Default events and attributes Our products report a set of default events and attributes. We will never send request parameters or any other attributes that are not in the default set, unless someone has explicitly enabled this via configuration. Adjust the data reported When evaluating security settings for a New Relic product, review the default events and attributes. The default attributes don't contain sensitive data. In general, it's simply the data needed for effective performance monitoring. Our products don't send other data unless you change the default security settings. Depending on your requirements, either or both of these situations may apply: If the default list contains data you're concerned about, you can disable those attributes from being collected. For how to edit that, see the documentation for the product you're using. If you need to send attributes not reported by default, you can enable those attributes to be reported. In that case, do not use high security mode: this will disable the ability to collect custom attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.07796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security for New Relic-reported <em>events</em> and attributes ",
        "sections": "<em>Default</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "By <em>default</em>, New Relic products report a variety of <em>data</em> used in our UI charts and that is available for querying. Our products will not transmit sensitive information without being explicitly instrumented to do so. <em>Default</em> <em>events</em> and attributes Our products report a set of <em>default</em> <em>events</em>"
      },
      "id": "60a8ea67e7b9d25ec7aeabfe"
    },
    {
      "sections": [
        "Events reported by synthetic monitoring"
      ],
      "title": "Events reported by synthetic monitoring",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Default events"
      ],
      "external_id": "908d6d1bc27321c7d0c330318504e4681c25a400",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/understand-data/event-data/events-reported-synthetic-monitoring/",
      "published_at": "2022-01-12T02:05:55Z",
      "updated_at": "2021-10-23T17:25:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Synthetic monitoring in New Relic reports event data that is displayed in some UI displays and is also available for querying and charting. Select an event name in the following table to see its attributes. Event Description SyntheticCheck SyntheticCheck returns metrics from one run of a specific monitor. These metrics include duration information for the monitor, location of the monitor check, size of the request and response headers, the type of monitor, and a timestamp. Each time a synthetic monitor runs a check, details about the check are captured in theSyntheticCheck event type. SyntheticCheck events contain details specific to the check to provide visibility such as the status, type of monitor, and size of request and response headers. SyntheticRequest SyntheticRequest returns results from individual HTTP requests made during a check. The data gathered include job information, location, type of content for request, duration information, request size, and page load information. With each simple or scripted monitor check, we capture each individual HTTP request made during the check. The HTTP details are captured at a more granular level than the SyntheticCheck event type. SyntheticPrivateLocationStatus Every monitor check running on a private location triggers capacity details for that private location. These details are captured in a SyntheticPrivateLocationStatus event. This provides visibility into the capacity of a private location and whether additional minions are required to support the workload. SyntheticPrivateMinion If you have private locations, such as those inside your firewall, you can view information regarding those locations with the SyntheticPrivateMinion event. Each private minion running sends health details to SyntheticPrivateMinion every 30 seconds. This allows you to understand the health of the private minion running at the location. Related documentation: Report custom events Extend data retention See example NRQL queries",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.07721,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Events</em> reported by synthetic monitoring",
        "sections": "<em>Events</em> reported by synthetic monitoring",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "Synthetic monitoring in New Relic reports <em>event</em> <em>data</em> that is displayed in some UI displays and is also available for querying and charting. Select an <em>event</em> name in the following table to see its attributes. <em>Event</em> Description SyntheticCheck SyntheticCheck returns metrics from one run of a specific"
      },
      "id": "609f8faf64441f8e99d2a1d5"
    },
    {
      "sections": [
        "Data requirements and limits for custom event data",
        "General requirements",
        "Important",
        "Reserved words",
        "Event type limits"
      ],
      "title": "Data requirements and limits for custom event data",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "77720ef366038ba648a5fbf3cf34e8e48b38440a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/data-requirements-limits-custom-event-data/",
      "published_at": "2022-01-12T02:18:23Z",
      "updated_at": "2021-10-23T21:58:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains general requirements and rules for inserting and using custom events and their associated attributes. Additional requirements may apply based on the method you use. You can report custom events to New Relic in several ways, including: APM agent APIs Event API (There are additional requirements when using the Event API.) Browser monitoring agent APIs (There are additional requirements with the custom PageAction event.) Mobile monitoring SDK General requirements When reporting custom events and attributes, follow these general requirements for supported data types, naming syntax, and size: Requirement Description Payload Total maximum size or length: 1MB (10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. The Event API has additional HTTP rate limits. Attribute data types Attribute values can be either a string or a numeric integer or float. If your attribute values contain date information, define it as an unformatted Unix timestamp (in seconds or milliseconds) by using the Insights data formatter. Attribute size Maximum name size: 255 bytes. Maximum attribute value size: Custom attributes sent by the agent: 255 bytes Attributes attached to custom events sent using the Event API: 4096 characters Maximum total attributes per event: 254. Exception: If you use an APM agent API, the max is 64. Maximum total attributes per event type: 48,000. Important Charts may only display the first 255 characters of attribute values. For complete attribute values, use the JSON chart type or Query API. Naming syntax Attribute names can be a combination of alphanumeric characters, colons (:), periods (.), and underscores (_). Event types (using the eventType attribute) can be a combination of alphanumeric characters, colons (:), and underscores (_). If the name begins with anything other than an alphabetical character, enclose the name with backticks in your NRQL query. For example: FROM `0_hello` SELECT count(*) Copy Do not use words reserved for use by NRQL. Null values The database does not store any data with a null value. Reserved words Avoid using the following reserved words as names for events and attributes. Otherwise, unexpected results may occur. Important This is not a complete list. In general, avoid using MySQL-reserved words to avoid collision with future New Relic functionality. Keyword Description accountId This is a reserved attribute name. If it's included, it will be dropped during ingest. appId Value must be an integer. If it is not an integer, the attribute name and value will be dropped during ingest. eventType The event type as stored in New Relic. New Relic agents and scripts normally report this as eventType. Can be a combination of alphanumeric characters, colons (:), and underscores (_). Be sure to review the prohibited eventType values and eventType limits. Prohibited eventType values For your eventType value, avoid using: Metric, MetricRaw, and strings prefixed with Metric[0-9] (such as Metric2 or Metric1Minute). Public_ and strings prefixed with Public_. These event types are reserved for use by New Relic. Events passed in with these eventType values will be dropped. timestamp Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. It must be +/-1 day (24 hours) of the current time on the server. Log forwarding terms The following keys are reserved by the Infrastructure agent's log forwarding feature: entity.guid, log, hostname, plugin.type, fb.input. If used, they are dropped during ingest and a warning is added to the logs. NRQL syntax terms If you need to use NRQL syntax terms as attribute names, including dotted attributes, they must be enclosed in backticks; for example, `LIMIT` or `consumer.offset`. Otherwise, avoid using these reserved words: ago, and, as, auto, begin, begintime, compare, day, days, end, endtime, explain, facet, from, hour, hours, in, is, like, limit, minute, minutes, month, months, not, null, offset, or, raw, second, seconds, select, since, timeseries, until, week, weeks, where, with Event type limits The current limit for total number of eventType values is 250 per child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop data. Event types include: Default events from New Relic agents Custom events from New Relic agents Custom events from Insights custom event inserter",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.30388,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Data</em> requirements and limits for custom <em>event</em> <em>data</em>",
        "sections": "<em>Data</em> requirements and limits for custom <em>event</em> <em>data</em>",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": " child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop <em>data</em>. <em>Event</em> types include: <em>Default</em> <em>events</em> from New Relic agents Custom <em>events</em> from New Relic agents Custom <em>events</em> from <em>Insights</em> custom <em>event</em> inserter"
      },
      "id": "609fa5cfe7b9d2bf16c3eb69"
    }
  ],
  "/docs/data-apis/understand-data/event-data/events-reported-apm": [
    {
      "sections": [
        "Default events reported by New Relic products"
      ],
      "title": "Default events reported by New Relic products",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Default events"
      ],
      "external_id": "598dfde069ba5a8bbbd5834c44b9740d6b338cdc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/understand-data/event-data/default-events-reported-new-relic-products/",
      "published_at": "2022-01-12T03:45:20Z",
      "updated_at": "2021-10-23T17:29:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic products report different types of data. One type of data reported is event data. Events are displayed in UI charts and tables, and also made available for querying. To understand the types of data available, see Data available via NRQL. Learn more about the events reported by New Relic products: APM default events Browser default events Infrastructure default events Mobile default events Synthetics default events NrAuditEvent events for understanding changes to your account",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 313.15808,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>events</em> <em>reported</em> <em>by</em> New Relic products",
        "sections": "Default <em>events</em> <em>reported</em> <em>by</em> New Relic products",
        "tags": "Default <em>events</em>",
        "body": "New Relic products <em>report</em> different types of data. One type of data <em>reported</em> is <em>event</em> data. <em>Events</em> are displayed in UI charts and tables, and also made available for querying. To understand the types of data available, see Data available via NRQL. Learn more about the <em>events</em> <em>reported</em> by New Relic"
      },
      "id": "609f8faf64441f8af9d2a1f0"
    },
    {
      "sections": [
        "Manage error data",
        "View logs for your APM and infrastructure data",
        "Error data types: events and trace details",
        "Events",
        "Trace details",
        "Caps on error reporting",
        "Charting error rates and counts",
        "Report custom errors",
        "Ignore errors",
        "Reduce noise with expected errors",
        "Disable error traces",
        "Delete error traces",
        "Caution"
      ],
      "title": "Manage error data",
      "type": "docs",
      "tags": [
        "APM",
        "APM UI pages",
        "Error analytics"
      ],
      "external_id": "29a2ebdc7b91029a1fada50791b90e9dc548f17e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apm/apm-ui-pages/error-analytics/manage-error-data/",
      "published_at": "2022-01-12T08:45:52Z",
      "updated_at": "2021-11-14T09:35:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's APM Errors page helps you identify, triage, and fix errors in your services. The Errors page uses data collected by the APM agent to display stack traces, transaction attributes such as HTTP header values, and any other custom attributes, so you can understand the context of the error and fix it. View logs for your APM and infrastructure data You can also bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. Error data types: events and trace details By default, our APM agents collect two type of error data: Events Trace details Events The error event data type includes default attributes, as well as any custom attributes instrumented in your service. It doesn't include a stack trace. Find your events data in the Errors UI as follows: The Errors column in the Error traces table. The Top 5 errors chart. When you’ve drilled into a grouping of errors, those errors not displaying a stack trace are based on this type of data. You can disable Show only errors with stack trace to show errors that have this type of data collected, but no associated trace details. Events are subject to sampling (see Caps on error reporting and Charting error rates and counts). For more on error event data, see Events reported by APM. Trace details The trace details error data type includes stack traces and attributes, and supplements events with more data. It's expected that more events will be reported than trace details--see Caps on error reporting. Find your trace details data in the Errors UI as follows: The Stack traces column of the Error traces table. When you’ve drilled into a grouping of errors, those errors with a stack trace use this type of data. Show only errors with stack trace is enabled by default, to constrain the errors shown to just those that have this type of data collected. This data is governed by specific retention rules for Error details. Caps on error reporting New Relic caps error reporting at: 100 events per minute per agent instance 20 trace details per minute per agent instance These caps prevent error reporting from negatively impacting application performance. Examples: App running across five EC2 instances, one JVM each. New Relic caps error reporting at: 100 events per minute x 5 instances = 500 events per minute 20 trace details per minute x 5 instances = 100 trace details per minute App running on one host with ten instances. New Relic caps error reporting at: 100 events per minute x 10 instances = 1000 events per minute 20 trace details per minute x 10 instances = 200 events per minute Charting error rates and counts The Error rate chart is driven by a query on metric timeslice data, which is an unsampled aggregate data type that is accurate but has very limited dimensionality. This data can't be faceted or filtered as flexibly as error event data. You can reproduce this chart in a dashboard, or explore the metric timeslice data further by clicking the ... menu on the Error rate chart, and then using the View query or Add to dashboard options. To chart faceted error counts using event data, as in the Top 5 errors chart, use an NRQL event query. Click the ... menu on the Top 5 errors chart and choose View query for a starting point in creating your chart. Since event data can be sampled (see Caps on error reporting), you can use the EXTRAPOLATE keyword to get an accurate error count, even if sampling is occurring. Report custom errors You can report errors not collected by default with our agents using our agent APIs. For more, see the documentation on the API. Ignore errors You can prevent certain errors that would normally be reported to New Relic from being collected using our agent APIs or the server-side configuration UI. For more details, see Manage errors in APM. Reduce noise with expected errors Sometimes you want to collect error data, but not have those errors wake you up through alerts. Using the agent API, you can mark such errors as “expected”. They’ll still be visible in the Errors page, but won’t affect your service’s error rate or Apdex metrics. Disable error traces To prevent certain errors from being reported to New Relic, disable them in your agent's configuration file. For most agents, you can ignore certain error codes or disable errors completely. For more information, see your specific agent's configuration documentation: C SDK Go (not applicable; the agent only reports errors when configured to do so) Java .NET Node.js PHP Python Ruby Delete error traces Caution You cannot recover error traces after you delete them. Deleting errors is currently only available in the legacy Errors Classic UI. If you want to... Do this... Delete all error traces for your app If you have permissions to delete all error traces for an app: Go to one.newrelic.com > APM > (select an app) > More views > Errors (classic). Select Delete all errors. Delete all error traces for your account To delete all error traces for your New Relic account, get support at support.newrelic.com. Delete individual error traces To delete individual error traces, use APM's Errors (classic) page. Drill into an error from the table of errors, then click Delete this error. In addition to deleting error traces, you may also want to delete transaction traces or database/slow SQL traces. This will remove potentially sensitive data while retaining your other application data (such as Apdex, deployment information, etc.).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.38696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Error data types: <em>events</em> and trace details",
        "tags": "<em>APM</em>",
        "body": " in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. Error data types: <em>events</em> and trace details By default, our <em>APM</em> agents collect two type of error data: <em>Events</em> Trace details <em>Events</em> The error <em>event</em> data type includes default attributes"
      },
      "id": "6044077e28ccbcab752c60d1"
    },
    {
      "sections": [
        "APM SLA reports",
        "Important",
        "View SLA reports",
        "View metric trends",
        "Analyze your data"
      ],
      "title": "APM SLA reports",
      "type": "docs",
      "tags": [
        "APM",
        "Reports",
        "Service level agreements"
      ],
      "external_id": "f66a2ccdfd774a7befcd9df4bd8a9d7f633e234c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apm/reports/service-level-agreements/apm-sla-reports/",
      "published_at": "2022-01-12T06:27:54Z",
      "updated_at": "2022-01-08T02:01:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "APM provides service level agreement (SLA) reports. SLA reports help you better understand your application performance by showing application downtime and trends over time. SLA reports for an application only include web transactions. If your application creates only non-web transactions, New Relic does not produce SLA reports for that app. SLA reports can be viewed in APM or downloaded as comma-separated value (.csv) files. Depending on your account level's data retention policy, you can view daily, weekly, or monthly reports. Important On top of the standard APM SLA reports described in this document, New Relic has developed an advanced service level management feature, with improved functionality. Check out our beta now! View SLA reports To view the SLA reports for your app: Go to one.newrelic.com > Applications > (select an app) > Reports > SLA. The report defaults to the Weekly SLA report tab. SLA report data shows the account owner's time zone, with periods beginning and ending at midnight in that time zone. If you have not enabled browser for your app, the SLA report shows links to requests, response time, and Apdex only for your application server. If you want to... Do this Show or hide details Select the End user tier (if available) or Application server heading. View another time period Select the tab for daily, weekly, or monthly SLA reports if available. Save or export the report Select Download this report as .csv to create a report file with comma-separated values. View metric trends To drill down into detailed information, select the link. This includes: End users (from browser): Page views, load time, and Apdex Application server (from APM agents, such as Java or Ruby): Requests, response time, and Apdex The metric detail window below the report list shows trends over the selected period (12 days, weeks, or months). Use any of New Relic's available standard page functions to drill down into detailed information. In addition: To view other details, select its link. To clear the details and return to the main SLA report, select the tab. Analyze your data APM includes several reports in the user interface. To gather, analyze, and visualize data about your software in other formats, use query builder.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.198265,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>APM</em> SLA <em>reports</em>",
        "sections": "<em>APM</em> SLA <em>reports</em>",
        "tags": "<em>APM</em>",
        "body": " values. View metric trends To drill down into detailed information, select the link. This includes: End users (from browser): Page views, load time, and Apdex Application server (from <em>APM</em> agents, such as Java or Ruby): Requests, response time, and Apdex The metric detail window below the <em>report</em> list"
      },
      "id": "603ebe4d196a679981a83dc0"
    }
  ],
  "/docs/data-apis/understand-data/event-data/events-reported-browser-monitoring": [
    {
      "sections": [
        "Build a custom New Relic One application",
        "Get started",
        "New Relic One: a programmable platform",
        "Tip"
      ],
      "title": "Build a custom New Relic One application ",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "Build on New Relic One"
      ],
      "external_id": "0fd7afcf4cd3c15157668bf349e84968062140ed",
      "image": "https://docs.newrelic.com/static/2caff7bdf3bb0fb46bee7c214448c921/c1b63/new-relic-one-browser-analyzer-example-application_0.png",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/build-new-relic-one/build-custom-new-relic-one-application/",
      "published_at": "2022-01-12T03:04:38Z",
      "updated_at": "2021-07-27T13:37:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic gives you a framework to build your own React JavaScript applications that: Reside on the New Relic One platform, alongside your other dashboards and data. Feature visualizations that you've tailored specifically for your organization. Display data from any source you want, whether from a New Relic-monitored entity or data from another service or API. Get started Keep reading to learn more about what you can do with New Relic One apps. If you want to get started building quickly, first read the requirements. New Relic One: a programmable platform We strive to have an automated user experience that provides optimal value for all users. But we also know that some organizations have unique business needs that can’t be met with our standard visualization options. Now, we give you control over the fundamental building blocks of our platform. Using the same tools our engineers use to build New Relic One, you can build custom applications that align with your unique organizational structure and business needs. If you know how to use React, GraphQL, and NRQL (our query language), building an application will take you only a few minutes. Check out these guides for help building custom applications. Solve any data-driven challenge, no matter how complex. You can: Use our APIs to get data into New Relic from any source. Visualize that data in your custom applications. one.newrelic.com: Here’s an example of a custom application built on New Relic One. This application gives a highly detailed analysis of a website, using the PageView events reported from New Relic's browser monitoring. Tip If your visualization needs are relatively simple, consider using custom charts and custom dashboards. Now, visit our developer site and start building!",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 394.40915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " a highly detailed analysis of a website, using the PageView <em>events</em> <em>reported</em> from New Relic&#x27;s <em>browser</em> <em>monitoring</em>. Tip If your visualization needs are relatively simple, consider using custom charts and custom dashboards. Now, visit our developer site and start building!"
      },
      "id": "603eaaa6e7b9d251572a07d0"
    },
    {
      "sections": [
        "Overview of data retention (original pricing model)",
        "Important",
        "Limits on editing and deleting data",
        "Product and API data retention policies",
        "APM",
        "APM data retention policies",
        "Legacy APM data retention policies",
        "Browser",
        "Infrastructure",
        "Tip",
        "Infrastructure downsampling and data retention",
        "Integration compute unit event limits",
        "Insights",
        "Logs",
        "Logs in context data retention",
        "Mobile",
        "Mobile data retention policies",
        "Standard Mobile (legacy) data retention policies",
        "Synthetics",
        "Synthetics data retention policies",
        "Metric API",
        "Trace API",
        "Data components",
        "Event data: reported by most products",
        "Metric timeslice data: reported by APM, Browser, and Mobile",
        "Aggregate metric timeslice data: reported by APM, Browser, and Mobile",
        "Key metrics: reported by APM, Browser, and Mobile",
        "Trace data: reported by APM, Browser, and Mobile"
      ],
      "title": "Overview of data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "22d5ab9f4d623ead28ee7bb82c118d91804dee22",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/overview-data-retention-components/",
      "published_at": "2022-01-12T07:48:35Z",
      "updated_at": "2022-01-12T07:48:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original Product-based pricing. If you're on our New Relic One pricing model, see Manage your data. Not sure which you're on? See Overview of pricing models. If you're on the original product-based pricing model, you retain your existing subscriptions and data retention values. You manage these existing retention settings from the Data management hub in New Relic One. To manage your retention settings, go to the Data management UI. You'll see your existing retention settings. Adjust retention values by clicking Edit retention. New Relic stores different types of data for different periods of time. The retention period for a type of data will vary depending on the product, the subscription level, and the feature. Limits on editing and deleting data Once telemetry data (events, metrics, logs, traces) is reported to New Relic and available for querying, that data cannot be edited or deleted. This is a purposeful design decision that optimizes New Relic's speed and performance. Data will expire after its data retention ends. If you sent unwanted data or sensitive data to New Relic that must be removed, contact your account representative at New Relic, or get support at support.newrelic.com. Product and API data retention policies Select a New Relic product to see details about its data retention: APM Specific retention policies apply to each APM subscription level, including Lite, Essentials, Pro, and Enterprise. This includes metric timeslice data, key metrics, trace data, and event data. In addition to retention limits, your data is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate metric data description. APM data retention policies For accounts on our original product-based pricing, APM data retention policies depend on your APM product subscription level. Component Lite Essentials Pro Metric timeslice data 24 hours 3 days 90 days Key metrics none forever forever Distributed tracing and logs in context none none 8 days Other trace data * 1 day 3 days 7 days Event data * * 1 day 3 days 8 days * If you view a transaction trace in the New Relic UI, New Relic stores that trace data for up to one year. * * Learn about how to extend the retention of event data. Legacy APM data retention policies Component Standard Startup Small Business Enterprise Metric timeslice data 7 days 14 days 30 days 90 days Key metrics none none none forever Trace data 7 days 7 days 7 days 7 days Event data none 8 days 8 days 8 days Browser For accounts on our original product-based pricing, Browser data is stored depending on your subscription level: Component Lite Pro Metric data 24 hours 90 days Key metrics 24 hours forever Trace data 7 days 7 days Event data * 1 day 8 days SPA data Unavailable 8 days * Learn about how to extend the retention of event data. Infrastructure For accounts on our original product-based pricing, Infrastructure data retention policies depend on your Infrastructure subscription level and your infrastructure compute units pricing model. Data retention rules apply the same whether that data is displayed in the UI or queried. Tip Infrastructure data retention is not governed by your Insights subscription. Infrastructure downsampling and data retention Types of data are stored depending on your subscription level: Component Essentials Pro Infrastructure data 3 months 13 months Host count Host count will stop reflecting a host whose agent is no longer reporting after three minutes. However, host data will be available, subject to other retention criteria. 3 minutes 3 minutes Inventory attributes removed Inventory attributes for a host are retained for 24 hours after the agent stops reporting. 24 hours 24 hours Integration data Not available with Essentials Limit of 2275 integration events per compute unit per month In addition, Infrastructure downsamples your data on the fly, as it's generated. All Infrastructure metric data types (including On-Host Integrations metrics) will display different granularity depending on the age of the data and the size of the time window. The following table illustrates when different downsampled buckets will be used, both in the Infrastructure UI and for queries: Bucket Size Used For Time Windows Covering... Data Retained For... Raw (5, 10 or 15 second) 0 to 59 minutes 7 days 1 minute 60 minutes to 6 hours 30 days 10 minutes 6 hours to 3 days Full account retention period 1 hour 3 days to 14 days Full account retention period 3 hours 14+ days Full account retention period Integration compute unit event limits The 2275 limit on integration events per compute unit per month is a limit on total Infrastructure integration events. It's not a limit of 2275 for each integration's events. Additional details and clarifications about this limit: This limit applies to all events from all integrations (cloud integrations and on-host integrations). The events are all handled the same. Default data received by the Infrastructure agent does not count against the 2275 event per compute unit limit. If you exceed your limit, we do not enforce this limit. If you exceed your limit, we'll review pricing options with you to ensure you get the most cost-effective pricing for your organization's integrations. Insights For accounts on our original product-based pricing, an Insights subscription extends your event data retention. An Insights Pro subscription allows you to customize the length of your event data retention. Logs For accounts on our original product-based pricing, log data can be retained for up to 30 days by New Relic. Shorter retention periods of 8 or 15 days are also available. Logs in context data retention New Relic Logs logs in context data retention policy is based on your current APM product subscription level. For more information, see APM data retention. Mobile For accounts on our original product-based pricing, Mobile data retention policies depend on your New Relic Mobile product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and event data. For information about your subscription-related data usage, see Mobile subscription usage. Mobile data retention policies Component Lite Enterprise Overview page data 24 hours 90 days Crash data and stack traces 24 hours 90 days HTTP requests (except errors) as metric data Unavailable 90 days HTTP request errors as metric data Unavailable 3 days Interaction traces Unavailable 90 days Custom events * Unavailable 1 day Mobile events * 1 day 8 days MobileBreadcrumb events * Unavailable 3 days MobileCrash events * 1 day 90 days MobileHandledException events * Unavailable 3 days MobileJSError events (React Native beta) * Unavailable 3 days MobileRequest events * Unavailable 3 days MobileRequestError events * Unavailable 3 days MobileSession events * 1 day 90 days * Learn how to extend retention of event data. Standard Mobile (legacy) data retention policies Unless otherwise noted, Insights event data is unavailable for Standard subscriptions. Component Standard Overview page data 7 days Crash data and stack traces 7 days HTTP requests (except errors) as metric data 7 days HTTP request errors as metric data 3 days Interaction traces Unavailable MobileCrash events 8 days MobileSession events 8 days Synthetics For accounts on our original product-based pricing, Synthetics data retention policies depend on your Synthetics product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and Insights events. Tip Synthetics events do not count against an Insights Pro subscription. Your current subscription level appears in the right column of the Account summary page: Go to synthetics.newrelic.com > (account dropdown) > Account settings > Account > Summary. Synthetics data retention policies Component Lite Pro Synthetics monitor data 2 weeks 13 months Event data 2 weeks 13 months Also see the data retention details for APIs, including: Metric API All raw metric data points will be retained for 30 days. All additional aggregated data derived from the raw metric data points (for example, one-minute rollups) will be retained for 13 months. Any change to the retention period beyond such periods may result in a charge to you. Trace API See Trace API general requirements and limits. Data components For accounts on our original product-based pricing, the section below provides an explanation of some types of data components governed by the data retention rules of specific products: Event data: reported by most products See Event data retention for information on the event data type. Metric timeslice data: reported by APM, Browser, and Mobile Metric timeslice data is a specific type of data that is used for most metric charts and reports in APM, mobile, and browser. Important Note that metric timeslice data differs from other metric data types. All metric timeslice data is aggregated, but New Relic deals with fresh data and old data in different ways. Fresh data has specific policies applied to the data to keep granular views of performance as aggregate metrics. As data ages and becomes less useful in a granular state, we summarize that data and only keep key metrics. Aggregate metric timeslice data: reported by APM, Browser, and Mobile Aggregate metric timeslice data summarizes calls to specific methods in your application: how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on your subscription level. Subscription Level Aggregate retention Enterprise Aggregates (averages) to 1-hour periods after 8 days. After 90 days, the permanent metric data continues to be stored in 1-hour periods. This means you cannot obtain data granularity of less than 1 hour after 8 days, and only a subset of metrics are available after 90 days. Pro After 8 days Essentials After 3 days. Legacy Small Business, Startup, and Standard After 3 days. Lite After 2 hours. We retain your most recent data in one-minute increments. We also aggregate permanent metric data to day-size increments after 90 days. When looking at older data in small time windows, you may notice that charts show less detail. As data ages, it is aggregated into larger segments. Key metrics: reported by APM, Browser, and Mobile New Relic retains certain \"key\" metric timeslice data aggregations forever for Enterprise and Pro customers, for any number of applications. New Relic aggregates permanent key metric data to hour-size increments after 90 days. Product Key metrics APM Apdex, for app server Throughput and page views Response time, plus breakdown into tiers or categories on your app's main Overview chart Page load time, plus breakdown into segments Error rate CPU usage Memory usage Browser Apdex, for browser Browser page load time Throughput total. After 90 days, there is no breakout by browser type, and only the combined average value is available for the time range requested. Mobile Crash rate: For iOS and for Android Network throughput Network response time Network error rates Interaction traces Trace data: reported by APM, Browser, and Mobile Depending on the product, New Relic retains different types of trace data: Product Trace data APM Types of trace data: Transaction traces Distributed tracing Slow query samples Error details See APM data retention details. Browser Types of trace data: Session traces Browser traces JavaScript errors* See Browser data retention details. Mobile Types of trace data: App crash stack traces Interaction traces See Mobile data retention details. * JavaScript errors in the stack trace UI are saved as trace data. JS errors are also saved as events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 90.8625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Event</em> data: <em>reported</em> <em>by</em> most products",
        "body": " on the <em>event</em> data type. Metric timeslice data: <em>reported</em> by APM, <em>Browser</em>, and Mobile Metric timeslice data is a specific type of data that is used for most metric charts and reports in APM, mobile, and <em>browser</em>. Important Note that metric timeslice data differs from other metric data types. All metric"
      },
      "id": "6043f75364441f6967378ec6"
    },
    {
      "sections": [
        "Python agent configuration",
        "Configuration methods and precedence",
        "Agent configuration file",
        "Tip",
        "Server-side configuration",
        "Important",
        "Environment variables",
        "Per-request configuration",
        "Example: Apache/mod_wsgi app name",
        "newrelic.set_background_task",
        "newrelic.ignore_transaction",
        "newrelic.suppress_apdex_metric",
        "newrelic.suppress_transaction_trace",
        "newrelic.disable_browser_autorum",
        "Multiple environment configuration",
        "General configuration settings",
        "license_key (REQUIRED)",
        "app_name (HIGHLY RECOMMENDED)",
        "monitor_mode",
        "developer_mode",
        "log_file",
        "log_level",
        "high_security",
        "proxy_scheme, proxy_host, proxy_port, proxy_user, proxy_pass",
        "audit_log_file",
        "Caution",
        "labels (tags)",
        "Two tags",
        "process_host.display_name",
        "api_key",
        "ca_bundle_path",
        "apdex_t",
        "Attributes",
        "attributes.enabled",
        "attributes.include",
        "attributes.exclude",
        "Transaction tracer configuration",
        "transaction_tracer.enabled",
        "transaction_tracer.transaction_threshold",
        "transaction_tracer.record_sql",
        "transaction_tracer.stack_trace_threshold",
        "transaction_tracer.explain_enabled",
        "transaction_tracer.explain_threshold",
        "transaction_tracer.attributes.enabled",
        "transaction_tracer.attributes.include",
        "transaction_tracer.attributes.exclude",
        "transaction_tracer.function_trace",
        "Transaction segment configuration",
        "transaction_segments.attributes.enabled",
        "transaction_segments.attributes.include",
        "transaction_segments.attributes.exclude",
        "Error collector configuration",
        "error_collector.enabled",
        "error_collector.ignore_classes",
        "error_collector.ignore_status_codes",
        "error_collector.expected_classes",
        "error_collector.expected_status_codes",
        "error_collector.attributes.enabled",
        "error_collector.attributes.include",
        "error_collector.attributes.exclude",
        "error_collector.capture_events",
        "Browser monitoring settings",
        "browser_monitoring.enabled",
        "browser_monitoring.auto_instrument",
        "browser_monitoring.content_type",
        "Instrument xhtml+xml page responses",
        "browser_monitoring.attributes.enabled",
        "browser_monitoring.attributes.include",
        "browser_monitoring.attributes.exclude",
        "Transaction events settings",
        "transaction_events.enabled",
        "transaction_events.attributes.enabled",
        "transaction_events.attributes.include",
        "transaction_events.attributes.exclude",
        "Custom events settings",
        "custom_insights_events.enabled",
        "Datastore tracer settings",
        "datastore_tracer.instance_reporting.enabled",
        "datastore_tracer.database_name_reporting.enabled",
        "Distributed tracing settings",
        "distributed_tracing.enabled",
        "Span event configuration",
        "span_events.enabled",
        "span_events.attributes.enabled",
        "span_events.attributes.include",
        "span_events.attributes.exclude",
        "Event harvest configuration",
        "Usage example",
        "event_harvest_config.harvest_limits.analytic_event_data",
        "event_harvest_config.harvest_limits.custom_event_data",
        "event_harvest_config.harvest_limits.span_event_data",
        "event_harvest_config.harvest_limits.error_event_data",
        "Event loop visibility settings",
        "event_loop_visibility.enabled",
        "event_loop_visibility.blocking_threshold",
        "Garbage collection runtime metrics settings",
        "gc_runtime_metrics.enabled",
        "gc_runtime_metrics.top_object_count_limit",
        "Other configuration settings",
        "utilization.detect_aws",
        "utilization.detect_azure",
        "utilization.detect_gcp",
        "utilization.detect_pcf",
        "utilization.detect_docker",
        "slow_sql.enabled",
        "thread_profiler.enabled",
        "cross_application_tracer.enabled",
        "strip_exception_messages.enabled",
        "strip_exception_messages.whitelist",
        "startup_timeout",
        "shutdown_timeout",
        "compressed_content_encoding",
        "Heroku",
        "heroku.use_dyno_names",
        "heroku.dyno_name_prefixes_to_shorten",
        "Built-in instrumentation",
        "Example: Disabling MySQLdb database query instrumentation"
      ],
      "title": "Python agent configuration",
      "type": "docs",
      "tags": [
        "Agents",
        "Python agent",
        "Configuration"
      ],
      "external_id": "923b7c4b9b48b55402bc7793ef3e12b0bdcfa8dc",
      "image": "https://docs.newrelic.com/static/0bbb623699fa652795cba242b01caa6b/8c557/diagram-python-config-precedence_0.png",
      "url": "https://docs.newrelic.com/docs/apm/agents/python-agent/configuration/python-agent-configuration/",
      "published_at": "2022-01-12T11:33:43Z",
      "updated_at": "2022-01-12T11:33:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Python agent lets you change the default agent behavior agent using configuration options. The only required Python agent configuration setting is the license key. The license key identifies the account where the agent reports application data. Depending on how you are hosting your application, the license key can be provided via a configuration file or an environment variable. Configuration methods and precedence The primary way to configure the Python agent is via the configuration file, which is generated as part of the standard install process. It is also possible to set a limited number of configuration options using server-side configuration in the UI or by using environment variables. You can also specify some settings on a per-request basis by passing settings with the WSGI request environ dictionary. The Python agent follows this order of precedence for configuration: With the Python agent, per-request options override server-side config. If enabled, server-side config overrides all corresponding values in the agent config file, even if the server-side values are left blank. The agent config file overrides environment variables. Environment variables override the agent defaults. Here are detailed descriptions of each configuration method: Agent configuration file Typically you configure your Python agent from a local configuration file on the agent's host system. Supply the path to the config file at startup using one of these methods: When you call newrelic.agent.initialize(), provide the path to the config file as the first argument. OR Set the NEW_RELIC_CONFIG_FILE environment variable. If you use the newrelic-admin wrapper script, you must use the environment variable because the wrapper script calls the agent automatically. The configuration file uses a structure similar to Microsoft Windows .ini files. For more information, see the Python ConfigParser module's file format documentation. Tip A sample configuration file is included with the Python agent as newrelic/newrelic.ini. You can also generate one from the newrelic-admin script using the generate-config command, or download a copy from our download repo. Server-side configuration Server-side configuration allows you to configure certain settings in the New Relic One UI. This applies your changes automatically to all agents even if they run across multiple hosts. Where available, this document includes the UI labels for server-side config under individual config options as the Server-side label. Important If server-side config is enabled, the agent ignores any value in the config file that could be set in the UI. Even if the UI value is empty, the agent treats this as an empty string and does not use the agent config file. Environment variables Environment variables allow you to override the defaults for certain core settings. If the equivalent setting is explicitly listed in the agent config file, the config file settings take precedence over the environment variable. Where available, environment variables are documented below under individual config options as the Environ variable. For simple configurations, you can use the environment variables in conjunction with server-side configuration and avoid the agent config file altogether. This is the default setup with Heroku, where installing the New Relic add-on automatically populates the necessary environment variables. If you're using New Relic APM and CodeStream, see how to associate repositories and how to associate build SHAs or release tags with errors inbox. Environment variable Configuration setting NEW_RELIC_LICENSE_KEY license_key NEW_RELIC_APP_NAME app_name NEW_RELIC_MONITOR_MODE monitor_mode NEW_RELIC_DEVELOPER_MODE developer_mode NEW_RELIC_LOG log_file NEW_RELIC_LOG_LEVEL log_level NEW_RELIC_HIGH_SECURITY high_security NEW_RELIC_PROXY_SCHEME proxy_scheme NEW_RELIC_PROXY_HOST proxy_host NEW_RELIC_PROXY_PORT proxy_port NEW_RELIC_PROXY_USER proxy_user NEW_RELIC_PROXY_PASS proxy_pass NEW_RELIC_AUDIT_LOG audit_log_file NEW_RELIC_STARTUP_TIMEOUT startup_timeout NEW_RELIC_SHUTDOWN_TIMEOUT shutdown_timeout NEW_RELIC_LABELS labels NEW_RELIC_PROCESS_HOST_DISPLAY_NAME process_host.display_name NEW_RELIC_API_KEY api_key NEW_RELIC_CA_BUNDLE_PATH ca_bundle_path NEW_RELIC_DISTRIBUTED_TRACING_ENABLED distributed_tracing.enabled NEW_RELIC_ANALYTICS_EVENTS_MAX_SAMPLES_STORED event_harvest_config.harvest_limits.analytic_event_data NEW_RELIC_CUSTOM_INSIGHTS_EVENTS_MAX_SAMPLES_STORED event_harvest_config.harvest_limits.custom_event_data NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED event_harvest_config.harvest_limits.span_event_data NEW_RELIC_ERROR_COLLECTOR_MAX_EVENT_SAMPLES_STORED event_harvest_config.harvest_limits.error_event_data NEW_RELIC_FEATURE_FLAG feature_flag Per-request configuration For certain WSGI servers, you can override the app name and capture attributes settings on a per-request basis. This is possible with WSGI servers where you can define additional key/value pairs that are passed into the per-request WSGI environ dictionary. Set these values with the strings on, off, true, false, 1 and 0. If set from a configuration mechanism implemented using Python code, Python objects evaluating to True or False will also be accepted. Example: Apache/mod_wsgi app name In the Apache/mod_wsgi server, you can use the SetEnv directive to override config settings (optionally inside a Location or Directory block). For example, you could override the app name for a complete virtual host, or for a subset of URLs handled by the WSGI application for that virtual host. In addition to being able to override certain agent configuration settings, you can set other per-request configuration settings with their WSGI environment key: newrelic.set_background_task If set to true, this web transaction will instead be reported as a non-web transaction. newrelic.ignore_transaction If set to true, this web transaction will not be reported. newrelic.suppress_apdex_metric If set to true, no Apdex metric will be generated for this web transaction. newrelic.suppress_transaction_trace If set to true, this web transaction cannot be recorded in a transaction trace. newrelic.disable_browser_autorum If set to true, this disables automatic insertion of the JavaScript header/footer for page load timing (sometimes referred to as real user monitoring or RUM). Only applicable if auto-insertion is available for your web framework. Important Using a WSGI middleware to set these values will not work where the Python agent's own WSGI application wrapper was applied at an outer scope. In these cases you must make calls to the agent API to achieve the same outcome. Multiple environment configuration The agent reads its primary configuration from an agent config section called newrelic. You can provide overrides for specific deployment environments (for example, Development, Staging, Production) in additional sections. Preface these sections with [newrelic:environment], where environment is replaced with the name of your environment. To specify that the agent should use an environment-based configuration, use one of these methods: When you call newrelic.agent.initialize(), provide the environment name as the second argument. OR Set the NEW_RELIC_ENVIRONMENT environment variable to the environment name. If no environment is specified, the agent will use the default settings as specified in the newrelic agent config section. The basic structure of the configuration file is: [newrelic] ... default settings [newrelic:development] ... override settings [newrelic:staging] ... override settings [newrelic:production] ... override settings Copy General configuration settings These settings are available in the agent configuration file. license_key (REQUIRED) Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_LICENSE_KEY Specifies the license key of your New Relic account. This key associates your app's metrics with your New Relic account. app_name (HIGHLY RECOMMENDED) Type String Default Python Application Set in Per-request option, config file, environment variable Per-request option newrelic.app_name Environ variable NEW_RELIC_APP_NAME The application name used to aggregate data in the New Relic One UI. To report data to multiple apps at the same time, specify a list of names separated with a semicolon ;. Do not put a space before the semicolon, which causes the Python config parser to interpret the name as an embedded comment. monitor_mode Type Boolean Default true Set in Config file, environment variable Environ variable NEW_RELIC_MONITOR_MODE When true, the agent collects performance data about your app and reports this data to our data collector. developer_mode Type Boolean Default false Set in Config file, environment variable Environ variable NEW_RELIC_DEVELOPER_MODE When true, the agent will instrument your web app, but will not send any actual data. In this offline mode, you will not be billed for an active agent. Use developer mode to test new versions of the agent, or test the agent against third-party packages in a developer environment. Offline mode is not a way of running the APM locally, because the metrics the agent collects are not reported anywhere. log_file Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_LOG Sets the name of a log file, which is useful for debugging issues with the agent. This is not set by default, since the agent does not know your web app process's parent user or what directories that process has permission to write to. For detailed information, see Python agent logging. Whatever you set this to, ensure the permissions for the containing directory and the file itself are correct, and that the user that your web application runs as can write to the file. Tip Use an absolute path unless you are sure what the working directory of your application will be at startup. If you can't write out a log file, you can also use stderr and output to standard error output. This would normally result in output appearing in your web server log. log_level Type String Default info Set in Config file, environment variable Environ variable NEW_RELIC_LOG_LEVEL Sets the level of detail of log messages, if you've set the log file location. This log_level will not affect the Python logging module log level. Possible values, in increasing order of detail, are critical, error, warning, info, and debug. To report agent issues, the most useful setting is debug. However, debug generates a lot of information very quickly, so do not keep the agent at this level for longer than it takes to reproduce your problem. high_security Type Boolean Default false Set in Config file, environment variable Environ variable NEW_RELIC_HIGH_SECURITY High security mode enforces certain security settings and prevents them from being overridden, so that no sensitive data is sent to us. Enabling high security mode means that request parameters are not collected, and you cannot send raw SQL. To activate high security mode, set it to true in the local .ini configuration file and activate it in the Account Settings in the UI. For more information, see High security. proxy_scheme, proxy_host, proxy_port, proxy_user, proxy_pass Type Strings Default (none) Set in Config file, environment variable Environ variables NEW_RELIC_PROXY_SCHEME NEW_RELIC_PROXY_HOST NEW_RELIC_PROXY_PORT NEW_RELIC_PROXY_USER NEW_RELIC_PROXY_PASS By default, the Python agent attempts to directly connect to our servers. If there is a firewall between your host and the our collector that requires you to use an HTTP proxy, set proxy_host and proxy_port to the required values for your HTTP proxy. If proxy authentication is implemented by the HTTP proxy, also set proxy_user and proxy_pass. The proxy_scheme setting dictates what protocol scheme is used to talk to the HTTP proxy. When set to http, the agent uses a SSL tunnel through the HTTP proxy for end-to-end encryption. Instead of setting the proxy_scheme, proxy_host and proxy_port settings, you can also set the proxy_host setting to a valid URI for the proxy. Include the scheme, host, and port; for example, http://proxy-host:8000. This also works if you set the details of the HTTP proxy with the NEW_RELIC_PROXY_HOST environment variable. Tip Python agent versions 2.0.0 or earlier do not provide the proxy_scheme setting, and the protocol scheme defaults to http or https depending whether ssl is disabled or enabled. If you are upgrading from an older agent version and your config file doesn't include proxy_scheme, ensure you add the setting and set it appropriately. If you don't, the agent will continue to base the protocol scheme on the ssl setting for backwards compatibility. As proxies are usually only configured to accept proxy requests via the http protocol scheme, not setting proxy_scheme may result in a failure. audit_log_file Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_AUDIT_LOG Sets the name of the audit log file. If set, the agent logs details of messages passed back and forth between the monitored process and the collector. This allows you to evaluate the security of the Python agent. Use an absolute path unless you are sure what your app's working directory will be at startup. Whatever you set this to, ensure the permissions for the containing directory and the file itself are correct. Also ensure your web app's parent user can write to the file. Caution Do not use audit logging on an ongoing basis, especially in a production environment. Because the agent does not truncate or rotate the log file, the log file can grow very quickly. labels (tags) Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_LABELS Adds tags. Specify name:value separated by a colon :, and separate additional tags with semicolons ;. Two tags Server:One;Data Center:Primary Copy process_host.display_name Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_PROCESS_HOST_DISPLAY_NAME Sets the hostname to be displayed in the APM UI. If set, this overrides the default hostname that the agent captures automatically. api_key Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_API_KEY Sets the api_key to be used with newrelic-admin record-deploy. ca_bundle_path Type String Default (none) Set in Config file, environment variable Environ variable NEW_RELIC_CA_BUNDLE_PATH Manual override for the path to your local CA bundle. This CA bundle will be used to validate the SSL certificate presented by our data collection service. Tip This configuration option is only available in Python agent versions 4.2.0 and newer. apdex_t Type Float Default 0.5 Set in Config file, environment variable Environ variable NEW_RELIC_APDEX_T We'll record transaction traces when they exceed this threshold. The format is a number of seconds (decimal points allowed). See our glossary entry for apdex_t Attributes Attributes are key-value pairs that provide information for transaction traces, traced errors, browser monitoring, and transaction events. In addition to configuring attributes for all four destinations with the general attribute settings below, they can also be configured on a per-destination basis. For more information, see Python agent attributes, Enabling and disabling attributes, and Attribute examples. attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes. attributes.include Type List of Strings Default (none) Set in Config file If attributes are enabled, attribute keys found in this list will be sent to us. Keys in the list should be space-separated as shown below: key1 key2 key3 Copy Rules for attributes can be found on the agent attributes page. attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent to us. Keys in the list should be space-separated as shown below: key1 key2 key3 Copy Rules for attributes can be found on the agent attributes page. Transaction tracer configuration Important Do not use brackets [suffix] at the end of your transaction name. The agent automatically strips brackets from the name. Instead, use parentheses (suffix) or other symbols if needed. For more information about transaction traces, see Transaction traces. transaction_tracer.enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable transaction tracing? If enabled, the transaction tracer captures deep information about slow transactions. transaction_tracer.transaction_threshold Type Positive float or string (apdex_f) Default apdex_f Set in Server-side config, config file Server-side label Threshold Threshold in seconds for when to collect a transaction trace. When the response time of a controller action exceeds this threshold, the agent records a transaction trace. Valid values are any positive float, or apdex_f (four times apdex_t). transaction_tracer.record_sql Type String Default obfuscated Set in Server-side config, config file Server-side label Record SQL? When the transaction tracer is enabled, the agent can record SQL statements. The recorder has three modes: off (sends no SQL), raw (sends the SQL statement in its original form), and obfuscated (strips out numeric and string literals). Most web frameworks (including Django) parameterize SQL queries so they do not actually contain the values used to fill out the query. If you use raw mode with one of these frameworks, the Python agent will only see the SQL prior to insertion of values. The parametrized SQL will look much like obfuscated mode. transaction_tracer.stack_trace_threshold Type Float Default 0.5 Set in Server-side config, config file Server-side label Stack trace threshold Threshold in seconds for when to collect stack traces from SQL calls. When SQL statements exceed this threshold, the agent captures the current stack trace. This is helpful for pinpointing where long SQL calls originate in an application. transaction_tracer.explain_enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable SQL query plans? Determines whether the Python agent will capture query plans for slow SQL queries. Only supported in MySQL and PostgreSQL. transaction_tracer.explain_threshold Type Float Default 0.5 Set in Server-side config, config file Server-side label Query plan threshold Queries in transaction traces that exceed this threshold will report slow query data and any available explain plans. Explain plan collection will not happen if transaction_tracer.explain_enabled is false. transaction_tracer.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes for transaction traces. If attributes.enabled at the root level is false, no attributes will be sent to transaction traces regardless on how this configuration setting (transaction_tracer.attributes.enabled) is set. transaction_tracer.attributes.include Type List of strings Default (none) Set in Config file If attributes are enabled for transaction traces, all attribute keys found in this list will be sent to us in transaction traces. For more information, see the agent attribute rules. transaction_tracer.attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent in transaction traces. For more information, see the agent attribute rules. transaction_tracer.function_trace Type String Default (none) Set in Config file For the specified functions or methods, the agent will capture additional function timing instrumentation. Specify these names in the form module:function or module:class.function. Wildcarding (globbing) for function and class names is possible using patterns supported by the fnmatch module. Module paths are not supported by wildcards. Specify the patterns in the form module:function* or module:class.*. For example, if you want to add function tracing to all validation functions in the below file: my-app/common/utils.py def validate_credentials(): … def validate_status(): … def format_message(): … Copy Add the following line to the agent config file to include function tracing to all validation functions in my-app/common/utils.py by using wildcarding. my-app/newrelic.ini [newrelic] ... transaction_tracer.function_trace = common.utils:validate* Copy Important Wilcarding requires Python agent version 6.4.4.161 or higher. Transaction segment configuration Here are Transaction segment settings available via the agent configuration file. transaction_segments.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes for segments of transaction traces. If attributes.enabled at the root level is false, no attributes will be sent to segments of transaction traces regardless on how this configuration setting (transaction_segments.attributes.enabled) is set. transaction_segments.attributes.include Type List of strings Default (none) Set in Config file If attributes are enabled for segments of transaction traces, all attribute keys found in this list will be sent in segments of transaction traces. For more information, see the agent attribute rules. transaction_segments.attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent in segments of transaction traces. For more information, see the agent attribute rules. Error collector configuration Here are error collector settings available via the agent configuration file. Tip For an overview of error configuration in APM, see Manage errors in APM. error_collector.enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable error collection? If enabled, the error collector captures information about uncaught exceptions. error_collector.ignore_classes Type String Default (none) Set in Server-side config, config file Server-side label Ignore these errors To stop collecting specific errors, set this to a space-separated list of the Python exception type names to ignore. Use the form module:class for the exception name. Tip Before version 6.4.0 of the agent, this setting was named error_collector.ignore_errors. If your configuration file still uses ignore_errors, update your agent to use ignore_classes. error_collector.ignore_status_codes Type String Default 100-102 200-208 226 300-308 404 Set in Server-side config, config file Server-side label Ignore these status codes Lists HTTP status codes which the agent should ignore rather than record as errors. List additional status codes as integers separated by spaces, and specify ranges with a hyphen - separator between the start and end values. To add one of the default codes to your allow list, preface the code with an exclamation point !. This setting is only compatible with some web frameworks, as some frameworks do not use exceptions to return HTTP responses. Tip This configuration option can only be set in server-side configuration in Python agent versions 6.4.0 and newer. error_collector.expected_classes Type String Default (none) Set in Server-side config, config file Server-side label Expect these error class names Prevents specified exception classes from affecting error rate or Apdex score while still reporting the errors to APM. Set this to a space-separated list of the Python exception type names to be expected. Use the form module:class for the exception name. Tip This configuration option is only available in Python agent versions 6.4.0 and newer. error_collector.expected_status_codes Type String Default (none) Set in Server-side config, config file Server-side label Expect these status codes Prevents specified HTTP status codes from affecting error rate or Apdex score while still reporting the errors to APM. List status codes as integers separated by spaces and specify ranges with a hyphen - separator between the start and end values. To negate one of the codes in your list, preface the code with an exclamation point !. This setting is only compatible with some web frameworks, as some frameworks do not use exceptions to return HTTP responses. Tip This configuration option is only available in Python Agent versions 6.4.0 and newer. error_collector.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes for traced errors. If attributes.enabled is false at the root level, then no attributes will be sent to traced errors regardless on how this configuration setting (error_collector.attributes.enabled) is set. error_collector.attributes.include Type List of strings Default (none) Set in Config file If attributes are enabled for traced errors, all attribute keys found in this list will be sent to in traced errors. For more information, see the agent attribute rules. error_collector.attributes.exclude Type List of strings Default (none) Set in Config file Attribute keys found in this list will not be sent to in traced errors. For more information, see the agent attribute rules. error_collector.capture_events Type Boolean Default true Set in Config file If enabled, the error collector captures event data for advanced analytics. For more information, see APM Errors. Browser monitoring settings Here are browser monitoring settings available via the agent configuration file. browser_monitoring.enabled Type Boolean Default true Set in Config file Enables browser monitoring. For more information, see Page load timing in Python. Important Before enabling browser monitoring in the config file, enable it in the application settings in the browser monitoring UI. browser_monitoring.auto_instrument Type Boolean Default true Set in Config file For supported Python web frameworks, this setting enables auto-insertion of the browser monitoring JavaScript fragments. browser_monitoring.content_type Type String Default text/html Set in Config file Specify the HTML Content-Type(s) that our browser monitoring agent should auto-instrument. Add additional entries in a space-separated list. Instrument xhtml+xml page responses If you are generating HTML page responses and using the Content-Type of application/xhtml+xml, you can override the allowed content types to list both this content type and the default text/html by using: browser_monitoring.content_type = text/html application/xhtml+xml Copy Important The browser monitoring JavaScript snippet prevents the page from validating as application/xhtml+xml, although the page should load and render in end-user browsers. browser_monitoring.attributes.enabled Type Boolean Default false Set in Config file This setting can be used to turn on or off all attributes for browser monitoring. This is the data which gets sent to page view events. If attributes.enabled is false at the root level, no attributes will be sent up in browser monitoring regardless on how the configuration setting (browser_monitoring.attributes.enabled) is set. browser_monitoring.attributes.include Type List of Strings Default (none) Set in Config file If attributes are enabled for browser_monitoring, all attribute keys found in this list will be sent in page views. For more information, see the agent attribute rules. browser_monitoring.attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent in page views. For more information, see the agent attribute rules. Transaction events settings Here are Transaction events settings available via the agent configuration file. Tip These configuration settings used to be called analytic_events. If your configuration file still uses analytic_events, update your agent to use transaction_events. transaction_events.enabled Type Boolean Default true Set in Config file Transaction event data allows the use of additional information such as histograms and percentiles. transaction_events.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off all attributes for transaction events. If attributes.enabled is false at the root level, then no attributes will be sent to transaction events regardless on how this configuration setting (transaction_events.attributes.enabled) is set. transaction_events.attributes.include Type List of Strings Default (none) Set in Config file If attributes are enabled for transaction events, all attribute keys found in this list will be sent in transaction events. For more information, see the agent attribute rules. transaction_events.attributes.exclude Type List of Strings Default (none) All attribute keys found in this list will not be sent to in transaction events. Note that excluding attributes from transaction events does not exclude from span events. For more information, see the agent attribute rules. Custom events settings Here are custom events settings available via the agent configuration file. custom_insights_events.enabled Type Boolean Default true Set in Config file Allow recording of events to the Insights custom events API via record_custom_event(). Datastore tracer settings These datastore tracer settings are available via the agent configuration file: datastore_tracer.instance_reporting.enabled Type Boolean Default true Set in Config file When enabled, the agent collects datastore instance metrics (such as host and port) for some database drivers. These are also reported on slow query traces and transaction traces. datastore_tracer.database_name_reporting.enabled Type Boolean Default true Set in Config file When enabled, the agent collects database name for some database drivers. The database name is reported on slow query traces and transaction traces. Distributed tracing settings Important Starting in Python agent version 7.0.0.166 or higher, distributed tracing is enabled by default. Enabling distributed tracing disables cross application tracing and has other effects on APM features. If migrating from cross application tracing, read the transition guide. Distributed tracing lets you see the path that a request takes as it travels through a distributed system. Settings include: distributed_tracing.enabled Type Boolean Default true Set in Config file, environment variable Environ variable NEW_RELIC_DISTRIBUTED_TRACING_ENABLED Enables Distributed Tracing Span event configuration Span events are collected for distributed tracing. Distributed tracing must be enabled to report span events. Configuration options include: span_events.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off whether the Python agent sends spans. span_events.attributes.enabled Type Boolean Default true Set in Config file This setting can be used to turn on or off for all attributes for span events. If attributes.enabled at the root level is false, no attributes will be sent to span events regardless on how this configuration setting (span_events.attributes.enabled) is set. For more information, see the agent attribute rules. span_events.attributes.include Type List of strings Default (none) Set in Config file If attributes are enabled for span events, all attribute keys found in this list will be sent in span events. For more information, see the agent attribute rules. span_events.attributes.exclude Type List of Strings Default (none) Set in Config file All attribute keys found in this list will not be sent in span events. For more information, see the agent attribute rules. Event harvest configuration Event harvest settings limit the amount of event type data sent to New Relic. When you use these settings, consider these important points: Event harvest settings affect the limits for a single instance of the agent, and not across the entire application. See the usage example below for how to set limits across an entire application. Real time streaming sends data every five seconds (12 times per minute), but the event harvest settings still affect the rate in events per minute. Enabling or disabling real time streaming does not require changing these settings. With real time streaming (enabled by default), New Relic will display the event harvest limits for entities in five second intervals. This means, for example, when you set a limit value of 1200 in the config file, you'll see it as 100 in New Relic. Usage example Let's say an application is deployed across 10 hosts, each running four processes per host. To limit the number of span events to 10,000 events per minute for the entire application, divide that number by 10 hosts. Then divide again by four processes per host. 10000 / (10 * 4) = 250 Based on that calculation, the final setting is: event_harvest_config.harvest_limits.span_event_data = 250 Event harvest configuration settings include: event_harvest_config.harvest_limits.analytic_event_data Type Integer Default 1200 Set in Config file Environ variable NEW_RELIC_ANALYTICS_EVENTS_MAX_SAMPLES_STORED Limit for analytic events per minute sent by an instance of the Python agent to New Relic. event_harvest_config.harvest_limits.custom_event_data Type Integer Default 1200 Set in Config file Environ variable NEW_RELIC_CUSTOM_INSIGHTS_EVENTS_MAX_SAMPLES_STORED Limit for custom events per minute sent by an instance of the Python agent to New Relic. Custom events are created through the Python Telemetry SDK. event_harvest_config.harvest_limits.span_event_data Type Integer Default 1000 Set in Config file Environ variable NEW_RELIC_SPAN_EVENTS_MAX_SAMPLES_STORED Limit for span events per minute sent by an instance of the Python agent to New Relic. event_harvest_config.harvest_limits.error_event_data Type Integer Default 100 Set in Config file Environ variable NEW_RELIC_ERROR_COLLECTOR_MAX_EVENT_SAMPLES_STORED Limit for error events per minute sent by an instance of the Python agent to New Relic. Event loop visibility settings Important Requires Python agent version 5.0.0.124 or higher. Event loop visibility surfaces information about transactions that block the event loop. The agent will generate information about transactions that have waited a significant amount of time to acquire control of the event loop. Settings include: event_loop_visibility.enabled Type Boolean Default true Set in Config file Set this to false to disable event loop information. event_loop_visibility.blocking_threshold Type Float Default 0.1 Set in Config file Threshold in seconds for how long a transaction must block the event loop before generating event loop information. Garbage collection runtime metrics settings Important Requires Python agent version 6.2.0.156 or higher. These garbage collection runtime metrics settings are available via the agent configuration file: gc_runtime_metrics.enabled Type Boolean Default false Set in Config file When enabled, the agent will generate and send garbage collection metrics. gc_runtime_metrics.top_object_count_limit Type Integer Default 5 Set in Config file The agent reports object count metrics for the most common object types being collected by the garbage collector. For each object type, this setting allows you to set the maximum number of individual metrics that will be sampled. Other configuration settings Here are assorted other settings available via the agent configuration file. utilization.detect_aws Type Boolean Default true If true, the agent automatically detects that it is running in an AWS environment. utilization.detect_azure Type Boolean Default true If true, the agent automatically detects that it is running in an Azure environment. utilization.detect_gcp Type Boolean Default true If true, the agent automatically detects that it is running in a Google Cloud Platform environment. utilization.detect_pcf Type Boolean Default true If true, the agent automatically detects that it is running in a Pivotal Cloud Foundry environment. utilization.detect_docker Type Boolean Default true If true, the agent automatically detects that it is running in Docker. slow_sql.enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable Slow SQL? If enabled, the agent captures details from long-running SQL database queries. thread_profiler.enabled Type Boolean Default true Set in Server-side config, config file Server-side label Enable thread profiler? Enables you to schedule thread profiling sessions. The thread profiler will periodically capture a snapshot of the call stack for each active thread in the application to construct a statistically representative call tree. cross_application_tracer.enabled Type Boolean Default false Set in Config file Enables cross application tracing, which connect your apps and services within your service-oriented architecture. strip_exception_messages.enabled Type Boolean Default false Set in Config file If enabled, exception messages will be stripped from error traces before they are sent to the collector, in order to prevent the inadvertent capture of sensitive information. This option is automatically enabled in high security mode. strip_exception_messages.whitelist Type String Default (none) Set in Config file Exceptions listed in your allow list will not have their messages stripped, even if strip_exception_messages.enabled is true. The allow list is a space-separated string of exception types, each in the form of module:exception_name. List built-in exceptions as exception_name; you do not need to prepend module: to them. Example: Built-in exception and user-defined exception KeyError my_module:MyException Copy startup_timeout Type Float Default 0.0 Set in Config file, environment variable Environ variable NEW_RELIC_STARTUP_TIMEOUT By default, the agent starts when it receives the first transaction (either web or non-web). The agent then starts in parallel, ensuring that this initial request is not delayed. However, the agent does not record the details of this initial request because the agent cannot collect data until registration is complete. To override this, you can set a startup timeout in seconds. The agent will then pause the initial transaction and wait for registration to complete. Important Since startup_timeout delays your app start, we recommend only setting a startup timeout for background task queuing systems, not web applications. shutdown_timeout Type Float Default 2.5 Set in Config file, environment variable Environ variable NEW_RELIC_SHUTDOWN_TIMEOUT On process shutdown, the agent attempts one final upload to the collector. To prevent the agent running indefinitely in case of an issue, the process shuts down normally if the shutdown_timeout threshold is reached. This shutdown can result in data loss, but the agent prioritizes key metric data during the upload process. For background task queuing systems, especially those which run a small number of tasks per process, you may want to increase the shutdown timeout to ensure the agent can upload all data on process shutdown. Tip The agent defaults to a 2.5 second timeout because Apache and many other web servers have a 3.0 second process termination timeout. The agent exits at 2.5 seconds to allow atexit cleanup code registered for the process to run. compressed_content_encoding Type String Default gzip Set in Config file If the data compression threshold is reached in the payload, the agent compresses data, using gzip compression by default. The config option compression_content_encoding can be set to deflate to use deflate compression. Heroku heroku.use_dyno_names Type Boolean Default true Environ variable NEW_RELIC_HEROKU_USE_DYNO_NAMES If true, the agent uses Heroku dyno names as the hostname. heroku.dyno_name_prefixes_to_shorten Type Array Default [\"scheduler\", \"run\"] Environ variable NEW_RELIC_HEROKU_DYNO_NAME_PREFIXES_TO_SHORTEN Ordinarily the agent reports dyno names with a trailing dot and process ID (for example, worker.3). You can remove this trailing data by specifying the prefixes you want to report without trailing data (for example, worker). Built-in instrumentation The Python agent instruments a range of Python packages/modules. This instrumentation only occurs when the target Python package/module is imported by an application. To disable default instrumentation, provide a special import-hook section corresponding to the name of the module that triggered instrumentation. Then set the enabled setting to false to disable instrumentation of that module. Example: Disabling MySQLdb database query instrumentation Add the following to the configuration file: [import-hook:MySQLdb] enabled = false Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 90.5598,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Browser</em> <em>monitoring</em> settings",
        "body": " of seconds (decimal points allowed). See our glossary entry for apdex_t Attributes Attributes are key-value pairs that provide information for transaction traces, traced errors, <em>browser</em> <em>monitoring</em>, and transaction <em>events</em>. In addition to configuring attributes for all four destinations with the general"
      },
      "id": "617dc153196a67cf3df7bf56"
    }
  ],
  "/docs/data-apis/understand-data/event-data/events-reported-mobile-monitoring": [
    {
      "sections": [
        "New Relic mobile monitoring application token",
        "Tip"
      ],
      "title": "New Relic mobile monitoring application token",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile",
        "Maintenance"
      ],
      "external_id": "4321d8be4040736dd9ca00e165cf6e0df8d2db5c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/maintenance/viewing-your-application-token/",
      "published_at": "2022-01-12T02:51:38Z",
      "updated_at": "2022-01-08T09:58:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "One of our data ingest keys is our mobile monitoring application token. For each mobile app you monitor, we automatically assign a unique, 40-character hexadecimal string to that app, for both the US and EU datacenter. This token is required to authenticate your app's data. To view your application token: Go to one.newrelic.com, click Mobile, and select a monitored app. Select Application or Installation. Tip Data reported from your mobile app is tied to a named app based on the application token. All apps with a single application token will report to the same place in our mobile monitoring UI, regardless of the app's name when installed on a mobile device. We recommend using a different application token for each platform to separate iOS and Android versions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.68341,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic <em>mobile</em> <em>monitoring</em> application token",
        "sections": "New Relic <em>mobile</em> <em>monitoring</em> application token",
        "tags": "<em>Mobile</em> <em>monitoring</em>",
        "body": " token: Go to one.newrelic.com, click <em>Mobile</em>, and select a monitored app. Select Application or Installation. Tip Data <em>reported</em> from your <em>mobile</em> app is tied to a named app based on the application token. All apps with a single application token will <em>report</em> to the same place in our <em>mobile</em> <em>monitoring</em>"
      },
      "id": "603ebe1964441fd8864e88aa"
    },
    {
      "sections": [
        "Overview of data retention (original pricing model)",
        "Important",
        "Limits on editing and deleting data",
        "Product and API data retention policies",
        "APM",
        "APM data retention policies",
        "Legacy APM data retention policies",
        "Browser",
        "Infrastructure",
        "Tip",
        "Infrastructure downsampling and data retention",
        "Integration compute unit event limits",
        "Insights",
        "Logs",
        "Logs in context data retention",
        "Mobile",
        "Mobile data retention policies",
        "Standard Mobile (legacy) data retention policies",
        "Synthetics",
        "Synthetics data retention policies",
        "Metric API",
        "Trace API",
        "Data components",
        "Event data: reported by most products",
        "Metric timeslice data: reported by APM, Browser, and Mobile",
        "Aggregate metric timeslice data: reported by APM, Browser, and Mobile",
        "Key metrics: reported by APM, Browser, and Mobile",
        "Trace data: reported by APM, Browser, and Mobile"
      ],
      "title": "Overview of data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "22d5ab9f4d623ead28ee7bb82c118d91804dee22",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/overview-data-retention-components/",
      "published_at": "2022-01-12T07:48:35Z",
      "updated_at": "2022-01-12T07:48:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original Product-based pricing. If you're on our New Relic One pricing model, see Manage your data. Not sure which you're on? See Overview of pricing models. If you're on the original product-based pricing model, you retain your existing subscriptions and data retention values. You manage these existing retention settings from the Data management hub in New Relic One. To manage your retention settings, go to the Data management UI. You'll see your existing retention settings. Adjust retention values by clicking Edit retention. New Relic stores different types of data for different periods of time. The retention period for a type of data will vary depending on the product, the subscription level, and the feature. Limits on editing and deleting data Once telemetry data (events, metrics, logs, traces) is reported to New Relic and available for querying, that data cannot be edited or deleted. This is a purposeful design decision that optimizes New Relic's speed and performance. Data will expire after its data retention ends. If you sent unwanted data or sensitive data to New Relic that must be removed, contact your account representative at New Relic, or get support at support.newrelic.com. Product and API data retention policies Select a New Relic product to see details about its data retention: APM Specific retention policies apply to each APM subscription level, including Lite, Essentials, Pro, and Enterprise. This includes metric timeslice data, key metrics, trace data, and event data. In addition to retention limits, your data is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate metric data description. APM data retention policies For accounts on our original product-based pricing, APM data retention policies depend on your APM product subscription level. Component Lite Essentials Pro Metric timeslice data 24 hours 3 days 90 days Key metrics none forever forever Distributed tracing and logs in context none none 8 days Other trace data * 1 day 3 days 7 days Event data * * 1 day 3 days 8 days * If you view a transaction trace in the New Relic UI, New Relic stores that trace data for up to one year. * * Learn about how to extend the retention of event data. Legacy APM data retention policies Component Standard Startup Small Business Enterprise Metric timeslice data 7 days 14 days 30 days 90 days Key metrics none none none forever Trace data 7 days 7 days 7 days 7 days Event data none 8 days 8 days 8 days Browser For accounts on our original product-based pricing, Browser data is stored depending on your subscription level: Component Lite Pro Metric data 24 hours 90 days Key metrics 24 hours forever Trace data 7 days 7 days Event data * 1 day 8 days SPA data Unavailable 8 days * Learn about how to extend the retention of event data. Infrastructure For accounts on our original product-based pricing, Infrastructure data retention policies depend on your Infrastructure subscription level and your infrastructure compute units pricing model. Data retention rules apply the same whether that data is displayed in the UI or queried. Tip Infrastructure data retention is not governed by your Insights subscription. Infrastructure downsampling and data retention Types of data are stored depending on your subscription level: Component Essentials Pro Infrastructure data 3 months 13 months Host count Host count will stop reflecting a host whose agent is no longer reporting after three minutes. However, host data will be available, subject to other retention criteria. 3 minutes 3 minutes Inventory attributes removed Inventory attributes for a host are retained for 24 hours after the agent stops reporting. 24 hours 24 hours Integration data Not available with Essentials Limit of 2275 integration events per compute unit per month In addition, Infrastructure downsamples your data on the fly, as it's generated. All Infrastructure metric data types (including On-Host Integrations metrics) will display different granularity depending on the age of the data and the size of the time window. The following table illustrates when different downsampled buckets will be used, both in the Infrastructure UI and for queries: Bucket Size Used For Time Windows Covering... Data Retained For... Raw (5, 10 or 15 second) 0 to 59 minutes 7 days 1 minute 60 minutes to 6 hours 30 days 10 minutes 6 hours to 3 days Full account retention period 1 hour 3 days to 14 days Full account retention period 3 hours 14+ days Full account retention period Integration compute unit event limits The 2275 limit on integration events per compute unit per month is a limit on total Infrastructure integration events. It's not a limit of 2275 for each integration's events. Additional details and clarifications about this limit: This limit applies to all events from all integrations (cloud integrations and on-host integrations). The events are all handled the same. Default data received by the Infrastructure agent does not count against the 2275 event per compute unit limit. If you exceed your limit, we do not enforce this limit. If you exceed your limit, we'll review pricing options with you to ensure you get the most cost-effective pricing for your organization's integrations. Insights For accounts on our original product-based pricing, an Insights subscription extends your event data retention. An Insights Pro subscription allows you to customize the length of your event data retention. Logs For accounts on our original product-based pricing, log data can be retained for up to 30 days by New Relic. Shorter retention periods of 8 or 15 days are also available. Logs in context data retention New Relic Logs logs in context data retention policy is based on your current APM product subscription level. For more information, see APM data retention. Mobile For accounts on our original product-based pricing, Mobile data retention policies depend on your New Relic Mobile product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and event data. For information about your subscription-related data usage, see Mobile subscription usage. Mobile data retention policies Component Lite Enterprise Overview page data 24 hours 90 days Crash data and stack traces 24 hours 90 days HTTP requests (except errors) as metric data Unavailable 90 days HTTP request errors as metric data Unavailable 3 days Interaction traces Unavailable 90 days Custom events * Unavailable 1 day Mobile events * 1 day 8 days MobileBreadcrumb events * Unavailable 3 days MobileCrash events * 1 day 90 days MobileHandledException events * Unavailable 3 days MobileJSError events (React Native beta) * Unavailable 3 days MobileRequest events * Unavailable 3 days MobileRequestError events * Unavailable 3 days MobileSession events * 1 day 90 days * Learn how to extend retention of event data. Standard Mobile (legacy) data retention policies Unless otherwise noted, Insights event data is unavailable for Standard subscriptions. Component Standard Overview page data 7 days Crash data and stack traces 7 days HTTP requests (except errors) as metric data 7 days HTTP request errors as metric data 3 days Interaction traces Unavailable MobileCrash events 8 days MobileSession events 8 days Synthetics For accounts on our original product-based pricing, Synthetics data retention policies depend on your Synthetics product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and Insights events. Tip Synthetics events do not count against an Insights Pro subscription. Your current subscription level appears in the right column of the Account summary page: Go to synthetics.newrelic.com > (account dropdown) > Account settings > Account > Summary. Synthetics data retention policies Component Lite Pro Synthetics monitor data 2 weeks 13 months Event data 2 weeks 13 months Also see the data retention details for APIs, including: Metric API All raw metric data points will be retained for 30 days. All additional aggregated data derived from the raw metric data points (for example, one-minute rollups) will be retained for 13 months. Any change to the retention period beyond such periods may result in a charge to you. Trace API See Trace API general requirements and limits. Data components For accounts on our original product-based pricing, the section below provides an explanation of some types of data components governed by the data retention rules of specific products: Event data: reported by most products See Event data retention for information on the event data type. Metric timeslice data: reported by APM, Browser, and Mobile Metric timeslice data is a specific type of data that is used for most metric charts and reports in APM, mobile, and browser. Important Note that metric timeslice data differs from other metric data types. All metric timeslice data is aggregated, but New Relic deals with fresh data and old data in different ways. Fresh data has specific policies applied to the data to keep granular views of performance as aggregate metrics. As data ages and becomes less useful in a granular state, we summarize that data and only keep key metrics. Aggregate metric timeslice data: reported by APM, Browser, and Mobile Aggregate metric timeslice data summarizes calls to specific methods in your application: how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on your subscription level. Subscription Level Aggregate retention Enterprise Aggregates (averages) to 1-hour periods after 8 days. After 90 days, the permanent metric data continues to be stored in 1-hour periods. This means you cannot obtain data granularity of less than 1 hour after 8 days, and only a subset of metrics are available after 90 days. Pro After 8 days Essentials After 3 days. Legacy Small Business, Startup, and Standard After 3 days. Lite After 2 hours. We retain your most recent data in one-minute increments. We also aggregate permanent metric data to day-size increments after 90 days. When looking at older data in small time windows, you may notice that charts show less detail. As data ages, it is aggregated into larger segments. Key metrics: reported by APM, Browser, and Mobile New Relic retains certain \"key\" metric timeslice data aggregations forever for Enterprise and Pro customers, for any number of applications. New Relic aggregates permanent key metric data to hour-size increments after 90 days. Product Key metrics APM Apdex, for app server Throughput and page views Response time, plus breakdown into tiers or categories on your app's main Overview chart Page load time, plus breakdown into segments Error rate CPU usage Memory usage Browser Apdex, for browser Browser page load time Throughput total. After 90 days, there is no breakout by browser type, and only the combined average value is available for the time range requested. Mobile Crash rate: For iOS and for Android Network throughput Network response time Network error rates Interaction traces Trace data: reported by APM, Browser, and Mobile Depending on the product, New Relic retains different types of trace data: Product Trace data APM Types of trace data: Transaction traces Distributed tracing Slow query samples Error details See APM data retention details. Browser Types of trace data: Session traces Browser traces JavaScript errors* See Browser data retention details. Mobile Types of trace data: App crash stack traces Interaction traces See Mobile data retention details. * JavaScript errors in the stack trace UI are saved as trace data. JS errors are also saved as events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.0175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Metric timeslice data: <em>reported</em> <em>by</em> APM, Browser, and <em>Mobile</em>",
        "body": "Session <em>events</em> * 1 day 90 days * Learn how to extend retention of <em>event</em> data. Standard <em>Mobile</em> (legacy) data retention policies Unless otherwise noted, Insights <em>event</em> data is unavailable for Standard subscriptions. Component Standard Overview page data 7 days Crash data and stack traces 7 days HTTP requests"
      },
      "id": "6043f75364441f6967378ec6"
    },
    {
      "sections": [
        "Get started with New Relic observability",
        "Get your data into New Relic with our quickstarts",
        "Some technical detail",
        "Guided install for New Relic",
        "All the answers in one place"
      ],
      "title": "Get started with New Relic observability",
      "type": "docs",
      "tags": [
        "Observe everything",
        "Get started"
      ],
      "external_id": "30f87d5f702f926efec49b59591679fa93627ad5",
      "image": "https://docs.newrelic.com/static/44970161aec793f3141cfcdc0fc96a57/c1b63/observability-2.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/get-started-full-stack-observability/",
      "published_at": "2022-01-12T18:26:43Z",
      "updated_at": "2022-01-12T18:26:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "True observability is the power of knowing what's happening across your digital system and why it's happening—at any time, whatever solution you’re using. It’s getting the whole picture of everything that enables your applications and devices to deliver value to your customers, from the container running a microservice in the cloud to a mobile website's shopping cart button. Our platform goes beyond simple monitoring by offering you observability: data insights to help you make proactive and predictive improvements to your environment. A comparison between simple monitoring and observability. Get your data into New Relic with our quickstarts New Relic I/O is a rich catalog of open-source quickstarts that automatically include integrations, dashboards, and alerts for you to use immediately. Popular technologies such as Node.js, Python, and Ruby have full-featured quickstarts, while others contain a mixture of tools. Each quickstart is created by observability experts around the world, vetted by New Relic, and ready for you to install with one click. Leverage community expertise and get more value out of your telemetry data with New Relic I/O, your hub for instant observability. Ready to get started? Find your quickstart in New Relic I/O: New Relic I/O New Relic I/O is open source, which means that you can modify and improve existing quickstarts, or build new ones, to suit your needs. We thoroughly review external edits to our quickstarts for value and quality. Interested in contributing to the community? Check out our contributor guide in GitHub. Some technical detail New Relic quickstarts use open source installation recipes to instrument integrations using our guided install process. Guided install for New Relic Alternatively, if you're comfortable with the command line, our guided install discovers the applications, infrastructure, and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. If your account reports data through our EU data center, click EU Guided install. Guided install EU Guided install All the answers in one place Once your data is in New Relic, we give you a UI with tools to cut through the layers of complexity surrounding your systems. This is all in one platform so you don't need to switch between diagnostic applications. You can interrogate your data for patterns, discover them using our data platform, or get proactive results from our machine learning tools. All our observability tools are interconnected and accessible in New Relic One. Data reported to New Relic can be categorized as metrics, events, logs, and traces. This data feeds our platform's analytics and monitoring capabilities. New Relic links your data in a meaningful way so that you can explore it, build dashboards, and set up alerts. Our out-of-the-box observability UI experiences allow to visualize, analyze, and troubleshoot your entire software stack in one unified platform. The New Relic Explorer consolidates all the entities in your system, and how they're connected, in a single place, so you can easily detect performance trends and issues. By automatically connecting infrastructure health with application performance and end-user behavior, you can cut through the noise to find useful signals.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.95941,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " running a microservice in the cloud to a <em>mobile</em> website&#x27;s shopping cart button. Our platform goes beyond simple <em>monitoring</em> by offering you observability: data insights to help you make proactive and predictive improvements to your environment. A comparison between simple <em>monitoring</em> and observability"
      },
      "id": "61743c6764441f60375fd317"
    }
  ],
  "/docs/data-apis/understand-data/event-data/events-reported-synthetic-monitoring": [
    {
      "sections": [
        "Security for New Relic-reported events and attributes",
        "Default events and attributes",
        "Adjust the data reported"
      ],
      "title": "Security for New Relic-reported events and attributes ",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Default events"
      ],
      "external_id": "446305a7d17c6dfb44e9e87520a1c08b79f5bcf9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/understand-data/event-data/customized-security-settings-insights/",
      "published_at": "2022-01-12T03:45:19Z",
      "updated_at": "2021-10-23T17:29:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "By default, New Relic products report a variety of data used in our UI charts and that is available for querying. Our products will not transmit sensitive information without being explicitly instrumented to do so. Default events and attributes Our products report a set of default events and attributes. We will never send request parameters or any other attributes that are not in the default set, unless someone has explicitly enabled this via configuration. Adjust the data reported When evaluating security settings for a New Relic product, review the default events and attributes. The default attributes don't contain sensitive data. In general, it's simply the data needed for effective performance monitoring. Our products don't send other data unless you change the default security settings. Depending on your requirements, either or both of these situations may apply: If the default list contains data you're concerned about, you can disable those attributes from being collected. For how to edit that, see the documentation for the product you're using. If you need to send attributes not reported by default, you can enable those attributes to be reported. In that case, do not use high security mode: this will disable the ability to collect custom attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.07794,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security for New Relic-reported <em>events</em> and attributes ",
        "sections": "<em>Default</em> <em>events</em> and attributes",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "By <em>default</em>, New Relic products report a variety of <em>data</em> used in our UI charts and that is available for querying. Our products will not transmit sensitive information without being explicitly instrumented to do so. <em>Default</em> <em>events</em> and attributes Our products report a set of <em>default</em> <em>events</em>"
      },
      "id": "60a8ea67e7b9d25ec7aeabfe"
    },
    {
      "sections": [
        "Default events reported by New Relic products"
      ],
      "title": "Default events reported by New Relic products",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Default events"
      ],
      "external_id": "598dfde069ba5a8bbbd5834c44b9740d6b338cdc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/understand-data/event-data/default-events-reported-new-relic-products/",
      "published_at": "2022-01-12T03:45:20Z",
      "updated_at": "2021-10-23T17:29:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic products report different types of data. One type of data reported is event data. Events are displayed in UI charts and tables, and also made available for querying. To understand the types of data available, see Data available via NRQL. Learn more about the events reported by New Relic products: APM default events Browser default events Infrastructure default events Mobile default events Synthetics default events NrAuditEvent events for understanding changes to your account",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.07794,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Default</em> <em>events</em> reported by New Relic products",
        "sections": "<em>Default</em> <em>events</em> reported by New Relic products",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": "New Relic products report different types of <em>data</em>. One type of <em>data</em> reported is <em>event</em> <em>data</em>. <em>Events</em> are displayed in UI charts and tables, and also made available for querying. To understand the types of <em>data</em> available, see <em>Data</em> available via NRQL. Learn more about the <em>events</em> reported by New Relic"
      },
      "id": "609f8faf64441f8af9d2a1f0"
    },
    {
      "sections": [
        "Data requirements and limits for custom event data",
        "General requirements",
        "Important",
        "Reserved words",
        "Event type limits"
      ],
      "title": "Data requirements and limits for custom event data",
      "type": "docs",
      "tags": [
        "Insights",
        "Event data sources",
        "Custom events"
      ],
      "external_id": "77720ef366038ba648a5fbf3cf34e8e48b38440a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/custom-data/custom-events/data-requirements-limits-custom-event-data/",
      "published_at": "2022-01-12T02:18:23Z",
      "updated_at": "2021-10-23T21:58:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains general requirements and rules for inserting and using custom events and their associated attributes. Additional requirements may apply based on the method you use. You can report custom events to New Relic in several ways, including: APM agent APIs Event API (There are additional requirements when using the Event API.) Browser monitoring agent APIs (There are additional requirements with the custom PageAction event.) Mobile monitoring SDK General requirements When reporting custom events and attributes, follow these general requirements for supported data types, naming syntax, and size: Requirement Description Payload Total maximum size or length: 1MB (10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. The Event API has additional HTTP rate limits. Attribute data types Attribute values can be either a string or a numeric integer or float. If your attribute values contain date information, define it as an unformatted Unix timestamp (in seconds or milliseconds) by using the Insights data formatter. Attribute size Maximum name size: 255 bytes. Maximum attribute value size: Custom attributes sent by the agent: 255 bytes Attributes attached to custom events sent using the Event API: 4096 characters Maximum total attributes per event: 254. Exception: If you use an APM agent API, the max is 64. Maximum total attributes per event type: 48,000. Important Charts may only display the first 255 characters of attribute values. For complete attribute values, use the JSON chart type or Query API. Naming syntax Attribute names can be a combination of alphanumeric characters, colons (:), periods (.), and underscores (_). Event types (using the eventType attribute) can be a combination of alphanumeric characters, colons (:), and underscores (_). If the name begins with anything other than an alphabetical character, enclose the name with backticks in your NRQL query. For example: FROM `0_hello` SELECT count(*) Copy Do not use words reserved for use by NRQL. Null values The database does not store any data with a null value. Reserved words Avoid using the following reserved words as names for events and attributes. Otherwise, unexpected results may occur. Important This is not a complete list. In general, avoid using MySQL-reserved words to avoid collision with future New Relic functionality. Keyword Description accountId This is a reserved attribute name. If it's included, it will be dropped during ingest. appId Value must be an integer. If it is not an integer, the attribute name and value will be dropped during ingest. eventType The event type as stored in New Relic. New Relic agents and scripts normally report this as eventType. Can be a combination of alphanumeric characters, colons (:), and underscores (_). Be sure to review the prohibited eventType values and eventType limits. Prohibited eventType values For your eventType value, avoid using: Metric, MetricRaw, and strings prefixed with Metric[0-9] (such as Metric2 or Metric1Minute). Public_ and strings prefixed with Public_. These event types are reserved for use by New Relic. Events passed in with these eventType values will be dropped. timestamp Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. It must be +/-1 day (24 hours) of the current time on the server. Log forwarding terms The following keys are reserved by the Infrastructure agent's log forwarding feature: entity.guid, log, hostname, plugin.type, fb.input. If used, they are dropped during ingest and a warning is added to the logs. NRQL syntax terms If you need to use NRQL syntax terms as attribute names, including dotted attributes, they must be enclosed in backticks; for example, `LIMIT` or `consumer.offset`. Otherwise, avoid using these reserved words: ago, and, as, auto, begin, begintime, compare, day, days, end, endtime, explain, facet, from, hour, hours, in, is, like, limit, minute, minutes, month, months, not, null, offset, or, raw, second, seconds, select, since, timeseries, until, week, weeks, where, with Event type limits The current limit for total number of eventType values is 250 per child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop data. Event types include: Default events from New Relic agents Custom events from New Relic agents Custom events from Insights custom event inserter",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.30388,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Data</em> requirements and limits for custom <em>event</em> <em>data</em>",
        "sections": "<em>Data</em> requirements and limits for custom <em>event</em> <em>data</em>",
        "tags": "<em>Event</em> <em>data</em> <em>sources</em>",
        "body": " child account in a given 24-hour time period. If a user exceeds this limit, New Relic may filter or drop <em>data</em>. <em>Event</em> types include: <em>Default</em> <em>events</em> from New Relic agents Custom <em>events</em> from New Relic agents Custom <em>events</em> from <em>Insights</em> custom <em>event</em> inserter"
      },
      "id": "609fa5cfe7b9d2bf16c3eb69"
    }
  ],
  "/docs/data-apis/understand-data/event-data/nrauditevent-event-data-query-examples": [
    {
      "sections": [
        "Query and alert on billing/usage data",
        "Available data types",
        "Query examples",
        "Data usage queries",
        "Daily data usage",
        "Daily usage by source",
        "Metrics ingest by source",
        "Month-to-date data usage",
        "Month-to-date estimated data cost",
        "User count queries",
        "Month-to-date full platform users",
        "Projected monthly full platform user count",
        "Count full platform users and basic users",
        "Set usage alerts",
        "Caution",
        "Ingested gigabytes exceed a fixed value",
        "Usage exceeds fixed threshold for GBs",
        "Usage exceeds fixed threshold for users",
        "Usage exceeds fixed threshold for estimated cost",
        "Available attributes"
      ],
      "title": "Query and alert on billing/usage data",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One pricing and billing"
      ],
      "external_id": "e22ae9e26686d11726a82ad4036ff58520b4a439",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/usage-queries-alerts/",
      "published_at": "2022-01-12T08:17:53Z",
      "updated_at": "2022-01-08T04:23:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For accounts on our New Relic One pricing model, we provide a View your usage UI for understanding billing-related usage and a Manage your data UI for managing billing-related data. Additionally, you can: Query your usage data to get more detail than is available in the UI. Set up NRQL alert conditions to get notifications about changes in your usage. Note that account hierarchy may affect queried data. See Account structure. Available data types Usage data is attached to these events: NrConsumption records usage every hour, and is the equivalent of \"real-time\" usage. Use this event to observe usage trends over time. NrMTDConsumption generates aggregate values from the NrConsumption event. Use this event to see usage or estimated cost for a billing period. NrUsage records usage every hour and is used to see usage reported per product. To see changes made to your account (for example, user management changes), you can query NrAuditEvent. Query examples The View your usage UI displays your data usage and billable user count. If you need more detail than that UI provides, you can use these NRQL queries. For definitions of some of the attributes used in these queries, see Attributes. Data usage queries Here are some data usage query examples: Daily data usage This query totals your billable ingested data, and displays a daily value for the past three months: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE 3 months ago TIMESERIES 1 day Copy Daily usage by source This query totals your billable ingested data, and displays a daily value for the past three months faceted by the source: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE 3 months ago FACET usageMetric TIMESERIES 1 day Copy Metrics ingest by source This query breaks down Metric data by the top ten metric names. You could also facet by appName or host to adjust the analysis. FROM Metric SELECT bytecountestimate()/10e8 as 'GB Estimate' SINCE '2021-04-01' UNTIL '2021-04-08' FACET metricName LIMIT 10 TIMESERIES 1 day Copy Month-to-date data usage This query shows the current full platform ingested data. In other words, it shows how much you'd be billed for your data for that month if you were billed right now. FROM NrMTDConsumption SELECT latest(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE this month Copy Month-to-date estimated data cost This query shows the estimated cost of your ingested data: FROM NrMTDConsumption SELECT latest(estimatedCost) WHERE productLine = 'DataPlatform' SINCE this month Copy User count queries Here are some user-related query examples. For details on how users are counted, see User count calculations. Month-to-date full platform users This query shows the billable full platform users for the month. In other words, it shows how much you'd be billed for your users for that month if you were billed right now. FROM NrMTDConsumption SELECT latest(usersBillable) SINCE this month Copy This query shows how many full platform users were counted by hour. This is useful for seeing how the full platform user count changed over time. from NrConsumption SELECT max(FullUsers) SINCE 10 days ago TIMESERIES 1 hour Copy Projected monthly full platform user count This query shows a projected count of monthly users. This query would not be good for using in a dashboard; it requires values based on a) the days remaining in the month, b) the start of the month. Here's an example querying the projected end-of-month count with 10 days left in that month: FROM NrMTDConsumption SELECT predictLinear(FullUsers, 10 days) SINCE '2020-09-01' Copy Count full platform users and basic users The usage UI shows the count of full platform users and basic users. The query used is: FROM NrUsage SELECT max(usage) SINCE 10 days ago WHERE productLine='FullStackObservability' WHERE metric in ('FullUsers', 'BasicUsers') FACET metric Copy To see the count of full and basic users over time: FROM NrUsage SELECT max(usage) SINCE 10 days ago WHERE productLine='FullStackObservability' WHERE metric in ('FullUsers', 'BasicUsers') FACET metric TIMESERIES 1 hour Copy Set usage alerts To help manage your billable data, you can set alerts to notify you of unexpected increases in usage. Learn how to create alerts with NRQL queries here. Caution When creating alert conditions, you should use the Event Timer method, which works very well with infrequent data. Here are some NRQL alert condition examples. For attribute definitions, see Attributes. Ingested gigabytes exceed a fixed value This query will create an alert when your hourly usage exceeds a fixed value: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' Copy If you have multiple child accounts, you may want to set threshold alerts for a specific subaccount: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' AND consumingAccountId = YOUR_CHILD-ACCOUNT_ID Copy Usage exceeds fixed threshold for GBs This query will create an alert when your usage exceeds fixed monthly threshold for GBs: FROM NrMTDConsumption SELECT latest(GigabytesIngested) WHERE productLine = 'DataPlatform' Copy Usage exceeds fixed threshold for users This query will create an alert when your usage exceeds fixed monthly threshold for billable users: FROM NrMTDConsumption SELECT latest(usersBillable) Copy Usage exceeds fixed threshold for estimated cost This query will create an alert when your usage exceeds fixed threshold for estimated cost: FROM NrMTDConsumption SELECT latest(estimatedCost) WHERE productLine = 'DataPlatform' Copy Available attributes Below are some of the important attributes attached to usage events. Attribute Description productLine The category of usage. There are three options: DataPlatform, FullStackObservability, and ProactiveDetection. (Starting November 1, 2021, IncidentIntelligence is no longer a billing factor). For more details about these categories, see New Relic platform. metric Consolidates multiple categories of usage into a single metric. Helpful when faceting by productLine. consumingAccountId ID of the New Relic account that is directly responsible for the stored events, as determined from the license key used. estimatedCost Calculates a cost estimate based on usage and metric cost. This is an estimate of costs to date, not your monthly invoice. For more attributes, see the data dictionary.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.23193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Query</em> <em>and</em> alert on billing&#x2F;usage <em>data</em>",
        "sections": "<em>Query</em> <em>and</em> alert on billing&#x2F;usage <em>data</em>",
        "tags": "Accounts <em>and</em> billing",
        "body": ", user management changes), you can <em>query</em> <em>NrAuditEvent</em>. <em>Query</em> <em>examples</em> The View your usage UI displays your <em>data</em> usage and billable user count. If you need more detail than that UI provides, you can use these NRQL queries. For definitions of some of the attributes used in these queries, see Attributes"
      },
      "id": "6175f12b64441f53a35fc21c"
    },
    {
      "sections": [
        "Log (audit) all data your New Relic agent transmits",
        "Caution",
        "APM agent audit logging",
        "Infrastructure agent logging",
        "New Relic account-related logging"
      ],
      "title": "Log (audit) all data your New Relic agent transmits",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Troubleshooting"
      ],
      "external_id": "66cd6ec040e070623d345e5319b1246d2ba4c3b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/solve-common-issues/troubleshooting/log-audit-all-data-your-new-relic-agent-transmits/",
      "published_at": "2022-01-12T05:59:00Z",
      "updated_at": "2021-12-14T04:18:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Every New Relic agent includes strong safeguards to ensure data security. For example, New Relic automatically encrypts sensitive information before it is transmitted. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website. If you need to record and view information about all data your app transmits to New Relic, you can enable audit logging for short periods of time. This is useful, for example, with debugging or auditing, when you need detailed information about what exactly is being transmitted. Caution Be sure to disable audit logging as soon as you are finished using it. This feature causes additional overhead, which may overload the audit log file if left turned on for extended periods of time. APM agent audit logging For details about the audit logging options for your APM agent's configuration file, see the agent-specific documentation: Agent Configuration file C SDK When starting the C SDK daemon, add -auditlog <file> to the daemon configuration file. For example: ./newrelic-daemon -f -logfile stdout -loglevel debug -auditlog audit.log Copy Go Logging is optional with the Go agent. If you are using newrelic.NewLogger(w) and want more detailed output, change newrelic.NewLogger(w) to newrelic.NewDebugLogger(w). For more information, see the New Relic Go logging documentation on GitHub. Java Set audit_mode to true. .NET Set auditLog to true. Node.js New Relic's Node.js agent does not use separate audit logs because the payload is already available in the configuration logs. To view increasing levels of detail, use your config file's logging level variables. PHP Use PHP newrelic.daemon.auditlog (for newrelic.ini) or auditlog (for newrelic.cfg). Python Use Python audit_log_file values. Ruby Use audit_log values. For more information, see Ruby agent audit log. Infrastructure agent logging You can generate infrastructure monitoring logs for troubleshooting our infrastructure agent. New Relic account-related logging To audit changes to your New Relic account, run NRQL queries with NrAuditEvent. To customize your query, use any of the available NrAuditEvent attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 309.1203,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Log (<em>audit</em>) all <em>data</em> your New Relic agent transmits",
        "sections": "Log (<em>audit</em>) all <em>data</em> your New Relic agent transmits",
        "body": "_log_file values. Ruby Use <em>audit</em>_log values. For more information, see Ruby agent <em>audit</em> log. Infrastructure agent logging You can generate infrastructure monitoring logs for troubleshooting our infrastructure agent. New Relic account-related logging To <em>audit</em> changes to your New Relic account, run NRQL queries with <em>NrAuditEvent</em>. To customize your <em>query</em>, use any of the available <em>NrAuditEvent</em> attributes."
      },
      "id": "61bf9c95196a67c384eee313"
    },
    {
      "sections": [
        "Synthetic monitoring audit log: Track changes made by users",
        "Feature description",
        "Query details",
        "Example use case: Finding changes made by a user"
      ],
      "title": "Synthetic monitoring audit log: Track changes made by users",
      "type": "docs",
      "tags": [
        "Synthetics",
        "Synthetic monitoring",
        "Administration"
      ],
      "external_id": "4673ae884e9d00a1c90e9577f2b8ff229b73b543",
      "image": "",
      "url": "https://docs.newrelic.com/docs/synthetics/synthetic-monitoring/administration/synthetic-monitoring-audit-log-track-changes-made-users/",
      "published_at": "2022-01-12T18:29:12Z",
      "updated_at": "2021-03-16T18:11:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic lets you see a 13-month history of synthetic monitoring audit events. Feature description When you take specific actions in synthetic monitoring like creating or editing a monitor, an NrAuditEvent is generated. This event includes details about the action taken and which user took that action. This data is stored for 13 months. This historical data may be helpful if you'd like to investigate how a problem with your account was created and who made that change. Synthetic monitoring's changes tracked include: Monitors Creation Edits (including location change, mute/unmute, and enable/disable) Script creation, edits, validation (including secure credentials used) Deletion Monitor downtimes Creation Edits Deletion Secure credentials Creation Edits Views Deletion Private locations Creation Edits (including clearing queues) Deletion For details on how to query this data, see Query details. Query details To query changes, use the query builder to explore the NrAuditEvent and its associated attributes. For an introduction to using the NrAuditEvent event, see Query account audit logs. Supported actionIdentifier events currently include: Monitors synthetics_monitor.create synthetics_monitor.update synthetics_monitor.create_script synthetics_monitor.update_script synthetics_monitor.validate_script synthetics_monitor.delete Monitor downtimes synthetics_monitor_downtime.create synthetics_monitor_downtime.update synthetics_monitor_downtime.delete Secure credentials synthetics_secure_credential.create synthetics_secure_credential.update synthetics_secure_credential.view synthetics_secure_credential.delete Private locations synthetics_private_location.create​ synthetics_private_location.update​ synthetics_private_location.delete How the change was made: The actorAPIKey attribute indicates if the change was made via the API or by a user via the UI. When this value is null, it's a user update; when not null, it's an API update. For examples of synthetic monitoring's audit log queries, see: The example use case. The synthetic monitoring specific examples in Audit query examples. Example use case: Finding changes made by a user Here's an example of using the synthetic monitoring audit log to solve a common problem: You are a manager at a company that uses synthetic monitoring. A new employee has been playing with your company's accounts to learn how synthetic monitoring works. Unfortunately, this employee was accidentally given full access to the production accounts, instead of the pre-production accounts. You want to determine what synthetic monitors this employee created, deleted, and updated, so that you will know which monitors need to be fixed. Instead of having to review every monitor in the account, you open the query builder and run the following NRQL query of the NrAuditEvent event: SELECT count(*) FROM NrAuditEvent WHERE actionIdentifier = 'synthetics_monitor.update_script' AND actorEmail = 'EMPLOYEE_EMAIL' FACET actionIdentifier, description SINCE 1 week ago LIMIT 1000 Copy The query will return all the synthetic monitors that the employee has updated, deleted, created, disable, or muted. One by one, you and the employee review the list and update the edited monitors.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 308.3359,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Synthetic monitoring <em>audit</em> log: Track changes made by users",
        "sections": "Synthetic monitoring <em>audit</em> log: Track changes made by users",
        "body": " details. <em>Query</em> details To <em>query</em> changes, use the <em>query</em> builder to explore the <em>NrAuditEvent</em> and its associated attributes. For an introduction to using the <em>NrAuditEvent</em> <em>event</em>, see <em>Query</em> account <em>audit</em> logs. Supported actionIdentifier events currently include: Monitors synthetics_monitor.create"
      },
      "id": "603eb96fe7b9d251b82a07cd"
    }
  ],
  "/docs/data-apis/understand-data/event-data/query-account-audit-logs-nrauditevent": [
    {
      "sections": [
        "Log (audit) all data your New Relic agent transmits",
        "Caution",
        "APM agent audit logging",
        "Infrastructure agent logging",
        "New Relic account-related logging"
      ],
      "title": "Log (audit) all data your New Relic agent transmits",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Troubleshooting"
      ],
      "external_id": "66cd6ec040e070623d345e5319b1246d2ba4c3b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/solve-common-issues/troubleshooting/log-audit-all-data-your-new-relic-agent-transmits/",
      "published_at": "2022-01-12T05:59:00Z",
      "updated_at": "2021-12-14T04:18:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Every New Relic agent includes strong safeguards to ensure data security. For example, New Relic automatically encrypts sensitive information before it is transmitted. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website. If you need to record and view information about all data your app transmits to New Relic, you can enable audit logging for short periods of time. This is useful, for example, with debugging or auditing, when you need detailed information about what exactly is being transmitted. Caution Be sure to disable audit logging as soon as you are finished using it. This feature causes additional overhead, which may overload the audit log file if left turned on for extended periods of time. APM agent audit logging For details about the audit logging options for your APM agent's configuration file, see the agent-specific documentation: Agent Configuration file C SDK When starting the C SDK daemon, add -auditlog <file> to the daemon configuration file. For example: ./newrelic-daemon -f -logfile stdout -loglevel debug -auditlog audit.log Copy Go Logging is optional with the Go agent. If you are using newrelic.NewLogger(w) and want more detailed output, change newrelic.NewLogger(w) to newrelic.NewDebugLogger(w). For more information, see the New Relic Go logging documentation on GitHub. Java Set audit_mode to true. .NET Set auditLog to true. Node.js New Relic's Node.js agent does not use separate audit logs because the payload is already available in the configuration logs. To view increasing levels of detail, use your config file's logging level variables. PHP Use PHP newrelic.daemon.auditlog (for newrelic.ini) or auditlog (for newrelic.cfg). Python Use Python audit_log_file values. Ruby Use audit_log values. For more information, see Ruby agent audit log. Infrastructure agent logging You can generate infrastructure monitoring logs for troubleshooting our infrastructure agent. New Relic account-related logging To audit changes to your New Relic account, run NRQL queries with NrAuditEvent. To customize your query, use any of the available NrAuditEvent attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 396.08276,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Log</em> (<em>audit</em>) all data your New Relic agent transmits",
        "sections": "<em>Log</em> (<em>audit</em>) all data your New Relic agent transmits",
        "body": "_file values. Ruby Use <em>audit_log</em> values. For more information, see Ruby agent <em>audit</em> <em>log</em>. Infrastructure agent logging You can generate infrastructure monitoring <em>logs</em> for troubleshooting our infrastructure agent. New Relic <em>account</em>-related logging To <em>audit</em> changes to your New Relic <em>account</em>, run NRQL queries with <em>NrAuditEvent</em>. To customize your <em>query</em>, use any of the available <em>NrAuditEvent</em> attributes."
      },
      "id": "61bf9c95196a67c384eee313"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/understand-data/event-data/nrauditevent-event-data-query-examples/",
      "sections": [
        "NrAuditEvent event data and query examples",
        "Available events and attributes",
        "Example queries",
        "General account changes",
        "What changes have been made to the New Relic account?",
        "What type of account change was made the most?",
        "What trends appear in account changes?",
        "What user management changes have been done?",
        "Synthetics: What changes have been made to a monitor?",
        "Workloads: What changes were made to any workload configuration?",
        "Changes made by specific users",
        "What account changes have been made by any user?",
        "What account changes have been made by a specific user?",
        "Who made the most changes to the account?",
        "Synthetics: What monitors were created by a specific user?",
        "Changes made using the API",
        "What account changes have been made using an API key?"
      ],
      "published_at": "2022-01-12T02:23:36Z",
      "title": "NrAuditEvent event data and query examples",
      "updated_at": "2021-11-25T12:48:11Z",
      "type": "docs",
      "external_id": "3619285ba0ba23c989f4abdd90607e4b8ba6aa13",
      "document_type": "page",
      "popularity": 1,
      "body": "To view changes made in your New Relic account, you can query NrAuditEvent events. Available events and attributes The NrAuditEvent is created to record configuration changes made in our products. The data gathered for this event includes the type of account change, actor (user or API key) that made the change, a human-readable description of the action taken, and a timestamp for the change. To see all the attributes attached to this event, see NrAuditEvent. Example queries These examples show some of the ways you can run NRQL queries of the NrAuditEvent event. General account changes What changes have been made to the New Relic account? To view all changes to your New Relic account for a specific time frame, run this basic NRQL query: SELECT * from NrAuditEvent SINCE 1 day ago Copy What type of account change was made the most? To query what type of change to the account users was made the most frequently during a specific time frame, include the actionIdentifier attribute in your query. For example: SELECT count(*) AS Actions FROM NrAuditEvent FACET actionIdentifier SINCE 1 week ago Copy What trends appear in account changes? When you include TIMESERIES in a NRQL query, the results are shown as a line graph. For example: SELECT count(*) from NrAuditEvent TIMESERIES facet actionIdentifier since 1 week ago Copy What user management changes have been done? Note that your users' user model will impact these queries. If your users are on our original user model, you can only query per account. If your users are on the New Relic One user model, you should query the top-level account in your New Relic organization. To see all the changes made to users, you could use: SELECT * FROM NrAuditEvent WHERE targetType = 'user' SINCE this month Copy If you wanted to narrow that down to see changes to user type (full platform user vs basic user), you could use: SELECT * FROM NrAuditEvent WHERE targetType = 'user' AND actionIdentifier IN ('user.self_upgrade', 'user.change_type') SINCE this month Copy Synthetics: What changes have been made to a monitor? To query Synthetics monitor updates during a specific time frame, include the actionIdentifier attribute in your query. For example: SELECT count(*) FROM NrAuditEvent WHERE actionIdentifier = 'synthetics_monitor.update_script' FACET actionIdentifier, description, actorEmail SINCE 1 week ago LIMIT 1000 Copy For more information about this Synthetics feature, see Synthetics audit log. Workloads: What changes were made to any workload configuration? To query what configuration changes were made to any workload, use the query below. The targetId attribute contains the GUID of the workload that was modified, which you can use for searches. Since changes on workloads are often automated, you might want to include the actorType attribute to know if the change was done directly by a user through the UI or through the API. SELECT timestamp, actorEmail, actorType, description, targetId FROM NrAuditEvent WHERE targetType = 'workload' SINCE 1 week ago LIMIT MAX Copy Changes made by specific users What account changes have been made by any user? To see detailed information about any user who made changes to the account during a specific time frame, include actorType = 'user' in the query. For example: SELECT actionIdentifier, description, actorEmail, actorId, targetType, targetId FROM NrAuditEvent WHERE actorType = 'user' SINCE 1 week ago Copy What account changes have been made by a specific user? To query account activities made by a specific person during the selected time frame, you must know their actorId. For example: SELECT actionIdentifier FROM NrAuditEvent WHERE actorId = 829034 SINCE 1 week ago Copy Who made the most changes to the account? To identify who (actorType) has made the most changes to the account, include the actorEmail attribute in your query. For example: SELECT count(*) as Users FROM NrAuditEvent WHERE actorType = 'user' FACET actorEmail SINCE 1 week ago Copy Synthetics: What monitors were created by a specific user? To query Synthetics monitor updates made by a specific user, include the actionIdentifier and actorEmail attribute in your query. For example: SELECT count(*) FROM NrAuditEvent WHERE actionIdentifier = 'synthetics_monitor.update_script' FACET actorEmail, actionIdentifier, description SINCE 1 week ago LIMIT 1000 Copy Changes made using the API What account changes have been made using an API key? To see detailed information about changes to the account that were made using an API key during a specific time frame, include actorType = 'api_key' in the query. For example: SELECT actionIdentifier, description, targetType, targetId, actorAPIKey, actorId, actorEmail FROM NrAuditEvent WHERE actorType = 'api_key' SINCE 1 week ago Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 385.06418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>NrAuditEvent</em> <em>event</em> data and <em>query</em> examples",
        "sections": "<em>NrAuditEvent</em> <em>event</em> data and <em>query</em> examples",
        "body": "To view changes made in your New Relic <em>account</em>, you can <em>query</em> <em>NrAuditEvent</em> events. Available events and attributes The <em>NrAuditEvent</em> is created to record configuration changes made in our products. The data gathered for this <em>event</em> includes the type of <em>account</em> change, actor (user or API key"
      },
      "id": "60a8e35ce7b9d2b07caeabdd"
    },
    {
      "sections": [
        "Data privacy with New Relic",
        "Tip",
        "Personal data transfer (Privacy Shield and SCC)",
        "Compliance with legal requirements",
        "Privacy by design and by default",
        "Personal data requests (GDPR, CCPA, etc.)",
        "Events and attributes",
        "Dropping data at ingest",
        "Technical security controls",
        "Organizational security controls",
        "Account security",
        "Retention of your data",
        "New Relic account emails",
        "Account changes (NrAuditEvent)",
        "Account usage",
        "Security for products and services",
        "Alerts and Applied Intelligence",
        "APIs",
        "APM",
        "Browser monitoring",
        "Diagnostics",
        "Infrastructure monitoring",
        "Integrations and serverless monitoring",
        "Logs management",
        "Mobile monitoring",
        "Pixie auto-telemetry data",
        "Synthetic monitoring"
      ],
      "title": "Data privacy with New Relic",
      "type": "docs",
      "tags": [
        "Security",
        "Security and Privacy",
        "Data privacy"
      ],
      "external_id": "d46953520476285467540433180d483815efecc6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/",
      "published_at": "2022-01-12T17:55:49Z",
      "updated_at": "2022-01-04T18:06:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic takes your data privacy seriously. Our principles-based approach aims to go beyond the legal requirements for consent. We understand your concerns when you entrust us with your data, and we always strive to embrace your expectations and preferences. This document provides links to detailed information about the privacy and security measures we take to protect you and your customers' data privacy. Our monitoring tools are data-agnostic; they don't require sensitive materials, and many of them don't require any personal data. You are responsible for ensuring that your systems are appropriately set up and configured so that they don't send inappropriate personal data or sensitive materials to New Relic monitoring tools. For additional information about policies, credentials, audits, and other resources, see our New Relic security website. Tip New Relic now offers the option of HIPPA-enabled accounts for customers meeting certain requirements. To learn more, see HIPAA readiness at New Relic. Personal data transfer (Privacy Shield and SCC) The Schrems case ruling invalidates Privacy Shield. However, it explicitly reaffirms the validity of Standard Contractual Clauses (SCC) as an appropriate legal mechanism to transfer personal data outside of the European Union. You can find more information in How the Demise of Privacy Shield Affects Your New Relic Account. If you want to send personal data from the EU, we offer an appropriate data processing agreement (DPA) with SCC to govern the transfer of that data in accordance with the Schrems decision. For more information, consult our Data Processing Addendum FAQ, or download our pre-signed DPA (PDF|697 KB). Compliance with legal requirements We always strive to comply with all applicable laws as they take effect. This includes the European Union's General Data Protection Regulation (GDPR) and all relevant US State laws, such as the California Consumer Privacy Act (CCPA). Our disk-based encryption provides additional security while your data is at rest (FIPS 140-2 compliant). In addition, we are authorized for Moderate Impact SaaS Services (FedRAMP Authorized Moderate) for accounts that meet specific criteria. For privacy-related details about New Relic's contractual and regulatory commitments for services, see: Terms of Service or Master Subscription Agreement Data Protection Agreement Services Privacy Notice For more information about annual audits, see Regulatory audits for New Relic services. Privacy by design and by default New Relic follows \"privacy by design\" principles as part of our overarching security program. For example, when New Relic agents capture a webpage or referrer URL, all query parameters are stripped by default. Here are examples of how we incorporate privacy considerations into our data and security practices. Personal data requests (GDPR, CCPA, etc.) New Relic strives to comply with all applicable laws as they take effect. This includes the European Union's GDPR and ePrivacy Directive and all applicable privacy laws, such as the California Consumer Privacy Act (CCPA) in the US. For more information about our process when responding to requests to access or delete personal data, see New Relic personal data requests. Events and attributes You can query events and attributes, as well as create charts and alert conditions about this data. For a complete list of all events and attributes tracked by New Relic agents, see our data dictionary. Events and attributes example: If you use the Infrastructure ProcessSample event's commandLine attribute, by default we strip options and arguments from the full command line to prevent accidental leakage of sensitive information. Dropping data at ingest Dropping data gives you control over the data that you send to New Relic, including any personal data that you configured to be collected. By dropping specific events or attributes from events, you determine what data New Relic ultimately stores so that you can query, alert on, and analyze it. For more information, see Drop data using NerdGraph. When our agents refer to data obfuscation, the agent actually removes the data before sending it to New Relic. The data cannot be recovered. For example, with APM queries, the Record SQL? value defaults to obfuscated. This strips the string literals and numeric sequences and then replaces them with the ? character. You can mask sensitive information in HTTP or HTTPS requests. For example, queries about distributed traces and transaction traces are obfuscated by default, in which case they cannot be recovered. For more information, see the documentation for specific New Relic services, including: APM transaction traces Distributed tracing Technical security controls We use a comprehensive set of technical controls to support general security needs as well as security for data we receive. For more information, see our documentation about data security, data encryption, and high security mode for APM agents. Organizational security controls New Relic maintains a number of internal policies and procedures to guide employees in privacy-related subjects such as data classification and handling, data retention, handling of personal data, fulfilling personal data requests, incident response, etc. All employees must complete the security and privacy training upon hiring and renew this training annually. Account security Our role-based account structure gives you direct control over who can access or change your account settings. For more information, see Users and roles. Retention of your data The New Relic One platform gives you a single source of truth for all your operational data, empowering you to ask and answer any question in milliseconds. This platform stores different types of data for different periods of time. The Data retention page in our UI provides information on how long your data will be stored in the New Relic database (NRDB). For more information, see Manage data retention. New Relic account emails By default, we communicate with you for a variety of purposes related to your status as New Relic subscribers. This includes product engagement, support, alert notifications, updates, billings, etc. Individual users can unsubscribe from certain communications. General email preferences are managed through the account user interface. For more information, see Account email settings. Alert notification emails are managed through the alerting UI. Account changes (NrAuditEvent) To view changes made to your account's users or to record configuration changes, query NrAuditEvent events. To be notified about account changes, create NRQL alert conditions. For more about available NrAuditEvent attributes, see our data dictionary. Account usage For more about usage, see Manage data. Security for products and services We publish security bulletins with detailed information about vulnerabilities, remediation strategies, and applicable updates for affected software. To receive notifications for future advisories, use either of these options: Subscribe to our security bulletins RSS feed. Select the Watching option in our Explorers Hub's Security notifications community channel to receive email alerts. The following summarizes how individual New Relic products and components ensure security, with links to additional details. Alerts and Applied Intelligence By default, our alerting services do not record any personal data. In addition, they automatically set default permissions for individual account users and access levels within account structures. For more information, see our documentation about Applied Intelligence, as well as our rules and limits for alerts. APIs APIs simply are interfaces for data exchange automation. APIs have no knowledge of the content being transferred. We require authorized users to provide their API keys to monitor subscription usage, manage account user permissions, query data, and perform other automated tasks. For more information, see Introduction to New Relic APIs. APM APM agents monitor your applications' performance. By default, APM agents do not record any personal data. For more information, see our APM security documentation. Browser monitoring Our browser monitoring agent allows you to monitor the performance of their websites. For more information, see: Browser security documentation Visitor's IP address New Relic cookies used by browser Enabling or disabling cookie collection for session tracking Diagnostics The New Relic Diagnostics service inspects relevant system information and any other necessary information (such as logs and config files) to perform diagnostic checks that assess configuration and operability. By default, this data is not transmitted to New Relic. You do have the option to upload this information to a support ticket over HTTPS. For more information, see the Diagnostics security documentation. Infrastructure monitoring The Infrastructure agent allows you to monitor the performance of components in your ecosystem, such as servers, platforms, operating systems, databases, etc. Infrastructure may record the userID and username of users connecting to Infrastructure resources. For more information, see the security documentation for infrastructure monitoring. Integrations and serverless monitoring Our integrations services allow you to retrieve and load data into the New Relic database from a variety of sources, including: Cloud-based integrations On-host integrations in containerized environments, such as Kubernetes On-host integrations built by New Relic On-host integrations built by the open-source community On-host integrations built by you Depending on the integration, different types of data may be recorded so that you can monitor the integrations in New Relic. The integration services are data agnostic. They will have no knowledge of whether the imported data contains any personal information. For more information, see the documentation for the specific integration, including: Amazon Web Services (AWS) Google Cloud Platform (GCP) Kubernetes Microsoft Azure On-host integrations Serverless function monitoring Logs management Due to the nature of our logs management service, you have direct control over what data is reported to New Relic. To ensure data privacy and to limit the types of information New Relic receives, no customer data is captured except what you supply in your API calls or log forwarder configuration. All data for the logs service is then reported to New Relic over HTTPS. The logs service does mask number patterns that appear to be for items such as credit cards or Social Security numbers. For more information, see the Logs security documentation. Mobile monitoring By default, our mobile monitoring service collects two pieces of personal data: The IP address is used to derive high-level geographical data, and then is discarded. A device ID is generated by New Relic and is used for billing purposes. For more information, see our security documentation for mobile monitoring. Pixie auto-telemetry data Auto-telemetry with Pixie is New Relic One's integration of Community Cloud for Pixie, a managed version of Pixie open source software. The data that Pixie collects is stored entirely within your Kubernetes cluster. This data does not persist outside of your environment, and it will never be stored by Community Cloud for Pixie. This means that your sensitive data remains within your environment and control. For example, you can: Control who has access to your Pixie data. Manage auto-update and two-way communication. For more information, see our security documentation for auto-telemetry with Pixie data. Synthetic monitoring The synthetic monitoring service uses monitors distributed throughout data centers around the world. It captures what is essentially performance data of simulated traffic. By default, it does not capture any personal data. For more information, see the data privacy and security documentation for synthetic monitoring. If you configure the synthetic service to monitor areas of websites that are located behind a login page, take care to create a non-personal login dedicated to this purpose. This will reduce the risk of unintended personal data exposure. For example, to securely store sensitive information, such as passwords, API keys, and user names, you can use secured credentials for scripted browsers and API tests. The synthetic monitoring service also supports a variety of authentication mechanisms. Depending on the type of monitor you choose, this includes Basic, Digest, NTLM, and NTLMv2. You can also control which of your users can access your monitors and private locations. For more information, see our documentation about user role-based permissions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 364.5337,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Account</em> changes (<em>NrAuditEvent</em>)",
        "body": " preferences are managed through the <em>account</em> user interface. For more information, see <em>Account</em> email settings. Alert notification emails are managed through the alerting UI. <em>Account</em> changes (<em>NrAuditEvent</em>) To view changes made to your <em>account</em>&#x27;s users or to record configuration changes, <em>query</em> <em>NrAuditEvent</em>"
      },
      "id": "603ec2d4e7b9d22fba2a07c6"
    }
  ],
  "/docs/data-apis/understand-data/metric-data/metric-data-type": [
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.72841,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Understand</em> <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "<em>Understand</em> <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.76096,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick start: Send metric <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.00429,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "sections": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " for cost, performance, and in some cases, compliance reasons. Our <em>data</em> management hub provides the tools you need to <em>understand</em> and control where your <em>data</em> is coming from, and adjust what’s stored and for how long. Important If you&#x27;re on our original product-based pricing model, you&#x27;ll see your <em>data</em> <em>ingest</em>"
      },
      "id": "603e96ff28ccbcf8bceba796"
    }
  ],
  "/docs/data-apis/understand-data/metric-data/query-apm-metric-timeslice-data-nrql": [
    {
      "sections": [
        "Extract metric timeslice data",
        "Time based data",
        "Time range considerations",
        "Important",
        "Tip",
        "Controlling time period output",
        "Data retention",
        "Extracting non-existent metric timeslice data"
      ],
      "title": "Extract metric timeslice data",
      "type": "docs",
      "tags": [
        "APIs",
        "REST API v2",
        "Basic functions"
      ],
      "external_id": "2a144a4b775dd2332592a5d92c199a07c08f49fa",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/rest-api-v2/basic-functions/extract-metric-timeslice-data/",
      "published_at": "2022-01-12T09:05:22Z",
      "updated_at": "2021-03-13T01:07:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "One type of New Relic data is metric timeslice data. There are several ways to query metric timeslice data: You can query APM metric timeslice data via NRQL (and therefore via our NerdGraph API). You can query any metric timeslice data via the REST API This doc explains how to do this with the REST API. Note that the API is not intended for bulk data extraction of minute-by-minute data points. Time based data All time values returned by the REST API and the API Explorer are UTC (Universal Time Coordinated). Be sure to adjust the time values for data collection as necessary. Time range considerations Important The minimum time range for data requests is one minute (60 seconds). Requests for anything less will result in a 422 status code and no data will be returned. New Relic only collects data at one minute intervals. The API uses the same mechanism for requesting data as the UI: it depends on the time range for the data you request. The objective is to optimize the number of data points returned and provide an easily digestible graph and report. For example: If you request data from a time range of three hours or less, the API returns the one-minute data values originally collected. If you increase the time range to greater than three hours, the data values returned will be an average for two minutes. If you increase the time range to over six hours, the data values returned will be an average for five minutes, and so on. Tip If the initial time for a requested time range is older than eight days, ten evenly spaced data points will be returned for any time range less than four days in length. Here is a summary of the metric value retrieval for the time ranges available. Between this time range... and this time range Granularity of collected data data age ≤ 8 days data age > 8 days ≤ 3 hours 1 minute 10 evenly spaced data points > 3 hour ≤ 6 hours 2 minutes > 6 hours ≤ 14 hours 5 minutes > 14 hours ≤ 24 hours 10 minutes > 1 day (24 hrs) ≤ 4 days (96 hrs) 30 minutes > 4 days ≤ 7 days 1 hour 1 hour > 7 days ≤ 3 weeks 3 hours 3 hours > 3 weeks ≤ 6 weeks 6 hours 6 hours > 6 weeks ≤ 9 weeks 12 hours 12 hours > 63 days 3 days 3 days When the start time for a requested time range is older than eight days, data has been aggregated or averaged to one hour periods due to the data aggregation schedule. This means that for any one hour period, only a single data value is available. Obtaining data at less than an hourly period in the time range would cause oversampling, resulting in duplicate values being returned. Returning only ten values prevents oversampling and presents a smoother chart, which eliminates a possibly misleading \"plateau\" effect. Controlling time period output Sometimes the output data's granularity may be too fine, or the time period for the data returned may be too short. To control this, include the period= parameter in the query command as the number of seconds you want each time period to report. Make sure your specifications follow New Relic's data aggregation schedules. Example #1: Following New Relic's table summarizing granularity of collected data, the following API call would normally return data in 30-minute periods, since the request is for 4 days (from=2018-02-13 and to=2018-02-17). By adding period=3600, the data will be returned as 60-minute periods. curl -X GET 'https://api.newrelic.com/v2/applications/$APPID/metrics/data.xml' \\ -H 'Api-Key:$API_KEY' -i \\ -d'names[]=CPU/User+Time&from=2018-02-13T04:00:00+00:00&to=2018-02-17T04:00:00+00:00&period=3600' Copy You cannot specify a period smaller than the default for the time range you are requesting. For example: In the command example above, you can request 1-hour periods, since that is greater than the default (half hour) granularity for the time range. In the command example above, you cannot request 1-minute periods, since that is less than the default (half hour) granularity for the time range. Example #2: If you request a range > 7 days but ≤ 3 weeks, where the default period is 3 hours, you can specify periods such as 6, 12, or 24 hours. However, you cannot request 1-hour periods, because that is less than the default (3 hours). Data retention How long data is available depends on the data retention for specific types of data. Extracting non-existent metric timeslice data Situations may arise where non-existent metric names are requested. For example: The metric timeslice data has not been created for one application, but exists for another. When the same metric extraction query is used on both of these applications, it will not be located for one. The metric name was incorrectly specified. Important Metric values that have existed in the past, but are no longer collected, will return a zero value. A successful response will include a 200 status code and metadata about the request. The metadata will contain the names of the metrics requested and the status of the request for those names. Response Metadata Description Response Metric Data metrics_not_found Lists all metric names for which matching data was not found in the requested time period. Metric timeslice data will not be returned for these metrics metrics_found Lists all metric names for which matching data was found in the requested time period. Metric timeslice data will be returned for these metrics Here is an example of output for a valid metric name, HttpDispatcher. HTTP/1.1 200 OK etag: \"0dc87c63d8dff6b1a9714bdf7531ec09\" Content-Type: application/json cache-control: max-age=0, private, must-revalidate {   \"metric_data\": {     \"from\": \"2016-01-28T18:06:06+00:00\",     \"to\": \"2016-01-28T18:36:06+00:00\",     \"metrics_not_found\": [], <---<<< INDICATES NO INVALID METRIC NAMES REQUESTED     \"metrics_found\": [       \"HttpDispatcher\" <---<<< INDICATES THIS METRIC NAME WAS VALID     ],     \"metrics\": [ <---<<< DATA RETURNED       {         \"name\": \"HttpDispatcher\",         \"timeslices\": [           {             \"from\": \"2016-01-28T18:03:00+00:00\",             \"to\": \"2016-01-28T18:04:00+00:00\",             \"values\": {               \"average_response_time\": 364,               \"calls_per_minute\": 99800,               \"call_count\": 99770,               \"min_response_time\": 3.5,               \"max_response_time\": 85000,               \"average_exclusive_time\": 0,               \"average_value\": 0.364,               \"total_call_time_per_minute\": 36300,               \"requests_per_minute\": 99800,               \"standard_deviation\": 1900,               \"average_call_time\": 364 ... Copy Here is an example of output for a invalid metric name, Foo. HTTP/1.1 200 OK etag: \"e51782cf7c5a5596139a7f5340c3de23\" Content-Type: application/json cache-control: max-age=0, private, must-revalidate {   \"metric_data\": {     \"from\": \"2016-01-28T18:06:33+00:00\",     \"to\": \"2016-01-28T18:36:33+00:00\",     \"metrics_not_found\": [       \"Foo\" <---<<< INDICATES THIS METRIC NAME WAS INVALID     ],     \"metrics_found\": [], <---<<< INDICATES NO VALID METRIC NAMES FOUND     \"metrics\": [] <---<<< NO DATA RETURNED   } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1188.98,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Extract <em>metric</em> <em>timeslice</em> <em>data</em>",
        "sections": "Extract <em>metric</em> <em>timeslice</em> <em>data</em>",
        "body": "One type of New Relic <em>data</em> is <em>metric</em> <em>timeslice</em> <em>data</em>. There are several ways to <em>query</em> <em>metric</em> <em>timeslice</em> <em>data</em>: You can <em>query</em> <em>APM</em> <em>metric</em> <em>timeslice</em> <em>data</em> via <em>NRQL</em> (and therefore via our NerdGraph API). You can <em>query</em> any <em>metric</em> <em>timeslice</em> <em>data</em> via the REST API This doc explains how to do this with the REST"
      },
      "id": "60440691e7b9d201b8579a00"
    },
    {
      "sections": [
        "Overview of data retention (original pricing model)",
        "Important",
        "Limits on editing and deleting data",
        "Product and API data retention policies",
        "APM",
        "APM data retention policies",
        "Legacy APM data retention policies",
        "Browser",
        "Infrastructure",
        "Tip",
        "Infrastructure downsampling and data retention",
        "Integration compute unit event limits",
        "Insights",
        "Logs",
        "Logs in context data retention",
        "Mobile",
        "Mobile data retention policies",
        "Standard Mobile (legacy) data retention policies",
        "Synthetics",
        "Synthetics data retention policies",
        "Metric API",
        "Trace API",
        "Data components",
        "Event data: reported by most products",
        "Metric timeslice data: reported by APM, Browser, and Mobile",
        "Aggregate metric timeslice data: reported by APM, Browser, and Mobile",
        "Key metrics: reported by APM, Browser, and Mobile",
        "Trace data: reported by APM, Browser, and Mobile"
      ],
      "title": "Overview of data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "22d5ab9f4d623ead28ee7bb82c118d91804dee22",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/overview-data-retention-components/",
      "published_at": "2022-01-12T07:48:35Z",
      "updated_at": "2022-01-12T07:48:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original Product-based pricing. If you're on our New Relic One pricing model, see Manage your data. Not sure which you're on? See Overview of pricing models. If you're on the original product-based pricing model, you retain your existing subscriptions and data retention values. You manage these existing retention settings from the Data management hub in New Relic One. To manage your retention settings, go to the Data management UI. You'll see your existing retention settings. Adjust retention values by clicking Edit retention. New Relic stores different types of data for different periods of time. The retention period for a type of data will vary depending on the product, the subscription level, and the feature. Limits on editing and deleting data Once telemetry data (events, metrics, logs, traces) is reported to New Relic and available for querying, that data cannot be edited or deleted. This is a purposeful design decision that optimizes New Relic's speed and performance. Data will expire after its data retention ends. If you sent unwanted data or sensitive data to New Relic that must be removed, contact your account representative at New Relic, or get support at support.newrelic.com. Product and API data retention policies Select a New Relic product to see details about its data retention: APM Specific retention policies apply to each APM subscription level, including Lite, Essentials, Pro, and Enterprise. This includes metric timeslice data, key metrics, trace data, and event data. In addition to retention limits, your data is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate metric data description. APM data retention policies For accounts on our original product-based pricing, APM data retention policies depend on your APM product subscription level. Component Lite Essentials Pro Metric timeslice data 24 hours 3 days 90 days Key metrics none forever forever Distributed tracing and logs in context none none 8 days Other trace data * 1 day 3 days 7 days Event data * * 1 day 3 days 8 days * If you view a transaction trace in the New Relic UI, New Relic stores that trace data for up to one year. * * Learn about how to extend the retention of event data. Legacy APM data retention policies Component Standard Startup Small Business Enterprise Metric timeslice data 7 days 14 days 30 days 90 days Key metrics none none none forever Trace data 7 days 7 days 7 days 7 days Event data none 8 days 8 days 8 days Browser For accounts on our original product-based pricing, Browser data is stored depending on your subscription level: Component Lite Pro Metric data 24 hours 90 days Key metrics 24 hours forever Trace data 7 days 7 days Event data * 1 day 8 days SPA data Unavailable 8 days * Learn about how to extend the retention of event data. Infrastructure For accounts on our original product-based pricing, Infrastructure data retention policies depend on your Infrastructure subscription level and your infrastructure compute units pricing model. Data retention rules apply the same whether that data is displayed in the UI or queried. Tip Infrastructure data retention is not governed by your Insights subscription. Infrastructure downsampling and data retention Types of data are stored depending on your subscription level: Component Essentials Pro Infrastructure data 3 months 13 months Host count Host count will stop reflecting a host whose agent is no longer reporting after three minutes. However, host data will be available, subject to other retention criteria. 3 minutes 3 minutes Inventory attributes removed Inventory attributes for a host are retained for 24 hours after the agent stops reporting. 24 hours 24 hours Integration data Not available with Essentials Limit of 2275 integration events per compute unit per month In addition, Infrastructure downsamples your data on the fly, as it's generated. All Infrastructure metric data types (including On-Host Integrations metrics) will display different granularity depending on the age of the data and the size of the time window. The following table illustrates when different downsampled buckets will be used, both in the Infrastructure UI and for queries: Bucket Size Used For Time Windows Covering... Data Retained For... Raw (5, 10 or 15 second) 0 to 59 minutes 7 days 1 minute 60 minutes to 6 hours 30 days 10 minutes 6 hours to 3 days Full account retention period 1 hour 3 days to 14 days Full account retention period 3 hours 14+ days Full account retention period Integration compute unit event limits The 2275 limit on integration events per compute unit per month is a limit on total Infrastructure integration events. It's not a limit of 2275 for each integration's events. Additional details and clarifications about this limit: This limit applies to all events from all integrations (cloud integrations and on-host integrations). The events are all handled the same. Default data received by the Infrastructure agent does not count against the 2275 event per compute unit limit. If you exceed your limit, we do not enforce this limit. If you exceed your limit, we'll review pricing options with you to ensure you get the most cost-effective pricing for your organization's integrations. Insights For accounts on our original product-based pricing, an Insights subscription extends your event data retention. An Insights Pro subscription allows you to customize the length of your event data retention. Logs For accounts on our original product-based pricing, log data can be retained for up to 30 days by New Relic. Shorter retention periods of 8 or 15 days are also available. Logs in context data retention New Relic Logs logs in context data retention policy is based on your current APM product subscription level. For more information, see APM data retention. Mobile For accounts on our original product-based pricing, Mobile data retention policies depend on your New Relic Mobile product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and event data. For information about your subscription-related data usage, see Mobile subscription usage. Mobile data retention policies Component Lite Enterprise Overview page data 24 hours 90 days Crash data and stack traces 24 hours 90 days HTTP requests (except errors) as metric data Unavailable 90 days HTTP request errors as metric data Unavailable 3 days Interaction traces Unavailable 90 days Custom events * Unavailable 1 day Mobile events * 1 day 8 days MobileBreadcrumb events * Unavailable 3 days MobileCrash events * 1 day 90 days MobileHandledException events * Unavailable 3 days MobileJSError events (React Native beta) * Unavailable 3 days MobileRequest events * Unavailable 3 days MobileRequestError events * Unavailable 3 days MobileSession events * 1 day 90 days * Learn how to extend retention of event data. Standard Mobile (legacy) data retention policies Unless otherwise noted, Insights event data is unavailable for Standard subscriptions. Component Standard Overview page data 7 days Crash data and stack traces 7 days HTTP requests (except errors) as metric data 7 days HTTP request errors as metric data 3 days Interaction traces Unavailable MobileCrash events 8 days MobileSession events 8 days Synthetics For accounts on our original product-based pricing, Synthetics data retention policies depend on your Synthetics product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and Insights events. Tip Synthetics events do not count against an Insights Pro subscription. Your current subscription level appears in the right column of the Account summary page: Go to synthetics.newrelic.com > (account dropdown) > Account settings > Account > Summary. Synthetics data retention policies Component Lite Pro Synthetics monitor data 2 weeks 13 months Event data 2 weeks 13 months Also see the data retention details for APIs, including: Metric API All raw metric data points will be retained for 30 days. All additional aggregated data derived from the raw metric data points (for example, one-minute rollups) will be retained for 13 months. Any change to the retention period beyond such periods may result in a charge to you. Trace API See Trace API general requirements and limits. Data components For accounts on our original product-based pricing, the section below provides an explanation of some types of data components governed by the data retention rules of specific products: Event data: reported by most products See Event data retention for information on the event data type. Metric timeslice data: reported by APM, Browser, and Mobile Metric timeslice data is a specific type of data that is used for most metric charts and reports in APM, mobile, and browser. Important Note that metric timeslice data differs from other metric data types. All metric timeslice data is aggregated, but New Relic deals with fresh data and old data in different ways. Fresh data has specific policies applied to the data to keep granular views of performance as aggregate metrics. As data ages and becomes less useful in a granular state, we summarize that data and only keep key metrics. Aggregate metric timeslice data: reported by APM, Browser, and Mobile Aggregate metric timeslice data summarizes calls to specific methods in your application: how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on your subscription level. Subscription Level Aggregate retention Enterprise Aggregates (averages) to 1-hour periods after 8 days. After 90 days, the permanent metric data continues to be stored in 1-hour periods. This means you cannot obtain data granularity of less than 1 hour after 8 days, and only a subset of metrics are available after 90 days. Pro After 8 days Essentials After 3 days. Legacy Small Business, Startup, and Standard After 3 days. Lite After 2 hours. We retain your most recent data in one-minute increments. We also aggregate permanent metric data to day-size increments after 90 days. When looking at older data in small time windows, you may notice that charts show less detail. As data ages, it is aggregated into larger segments. Key metrics: reported by APM, Browser, and Mobile New Relic retains certain \"key\" metric timeslice data aggregations forever for Enterprise and Pro customers, for any number of applications. New Relic aggregates permanent key metric data to hour-size increments after 90 days. Product Key metrics APM Apdex, for app server Throughput and page views Response time, plus breakdown into tiers or categories on your app's main Overview chart Page load time, plus breakdown into segments Error rate CPU usage Memory usage Browser Apdex, for browser Browser page load time Throughput total. After 90 days, there is no breakout by browser type, and only the combined average value is available for the time range requested. Mobile Crash rate: For iOS and for Android Network throughput Network response time Network error rates Interaction traces Trace data: reported by APM, Browser, and Mobile Depending on the product, New Relic retains different types of trace data: Product Trace data APM Types of trace data: Transaction traces Distributed tracing Slow query samples Error details See APM data retention details. Browser Types of trace data: Session traces Browser traces JavaScript errors* See Browser data retention details. Mobile Types of trace data: App crash stack traces Interaction traces See Mobile data retention details. * JavaScript errors in the stack trace UI are saved as trace data. JS errors are also saved as events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.8205,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of <em>data</em> retention (original pricing model)",
        "sections": "<em>Metric</em> <em>timeslice</em> <em>data</em>: reported by <em>APM</em>, Browser, and Mobile",
        "tags": "Original <em>data</em> retention",
        "body": " <em>timeslice</em> <em>data</em>, key metrics, trace <em>data</em>, and event <em>data</em>. In addition to retention limits, your <em>data</em> is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate <em>metric</em> <em>data</em> description. <em>APM</em> <em>data</em> retention policies For accounts on our original product"
      },
      "id": "6043f75364441f6967378ec6"
    },
    {
      "sections": [
        "Introduction to New Relic REST API (v2)",
        "Setup",
        "Tip",
        "URL",
        "API key $API_KEY",
        "Query details (PAYLOAD)",
        "Examples"
      ],
      "title": "Introduction to New Relic REST API (v2)",
      "type": "docs",
      "tags": [
        "APIs",
        "REST API v2",
        "Get started"
      ],
      "external_id": "97d12808fc706366121b8c005edc2320a0c7797b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/rest-api-v2/get-started/introduction-new-relic-rest-api-v2/",
      "published_at": "2022-01-12T08:42:15Z",
      "updated_at": "2022-01-08T02:34:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's REST APIs let you retrieve data from, and push data to New Relic tools, and include configuration and delete capabilities. You can also use the API Explorer to understand the data available to you via the REST API, to obtain curl commands, and to see JSON responses. Setup The REST API command structure follows this template: curl -X GET <URL> -H \"Api-Key:$API_KEY\" -d '<PAYLOAD>' Copy The GET command could also be a POST or DELETE, depending on the query intent. To understand the placeholders, keep reading. Tip Our examples use curl as a common command line tool to pull metric timeslice data from the REST API. However, you can use any method to make your REST requests. The curl commands include target URLs, header information, and data which are relevant for any request mechanism. URL The API calls require a URL to specify the location from which the data will be accessed. You must replace the placeholder <URL> with the appropriate URL which will change depending on the type of data being requested. In general the URL follows this template: https://api.newrelic.com/v2/applications/$APP_ID/metrics/data.json Copy The $APPID specifies the exact application or product for which the data is being requested. The information following this parameter will vary depending on the data request. If you have an EU region account, the URL is: api. eu .newrelic.com/v2/applications/$APP_ID/metrics/data.json Copy Tip You can retrieve XML data instead of JSON by replacing .json with .xml. API key $API_KEY New Relic API calls require an API key. This may be one of several API keys: A user key: This is recommended. It's our latest key implementation and has fewer limitations than a REST API key. The user key is also used for our NerdGraph API. A REST API key: This is our older key implementation. For more information about it, see REST API key. If you use this key, the required header is X-Api-Key and not Api-Key. If you have a New Relic partnership account, you'll use a different key: see Partnership authentication. In our REST API examples, we borrow the API key placeholder $API_KEY from Unix shell programming. Be sure to replace that and other user-specific variables when forming calls. Query details (PAYLOAD) The < PAYLOAD> contains the query details, which define: The metric name you want to query and the value you want to retrieve The defined time range for retrieving metrics (Optional): The average of the metric timeslice data by using summarize Examples See the following docs for example REST API use cases: APM examples (how to retrieve metric timeslice data from APM) Browser examples (how to retrieve metric timeslice data from browser monitoring). The REST API only returns the Lite browser agent script. For more information, see this table comparing Lite, Pro, and Pro+SPA types. Infrastructure alert examples Alerts examples (create alert conditions and configure notification channels, and more)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.57486,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> details (PAYLOAD)",
        "body": " for retrieving metrics (Optional): The average of the <em>metric</em> <em>timeslice</em> <em>data</em> by using summarize Examples See the following docs for example REST API use cases: <em>APM</em> examples (how to retrieve <em>metric</em> <em>timeslice</em> <em>data</em> from <em>APM</em>) Browser examples (how to retrieve <em>metric</em> <em>timeslice</em> <em>data</em> from browser monitoring"
      },
      "id": "604428f528ccbc84422c60ef"
    }
  ],
  "/docs/data-apis/understand-data/metric-data/query-metric-data-type": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "48ab117ae50533224877d767224d85edd939db42",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2022-01-12T03:54:07Z",
      "updated_at": "2021-10-24T00:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Want to try out our StatsD integration? Create a New Relic account for free! No credit card required. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need a license key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query of the NrIntegrationError event: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic license key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_LICENSE_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_LICENSE_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=NEW_RELIC_LICENSE_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 397.93054,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Metric</em> <em>types</em>",
        "body": " <em>query</em> option. For example, you might run a NRQL <em>query</em> like: SELECT count(*) FROM <em>Metric</em> WHERE <em>metric</em>Name = &#x27;my<em>Metric</em>&#x27; and environment = &#x27;production&#x27; Copy For more on how to <em>query</em> the <em>Metric</em> <em>data</em> <em>type</em>, see <em>Query</em> <em>metric</em> <em>data</em>. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it."
      },
      "id": "6174af22e7b9d253c613b73d"
    },
    {
      "sections": [
        "Transition to New Relic One from Insights",
        "Important",
        "Features",
        "Improved query abilities",
        "Improved visualizations",
        "Steps for a successful transition"
      ],
      "title": "Transition to New Relic One from Insights",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "Core concepts"
      ],
      "external_id": "4af99cd8030909a71d21a359a60af5ac93b93a66",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/core-concepts/transition-new-relic-one-insights/",
      "published_at": "2022-01-12T03:17:18Z",
      "updated_at": "2021-07-21T20:51:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we're upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. Released in 2014, New Relic Insights was our original way to create custom queries, charts, and dashboards. With New Relic One, we have modernized the experience for you to access, analyze, and visualize your data. New Relic One offers an improved charts and dashboards experience, and it provides a platform where we can more rapidly bring new innovations to you. This transition guide can help you understand: What are some of the new and improved features you get with New Relic One charts, dashboards, and queries Why it's easy to transition to New Relic One What to know and considerations when you make the switch How to get the most out of using New Relic One Features You can scroll down to the transition details, but first here are some features we've added that show how New Relic One dashboards are a clear improvement over Insights dashboards. Improved query abilities With New Relic One, you get: Ability to query many accounts from the same widget: New Relic One lets you query across all your associated accounts in one place. Better querying and charting experiences: Query access is available globally, no matter where you are in New Relic One. Learn how to browse and query data in New Relic. Improved query experience: You can query both the Metric data type and metric timeslice data. Easy customization: Every visualization now has the query accessible. You can augment any curated chart just by changing the NRQL query. Improved visualizations Not only can you select a wide range of visualization options, you can also add more to your dashboards: Better display options: Make your data easier to understand by using visualizations other than dense, line-heavy charts. New Relic One also offers a better TV mode. Facet linking: You can filter your dashboards by faceted attributes, making your dashboards more interactive and easy to use. There's also support for cases. Learn more. More charts or widgets in an area: Insights restricted you to a 3-across limit. Now you can display up to 12 across your dashboard, providing increased data density along with improved tooltips and tracking across charts. Easier creation of multi-page dashboards: Insights referred to these as data apps. Your Insights data apps are preserved as multi-page dashboards in New Relic One. Chart consistency and flexibility: Dashboards include facet color consistency across widgets and faster loading times for more performant dashboards. Also, you can add any chart type to a dashboard in New Relic One! The New Relic Insights UI has served our users well for many years, but it's time to give you an even better experience. Join us and make the switch to New Relic One! Steps for a successful transition The transition to New Relic One has two parts: the UI and mobile app experience (April 12, 2021) and the Dashboard API (July 2021). Insights functionality Transition to New Relic One UI We have already taken care of your transition from Insights to New Relic One for you! As of April 12, 2021, your old Insights URLs redirect automatically to New Relic One. We recommend that you familiarize yourself with the new UI features available to you, as described in this transition guide. If you need to view any Insights charts embedded in other websites, go to one.newrelic.com > More > Manage Data. (These older embedded charts will continue to function as expected.) Mobile apps The Insights mobile app is deprecated as of April 11, 2021. Go to the Google Play Store 2 or Apple App store. Delete your old Insights mobile app, and download the New Relic One mobile app. tvOS apps and large displays New Relic's tvOS app is still available. No action is needed by you at this time. Some New Relic customers with the original pricing model may have set up dashboards on wall screens for restricted users with kiosk mode. No action is required for you to continue to view these dashboards. APIs In July of 2021, the Insights Dashboard API will be deprecated and replaced with NerdGraph functionality. For more on this change, and tips on how to migrate, see NerdGraph API for dashboards. Partnership accounts This applies only if your account is one of the few using our partnership account structure to deliver New Relic services to your direct customers. In this situation, the Insights EOL will not affect your customers’ pricing. This is simply an EOL for the UI, not an EOL for the account type. Questions If you have questions about the transition, please comment in our Explorers Hub post. Or, if you work with an account team, they will be happy to help you.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.84195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Improved <em>query</em> abilities",
        "body": " to browse and <em>query</em> <em>data</em> in New Relic. Improved <em>query</em> experience: You can <em>query</em> both the <em>Metric</em> <em>data</em> <em>type</em> and <em>metric</em> timeslice <em>data</em>. Easy customization: Every visualization now has the <em>query</em> accessible. You can augment any curated chart just by changing the NRQL <em>query</em>. Improved visualizations Not only"
      },
      "id": "6044171164441f454a378ee2"
    },
    {
      "sections": [
        "Analyze and monitor data trends with metrics",
        "Why create metrics from other data types?",
        "Available operations",
        "Mutations",
        "Create a rule",
        "Delete a rule",
        "Important",
        "Enable or disable a rule",
        "Queries",
        "List all rules for a New Relic account",
        "List rule by rule ID",
        "Use the NerdGraph GraphiQL API tool"
      ],
      "title": "Analyze and monitor data trends with metrics",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Convert data to metrics"
      ],
      "external_id": "06073bcfec2679ac1bc402dfe305426bbd9e2182",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/convert-to-metrics/analyze-monitor-data-trends-metrics/",
      "published_at": "2022-01-12T06:17:06Z",
      "updated_at": "2021-10-23T17:28:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can generate metric-type data from other types of data in New Relic, including events, logs, and spans. Metrics are aggregates of your data and are optimal for analyzing and monitoring trends over long time periods. This document explains: Reasons to use this feature Available operations How to use our NerdGraph API tool to perform operations Why create metrics from other data types? Using metrics allows for more efficient data storage. This in turn allows you to query your data and build charts more easily. The difference between metrics and other types of data in New Relic is based on time. For more information, see Understand data types. Events, logs, spans: These types of data represent a single record at a specific moment in time. For example, you may have an event for every request to the system. This data is ideal for in-depth troubleshooting and analysis. Metrics: These provide an aggregated view of your events, logs, or spans. Metrics are better for showing trends over longer time ranges. For example, you can aggregate the total number of requests per service to one metric and then examine this information month over month. Why use metrics? Comments Flexibility Metrics are dimensional. You can choose what metadata (like host name or app name) is attached to them. Common metric measurements, like average, sum, minimum, and maximum, are already calculated. Data aggregation and retention The data has already been pre-aggregated into longer-period time buckets. Data retention is 13 months. Query capabilities You can query using the Metric data type. When you create metrics, this does not delete your events or other types of data. However, metrics are better for longer-range querying and charting. To get started converting your data to metrics, create a rule. Available operations To show, create, and delete rules for generating metrics from events, logs, or spans, use NerdGraph, our GraphQL-format API. Before performing any operation, we recommend reading Intro to NerdGraph and exploring your data with the GraphiQL API tool. These operations fall under two basic request types: Mutations, which are operations that make changes to existing rules or settings (for example, creating a new metrics rule). Queries, for fetching existing data (for example, fetching existing metrics rules). All operations are role-based in NerdGraph as the currently logged-in New Relic user. Mutations Mutation operations for events to metrics, logs to metrics, or spans to metrics include: Create a rule See Create metrics. Delete a rule Important This operation modifies production settings, so we recommend thoroughly reviewing your changes before you run the operation. To delete a rule, you need the rule ID and the New Relic account ID. Example request: mutation { eventsToMetricsDeleteRule(deletes: {ruleId: \"12\", accountId: 123456}) { successes { id name nrql } failures { errors { description reason } submitted { ruleId accountId } } } } Copy In this request: Element Description mutation One of the basic API operation types. eventsToMetricsDeleteRule The method being called to delete a rule. deletes This takes two parameters: ruleId: The ID of the rule for events to metrics, logs to metrics, or spans to metrics. accountId: The New Relic account ID. successes and submitted blocks Here you define the data returned by a success or failure. Available parameters for these blocks: id (or ruleId for submitted) name description nrql enabled accountId Example response for the request: { \"data\": { \"eventsToMetricsDeleteRule\": { \"failures\": [], \"successes\": [ { \"id\": \"12\", \"name\": \"Test Rule\", \"nrql\": \"select summary(duration) as 'server.responseTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } Copy Enable or disable a rule Important This operation modifies production settings, so we recommend thoroughly reviewing your changes before you run the operation. To enable or disable an existing rule for events to metrics, logs to metrics, or spans to metrics, use the same eventsToMetricsUpdateRule operation. The only difference is whether enabled is set to true or false. Example request to enable an existing metrics rule: mutation { eventsToMetricsUpdateRule(updates: {ruleId: \"12\", accountId: 123456, enabled: true}) { successes { id name nrql } failures { errors { description reason } submitted { ruleId accountId } } } } Copy In this request: Element Description mutation One of the basic API operation types. eventsToMetricsUpdateRule The method being called to update an existing rule and either enable it or disable it. updates This takes three required parameters: ruleId: The ID of the rule for events to metrics, logs to metrics, or spans to metrics. accountId: The New Relic account ID. enabled: To enable a disabled rule, set this to true. To disable a rule, set this to false. successes and submitted blocks Here you define the data returned by a success or failure. Available parameters for these blocks: id (or ruleId for submitted) name description nrql enabled accountId Queries Query operations include: List all rules for a New Relic account You can list all rules in a New Relic account or return a specific rule. Example listing all rules for account 123456: query { actor { account(id:123456) { eventsToMetrics{ allRules{ rules{ id name enabled nrql description } } } } } } Copy In this request: Element Description query One of the basic API operation types. Used to query but not make changes. actor This specifies the current New Relic user. account(id: 123456) Specify the ID for the New Relic account where to retrieve data. eventsToMetrics Scope the data only for events-to-metrics, logs-to-metrics, or spans-to-metrics rules. allRules Returns all rules for that account. rules In the rules block, you can define what data you want returned. Available fields include: id name description nrql accountId enabled Example response: { \"data\": { \"actor\": { \"account\": { \"eventsToMetrics\": { \"allRules\": { \"rules\": [ { \"description\": \"Metric for total time\", \"enabled\": true, \"id\": \"1\", \"name\": \"Total Time Tx\", \"nrql\": \"select summary(totalTime) as 'server.totalTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" }, { \"description\": \"Metric for duration\", \"enabled\": true, \"id\": \"2\", \"name\": \"Duration Rule\", \"nrql\": \"select summary(duration) as 'server.responseTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } } } } Copy List rule by rule ID If you know the exact ID for a rule, then you can query for a specific rule. For example, you may have just created a rule and now you want to list its contents so you can review it. Example listing rule 36 for New Relic account 123456: query { actor { account(id: 123456) { eventsToMetrics { rulesById(ruleIds: \"36\") { rules { id name enabled nrql description accountId } } } } } } Copy For more details about the elements in this query, see List all rules. Example response: { \"data\": { \"actor\": { \"account\": { \"eventsToMetrics\": { \"rulesById\": { \"rules\": [ { \"accountId\": 123456, \"description\": \"Metric for total time\", \"enabled\": true, \"id\": \"36\", \"name\": \"Total Time Tx\", \"nrql\": \"select summary(totalTime) as 'server.totalTime' from Transaction where appName = 'Data Points Staging' facet name, appName, host\" } ] } } } } } } Copy Use the NerdGraph GraphiQL API tool You can use our GraphiQL tool to explore the data structure. You can also use it to build and run the operations to convert events, logs, and spans to metrics. To use this tool: Create the metrics operation's request with the required parameters. Go to api.newrelic.com/graphiql, and paste your query into the box. To execute the operation, press Play. Or, to get the cURL format, select Copy as cURL.) Validate the response in the response box. Optional: To verify that your rule-creation operation was performed successfully, run a list query for that rule ID.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.58267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Analyze and monitor <em>data</em> trends with <em>metrics</em>",
        "sections": "Why create <em>metrics</em> from other <em>data</em> <em>types</em>?",
        "tags": "Convert <em>data</em> to <em>metrics</em>",
        "body": " retention is 13 months. <em>Query</em> capabilities You can <em>query</em> using the <em>Metric</em> <em>data</em> <em>type</em>. When you create metrics, this does not delete your events or other types of <em>data</em>. However, metrics are better for longer-range querying and charting. To get started converting your <em>data</em> to metrics, create a rule"
      },
      "id": "603eb239e7b9d2b99d2a07bb"
    }
  ],
  "/docs/data-apis/understand-data/new-relic-data-types": [
    {
      "sections": [
        "Understand and manage data ingest",
        "Data ingestion UI",
        "Data ingestion sources",
        "Understand where data is coming from",
        "How ingested data is broken down",
        "Set alerts for data use",
        "Adjust your data ingest",
        "Drop unwanted data",
        "Disable agents and integrations",
        "Adjust APM data ingest",
        "Adjust infrastructure data ingest",
        "Adjust log data ingest"
      ],
      "title": "Understand and manage data ingest",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "f1c46558041c874a2076f781fa975a21105f60e4",
      "image": "https://docs.newrelic.com/static/82d3c36157005ac0efe40cd6e10fe06b/b23ad/data-facet.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-data-coming-new-relic/",
      "published_at": "2022-01-12T06:18:34Z",
      "updated_at": "2022-01-08T03:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you report data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our New Relic One pricing model, you're charged for the number of bytes written to our database that are above and beyond the free per-month amount. If you're trying to estimate the cost of your data ingest, see Calculate data ingest. Data ingestion UI To learn how to easily analyze the data your account is ingesting, watch this short video (3:18 minutes). The Data ingestion tab is located in the Data management UI. The Data ingestion UI shows your ingest rates for the time period specified by the time picker in the upper right. The page shows your daily average GBs, and the total GBs for that time range. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively manage your data ingest in various ways. To see the underlying NRQL query that is used to generate the chart, click View query. From the account dropdown, select Manage your data, and then select Data ingestion. For how to get more details about ingested data, see Get ingest details. Data ingestion sources The data ingestion UI chart shows you a high level breakdown of your billable data usage. The table below explains those sources. In this table, \"usage metric group\" refers to the value of that source's usageMetric attribute value on the NrConsumption event. Data sources Description Metrics In the data ingestion chart, Metrics is a combination of two types of metrics: metric timeslice data and dimensional metrics. Usage metric group: MetricsBytes. Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days. You are only billed for the initial ingest volume. You are not billed for subsequent rollups. APM This includes APM events, like Transaction and TransactionError. Usage metric group: ApmEventsBytes. Infrastructure Includes several categories of infrastructure monitoring events, described below. Infrastructure host data. Usage metric group:InfraHostBytes. Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data. Infrastructure process data stored in ProcessSample. Usage metric group: InfraProcessBytes. Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see Process metrics. Infrastructure integrations. Usage metric group: InfraIntegrationBytes. Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP. Logging Includes logs and any Log_<value> custom data partition created. Usage metric group: LoggingBytes. Log records are stored on the Log data type by default. Additional custom data partitions will create new data types, which are always prefixed with Log_ and are counted as part of the overall set of log data stored. With LogExtendedRecord, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data. As of September 2021, log storage as blobs replaces LogExtendedRecord. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our log blobs docs. Default Custom events. Usage metric group: CustomEventsBytes Mobile events Mobile events, including the general Mobile event, MobileRequestError, MobileBreadcrumb, MobileSession, MobileHandledException, MobileCrash. Usage metric group: MobileEventsBytes. Tracing Usage metric group: TracingBytes. This includes the Span data type and OpenTelemetry's SpanEvent. You are not charged for DistributedTraceSummary events. Browser events Browser events, including the namespaces of Browser, Browser:EventLog, Browser:JSErrors, and PcvPerf (PageView timing). Usage metric group: BrowserEventsBytes. Lambda AWS Lambda events. Usage metric group: ServerlessBytes. Understand where data is coming from You can inspect your data ingest to gain more information about your ingest health. From the data ingestion UI page, you can analyze your usage in more detail. Spending some time understanding your ingested data and where it comes from and how it changes over time can be valuable. You'll know your ingest baselines, and you'll be able to more easily spot anomalies, like ingest spikes, and understand their source. On the data ingestion chart, time is on the X axis and the bands representing data sources are located along the Y axis. Click on a data source band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. This image shows the data source band for June 15 right before it's clicked. A modal opens with the account, data source, and facet selected. You can do a handful of things on this page: Change the account, data source, or facet you want to drill down into. Change the time range. Review the results of the query in chart form. The chart displays the top 15 results for the facet query. Open the NRQL query in the Query builder where you'll find additional facets that you can use. For more about creating more detailed queries: Learn some NRQL basics. See some example usage-related queries. How ingested data is broken down Some of the usage data in this UI can vary depending on your account. This information is intended to help you understand how we're working with your ingest data: The chart on the Data ingestion page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem. If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a one hour period. The ingest value provided on the main Data ingestion chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate. Set alerts for data use For how to set alerts that will notify you when you're reaching data ingest levels you don't want to cross, see Query and alert on usage data. For example, you might set an alert on logs, which can accumulate quickly in an active system. Adjust your data ingest Here are some ideas for managing your data: Drop unwanted data On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional data dropping rules yourself. For how to drop log data, see Drop log data. Disable agents and integrations If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool. Adjust APM data ingest Options for adjusting APM data include: Configure the sampling rate for transaction events. Set appropriate Apdex scores, for example, for frequency of traces. Optimize custom instrumentation and/or custom metrics. Adjust infrastructure data ingest Options for adjusting infrastructure data include: Adjust sampling rate for network, storage, and system events. Disable process metrics. Adjust polling intervals: Polling for cloud integrations. For on-host integrations: edit the configuration file for a specific integration. Control the reporting of specific attributes. Manage Kubernetes events integration. Adjust log data ingest Options for adjusting log data ingest include: Use the log forwarder to filter log events on the sending side. Drop log data, either via the UI or with NerdGraph.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.7283,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Understand</em> <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "sections": "<em>Understand</em> <em>and</em> <em>manage</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " to identify which of your accounts are sending the most <em>data</em>. The page also provides the current month-to-date, and the projected end-of-month total <em>ingest</em> rates. With this information, you can proactively <em>manage</em> your <em>data</em> <em>ingest</em> in various ways. To see the underlying NRQL query that is used to generate"
      },
      "id": "603e978228ccbc8984eba79e"
    },
    {
      "sections": [
        "Report metrics via the Metric API",
        "Quick start: Send metric data",
        "Endpoint URL",
        "Tip",
        "HTTP request headers",
        "HTTP request body",
        "Structure",
        "JSON payload creating two metrics",
        "Required key-value pairs",
        "JSON payload with three metric types",
        "Share attributes across metrics with common",
        "Example of common attributes",
        "Response validation and status codes",
        "Missing data with 202 response",
        "Status codes"
      ],
      "title": "Report metrics via the Metric API",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "ad2f24e880ef009b256115c0db440b4bb4de9c7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/data-apis/ingest-apis/metric-api/report-metrics-metric-api/",
      "published_at": "2022-01-12T02:25:45Z",
      "updated_at": "2022-01-08T12:41:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric data. Quick start: Send metric data We report the metric types count, gauge, and summary. For more information on metrics see our documentation. Metric data is submitted to New Relic through an HTTP POST request. Each request is composed of one or more metric data points, which consist of a metric name, a timestamp, and a value. Follow this example to send your first metric data points to New Relic: Get the license key for the account you want to report data to. Insert the license key into the following JSON, and then send the JSON to our endpoint. Update the timestamp with a valid epoch timestamp. fix(Metric API): Remove unnecessary indentation This example creates a single metric data point for a metric named memory.heap, but you can create additional attributes or data points by specifying metric types or adding optional common blocks. curl -vvv -k -H \"Content-Type: application/json\" \\ -H \"Api-Key: NEW_RELIC_LICENSE_KEY\" \\ -X POST https://metric-api.newrelic.com/metric/v1 \\ --data '[{ \"metrics\":[{ \"name\":\"memory.heap\", \"type\":\"gauge\", \"value\":2.3, \"timestamp\":CURRENT_TIME, \"attributes\":{\"host.name\":\"dev.server.com\"} }] }]' Copy The metric should be available in New Relic in a few seconds. You can query the data from any NRQL interface using this query: FROM Metric SELECT max(memory.heap) TIMESERIES Copy For more on where data shows up, see Find Metric API data. Endpoint URL Use an HTTP POST when sending metric data to the metric API endpoint: https://metric-api.newrelic.com/metric/v1 Copy Tip If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. HTTP request headers Include the following HTTP request headers with the POST request. You can send some parameters as query parameters instead of request headers. Header Send as a query parameter? Details Content-Type No Required. Must be application/json. Content-Length No Required (usually set automatically by the HTTP client). The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes Required. A license key for the account you want to report data to. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if GZIP. The value must be GZIP or Identity. If no value is present, then Identity is assumed. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. HTTP request body The body of the HTTP POST request must be in JSON format. The following describes the requirements and recommendations for the JSON payload. The payload must be encoded as UTF-8. Structure The JSON payload uses this structure: The JSON payload is an array of maps. Each map must contain a metrics key whose value is an array containing one or more metric data points. A metric data point is identified by a name, value, and timestamp along with an optional set of attributes. JSON payload creating two metrics This example payload creates two metrics. service.errors.all is a count metric with three attributes and service.memory is a gauge metric with two attributes. [ { \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"service.response.statuscode\": \"400\", \"host.name\": \"dev.server.com\", \"service.name\": \"foo\" } }, { \"name\": \"service.memory\", \"type\": \"gauge\", \"value\": 2.7, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Required key-value pairs Each metric data point map in the metrics array uses the following key-value structure: Key Description name string Required. The name of the metric. The value must be less than 255 characters. value number or map Required. The value varies depending on the metric type. For gauge and count the value should be a single number. For summary, the value should be a map with key-value pairs specifying the count, sum, min, and max. timestamp long Required. The metric's start time in Unix time. Defaults to the current time in UTC timezone. This field also support seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and query. Metrics reported with a timestamp older than 48 hours ago or newer than 24 hours from the time they are reported are dropped. interval.ms positive long Required for count and summary metric types. The length of the time window. type Recommended. This should be one of the supported metric types. If you do not specify a type, then this will default to a gauge. attributes strings, JSON numbers, or booleans Recommended. A map of key value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Keys are case-sensitive and must be less than 255 characters. JSON payload with three metric types Here's an example payload containing one metric data point for each metric type: [ { \"metrics\": [ { \"name\": \"cache.misses\", \"type\": \"count\", \"value\": 15, \"timestamp\": [CURRENT_TIME](#optional-map-attributes), \"interval.ms\": 10000, \"attributes\": { \"cache.name\": \"myCache\", \"host.name\": \"dev.server.com\" } }, { \"name\": \"temperature\", \"type\": \"gauge\", \"value\": 15, \"timestamp\": CURRENT_TIME, \"attributes\": { \"city\": \"Portland\", \"state\": \"Oregon\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"interval.ms\": 10000, \"timestamp\": CURRENT_TIME, \"attributes\": { \"host.name\": \"dev.server.com\", \"app.name\": \"foo\" } } ] } ] Copy Share attributes across metrics with common If you want to include a set of attributes on multiple metrics (and not add the same attributes for each metric), you can use the common block. This is an optional map that specifies information that applies to all associated metric data points. Values in the common section will be overridden if the same key exists on a metric data point. The block can include: Attribute Description timestamp long The metric's start time in Unix time. This defaults to the current time in the UTC timezone. This field also supports seconds, microseconds, and nanoseconds. However, the data will be converted to milliseconds for storage and later querying. interval.ms positive long Required for count and summary.The length of the time window. attributes strings, JSON numbers, or booleans A map of key-value pairs associated with this specific metric. Values can be strings, JSON numbers, or booleans. Example of common attributes In the following example payload, three metrics are sent. All three metrics share app.name and host.name attributes, specified in the common block. Each metric also has a unique value for another attribute, server.response.statuscode. [ { \"common\" : { \"timestamp\": 1531414060739, \"interval.ms\": 10000, \"attributes\": { \"app.name\": \"foo\", \"host.name\": \"dev.server.com\" } }, \"metrics\": [ { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 9, \"attributes\": { \"service.response.statuscode\": \"400\" } }, { \"name\": \"service.errors.all\", \"type\": \"count\", \"value\": 4, \"attributes\": { \"service.response.statuscode\": \"500\" } }, { \"name\": \"service.response.duration\", \"type\": \"summary\", \"value\": { \"count\": 5, \"sum\": 0.004382655, \"min\": 0.0005093, \"max\": 0.001708826 }, \"attributes\": { \"service.response.statuscode\": \"200\" } } ] } ] Copy Response validation and status codes The Metric API returns a 202 response code for successful requests. When your data is accepted, an HTTP 202 response code is returned with a response structure like this: HTTP/1.1 202 Accepted Content-Type: application/json; charset=UTF-8 Content-Length: 52 Access-Control-Allow-Methods: GET, POST, PUT, HEAD, OPTIONS Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: * Connection: keep-alive {\"requestId\":\"f0e7bfff-001a-b000-0000-01682bcf4565\"} Copy Missing data with 202 response A 202 code indicates the API did receive your data, and that the data passed basic validation checks. Normally, your data will be available for querying within a few seconds. However, New Relic runs additional validation asynchronously after receiving your data. If you receive a 202 response but can't find your metric, this indicates that New Relic found an error during this asynchronous validation. You can find these errors by querying NrIntegrationError events in the account associated with the Insert API key you used. The requestId for each request will be tagged on the NrIntegrationError event. For more information, see Troubleshoot an NRIntegrationError event. Status codes The Metric API can return the following HTTP status codes: Status code Definition 202 Data accepted. 400 Structure of the request is invalid. 403 Authentication failure. 404 The request path is incorrect. 405 Used a request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. Payloads must be under 1MB (10^6 bytes). 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.76083,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick start: Send metric <em>data</em>",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": "Use the Metric API to send custom metrics to the New Relic platform. This document includes a quick start to send your first custom metric, plus detailed information on how to format and send your metric <em>data</em>. Quick start: Send metric <em>data</em> We report the metric types count, gauge, and summary"
      },
      "id": "6107859064441f8baf47abd9"
    },
    {
      "sections": [
        "New Relic's data management hub",
        "Important",
        "Where to find the data management hub",
        "Better cost, performance, and compliance",
        "Cost management",
        "Performance management",
        "Ingest and retention strategies"
      ],
      "title": "New Relic's data management hub",
      "type": "docs",
      "tags": [
        "Ingest and manage data",
        "Manage data"
      ],
      "external_id": "999fa6106dd47250e9a5d822aa2f92b6ea088c78",
      "image": "https://docs.newrelic.com/static/8a553ce9643c8513be3200af5d924250/c1b63/datamanagement_overview.png",
      "url": "https://docs.newrelic.com/docs/data-apis/manage-data/manage-your-data/",
      "published_at": "2022-01-12T06:19:11Z",
      "updated_at": "2022-01-08T02:14:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, we're super proud of NRDB, the New Relic database where we store your data. It gathers all your telemetry data in one place, gives you a connected view of all your data, and scales as your business grows. We invite you to send all your metrics, events, logs, and traces to NRDB, including those from third-party sources. We also recognize that some data might not be necessary for your business goals. You shouldn’t have to wade through data you don’t need to get to the data you do. And you definitely shouldn’t have to pay for it. That’s where our data management tools come in: they let you decide what data you send to New Relic and how long it should be stored. Data management hub: from the account dropdown in the top right of the UI, select Manage your data. Coupled with user management tools, data management helps you get maximum value from your investment in New Relic, all while safeguarding your data. Important Not yet a New Relic customer? Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Where to find the data management hub To find the data management UI: From one.newrelic.com click the account dropdown, and the click Manage your data. Better cost, performance, and compliance Collecting and storing data in New Relic allows you to analyze, visualize, and alert on all your metrics, events, logs, and traces from across all of your sources. However, it’s important to manage that data for cost, performance, and in some cases, compliance reasons. Our data management hub provides the tools you need to understand and control where your data is coming from, and adjust what’s stored and for how long. Important If you're on our original product-based pricing model, you'll see your data ingest, retention, and limits in the data management hub. But on the original pricing plan, you're not billed on data ingest. Not sure which plan you're on? See Overview of pricing and user model. Cost management Our ingest process helps you hone your data. For example, data might arrive at our processing front door compressed and of varying quality. Through ingest, that data is uncompressed, decorated with queryable attributes, and evaluated. Elements are dropped or trimmed, all before we write it to NRDB. That way, the data you store is only the data you want most. Want to estimate your data ingest and cost? See Calculate data ingest. Performance management While NRDB is a phenomenally scalable database, it’s also a reality that queries across huge datasets might not return results in a timely enough fashion for your needs. You get better performance if you limit the data we store, or convert it into a format that keeps it easily queryable. One solution for improving performance is to drop data to reduce the amount of data stored. Ingest and retention strategies Depending on your goals, whether to reduce costs, increase specific retention rates, or pare down your data to what’s most essential, we have a strategy for you. Learn about adjusting data ingest. Learn about dropping data. For dropping logs, see Drop log data. Learn how to adjust how long data is retained.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.00418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "sections": "New Relic&#x27;s <em>data</em> <em>management</em> hub",
        "tags": "<em>Ingest</em> <em>and</em> <em>manage</em> <em>data</em>",
        "body": " for cost, performance, and in some cases, compliance reasons. Our <em>data</em> management hub provides the tools you need to <em>understand</em> and control where your <em>data</em> is coming from, and adjust what’s stored and for how long. Important If you&#x27;re on our original product-based pricing model, you&#x27;ll see your <em>data</em> <em>ingest</em>"
      },
      "id": "603e96ff28ccbcf8bceba796"
    }
  ],
  "/docs/distributed-tracing/concepts/distributed-tracing-planning-guide": [
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.94511,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to <em>get</em> <em>started</em> right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting <em>started</em>? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.95609,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Infinite <em>Tracing</em> feature. If you are just getting <em>started</em> with <em>distributed</em> <em>tracing</em>, see our setup options. What is Infinite <em>Tracing</em>? Infinite <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on Infinite <em>Tracing</em> to make sampling decisions. You can configure Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.70053,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling Infinite <em>Tracing</em>. To learn more about this, see Intro to Infinite <em>Tracing</em> and Sampling considerations. To <em>get</em> <em>started</em> using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    }
  ],
  "/docs/distributed-tracing/concepts/how-new-relic-distributed-tracing-works": [
    {
      "sections": [
        "Distributed tracing: Planning guide",
        "Impact to APM features",
        "External services page has less detail",
        "Transaction trace UI displays service URLs, not transaction links",
        "Cross-application tracing will be disabled",
        "Impacts related to mobile monitoring",
        "Plan your rollout",
        "Enable distributed tracing"
      ],
      "title": "Distributed tracing: Planning guide",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "10263f2b6ec929f082153e28cbce07fe3a1f106a",
      "image": "https://docs.newrelic.com/static/7072dfa9e494767baabba420d78e7094/c1b63/distributed-trace-txn-trace-affects.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/distributed-tracing-planning-guide/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2022-01-08T03:46:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are new to New Relic distributed tracing, we recommend you read the following before you enable distributed tracing. Impact to APM features Our distributed tracing improves on APM's previous cross application tracing feature. Here are some key benefits: See more cross-service activity details and more complete end-to-end traces. Filter and query traces, as well as make custom charts. See the complete trace even when calls cross an organization's account boundaries. See Introduction to distributed tracing for more features. Enabling distributed tracing may affect some APM features you currently use. These changes affect only applications monitored by agents that have distributed tracing enabled—they don't apply on an account-level. We may provide backward compatibility with some or all of the affected features in future releases. For now, you should understand the following changes before enabling distributed tracing: External services page has less detail When distributed tracing is enabled for an application, external calls do not have internal transaction details at one.newrelic.com > APM > (select an app) > Monitor > External services > (select external service). To find that information, you would instead go to the Distributed tracing UI page, find the external call URLs, and see what their child spans are. Transaction trace UI displays service URLs, not transaction links When distributed tracing is enabled for an application, the transaction trace UI will no longer have the transaction name and link for the called service (see screenshot below). This will be replaced with the called service's URL. one.newrelic.com > APM > (select an app) > Monitor > Transactions > (select a transaction trace): This shows the transaction trace UI before distributed tracing is enabled, with a link to the associated transaction. With distributed tracing enabled, it will display the service's URL. If you wanted to get more detail about trace activity, you would go to the Distributed tracing UI page and examine that trace. Cross-application tracing will be disabled Enabling distributed tracing will disable the cross application tracing feature. Distributed tracing is an improved version of cross-application tracing and only one can be enabled at a time. Impacts related to mobile monitoring APM-related impacts include: When distributed tracing is enabled for an APM-monitored entity, legacy service maps will not show applications monitored by mobile. The App server drill-down feature of the legacy mobile HTTP requests UI page is not available. Plan your rollout If you're enabling distributed tracing for a large, distributed system, here are some tips: If you are a current APM user, see Impact to APM features. Determine the requests that are the most important for your business, or the most likely to require analysis and troubleshooting, and enable distributed tracing for those services. Enable tracing for services at roughly the same time so you can more easily gauge how complete your end-to-end traces are. When you look at traces in the distributed tracing UI, you'll see spans in the trace for external calls to other services. Then, you can enable distributed tracing for any of those services you want. If a service is fairly standalone and not often used in context with other services, you may not want to enable distributed tracing for it. Here's a visual representation of such a phased roll-out: If you are using APM for a large, monolithic service, there may be many sub-process spans per trace and APM limits may result in fewer traces than expected. You can solve this by using APM agent instrumentation to disable the reporting of unimportant data. Distributed tracing works by propagating header information from service to service in a request path. Some services may communicate through a proxy or other intermediary service that does not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable distributed tracing If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up distributed tracing. See the setup options.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.81458,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "sections": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " is enabled, with a link to the associated transaction. With <em>distributed</em> <em>tracing</em> enabled, it will display the service&#x27;s URL. If you wanted to <em>get</em> more detail about <em>trace</em> activity, you would go to the <em>Distributed</em> <em>tracing</em> UI page and examine that <em>trace</em>. Cross-application <em>tracing</em> will be disabled"
      },
      "id": "6072a60564441f3a629d8535"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.94511,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to <em>get</em> <em>started</em> right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting <em>started</em>? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.95609,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Infinite <em>Tracing</em> feature. If you are just getting <em>started</em> with <em>distributed</em> <em>tracing</em>, see our setup options. What is Infinite <em>Tracing</em>? Infinite <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on Infinite <em>Tracing</em> to make sampling decisions. You can configure Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    }
  ],
  "/docs/distributed-tracing/concepts/introduction-distributed-tracing": [
    {
      "sections": [
        "Distributed tracing: Planning guide",
        "Impact to APM features",
        "External services page has less detail",
        "Transaction trace UI displays service URLs, not transaction links",
        "Cross-application tracing will be disabled",
        "Impacts related to mobile monitoring",
        "Plan your rollout",
        "Enable distributed tracing"
      ],
      "title": "Distributed tracing: Planning guide",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "10263f2b6ec929f082153e28cbce07fe3a1f106a",
      "image": "https://docs.newrelic.com/static/7072dfa9e494767baabba420d78e7094/c1b63/distributed-trace-txn-trace-affects.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/distributed-tracing-planning-guide/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2022-01-08T03:46:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are new to New Relic distributed tracing, we recommend you read the following before you enable distributed tracing. Impact to APM features Our distributed tracing improves on APM's previous cross application tracing feature. Here are some key benefits: See more cross-service activity details and more complete end-to-end traces. Filter and query traces, as well as make custom charts. See the complete trace even when calls cross an organization's account boundaries. See Introduction to distributed tracing for more features. Enabling distributed tracing may affect some APM features you currently use. These changes affect only applications monitored by agents that have distributed tracing enabled—they don't apply on an account-level. We may provide backward compatibility with some or all of the affected features in future releases. For now, you should understand the following changes before enabling distributed tracing: External services page has less detail When distributed tracing is enabled for an application, external calls do not have internal transaction details at one.newrelic.com > APM > (select an app) > Monitor > External services > (select external service). To find that information, you would instead go to the Distributed tracing UI page, find the external call URLs, and see what their child spans are. Transaction trace UI displays service URLs, not transaction links When distributed tracing is enabled for an application, the transaction trace UI will no longer have the transaction name and link for the called service (see screenshot below). This will be replaced with the called service's URL. one.newrelic.com > APM > (select an app) > Monitor > Transactions > (select a transaction trace): This shows the transaction trace UI before distributed tracing is enabled, with a link to the associated transaction. With distributed tracing enabled, it will display the service's URL. If you wanted to get more detail about trace activity, you would go to the Distributed tracing UI page and examine that trace. Cross-application tracing will be disabled Enabling distributed tracing will disable the cross application tracing feature. Distributed tracing is an improved version of cross-application tracing and only one can be enabled at a time. Impacts related to mobile monitoring APM-related impacts include: When distributed tracing is enabled for an APM-monitored entity, legacy service maps will not show applications monitored by mobile. The App server drill-down feature of the legacy mobile HTTP requests UI page is not available. Plan your rollout If you're enabling distributed tracing for a large, distributed system, here are some tips: If you are a current APM user, see Impact to APM features. Determine the requests that are the most important for your business, or the most likely to require analysis and troubleshooting, and enable distributed tracing for those services. Enable tracing for services at roughly the same time so you can more easily gauge how complete your end-to-end traces are. When you look at traces in the distributed tracing UI, you'll see spans in the trace for external calls to other services. Then, you can enable distributed tracing for any of those services you want. If a service is fairly standalone and not often used in context with other services, you may not want to enable distributed tracing for it. Here's a visual representation of such a phased roll-out: If you are using APM for a large, monolithic service, there may be many sub-process spans per trace and APM limits may result in fewer traces than expected. You can solve this by using APM agent instrumentation to disable the reporting of unimportant data. Distributed tracing works by propagating header information from service to service in a request path. Some services may communicate through a proxy or other intermediary service that does not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable distributed tracing If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up distributed tracing. See the setup options.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.81445,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "sections": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " is enabled, with a link to the associated transaction. With <em>distributed</em> <em>tracing</em> enabled, it will display the service&#x27;s URL. If you wanted to <em>get</em> more detail about <em>trace</em> activity, you would go to the <em>Distributed</em> <em>tracing</em> UI page and examine that <em>trace</em>. Cross-application <em>tracing</em> will be disabled"
      },
      "id": "6072a60564441f3a629d8535"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.95598,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Infinite <em>Tracing</em> feature. If you are just getting <em>started</em> with <em>distributed</em> <em>tracing</em>, see our setup options. What is Infinite <em>Tracing</em>? Infinite <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on Infinite <em>Tracing</em> to make sampling decisions. You can configure Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.70042,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling Infinite <em>Tracing</em>. To learn more about this, see Intro to Infinite <em>Tracing</em> and Sampling considerations. To <em>get</em> <em>started</em> using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    }
  ],
  "/docs/distributed-tracing/concepts/quick-start": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1109.0613,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Distributed</em> <em>tracing</em>",
        "body": " Infinite <em>Tracing</em> feature. If you are just getting started with <em>distributed</em> <em>tracing</em>, see our <em>setup</em> <em>options</em>. What is Infinite <em>Tracing</em>? Infinite <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on Infinite <em>Tracing</em> to make sampling decisions. You can configure Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Distributed tracing: Planning guide",
        "Impact to APM features",
        "External services page has less detail",
        "Transaction trace UI displays service URLs, not transaction links",
        "Cross-application tracing will be disabled",
        "Impacts related to mobile monitoring",
        "Plan your rollout",
        "Enable distributed tracing"
      ],
      "title": "Distributed tracing: Planning guide",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "10263f2b6ec929f082153e28cbce07fe3a1f106a",
      "image": "https://docs.newrelic.com/static/7072dfa9e494767baabba420d78e7094/c1b63/distributed-trace-txn-trace-affects.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/distributed-tracing-planning-guide/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2022-01-08T03:46:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are new to New Relic distributed tracing, we recommend you read the following before you enable distributed tracing. Impact to APM features Our distributed tracing improves on APM's previous cross application tracing feature. Here are some key benefits: See more cross-service activity details and more complete end-to-end traces. Filter and query traces, as well as make custom charts. See the complete trace even when calls cross an organization's account boundaries. See Introduction to distributed tracing for more features. Enabling distributed tracing may affect some APM features you currently use. These changes affect only applications monitored by agents that have distributed tracing enabled—they don't apply on an account-level. We may provide backward compatibility with some or all of the affected features in future releases. For now, you should understand the following changes before enabling distributed tracing: External services page has less detail When distributed tracing is enabled for an application, external calls do not have internal transaction details at one.newrelic.com > APM > (select an app) > Monitor > External services > (select external service). To find that information, you would instead go to the Distributed tracing UI page, find the external call URLs, and see what their child spans are. Transaction trace UI displays service URLs, not transaction links When distributed tracing is enabled for an application, the transaction trace UI will no longer have the transaction name and link for the called service (see screenshot below). This will be replaced with the called service's URL. one.newrelic.com > APM > (select an app) > Monitor > Transactions > (select a transaction trace): This shows the transaction trace UI before distributed tracing is enabled, with a link to the associated transaction. With distributed tracing enabled, it will display the service's URL. If you wanted to get more detail about trace activity, you would go to the Distributed tracing UI page and examine that trace. Cross-application tracing will be disabled Enabling distributed tracing will disable the cross application tracing feature. Distributed tracing is an improved version of cross-application tracing and only one can be enabled at a time. Impacts related to mobile monitoring APM-related impacts include: When distributed tracing is enabled for an APM-monitored entity, legacy service maps will not show applications monitored by mobile. The App server drill-down feature of the legacy mobile HTTP requests UI page is not available. Plan your rollout If you're enabling distributed tracing for a large, distributed system, here are some tips: If you are a current APM user, see Impact to APM features. Determine the requests that are the most important for your business, or the most likely to require analysis and troubleshooting, and enable distributed tracing for those services. Enable tracing for services at roughly the same time so you can more easily gauge how complete your end-to-end traces are. When you look at traces in the distributed tracing UI, you'll see spans in the trace for external calls to other services. Then, you can enable distributed tracing for any of those services you want. If a service is fairly standalone and not often used in context with other services, you may not want to enable distributed tracing for it. Here's a visual representation of such a phased roll-out: If you are using APM for a large, monolithic service, there may be many sub-process spans per trace and APM limits may result in fewer traces than expected. You can solve this by using APM agent instrumentation to disable the reporting of unimportant data. Distributed tracing works by propagating header information from service to service in a request path. Some services may communicate through a proxy or other intermediary service that does not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable distributed tracing If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up distributed tracing. See the setup options.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 724.84155,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "sections": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "tags": "<em>Distributed</em> <em>tracing</em>",
        "body": " not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable <em>distributed</em> <em>tracing</em> If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up <em>distributed</em> <em>tracing</em>. See the <em>setup</em> <em>options</em>."
      },
      "id": "6072a60564441f3a629d8535"
    },
    {
      "sections": [
        "Account ID"
      ],
      "title": "Account ID",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account setup"
      ],
      "external_id": "3771d53b48cfcf2f29d303b1d77a3884e4bae480",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/account-id/",
      "published_at": "2022-01-12T08:15:05Z",
      "updated_at": "2022-01-12T08:15:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some New Relic procedures require use of your account ID (for example, some API calls). Your account ID is the ID number we assign to a New Relic account. Note that some New Relic organizations contain multiple accounts. Options for finding your account ID: If your organization has multiple accounts: From one.newrelic.com, use the account selector dropdown near the top of the page to switch between accounts and see their IDs. There are other options, depending on which user model you're on: New Relic One user model: Click the account dropdown, and then go to: Administration > Organization and access > Accounts to see account IDs. Original user model: Click the account dropdown, click Account settings, and then click API keys. The account ID is displayed there. For more on how account access works, see Factors affecting access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 241.06087,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "Account <em>setup</em>",
        "body": "Some New Relic procedures require use of your account ID (for example, some API calls). Your account ID is the ID number we assign to a New Relic account. Note that some New Relic organizations contain multiple accounts. <em>Options</em> for finding your account ID: If your organization has multiple"
      },
      "id": "61271c84196a67ed0c00b367"
    }
  ],
  "/docs/distributed-tracing/enable-configure/integrations-enable-distributed-tracing": [
    {
      "sections": [
        "Overview: Set up distributed tracing",
        "New Relic integrations",
        "New Relic integrations for third-party telemetry tools",
        "Set up your own solution with our Trace API",
        "Tip"
      ],
      "title": "Overview: Set up distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "cd81f363a9ee07640029b514cafe1f84ac04ef99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/overview-enable-distributed-tracing/",
      "published_at": "2022-01-12T06:20:22Z",
      "updated_at": "2021-12-04T21:47:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We offer a variety of ways to capture distributed tracing data, including: New Relic integrations (including APM, AWS Lambda, browser, and mobile monitoring) New Relic integrations for third-party telemetry tools New Relic Trace API New Relic integrations We offer a range of agents to capture trace data from your applications: Product Description Language agents See telemetry data from applications instrumented with our language-specific agents: C, Go, Java, .NET, Node.js, PHP, Python, and Ruby. Monitoring for AWS Lambda See Lambda function activity, and examine the functions in your traces. Browser See users' browser-side traces. Mobile See users' mobile traces. New Relic integrations for third-party telemetry tools If you are collecting data with these telemetry tools, you can send your data to New Relic: OpenTelemetry Kamon AWS X-Ray Set up your own solution with our Trace API Send data from your telemetry tool directly to New Relic without using an integration that exports the data. Tip Note that this may require more manual configuration than using an integration. Data format Description Trace API: New Relic format Convert your trace data to the New Relic format and send it to New Relic for viewing. Trace API: Data in Zipkin format Send your Zipkin trace data to New Relic for viewing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 236.97113,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview: Set up <em>distributed</em> <em>tracing</em>",
        "sections": "Overview: Set up <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "We offer a variety of ways to capture <em>distributed</em> <em>tracing</em> data, including: New Relic integrations (including APM, AWS Lambda, browser, and mobile monitoring) New Relic integrations for third-party telemetry tools New Relic <em>Trace</em> API New Relic integrations We offer a range of agents to capture <em>trace</em>"
      },
      "id": "6072a666196a67a2bf64a758"
    },
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Examine logs for trace details",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2022-01-12T06:20:28Z",
      "updated_at": "2021-11-13T20:42:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: APM: C APM: Golang APM: Java APM: .NET APM: Node.js APM: PHP APM: Python APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. Examine logs for trace details You can bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. From the Transactions page, click on a trace to go to the Trace details page. From the trace details page, click See logs. To view details related to an individual log message, click directly on the message. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. To set up Infinite Tracing, you need to install the Infinite Tracing gem. The gem is available in rubygems.org. For applications using Bundler, additionally include the Infinite Tracing gem in the Gemfile: gem 'newrelic-infinite_tracing' Copy If you're using Rails 3 or higher, or Rails 2.3 in the recommended configuration, Rails will automatically call Bundler.require and cause newrelic-infinite_tracing to be required during startup of your application. If you're using Sinatra or another framework, you must manually call require 'newrelic/infinite_tracing' or manually call Bundler.require. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever or -1. For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For Infinite Tracing, you also need to install the Infinite Tracing gem. For best results, update to the latest Ruby agent version and Infinite Tracing gem version, if applicable. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 7.0.0 or higher (includes W3C Trace Context) newrelic-infinite_tracing 7.0.0 or higher Supported environments: Ruby 2.5 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.32019,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents <em>and</em> <em>distributed</em> <em>tracing</em>",
        "sections": "<em>Configure</em> standard <em>distributed</em> <em>tracing</em> for your older agents",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " in the UI: <em>Understand</em> and use the <em>distributed</em> <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up Infinite <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Distributed tracing: Planning guide",
        "Impact to APM features",
        "External services page has less detail",
        "Transaction trace UI displays service URLs, not transaction links",
        "Cross-application tracing will be disabled",
        "Impacts related to mobile monitoring",
        "Plan your rollout",
        "Enable distributed tracing"
      ],
      "title": "Distributed tracing: Planning guide",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "10263f2b6ec929f082153e28cbce07fe3a1f106a",
      "image": "https://docs.newrelic.com/static/7072dfa9e494767baabba420d78e7094/c1b63/distributed-trace-txn-trace-affects.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/distributed-tracing-planning-guide/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2022-01-08T03:46:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are new to New Relic distributed tracing, we recommend you read the following before you enable distributed tracing. Impact to APM features Our distributed tracing improves on APM's previous cross application tracing feature. Here are some key benefits: See more cross-service activity details and more complete end-to-end traces. Filter and query traces, as well as make custom charts. See the complete trace even when calls cross an organization's account boundaries. See Introduction to distributed tracing for more features. Enabling distributed tracing may affect some APM features you currently use. These changes affect only applications monitored by agents that have distributed tracing enabled—they don't apply on an account-level. We may provide backward compatibility with some or all of the affected features in future releases. For now, you should understand the following changes before enabling distributed tracing: External services page has less detail When distributed tracing is enabled for an application, external calls do not have internal transaction details at one.newrelic.com > APM > (select an app) > Monitor > External services > (select external service). To find that information, you would instead go to the Distributed tracing UI page, find the external call URLs, and see what their child spans are. Transaction trace UI displays service URLs, not transaction links When distributed tracing is enabled for an application, the transaction trace UI will no longer have the transaction name and link for the called service (see screenshot below). This will be replaced with the called service's URL. one.newrelic.com > APM > (select an app) > Monitor > Transactions > (select a transaction trace): This shows the transaction trace UI before distributed tracing is enabled, with a link to the associated transaction. With distributed tracing enabled, it will display the service's URL. If you wanted to get more detail about trace activity, you would go to the Distributed tracing UI page and examine that trace. Cross-application tracing will be disabled Enabling distributed tracing will disable the cross application tracing feature. Distributed tracing is an improved version of cross-application tracing and only one can be enabled at a time. Impacts related to mobile monitoring APM-related impacts include: When distributed tracing is enabled for an APM-monitored entity, legacy service maps will not show applications monitored by mobile. The App server drill-down feature of the legacy mobile HTTP requests UI page is not available. Plan your rollout If you're enabling distributed tracing for a large, distributed system, here are some tips: If you are a current APM user, see Impact to APM features. Determine the requests that are the most important for your business, or the most likely to require analysis and troubleshooting, and enable distributed tracing for those services. Enable tracing for services at roughly the same time so you can more easily gauge how complete your end-to-end traces are. When you look at traces in the distributed tracing UI, you'll see spans in the trace for external calls to other services. Then, you can enable distributed tracing for any of those services you want. If a service is fairly standalone and not often used in context with other services, you may not want to enable distributed tracing for it. Here's a visual representation of such a phased roll-out: If you are using APM for a large, monolithic service, there may be many sub-process spans per trace and APM limits may result in fewer traces than expected. You can solve this by using APM agent instrumentation to disable the reporting of unimportant data. Distributed tracing works by propagating header information from service to service in a request path. Some services may communicate through a proxy or other intermediary service that does not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable distributed tracing If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up distributed tracing. See the setup options.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.40811,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "sections": "<em>Enable</em> <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " easily gauge how complete your end-to-end traces are. When you look at traces in the <em>distributed</em> <em>tracing</em> UI, you&#x27;ll see spans in the <em>trace</em> for external calls to other services. Then, you can <em>enable</em> <em>distributed</em> <em>tracing</em> for any of those services you want. If a service is fairly standalone and not often"
      },
      "id": "6072a60564441f3a629d8535"
    }
  ],
  "/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing": [
    {
      "sections": [
        "Enable distributed tracing for our telemetry tool integrations",
        "Sampling considerations",
        "Set up integrations"
      ],
      "title": "Enable distributed tracing for our telemetry tool integrations",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "ca05c9c79d80af7bc4f16230459e9811a23a94b6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/integrations-enable-distributed-tracing/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2021-12-04T21:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the telemetry tools Kamon, OpenTelemetry, or AWS X-Ray, you can get that data into New Relic with our telemetry integrations. Sampling considerations Because distributed systems can generate a lot of trace data, telemetry tools rely on data sampling (filtering). When you install a telemetry integration that reports trace data, you'll have an option to enable Infinite Tracing. Choosing Infinite Tracing has implications for how you configure sampling in your telemetry tool: Standard installation without Infinite Tracing: A standard installation assumes you want your telemetry tool to sample trace data before it's sent to us. (If your trace data exceeds our Trace API limits, we may also do additional sampling.) Install with Infinite Tracing: If you choose Infinite Tracing (read requirements), we assume your telemetry tool's sampling is set to 100%, so that all of that tool's trace data is sent to us. The trace observer selects the most important and actionable traces using tail-based sampling, and then that data is ingested via our Trace API. Set up integrations To set up your telemetry tool for sending distributed traces to New Relic, follow the instructions for your tool: OpenTelemetry Kamon AWS X-Ray",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 236.97166,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Enable</em> <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "sections": "<em>Enable</em> <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " a telemetry integration that reports <em>trace</em> data, you&#x27;ll have an option to <em>enable</em> Infinite <em>Tracing</em>. Choosing Infinite <em>Tracing</em> has implications for how you <em>configure</em> sampling in your telemetry tool: Standard installation without Infinite <em>Tracing</em>: A standard installation assumes you want your telemetry tool"
      },
      "id": "6072a66664441f271c9d8557"
    },
    {
      "sections": [
        "Overview: Set up distributed tracing",
        "New Relic integrations",
        "New Relic integrations for third-party telemetry tools",
        "Set up your own solution with our Trace API",
        "Tip"
      ],
      "title": "Overview: Set up distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "cd81f363a9ee07640029b514cafe1f84ac04ef99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/overview-enable-distributed-tracing/",
      "published_at": "2022-01-12T06:20:22Z",
      "updated_at": "2021-12-04T21:47:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We offer a variety of ways to capture distributed tracing data, including: New Relic integrations (including APM, AWS Lambda, browser, and mobile monitoring) New Relic integrations for third-party telemetry tools New Relic Trace API New Relic integrations We offer a range of agents to capture trace data from your applications: Product Description Language agents See telemetry data from applications instrumented with our language-specific agents: C, Go, Java, .NET, Node.js, PHP, Python, and Ruby. Monitoring for AWS Lambda See Lambda function activity, and examine the functions in your traces. Browser See users' browser-side traces. Mobile See users' mobile traces. New Relic integrations for third-party telemetry tools If you are collecting data with these telemetry tools, you can send your data to New Relic: OpenTelemetry Kamon AWS X-Ray Set up your own solution with our Trace API Send data from your telemetry tool directly to New Relic without using an integration that exports the data. Tip Note that this may require more manual configuration than using an integration. Data format Description Trace API: New Relic format Convert your trace data to the New Relic format and send it to New Relic for viewing. Trace API: Data in Zipkin format Send your Zipkin trace data to New Relic for viewing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 236.97113,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview: Set up <em>distributed</em> <em>tracing</em>",
        "sections": "Overview: Set up <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "We offer a variety of ways to capture <em>distributed</em> <em>tracing</em> data, including: New Relic integrations (including APM, AWS Lambda, browser, and mobile monitoring) New Relic integrations for third-party telemetry tools New Relic <em>Trace</em> API New Relic integrations We offer a range of agents to capture <em>trace</em>"
      },
      "id": "6072a666196a67a2bf64a758"
    },
    {
      "sections": [
        "Distributed tracing: Planning guide",
        "Impact to APM features",
        "External services page has less detail",
        "Transaction trace UI displays service URLs, not transaction links",
        "Cross-application tracing will be disabled",
        "Impacts related to mobile monitoring",
        "Plan your rollout",
        "Enable distributed tracing"
      ],
      "title": "Distributed tracing: Planning guide",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "10263f2b6ec929f082153e28cbce07fe3a1f106a",
      "image": "https://docs.newrelic.com/static/7072dfa9e494767baabba420d78e7094/c1b63/distributed-trace-txn-trace-affects.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/distributed-tracing-planning-guide/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2022-01-08T03:46:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are new to New Relic distributed tracing, we recommend you read the following before you enable distributed tracing. Impact to APM features Our distributed tracing improves on APM's previous cross application tracing feature. Here are some key benefits: See more cross-service activity details and more complete end-to-end traces. Filter and query traces, as well as make custom charts. See the complete trace even when calls cross an organization's account boundaries. See Introduction to distributed tracing for more features. Enabling distributed tracing may affect some APM features you currently use. These changes affect only applications monitored by agents that have distributed tracing enabled—they don't apply on an account-level. We may provide backward compatibility with some or all of the affected features in future releases. For now, you should understand the following changes before enabling distributed tracing: External services page has less detail When distributed tracing is enabled for an application, external calls do not have internal transaction details at one.newrelic.com > APM > (select an app) > Monitor > External services > (select external service). To find that information, you would instead go to the Distributed tracing UI page, find the external call URLs, and see what their child spans are. Transaction trace UI displays service URLs, not transaction links When distributed tracing is enabled for an application, the transaction trace UI will no longer have the transaction name and link for the called service (see screenshot below). This will be replaced with the called service's URL. one.newrelic.com > APM > (select an app) > Monitor > Transactions > (select a transaction trace): This shows the transaction trace UI before distributed tracing is enabled, with a link to the associated transaction. With distributed tracing enabled, it will display the service's URL. If you wanted to get more detail about trace activity, you would go to the Distributed tracing UI page and examine that trace. Cross-application tracing will be disabled Enabling distributed tracing will disable the cross application tracing feature. Distributed tracing is an improved version of cross-application tracing and only one can be enabled at a time. Impacts related to mobile monitoring APM-related impacts include: When distributed tracing is enabled for an APM-monitored entity, legacy service maps will not show applications monitored by mobile. The App server drill-down feature of the legacy mobile HTTP requests UI page is not available. Plan your rollout If you're enabling distributed tracing for a large, distributed system, here are some tips: If you are a current APM user, see Impact to APM features. Determine the requests that are the most important for your business, or the most likely to require analysis and troubleshooting, and enable distributed tracing for those services. Enable tracing for services at roughly the same time so you can more easily gauge how complete your end-to-end traces are. When you look at traces in the distributed tracing UI, you'll see spans in the trace for external calls to other services. Then, you can enable distributed tracing for any of those services you want. If a service is fairly standalone and not often used in context with other services, you may not want to enable distributed tracing for it. Here's a visual representation of such a phased roll-out: If you are using APM for a large, monolithic service, there may be many sub-process spans per trace and APM limits may result in fewer traces than expected. You can solve this by using APM agent instrumentation to disable the reporting of unimportant data. Distributed tracing works by propagating header information from service to service in a request path. Some services may communicate through a proxy or other intermediary service that does not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable distributed tracing If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up distributed tracing. See the setup options.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.40811,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "sections": "<em>Enable</em> <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " easily gauge how complete your end-to-end traces are. When you look at traces in the <em>distributed</em> <em>tracing</em> UI, you&#x27;ll see spans in the <em>trace</em> for external calls to other services. Then, you can <em>enable</em> <em>distributed</em> <em>tracing</em> for any of those services you want. If a service is fairly standalone and not often"
      },
      "id": "6072a60564441f3a629d8535"
    }
  ],
  "/docs/distributed-tracing/enable-configure/overview-enable-distributed-tracing": [
    {
      "sections": [
        "Enable distributed tracing for our telemetry tool integrations",
        "Sampling considerations",
        "Set up integrations"
      ],
      "title": "Enable distributed tracing for our telemetry tool integrations",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "ca05c9c79d80af7bc4f16230459e9811a23a94b6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/integrations-enable-distributed-tracing/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2021-12-04T21:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the telemetry tools Kamon, OpenTelemetry, or AWS X-Ray, you can get that data into New Relic with our telemetry integrations. Sampling considerations Because distributed systems can generate a lot of trace data, telemetry tools rely on data sampling (filtering). When you install a telemetry integration that reports trace data, you'll have an option to enable Infinite Tracing. Choosing Infinite Tracing has implications for how you configure sampling in your telemetry tool: Standard installation without Infinite Tracing: A standard installation assumes you want your telemetry tool to sample trace data before it's sent to us. (If your trace data exceeds our Trace API limits, we may also do additional sampling.) Install with Infinite Tracing: If you choose Infinite Tracing (read requirements), we assume your telemetry tool's sampling is set to 100%, so that all of that tool's trace data is sent to us. The trace observer selects the most important and actionable traces using tail-based sampling, and then that data is ingested via our Trace API. Set up integrations To set up your telemetry tool for sending distributed traces to New Relic, follow the instructions for your tool: OpenTelemetry Kamon AWS X-Ray",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 236.97166,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Enable</em> <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "sections": "<em>Enable</em> <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " a telemetry integration that reports <em>trace</em> data, you&#x27;ll have an option to <em>enable</em> Infinite <em>Tracing</em>. Choosing Infinite <em>Tracing</em> has implications for how you <em>configure</em> sampling in your telemetry tool: Standard installation without Infinite <em>Tracing</em>: A standard installation assumes you want your telemetry tool"
      },
      "id": "6072a66664441f271c9d8557"
    },
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Examine logs for trace details",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2022-01-12T06:20:28Z",
      "updated_at": "2021-11-13T20:42:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: APM: C APM: Golang APM: Java APM: .NET APM: Node.js APM: PHP APM: Python APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. Examine logs for trace details You can bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. From the Transactions page, click on a trace to go to the Trace details page. From the trace details page, click See logs. To view details related to an individual log message, click directly on the message. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. To set up Infinite Tracing, you need to install the Infinite Tracing gem. The gem is available in rubygems.org. For applications using Bundler, additionally include the Infinite Tracing gem in the Gemfile: gem 'newrelic-infinite_tracing' Copy If you're using Rails 3 or higher, or Rails 2.3 in the recommended configuration, Rails will automatically call Bundler.require and cause newrelic-infinite_tracing to be required during startup of your application. If you're using Sinatra or another framework, you must manually call require 'newrelic/infinite_tracing' or manually call Bundler.require. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever or -1. For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For Infinite Tracing, you also need to install the Infinite Tracing gem. For best results, update to the latest Ruby agent version and Infinite Tracing gem version, if applicable. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 7.0.0 or higher (includes W3C Trace Context) newrelic-infinite_tracing 7.0.0 or higher Supported environments: Ruby 2.5 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.32019,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents <em>and</em> <em>distributed</em> <em>tracing</em>",
        "sections": "<em>Configure</em> standard <em>distributed</em> <em>tracing</em> for your older agents",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " in the UI: <em>Understand</em> and use the <em>distributed</em> <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up Infinite <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Distributed tracing: Planning guide",
        "Impact to APM features",
        "External services page has less detail",
        "Transaction trace UI displays service URLs, not transaction links",
        "Cross-application tracing will be disabled",
        "Impacts related to mobile monitoring",
        "Plan your rollout",
        "Enable distributed tracing"
      ],
      "title": "Distributed tracing: Planning guide",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "10263f2b6ec929f082153e28cbce07fe3a1f106a",
      "image": "https://docs.newrelic.com/static/7072dfa9e494767baabba420d78e7094/c1b63/distributed-trace-txn-trace-affects.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/distributed-tracing-planning-guide/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2022-01-08T03:46:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are new to New Relic distributed tracing, we recommend you read the following before you enable distributed tracing. Impact to APM features Our distributed tracing improves on APM's previous cross application tracing feature. Here are some key benefits: See more cross-service activity details and more complete end-to-end traces. Filter and query traces, as well as make custom charts. See the complete trace even when calls cross an organization's account boundaries. See Introduction to distributed tracing for more features. Enabling distributed tracing may affect some APM features you currently use. These changes affect only applications monitored by agents that have distributed tracing enabled—they don't apply on an account-level. We may provide backward compatibility with some or all of the affected features in future releases. For now, you should understand the following changes before enabling distributed tracing: External services page has less detail When distributed tracing is enabled for an application, external calls do not have internal transaction details at one.newrelic.com > APM > (select an app) > Monitor > External services > (select external service). To find that information, you would instead go to the Distributed tracing UI page, find the external call URLs, and see what their child spans are. Transaction trace UI displays service URLs, not transaction links When distributed tracing is enabled for an application, the transaction trace UI will no longer have the transaction name and link for the called service (see screenshot below). This will be replaced with the called service's URL. one.newrelic.com > APM > (select an app) > Monitor > Transactions > (select a transaction trace): This shows the transaction trace UI before distributed tracing is enabled, with a link to the associated transaction. With distributed tracing enabled, it will display the service's URL. If you wanted to get more detail about trace activity, you would go to the Distributed tracing UI page and examine that trace. Cross-application tracing will be disabled Enabling distributed tracing will disable the cross application tracing feature. Distributed tracing is an improved version of cross-application tracing and only one can be enabled at a time. Impacts related to mobile monitoring APM-related impacts include: When distributed tracing is enabled for an APM-monitored entity, legacy service maps will not show applications monitored by mobile. The App server drill-down feature of the legacy mobile HTTP requests UI page is not available. Plan your rollout If you're enabling distributed tracing for a large, distributed system, here are some tips: If you are a current APM user, see Impact to APM features. Determine the requests that are the most important for your business, or the most likely to require analysis and troubleshooting, and enable distributed tracing for those services. Enable tracing for services at roughly the same time so you can more easily gauge how complete your end-to-end traces are. When you look at traces in the distributed tracing UI, you'll see spans in the trace for external calls to other services. Then, you can enable distributed tracing for any of those services you want. If a service is fairly standalone and not often used in context with other services, you may not want to enable distributed tracing for it. Here's a visual representation of such a phased roll-out: If you are using APM for a large, monolithic service, there may be many sub-process spans per trace and APM limits may result in fewer traces than expected. You can solve this by using APM agent instrumentation to disable the reporting of unimportant data. Distributed tracing works by propagating header information from service to service in a request path. Some services may communicate through a proxy or other intermediary service that does not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable distributed tracing If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up distributed tracing. See the setup options.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.40811,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "sections": "<em>Enable</em> <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " easily gauge how complete your end-to-end traces are. When you look at traces in the <em>distributed</em> <em>tracing</em> UI, you&#x27;ll see spans in the <em>trace</em> for external calls to other services. Then, you can <em>enable</em> <em>distributed</em> <em>tracing</em> for any of those services you want. If a service is fairly standalone and not often"
      },
      "id": "6072a60564441f3a629d8535"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/infinite-tracing-configure-proxy-support": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 378.09778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Infinite</em> <em>Tracing</em> feature. If you are just getting started with <em>distributed</em> <em>tracing</em>, see our setup options. What is <em>Infinite</em> <em>Tracing</em>? <em>Infinite</em> <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on <em>Infinite</em> <em>Tracing</em> to make sampling decisions. You can configure <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 340.9119,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling <em>Infinite</em> <em>Tracing</em>. To learn more about this, see Intro to <em>Infinite</em> <em>Tracing</em> and Sampling considerations. To get started using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.22345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to get started right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting started? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/infinite-tracing-configure-random-trace-filter": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 378.09778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Infinite</em> <em>Tracing</em> feature. If you are just getting started with <em>distributed</em> <em>tracing</em>, see our setup options. What is <em>Infinite</em> <em>Tracing</em>? <em>Infinite</em> <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on <em>Infinite</em> <em>Tracing</em> to make sampling decisions. You can configure <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 340.9119,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling <em>Infinite</em> <em>Tracing</em>. To learn more about this, see Intro to <em>Infinite</em> <em>Tracing</em> and Sampling considerations. To get started using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.22345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to get started right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting started? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/infinite-tracing-configure-span-attribute-trace-filter": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 378.0976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Infinite</em> <em>Tracing</em> feature. If you are just getting started with <em>distributed</em> <em>tracing</em>, see our setup options. What is <em>Infinite</em> <em>Tracing</em>? <em>Infinite</em> <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on <em>Infinite</em> <em>Tracing</em> to make sampling decisions. You can configure <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 340.91174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling <em>Infinite</em> <em>Tracing</em>. To learn more about this, see Intro to <em>Infinite</em> <em>Tracing</em> and Sampling considerations. To get started using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.22336,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to get started right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting started? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/infinite-tracing-configure-trace-observer-monitoring": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 378.0976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Infinite</em> <em>Tracing</em> feature. If you are just getting started with <em>distributed</em> <em>tracing</em>, see our setup options. What is <em>Infinite</em> <em>Tracing</em>? <em>Infinite</em> <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on <em>Infinite</em> <em>Tracing</em> to make sampling decisions. You can configure <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 340.91174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling <em>Infinite</em> <em>Tracing</em>. To learn more about this, see Intro to <em>Infinite</em> <em>Tracing</em> and Sampling considerations. To get started using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.22336,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to get started right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting started? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing": [
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 340.9116,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling <em>Infinite</em> <em>Tracing</em>. To learn more about this, see Intro to <em>Infinite</em> <em>Tracing</em> and Sampling considerations. To get started using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.22327,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to get started right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting started? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    },
    {
      "sections": [
        "Enable distributed tracing for our telemetry tool integrations",
        "Sampling considerations",
        "Set up integrations"
      ],
      "title": "Enable distributed tracing for our telemetry tool integrations",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "ca05c9c79d80af7bc4f16230459e9811a23a94b6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/integrations-enable-distributed-tracing/",
      "published_at": "2022-01-12T08:25:34Z",
      "updated_at": "2021-12-04T21:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the telemetry tools Kamon, OpenTelemetry, or AWS X-Ray, you can get that data into New Relic with our telemetry integrations. Sampling considerations Because distributed systems can generate a lot of trace data, telemetry tools rely on data sampling (filtering). When you install a telemetry integration that reports trace data, you'll have an option to enable Infinite Tracing. Choosing Infinite Tracing has implications for how you configure sampling in your telemetry tool: Standard installation without Infinite Tracing: A standard installation assumes you want your telemetry tool to sample trace data before it's sent to us. (If your trace data exceeds our Trace API limits, we may also do additional sampling.) Install with Infinite Tracing: If you choose Infinite Tracing (read requirements), we assume your telemetry tool's sampling is set to 100%, so that all of that tool's trace data is sent to us. The trace observer selects the most important and actionable traces using tail-based sampling, and then that data is ingested via our Trace API. Set up integrations To set up your telemetry tool for sending distributed traces to New Relic, follow the instructions for your tool: OpenTelemetry Kamon AWS X-Ray",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.16913,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Enable <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "sections": "Enable <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " a telemetry integration that reports <em>trace</em> data, you&#x27;ll have an option to enable <em>Infinite</em> <em>Tracing</em>. Choosing <em>Infinite</em> <em>Tracing</em> has implications for how you configure sampling in your telemetry tool: Standard installation without <em>Infinite</em> <em>Tracing</em>: A standard installation assumes you want your telemetry tool"
      },
      "id": "6072a66664441f271c9d8557"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/set-trace-observer": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 378.0974,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Infinite</em> <em>Tracing</em> feature. If you are just getting started with <em>distributed</em> <em>tracing</em>, see our setup options. What is <em>Infinite</em> <em>Tracing</em>? <em>Infinite</em> <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on <em>Infinite</em> <em>Tracing</em> to make sampling decisions. You can configure <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 340.9116,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling <em>Infinite</em> <em>Tracing</em>. To learn more about this, see Intro to <em>Infinite</em> <em>Tracing</em> and Sampling considerations. To get started using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.22327,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to get started right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting started? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    }
  ],
  "/docs/distributed-tracing/other-requirements/infinite-tracing-configuring-ssl-java-7-8": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2022-01-12T06:21:31Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to the advanced Infinite Tracing feature. If you are just getting started with distributed tracing, see our setup options. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing model: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. Instructions for setting up Infinite Tracing are in the specific docs for our solutions. To get started, see our quick start guide. Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 378.09726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Infinite</em> <em>Tracing</em> feature. If you are just getting started with <em>distributed</em> <em>tracing</em>, see our setup options. What is <em>Infinite</em> <em>Tracing</em>? <em>Infinite</em> <em>Tracing</em> allows you to send all your <em>trace</em> data to our cloud-based service and rely on <em>Infinite</em> <em>Tracing</em> to make sampling decisions. You can configure <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2022-01-12T08:26:17Z",
      "updated_at": "2022-01-08T03:18:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get the license key for the account you want to report data to. You'll be executing a curl request, below. Notes on this: Replace the license key placeholder with your license key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_LICENSE_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 340.91144,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> API",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " send-data instructions have options for enabling <em>Infinite</em> <em>Tracing</em>. To learn more about this, see Intro to <em>Infinite</em> <em>Tracing</em> and Sampling considerations. To get started using the <em>Trace</em> API, choose an option: Send a sample <em>trace</em>: This shows a curl example of sending a <em>trace</em> to New Relic. This is useful"
      },
      "id": "6071cfc864441fa88f9d8530"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Want to get started right away?",
        "Want to learn more before getting started?"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/2878076657e1173d9f8c92a6e7547a9f/83b75/intro-DT.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2022-01-12T06:19:55Z",
      "updated_at": "2022-01-05T01:43:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. Requests might pass through various services to reach completion, and these services could be in a variety of places: containers, serverless environments, virtual machines, different cloud providers, or on-premises. When you can see the path of an entire request across different services, you can quickly pinpoint failures or performance issues. Distributed tracing collects data as requests travel from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are eventually combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of a web transaction where agents measure the time spent in each service. Agents then send that timing information to New Relic as spans, and the spans are combined into one distributed trace. Want to get started right away? If you are familiar with distributed tracing and want to jump right in, check out the setup options. Want to learn more before getting started? See these distributed tracing topics: How does distributed tracing work, and what types of distributed tracing are available? How can I troubleshoot requests using the distributed tracing UI? How should I plan my rollout of distributed tracing? What is the advanced feature called Infinite Tracing?",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.2232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " Relic as spans, and the spans are combined into one <em>distributed</em> <em>trace</em>. Want to get started right away? If you are familiar with <em>distributed</em> <em>tracing</em> and want to jump right in, check out the setup options. Want to learn more before getting started? See these <em>distributed</em> <em>tracing</em> topics: How does"
      },
      "id": "6072a767e7b9d231f1a5c64c"
    }
  ]
}