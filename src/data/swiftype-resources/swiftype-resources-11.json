{
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80945,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80225,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Bigtable monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Bigtable Cluster data",
        "Bigtable Table data"
      ],
      "title": "Google Cloud Bigtable monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a7cd3b2dca9763ad525a473fb8a9ed19d60d0875",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-bigtable-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Bigtable data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Bigtable integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Cluster GcpBigtableClusterSample GcpBigtableCluster Table GcpBigtableTableSample GcpBigtableTable For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Bigtable data for Cluster and Table. Bigtable Cluster data Metric Unit Description cluster.CpuLoad Count CPU load of a cluster. cluster.CpuLoadHottestNode Count CPU load of the busiest node in a cluster. cluster.DiskLoad Count Utilization of HDD disks in a cluster. cluster.Node Count Number of nodes in a cluster. cluster.StorageUtilization Count Storage used as a fraction of total storage capacity. disk.BytesUsed Bytes Amount of compressed data for tables stored in a cluster. disk.StorageCapacity Bytes Capacity of compressed data for tables that can be stored in a cluster. Bigtable Table data Metric Unit Description replication.Latency Milliseconds Distribution of replication request latencies for a table. Includes only requests that have been received by the destination cluster. replication.MaxDelay Seconds Upper bound for replication delay between clusters of a table. Indicates the time frame during which latency information may not be accurate. server.Error Count Number of server requests for a table that failed with an error. server.Latencies Milliseconds Distribution of server request latencies for a table, measured when calls reach Cloud Bigtable. server.ModifiedRows Count Number of rows modified by server requests for a table. server.MultiClusterFailovers Count Number of failovers during multi-cluster requests. server.ReceivedBytes Bytes Number of uncompressed bytes of request data received by servers for a table. server.Request Count Number of server requests for a table. server.ReturnedRows Count Number of rows returned by server requests for a table. server.SentBytes Bytes Number of uncompressed bytes of response data sent by servers for a table. table.BytesUsed Bytes Amount of compressed data stored in a table.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80072,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Bigtable data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "6045082f196a67af34960f20"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-dataflow-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80945,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80225,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80072,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-dataproc-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80933,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8006,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-database-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80933,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8006,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-hosting-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80933,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8006,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-storage-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8092,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.802,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firestore-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8092,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.802,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80908,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80035,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-load-balancing-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80908,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80035,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-pubsub-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80173,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80023,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-router-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80173,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80023,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-run-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8001,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-spanner-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8001,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-sql-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8087,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80148,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.79996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8087,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80148,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.79996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8087,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80148,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.79996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-datastore-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80856,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80136,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.79984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-direct-interconnect-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80856,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80136,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.79984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-kubernetes-engine-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80844,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8012,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.7997,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.80844,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.7997,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    },
    {
      "sections": [
        "Google Cloud Bigtable monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Bigtable Cluster data",
        "Bigtable Table data"
      ],
      "title": "Google Cloud Bigtable monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a7cd3b2dca9763ad525a473fb8a9ed19d60d0875",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-bigtable-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Bigtable data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Bigtable integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Cluster GcpBigtableClusterSample GcpBigtableCluster Table GcpBigtableTableSample GcpBigtableTable For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Bigtable data for Cluster and Table. Bigtable Cluster data Metric Unit Description cluster.CpuLoad Count CPU load of a cluster. cluster.CpuLoadHottestNode Count CPU load of the busiest node in a cluster. cluster.DiskLoad Count Utilization of HDD disks in a cluster. cluster.Node Count Number of nodes in a cluster. cluster.StorageUtilization Count Storage used as a fraction of total storage capacity. disk.BytesUsed Bytes Amount of compressed data for tables stored in a cluster. disk.StorageCapacity Bytes Capacity of compressed data for tables that can be stored in a cluster. Bigtable Table data Metric Unit Description replication.Latency Milliseconds Distribution of replication request latencies for a table. Includes only requests that have been received by the destination cluster. replication.MaxDelay Seconds Upper bound for replication delay between clusters of a table. Indicates the time frame during which latency information may not be accurate. server.Error Count Number of server requests for a table that failed with an error. server.Latencies Milliseconds Distribution of server request latencies for a table, measured when calls reach Cloud Bigtable. server.ModifiedRows Count Number of rows modified by server requests for a table. server.MultiClusterFailovers Count Number of failovers during multi-cluster requests. server.ReceivedBytes Bytes Number of uncompressed bytes of request data received by servers for a table. server.Request Count Number of server requests for a table. server.ReturnedRows Count Number of rows returned by server requests for a table. server.SentBytes Bytes Number of uncompressed bytes of response data sent by servers for a table. table.BytesUsed Bytes Amount of compressed data stored in a table.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.7997,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Bigtable data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "6045082f196a67af34960f20"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis": [
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8011,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.7996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    },
    {
      "sections": [
        "Google Cloud Bigtable monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Bigtable Cluster data",
        "Bigtable Table data"
      ],
      "title": "Google Cloud Bigtable monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a7cd3b2dca9763ad525a473fb8a9ed19d60d0875",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-bigtable-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Bigtable data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Bigtable integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Cluster GcpBigtableClusterSample GcpBigtableCluster Table GcpBigtableTableSample GcpBigtableTable For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Bigtable data for Cluster and Table. Bigtable Cluster data Metric Unit Description cluster.CpuLoad Count CPU load of a cluster. cluster.CpuLoadHottestNode Count CPU load of the busiest node in a cluster. cluster.DiskLoad Count Utilization of HDD disks in a cluster. cluster.Node Count Number of nodes in a cluster. cluster.StorageUtilization Count Storage used as a fraction of total storage capacity. disk.BytesUsed Bytes Amount of compressed data for tables stored in a cluster. disk.StorageCapacity Bytes Capacity of compressed data for tables that can be stored in a cluster. Bigtable Table data Metric Unit Description replication.Latency Milliseconds Distribution of replication request latencies for a table. Includes only requests that have been received by the destination cluster. replication.MaxDelay Seconds Upper bound for replication delay between clusters of a table. Indicates the time frame during which latency information may not be accurate. server.Error Count Number of server requests for a table that failed with an error. server.Latencies Milliseconds Distribution of server request latencies for a table, measured when calls reach Cloud Bigtable. server.ModifiedRows Count Number of rows modified by server requests for a table. server.MultiClusterFailovers Count Number of failovers during multi-cluster requests. server.ReceivedBytes Bytes Number of uncompressed bytes of request data received by servers for a table. server.Request Count Number of server requests for a table. server.ReturnedRows Count Number of rows returned by server requests for a table. server.SentBytes Bytes Number of uncompressed bytes of response data sent by servers for a table. table.BytesUsed Bytes Amount of compressed data stored in a table.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.7996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Bigtable monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Bigtable data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "6045082f196a67af34960f20"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8083,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.8011,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.7996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your <em>GCP</em> Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic": [
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-09-14T20:40:51Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 278.44553,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.30382,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.29915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics": [
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.30382,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.29915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    },
    {
      "sections": [
        "Google Cloud Composer monitoring integration",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Composer Environment data",
        "Composer Workflow data"
      ],
      "title": "Google Cloud Composer monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a2cbc05bfa686de458fed2748ed111064b903ad4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration/",
      "published_at": "2021-09-14T20:39:42Z",
      "updated_at": "2021-09-14T20:39:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Composer data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Composer integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider Environment GcpComposerEnvironmentSample GcpComposerEnvironment Workflow GcpComposerWorkflowSample GcpComposerWorkflow For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Composer data for Environment and Workflow. Composer Environment data Metric Unit Description environment.api.Request Count Number of Composer API requests seen so far. environment.api.RequestLatencies Milliseconds Distribution of Composer API call latencies. environment.dag_processing.ParseError Count Number of errors raised during parsing DAG files. environment.dag_processing.Processes Count Number of currently running DAG parsing processes. environment.dag_processing.ProcessorTimeout Count Number of file processors terminated due to processing timeout. environment.dag_processing.TotalParseTime Seconds Number of seconds taken to scan and import all DAG files once. environment.DagbagSize Count The current dag bag size. environment.database.cpu.ReservedCores Count Number of cores reserved for the database instance. environment.database.cpu.UsageTime Seconds CPU usage time of the database instance. environment.database.cpu.Utilization Count CPU utilization ratio (from 0.0 to 1.0) of the database instance. environment.database.disk.BytesUsed Bytes Used disk space in bytes on the database instance. environment.database.disk.Quota Bytes Maximum data disk size in bytes of the database instance. environment.database.disk.Utilization Count Disk quota usage ratio (from 0.0 to 1.0) of the database instance. environment.database.memory.BytesUsed Bytes Memory usage of the database instance in bytes. environment.database.memory.Quota Bytes Maximum RAM size in bytes of the database instance. environment.database.memory.Utilization Count Memory utilization ratio (from 0.0 to 1.0) of the database instance. environment.executor.OpenSlots Count Number of open slots on executor. environment.executor.RunningTasks Count Number of running tasks on executor. environment.FinishedTaskInstance Count Overall task instances. environment.NumCeleryWorkers Count Number of Celery workers. environment.SchedulerHeartbeat Count Scheduler heartbeats. environment.TaskQueueLength Count Number of tasks in queue. environment.worker.PodEviction Count The number of Airflow worker pods evictions. environment.ZombieTaskKilled Count Number of zombie tasks killed. Composer Workflow data Metric Unit Description workflow.Run Count Number of workflow runs completed so far. workflow.RunDuration Seconds Duration of workflow run completion. workflow.task.Run Count Number of workflow tasks completed so far. workflow.task.RunDuration Seconds Duration of task completion.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.29816,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Composer monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Composer data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "60450ccd196a67293b960f66"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles": [
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-09-14T20:40:51Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 278.4454,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.30374,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.29907,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations": [
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-09-14T20:40:51Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 278.4454,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.30374,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.29907,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/getting-started/polling-intervals-gcp-integrations": [
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-09-14T20:40:51Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 278.44528,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.30365,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.29898,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/troubleshooting/gcp-integration-api-authentication-errors": [
    {
      "sections": [
        "GCP integration metrics",
        "BETA FEATURE",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-09-14T20:40:51Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.74716,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. <em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine"
      },
      "id": "603e8a5264441f524a4e8840"
    },
    {
      "sections": [
        "Google Memorystore for Redis",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Redis RedisInstance data"
      ],
      "title": "Google Memorystore for Redis",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "17f2891f284704c51e4e6cdac2c1b1de4b3aaf6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis/",
      "published_at": "2021-09-14T20:40:52Z",
      "updated_at": "2021-09-14T20:40:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Redis data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Redis integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider RedisInstance GcpRedisInstanceSample GcpRedisInstance For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Redis data for RedisInstance. Redis RedisInstance data Metric Unit Description clients.Blocked Count Number of blocked clients. clients.Connected Count Number of client connections. commands.Calls Count Total number of calls for this command in one minute. commands.TotalTime Microseconds The amount of time in microseconds that this command took in the last second. commands.UsecPerCall Count Average time per call over 1 minute by command. keyspace.AvgTtl Milliseconds Average TTL for keys in this database. keyspace.Keys Count Number of keys stored in this database. keyspace.KeysWithExpiration Count Number of keys with an expiration in this database. replication.master.slaves.Lag Bytes The number of bytes that replica is behind. replication.master.slaves.Offset Bytes The number of bytes that have been acknowledged by replicas. replication.MasterReplOffset Bytes The number of bytes that master has produced and sent to replicas. To be compared with replication byte offset of replica. replication.OffsetDiff Bytes The number of bytes that have not been replicated to the replica. This is the difference between replication byte offset (master) and replication byte offset (replica). replication.Role Count Returns a value indicating the node role. 1 indicates master and 0 indicates replica. server.Uptime Seconds Uptime in seconds. stats.CacheHitRatio Count Cache Hit ratio as a fraction. stats.connections.Total Count Total number of connections accepted by the server. stats.CpuUtilization s { CPU} CPU-seconds consumed by the Redis server, broken down by system/user space and parent/child relationship. stats.EvictedKeys Count Number of evicted keys due to maxmemory limit. stats.ExpiredKeys Count Total number of key expiration events. stats.KeyspaceHits Count Number of successful lookup of keys in the main dictionary. stats.KeyspaceMisses Count Number of failed lookup of keys in the main dictionary. stats.memory.Maxmemory Bytes Maximum amount of memory Redis can consume. stats.memory.SystemMemoryOverloadDuration Microseconds The amount of time in microseconds the instance is in system memory overload mode. stats.memory.SystemMemoryUsageRatio Count Memory usage as a ratio of maximum system memory. stats.memory.Usage Bytes Total number of bytes allocated by Redis. stats.memory.UsageRatio Count Memory usage as a ratio of maximum memory. stats.NetworkTraffic Bytes Total number of bytes sent to/from redis (includes bytes from commands themselves, payload data, and delimiters). stats.pubsub.Channels Count Global number of pub/sub channels with client subscriptions. stats.pubsub.Patterns Count Global number of pub/sub pattern with client subscriptions. stats.RejectConnections Count Number of connections rejected because of maxclients limit.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.62627,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Redis",
        "sections": "<em>Google</em> Memorystore for Redis",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Redis data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f27bd378ee1"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.62195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Memorystore for Memcached",
        "sections": "<em>Google</em> Memorystore for Memcached",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a <em>cloud</em> integration for reporting your GCP Memcache data to our <em>platform</em>. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/integrations/grafana-integrations/get-started/grafana-support-prometheus-promql": [
    {
      "sections": [
        "AWS integrations metrics",
        "Amazon Web Services Metrics"
      ],
      "title": "AWS integrations metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "ff880ffb0485570e05dfb7c815fe82949ccbd006",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/aws-integrations-metrics/",
      "published_at": "2021-09-14T20:49:04Z",
      "updated_at": "2021-09-14T20:49:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon Web Services Metrics We recommend using the AWS CloudWatch Metric Streams integration to ingest AWS services' metrics. For a complete list of available metrics, check the CloudWatch metrics listing documentation for each AWS service. The following table contains the metrics we collect from AWS API Polling integrations: Integration Dimensional Metric Name (new) Sample / Event Name (previous) AWS ALB aws.applicationelb.ActiveConnectionCount provider.activeConnectionCount AWS ALB aws.applicationelb.ClientTLSNegotiationErrorCount provider.clientTlsNegotiationErrorCount AWS ALB aws.applicationelb.ConsumedLCUs provider.consumedLcus AWS ALB aws.applicationelb.ELBAuthError provider.elbAuthError AWS ALB aws.applicationelb.ELBAuthFailure provider.elbAuthFailure AWS ALB aws.applicationelb.ELBAuthLatency provider.elbAuthLatency AWS ALB aws.applicationelb.ELBAuthRefreshTokenSuccess provider.elbAuthRefreshTokenSuccess AWS ALB aws.applicationelb.ELBAuthSuccess provider.elbAuthSuccess AWS ALB aws.applicationelb.ELBAuthUserClaimsSizeExceeded provider.elbAuthUserClaimsSizeExceeded AWS ALB aws.applicationelb.HTTP_Fixed_Response_Count provider.httpFixedResponseCount AWS ALB aws.applicationelb.HTTP_Redirect_Count provider.httpRedirectCount AWS ALB aws.applicationelb.HTTP_Redirect_Url_Limit_Exceeded_Count provider.httpRedirectUrlLimitExceededCount AWS ALB aws.applicationelb.HTTPCode_ELB_2XX_Count provider.httpCodeElb2XXCount AWS ALB aws.applicationelb.HTTPCode_ELB_3XX_Count provider.httpCodeElb3XXCount AWS ALB aws.applicationelb.HTTPCode_ELB_4XX_Count provider.httpCodeElb4XXCount AWS ALB aws.applicationelb.HTTPCode_ELB_500_Count provider.httpCodeElb500Count AWS ALB aws.applicationelb.HTTPCode_ELB_502_Count provider.httpCodeElb502Count AWS ALB aws.applicationelb.HTTPCode_ELB_503_Count provider.httpCodeElb503Count AWS ALB aws.applicationelb.HTTPCode_ELB_504_Count provider.httpCodeElb504Count AWS ALB aws.applicationelb.HTTPCode_ELB_5XX_Count provider.httpCodeElb5XXCount AWS ALB aws.applicationelb.IPv6ProcessedBytes provider.IpV6ProcessedBytes AWS ALB aws.applicationelb.IPv6RequestCount provider.IpV6RequestCount AWS ALB aws.applicationelb.LambdaTargetProcessedBytes provider.lambdaTargetProcessedBytes AWS ALB aws.applicationelb.NewConnectionCount provider.newConnectionCount AWS ALB aws.applicationelb.ProcessedBytes provider.processedBytes AWS ALB aws.applicationelb.RejectedConnectionCount provider.rejectedConnectionCount AWS ALB aws.applicationelb.RequestCount.byAlb provider.requestCount AWS ALB aws.applicationelb.RuleEvaluations provider.ruleEvaluations AWS ALB aws.applicationelb.RulesEvaluated provider.rulesEvaluated AWS ALB aws.applicationelb.HealthyHostCount provider.healthyHostCount AWS ALB aws.applicationelb.HTTPCode_Target_2XX_Count provider.httpCodeTarget2XXCount AWS ALB aws.applicationelb.HTTPCode_Target_3XX_Count provider.httpCodeTarget3XXCount AWS ALB aws.applicationelb.HTTPCode_Target_4XX_Count provider.httpCodeTarget4XXCount AWS ALB aws.applicationelb.HTTPCode_Target_5XX_Count provider.httpCodeTarget5XXCount AWS ALB aws.applicationelb.LambdaInternalError provider.lambdaInternalError AWS ALB aws.applicationelb.LambdaUserError provider.lambdaUserError AWS ALB aws.applicationelb.NonStickyRequestCount provider.nonStickyRequestCount AWS ALB aws.applicationelb.RequestCount.byTargetGroup provider.requestCount AWS ALB aws.applicationelb.RequestCountPerTarget provider.requestCountPerTarget AWS ALB aws.applicationelb.TargetConnectionErrorCount provider.targetConnectionErrorCount AWS ALB aws.applicationelb.TargetResponseTime provider.targetResponseTime AWS ALB aws.applicationelb.TargetTLSNegotiationErrorCount provider.targetTlsNegotiationErrorCount AWS ALB aws.applicationelb.UnHealthyHostCount provider.unHealthyHostCount AWS API Gateway aws.apigateway.4XXError.byApi provider.4xxError AWS API Gateway aws.apigateway.5XXError.byApi provider.5xxError AWS API Gateway aws.apigateway.CacheHitCount.byApi provider.cacheHitCount AWS API Gateway aws.apigateway.CacheMissCount.byApi provider.cacheMissCount AWS API Gateway aws.apigateway.Count.byApi provider.count AWS API Gateway aws.apigateway.IntegrationLatency.byApi provider.integrationLatency AWS API Gateway aws.apigateway.Latency.byApi provider.latency AWS API Gateway aws.apigateway.4XXError.byResourceWithMetrics provider.4xxError AWS API Gateway aws.apigateway.5XXError.byResourceWithMetrics provider.5xxError AWS API Gateway aws.apigateway.CacheHitCount.byResourceWithMetrics provider.cacheHitCount AWS API Gateway aws.apigateway.CacheMissCount.byResourceWithMetrics provider.cacheMissCount AWS API Gateway aws.apigateway.Count.byResourceWithMetrics provider.count AWS API Gateway aws.apigateway.IntegrationLatency.byResourceWithMetrics provider.integrationLatency AWS API Gateway aws.apigateway.Latency.byResourceWithMetrics provider.latency AWS API Gateway aws.apigateway.4XXError.byStage provider.4xxError AWS API Gateway aws.apigateway.5XXError.byStage provider.5xxError AWS API Gateway aws.apigateway.CacheHitCount.byStage provider.cacheHitCount AWS API Gateway aws.apigateway.CacheMissCount.byStage provider.cacheMissCount AWS API Gateway aws.apigateway.Count.byStage provider.count AWS API Gateway aws.apigateway.IntegrationLatency.byStage provider.integrationLatency AWS API Gateway aws.apigateway.Latency.byStage provider.latency AWS AppSync aws.appsync.4XXError provider.4XXError AWS AppSync aws.appsync.5XXError provider.5XXError AWS AppSync aws.appsync.Latency provider.latency AWS Athena aws.athena.ProcessedBytes provider.processedBytes AWS Athena aws.athena.TotalExecutionTime provider.totalExecutionTime AWS Auto Scaling aws.autoscaling.GroupDesiredCapacity provider.groupDesiredCapacity AWS Auto Scaling aws.autoscaling.GroupInServiceInstances provider.groupInServiceInstances AWS Auto Scaling aws.autoscaling.GroupMaxSize provider.groupMaxSize AWS Auto Scaling aws.autoscaling.GroupMinSize provider.groupMinSize AWS Auto Scaling aws.autoscaling.GroupPendingInstances provider.groupPendingInstances AWS Auto Scaling aws.autoscaling.GroupStandbyInstances provider.groupStandbyInstances AWS Auto Scaling aws.autoscaling.GroupTerminatingInstances provider.groupTerminatingInstances AWS Auto Scaling aws.autoscaling.GroupTotalInstances provider.groupTotalInstances AWS Auto Scaling aws.autoscaling.healthStatus healthStatus AWS Billing aws.billing.EstimatedCharges.byAccountCost provider.estimatedCharges AWS Billing aws.billing.EstimatedCharges.byAccountServiceCost provider.estimatedCharges AWS Billing aws.billing.actualAmount.byBudget provider.actualAmount AWS Billing aws.billing.forecastedAmount.byBudget provider.forecastedAmount AWS Billing aws.billing.limitAmount.byBudget provider.limitAmount AWS Billing aws.billing.EstimatedCharges.byServiceCost provider.estimatedCharges AWS CloudFront aws.cloudfront.4xxErrorRate provider.error4xxErrorRate AWS CloudFront aws.cloudfront.5xxErrorRate provider.error5xxErrorRate AWS CloudFront aws.cloudfront.BytesDownloaded provider.bytesDownloaded AWS CloudFront aws.cloudfront.BytesUploaded provider.bytesUploaded AWS CloudFront aws.cloudfront.Requests provider.requests AWS CloudFront aws.cloudfront.TotalErrorRate provider.totalErrorRate AWS CloudFront aws.lambda.Duration.byEdgeFunction provider.duration AWS CloudFront aws.lambda.Errors.byEdgeFunction provider.errors AWS CloudFront aws.lambda.Invocations.byEdgeFunction provider.invocations AWS CloudFront aws.lambda.Throttles.byEdgeFunction provider.throttles AWS Cognito aws.cognito.FederationSuccesses provider.federationSuccesses AWS Cognito aws.cognito.FederationThrottles provider.federationThrottles AWS Cognito aws.cognito.SignInSuccesses provider.signInSuccesses AWS Cognito aws.cognito.SignInThrottles provider.signInThrottles AWS Cognito aws.cognito.SignUpSuccesses provider.signUpSuccesses AWS Cognito aws.cognito.SignUpThrottles provider.signUpThrottles AWS Cognito aws.cognito.TokenRefreshSuccesses provider.tokenRefreshSuccesses AWS Cognito aws.cognito.TokenRefreshThrottles provider.tokenRefreshThrottles AWS Connect aws.connect.CallRecordingUploadError provider.callRecordingUploadError AWS Connect aws.connect.ContactFlowErrors provider.contactFlowErrors AWS Connect aws.connect.ContactFlowFatalErrors provider.contactFlowFatalErrors AWS Connect aws.connect.MisconfiguredPhoneNumbers provider.misconfiguredPhoneNumbers AWS Connect aws.connect.PublicSigningKeyUsage provider.publicSigningKeyUsage AWS Connect aws.connect.CallsBreachingConcurrencyQuota provider.callsBreachingConcurrencyQuota AWS Connect aws.connect.CallsPerInterval provider.callsPerInterval AWS Connect aws.connect.ConcurrentCalls provider.concurrentCalls AWS Connect aws.connect.ConcurrentCallsPercentage provider.concurrentCallsPercentage AWS Connect aws.connect.MissedCalls provider.missedCalls AWS Connect aws.connect.ThrottledCalls provider.throttledCalls AWS Connect aws.connect.CallBackNotDialableNumber provider.callBackNotDialableNumber AWS Connect aws.connect.LongestQueueWaitTime provider.longestQueueWaitTime AWS Connect aws.connect.QueueCapacityExceededError provider.queueCapacityExceededError AWS Connect aws.connect.QueueSize provider.queueSize AWS Direct Connect aws.dx.ConnectionBpsEgress provider.connectionBpsEgress AWS Direct Connect aws.dx.ConnectionBpsIngress provider.connectionBpsIngress AWS Direct Connect aws.dx.ConnectionCRCErrorCount provider.connectionCRCErrorCount AWS Direct Connect aws.dx.ConnectionLightLevelRx provider.connectionLightLevelRx AWS Direct Connect aws.dx.ConnectionLightLevelTx provider.connectionLightLevelTx AWS Direct Connect aws.dx.ConnectionPpsEgress provider.connectionPpsEgress AWS Direct Connect aws.dx.ConnectionPpsIngress provider.connectionPpsIngress AWS Direct Connect aws.dx.ConnectionState provider.connectionState AWS Direct Connect aws.dx.VirtualInterfaceBpsEgress provider.virtualInterfaceBpsEgress AWS Direct Connect aws.dx.VirtualInterfaceBpsIngress provider.virtualInterfaceBpsIngress AWS Direct Connect aws.dx.VirtualInterfacePpsEgress provider.virtualInterfacePpsEgress AWS Direct Connect aws.dx.VirtualInterfacePpsIngress provider.virtualInterfacePpsIngress AWS DocumentDB aws.docdb.BackupRetentionPeriodStorageUsed.byCluster provider.backupRetentionPeriodStorageUsed AWS DocumentDB aws.docdb.BufferCacheHitRatio.byCluster provider.bufferCacheHitRatio AWS DocumentDB aws.docdb.CPUUtilization.byCluster provider.cpuUtilization AWS DocumentDB aws.docdb.DatabaseConnections.byCluster provider.databaseConnections AWS DocumentDB aws.docdb.DBClusterReplicaLagMaximum.byCluster provider.dbClusterReplicaLagMaximum AWS DocumentDB aws.docdb.DBClusterReplicaLagMinimum.byCluster provider.dbClusterReplicaLagMinimum AWS DocumentDB aws.docdb.DBInstanceReplicaLag.byCluster provider.dbInstanceReplicaLag AWS DocumentDB aws.docdb.DiskQueueDepth.byCluster provider.diskQueueDepth AWS DocumentDB aws.docdb.EngineUptime.byCluster provider.engineUptime AWS DocumentDB aws.docdb.FreeableMemory.byCluster provider.freeableMemory AWS DocumentDB aws.docdb.FreeLocalStorage.byCluster provider.freeLocalStorage AWS DocumentDB aws.docdb.NetworkReceiveThroughput.byCluster provider.networkReceiveThroughput AWS DocumentDB aws.docdb.NetworkThroughput.byCluster provider.networkThroughput AWS DocumentDB aws.docdb.NetworkTransmitThroughput.byCluster provider.networkTransmitThroughput AWS DocumentDB aws.docdb.ReadIOPS.byCluster provider.readIOPS AWS DocumentDB aws.docdb.ReadLatency.byCluster provider.readLatency AWS DocumentDB aws.docdb.ReadThroughput.byCluster provider.readThroughput AWS DocumentDB aws.docdb.SnapshotStorageUsed.byCluster provider.snapshotStorageUsed AWS DocumentDB aws.docdb.SwapUsage.byCluster provider.swapUsage AWS DocumentDB aws.docdb.TotalBackupStorageBilled.byCluster provider.totalBackupStorageBilled AWS DocumentDB aws.docdb.VolumeBytesUsed.byCluster provider.volumeBytesUsed AWS DocumentDB aws.docdb.VolumeReadIOPs.byCluster provider.volumeReadIOPs AWS DocumentDB aws.docdb.VolumeWriteIOPs.byCluster provider.volumeWriteIOPs AWS DocumentDB aws.docdb.WriteIOPS.byCluster provider.writeIOPS AWS DocumentDB aws.docdb.WriteLatency.byCluster provider.writeLatency AWS DocumentDB aws.docdb.WriteThroughput.byCluster provider.writeThroughput AWS DocumentDB aws.docdb.BackupRetentionPeriodStorageUsed.byClusterByRole provider.backupRetentionPeriodStorageUsed AWS DocumentDB aws.docdb.BufferCacheHitRatio.byClusterByRole provider.bufferCacheHitRatio AWS DocumentDB aws.docdb.CPUUtilization.byClusterByRole provider.cpuUtilization AWS DocumentDB aws.docdb.DatabaseConnections.byClusterByRole provider.databaseConnections AWS DocumentDB aws.docdb.DBClusterReplicaLagMaximum.byClusterByRole provider.dbClusterReplicaLagMaximum AWS DocumentDB aws.docdb.DBClusterReplicaLagMinimum.byClusterByRole provider.dbClusterReplicaLagMinimum AWS DocumentDB aws.docdb.DBInstanceReplicaLag.byClusterByRole provider.dbInstanceReplicaLag AWS DocumentDB aws.docdb.DiskQueueDepth.byClusterByRole provider.diskQueueDepth AWS DocumentDB aws.docdb.EngineUptime.byClusterByRole provider.engineUptime AWS DocumentDB aws.docdb.FreeableMemory.byClusterByRole provider.freeableMemory AWS DocumentDB aws.docdb.FreeLocalStorage.byClusterByRole provider.freeLocalStorage AWS DocumentDB aws.docdb.NetworkReceiveThroughput.byClusterByRole provider.networkReceiveThroughput AWS DocumentDB aws.docdb.NetworkThroughput.byClusterByRole provider.networkThroughput AWS DocumentDB aws.docdb.NetworkTransmitThroughput.byClusterByRole provider.networkTransmitThroughput AWS DocumentDB aws.docdb.ReadIOPS.byClusterByRole provider.readIOPS AWS DocumentDB aws.docdb.ReadLatency.byClusterByRole provider.readLatency AWS DocumentDB aws.docdb.ReadThroughput.byClusterByRole provider.readThroughput AWS DocumentDB aws.docdb.SnapshotStorageUsed.byClusterByRole provider.snapshotStorageUsed AWS DocumentDB aws.docdb.SwapUsage.byClusterByRole provider.swapUsage AWS DocumentDB aws.docdb.TotalBackupStorageBilled.byClusterByRole provider.totalBackupStorageBilled AWS DocumentDB aws.docdb.VolumeBytesUsed.byClusterByRole provider.volumeBytesUsed AWS DocumentDB aws.docdb.VolumeReadIOPs.byClusterByRole provider.volumeReadIOPs AWS DocumentDB aws.docdb.VolumeWriteIOPs.byClusterByRole provider.volumeWriteIOPs AWS DocumentDB aws.docdb.WriteIOPS.byClusterByRole provider.writeIOPS AWS DocumentDB aws.docdb.WriteLatency.byClusterByRole provider.writeLatency AWS DocumentDB aws.docdb.WriteThroughput.byClusterByRole provider.writeThroughput AWS DocumentDB aws.docdb.BackupRetentionPeriodStorageUsed.byInstance provider.backupRetentionPeriodStorageUsed AWS DocumentDB aws.docdb.BufferCacheHitRatio.byInstance provider.bufferCacheHitRatio AWS DocumentDB aws.docdb.CPUUtilization.byInstance provider.cpuUtilization AWS DocumentDB aws.docdb.DatabaseConnections.byInstance provider.databaseConnections AWS DocumentDB aws.docdb.DBClusterReplicaLagMaximum.byInstance provider.dbClusterReplicaLagMaximum AWS DocumentDB aws.docdb.DBClusterReplicaLagMinimum.byInstance provider.dbClusterReplicaLagMinimum AWS DocumentDB aws.docdb.DBInstanceReplicaLag.byInstance provider.dbInstanceReplicaLag AWS DocumentDB aws.docdb.DiskQueueDepth.byInstance provider.diskQueueDepth AWS DocumentDB aws.docdb.EngineUptime.byInstance provider.engineUptime AWS DocumentDB aws.docdb.FreeableMemory.byInstance provider.freeableMemory AWS DocumentDB aws.docdb.FreeLocalStorage.byInstance provider.freeLocalStorage AWS DocumentDB aws.docdb.NetworkReceiveThroughput.byInstance provider.networkReceiveThroughput AWS DocumentDB aws.docdb.NetworkThroughput.byInstance provider.networkThroughput AWS DocumentDB aws.docdb.NetworkTransmitThroughput.byInstance provider.networkTransmitThroughput AWS DocumentDB aws.docdb.ReadIOPS.byInstance provider.readIOPS AWS DocumentDB aws.docdb.ReadLatency.byInstance provider.readLatency AWS DocumentDB aws.docdb.ReadThroughput.byInstance provider.readThroughput AWS DocumentDB aws.docdb.SnapshotStorageUsed.byInstance provider.snapshotStorageUsed AWS DocumentDB aws.docdb.SwapUsage.byInstance provider.swapUsage AWS DocumentDB aws.docdb.TotalBackupStorageBilled.byInstance provider.totalBackupStorageBilled AWS DocumentDB aws.docdb.VolumeBytesUsed.byInstance provider.volumeBytesUsed AWS DocumentDB aws.docdb.VolumeReadIOPs.byInstance provider.volumeReadIOPs AWS DocumentDB aws.docdb.VolumeWriteIOPs.byInstance provider.volumeWriteIOPs AWS DocumentDB aws.docdb.WriteIOPS.byInstance provider.writeIOPS AWS DocumentDB aws.docdb.WriteLatency.byInstance provider.writeLatency AWS DocumentDB aws.docdb.WriteThroughput.byInstance provider.writeThroughput AWS DynamoDB aws.dynamodb.ConsumedReadCapacityUnits.byGlobalSecondaryIndex provider.consumedReadCapacityUnits AWS DynamoDB aws.dynamodb.ConsumedWriteCapacityUnits.byGlobalSecondaryIndex provider.consumedWriteCapacityUnits AWS DynamoDB aws.dynamodb.OnlineIndexConsumedWriteCapacity provider.onlineIndexConsumedWriteCapacity AWS DynamoDB aws.dynamodb.OnlineIndexPercentageProgress provider.onlineIndexPercentageProgress AWS DynamoDB aws.dynamodb.OnlineIndexThrottleEvents provider.onlineIndexThrottleEvents AWS DynamoDB aws.dynamodb.ProvisionedReadCapacityUnits.byGlobalSecondaryIndex provider.provisionedReadCapacityUnits AWS DynamoDB aws.dynamodb.ProvisionedWriteCapacityUnits.byGlobalSecondaryIndex provider.provisionedWriteCapacityUnits AWS DynamoDB aws.dynamodb.ReadThrottleEvents.byGlobalSecondaryIndex provider.readThrottleEvents AWS DynamoDB aws.dynamodb.WriteThrottleEvents.byGlobalSecondaryIndex provider.writeThrottleEvents AWS DynamoDB aws.dynamodb.SystemErrors.byRegion provider.systemErrors AWS DynamoDB aws.dynamodb.UserErrors.byRegion provider.userErrors AWS DynamoDB aws.dynamodb.ConditionalCheckFailedRequests provider.conditionalCheckFailedRequests AWS DynamoDB aws.dynamodb.ConsumedReadCapacityUnits.byTable provider.consumedReadCapacityUnits AWS DynamoDB aws.dynamodb.ConsumedWriteCapacityUnits.byTable provider.consumedWriteCapacityUnits AWS DynamoDB aws.dynamoDb.ItemCount provider.itemCount AWS DynamoDB aws.dynamodb.PendingReplicationCount provider.pendingReplicationCount AWS DynamoDB aws.dynamodb.ProvisionedReadCapacityUnits.byTable provider.provisionedReadCapacityUnits AWS DynamoDB aws.dynamodb.ProvisionedWriteCapacityUnits.byTable provider.provisionedWriteCapacityUnits AWS DynamoDB aws.dynamodb.ReadThrottleEvents.byTable provider.readThrottleEvents AWS DynamoDB aws.dynamodb.ReplicationLatency provider.replicationLatency AWS DynamoDB aws.dynamodb.ReturnedItemCount provider.queryReturnedItemCount AWS DynamoDB aws.dynamodb.ReturnedItemCount.scan provider.scanReturnedItemCount AWS DynamoDB aws.dynamodb.SuccessfulRequestLatency.batchGet provider.batchGetSuccessfulRequestLatency AWS DynamoDB aws.dynamodb.SuccessfulRequestLatency.batchWrite provider.batchWriteSuccessfulRequestLatency AWS DynamoDB aws.dynamodb.SuccessfulRequestLatency.delete provider.deleteSuccessfulRequestLatency AWS DynamoDB aws.dynamodb.SuccessfulRequestLatency.get provider.getSuccessfulRequestLatency AWS DynamoDB aws.dynamodb.SuccessfulRequestLatency.put provider.putSuccessfulRequestLatency AWS DynamoDB aws.dynamodb.SuccessfulRequestLatency.query provider.querySuccessfulRequestLatency AWS DynamoDB aws.dynamodb.SuccessfulRequestLatency.scan provider.scanSuccessfulRequestLatency AWS DynamoDB aws.dynamodb.SuccessfulRequestLatency.update provider.updateSuccessfulRequestLatency AWS DynamoDB aws.dynamodb.SystemErrors.batchGet provider.batchGetSystemErrors AWS DynamoDB aws.dynamodb.SystemErrors.batchWrite provider.batchWriteSystemErrors AWS DynamoDB aws.dynamodb.SystemErrors.byTable provider.systemErrors AWS DynamoDB aws.dynamodb.SystemErrors.delete provider.deleteSystemErrors AWS DynamoDB aws.dynamodb.SystemErrors.get provider.getSystemErrors AWS DynamoDB aws.dynamodb.SystemErrors.put provider.putSystemErrors AWS DynamoDB aws.dynamodb.SystemErrors.query provider.querySystemErrors AWS DynamoDB aws.dynamodb.SystemErrors.scan provider.scanSystemErrors AWS DynamoDB aws.dynamodb.SystemErrors.update provider.updateSystemErrors AWS DynamoDB aws.dynamoDb.TableSizeBytes provider.tableSizeBytes AWS DynamoDB aws.dynamodb.ThrottledRequests.batchGet provider.batchGetThrottledRequests AWS DynamoDB aws.dynamodb.ThrottledRequests.batchWrite provider.batchWriteThrottledRequests AWS DynamoDB aws.dynamodb.ThrottledRequests.delete provider.deleteThrottledRequests AWS DynamoDB aws.dynamodb.ThrottledRequests.get provider.getThrottledRequests AWS DynamoDB aws.dynamodb.ThrottledRequests.put provider.putThrottledRequests AWS DynamoDB aws.dynamodb.ThrottledRequests.query provider.queryThrottledRequests AWS DynamoDB aws.dynamodb.ThrottledRequests.scan provider.scanThrottledRequests AWS DynamoDB aws.dynamodb.ThrottledRequests.update provider.updateThrottledRequests AWS DynamoDB aws.dynamodb.UserErrors.byTable provider.userErrors AWS DynamoDB aws.dynamodb.WriteThrottleEvents.byTable provider.writeThrottleEvents AWS EBS aws.ebs.BurstBalance provider.burstBalance AWS EBS aws.ebs.VolumeConsumedReadWriteOps provider.volumeConsumedReadWriteOps AWS EBS aws.ebs.VolumeIdleTime provider.volumeIdleTime AWS EBS aws.ebs.VolumeQueueLength provider.volumeQueueLength AWS EBS aws.ebs.VolumeReadBytes provider.volumeReadBytes AWS EBS aws.ebs.VolumeReadOps provider.volumeReadOps AWS EBS aws.ebs.VolumeThroughputPercentage provider.volumeThroughputPercentage AWS EBS aws.ebs.VolumeTotalReadTime provider.volumeTotalReadTime AWS EBS aws.ebs.VolumeTotalWriteTime provider.volumeTotalWriteTime AWS EBS aws.ebs.VolumeWriteBytes provider.volumeWriteBytes AWS EBS aws.ebs.VolumeWriteOps provider.volumeWriteOps AWS EC2 aws.ec2.CPUCreditBalance provider.cpuCreditBalance AWS EC2 aws.ec2.CPUCreditUsage provider.cpuCreditUsage AWS EC2 aws.ec2.CPUSurplusCreditBalance provider.cpuSurplusCreditBalance AWS EC2 aws.ec2.CPUSurplusCreditsCharged provider.cpuSurplusCreditsCharged AWS EC2 aws.ec2.CPUUtilization provider.cpuUtilization AWS EC2 aws.ec2.DiskReadBytes provider.diskReadBytes AWS EC2 aws.ec2.DiskReadOps provider.diskReadOps AWS EC2 aws.ec2.DiskWriteBytes provider.diskWriteBytes AWS EC2 aws.ec2.DiskWriteOps provider.diskWriteOps AWS EC2 aws.ec2.NetworkIn provider.networkInBytes AWS EC2 aws.ec2.NetworkOut provider.networkOutBytes AWS EC2 aws.ec2.NetworkPacketsIn provider.networkInPackets AWS EC2 aws.ec2.NetworkPacketsOut provider.networkOutPackets AWS EC2 aws.ec2.StatusCheckFailed provider.statusCheckFailed AWS EC2 aws.ec2.StatusCheckFailed_Instance provider.statusCheckFailedInstance AWS EC2 aws.ec2.StatusCheckFailed_System provider.statusCheckFailedSystem AWS ECS aws.ecs.activeServicesCount.byCluster provider.activeServicesCount AWS ECS aws.ecs.CPUReservation provider.cpuReservation AWS ECS aws.ecs.CPUUtilization.byCluster provider.cpuUtilization AWS ECS aws.ecs.MemoryReservation provider.memoryReservation AWS ECS aws.ecs.MemoryUtilization.byCluster provider.memoryUtilization AWS ECS aws.ecs.pendingTasksCount.byCluster provider.pendingTasksCount AWS ECS aws.ecs.registeredContainerInstancesCount.byCluster provider.registeredContainerInstancesCount AWS ECS aws.ecs.runningTasksCount.byCluster provider.runningTasksCount AWS ECS aws.ecs.CPUUtilization.byService provider.cpuUtilization AWS ECS aws.ecs.desiredCount.byService provider.desiredCount AWS ECS aws.ecs.MemoryUtilization.byService provider.memoryUtilization AWS ECS aws.ecs.pendingCount.byService provider.pendingCount AWS ECS aws.ecs.runningCount.byService provider.runningCount AWS EFS aws.efs.BurstCreditBalance provider.burstCreditBalance AWS EFS aws.efs.ClientConnections provider.clientConnections AWS EFS aws.efs.DataReadIOBytes provider.dataReadIOBytes AWS EFS aws.efs.DataWriteIOBytes provider.dataWriteIOBytes AWS EFS aws.efs.lastKnownSizeInBytes provider.lastKnownSizeInBytes AWS EFS aws.efs.MetadataIOBytes provider.metadataIOBytes AWS EFS aws.efs.PercentIOLimit provider.percentIOLimit AWS EFS aws.efs.PermittedThroughput provider.permittedThroughput AWS EFS aws.efs.TotalIOBytes provider.totalIOBytes AWS ElasticSearch aws.es.2xx provider.2xx AWS ElasticSearch aws.es.3xx provider.3xx AWS ElasticSearch aws.es.4xx provider.4xx AWS ElasticSearch aws.es.5xx provider.5xx AWS ElasticSearch aws.es.AutomatedSnapshotFailure provider.AutomatedSnapshotFailure AWS ElasticSearch aws.es.ClusterIndexWritesBlocked provider.ClusterIndexWritesBlocked AWS ElasticSearch aws.es.ClusterStatus.green provider.ClusterStatus.green AWS ElasticSearch aws.es.ClusterStatus.red provider.ClusterStatus.red AWS ElasticSearch aws.es.ClusterStatus.yellow provider.ClusterStatus.yellow AWS ElasticSearch aws.es.ClusterUsedSpace provider.ClusterUsedSpace AWS ElasticSearch aws.es.CPUCreditBalance provider.CPUCreditBalance AWS ElasticSearch aws.es.CPUUtilization.byCluster provider.CPUUtilization AWS ElasticSearch aws.es.DeletedDocuments provider.DeletedDocuments AWS ElasticSearch aws.es.DiskQueueDepth provider.DiskQueueDepth AWS ElasticSearch aws.es.ElasticsearchRequests provider.ElasticsearchRequests AWS ElasticSearch aws.es.FreeStorageSpace.byCluster provider.FreeStorageSpace AWS ElasticSearch aws.es.IndexingLatency.byCluster provider.IndexingLatency AWS ElasticSearch aws.es.IndexingRate.byCluster provider.IndexingRate AWS ElasticSearch aws.es.InvalidHostHeaderRequests provider.InvalidHostHeaderRequests AWS ElasticSearch aws.es.JVMGCOldCollectionCount.byCluster provider.JVMGCOldCollectionCount AWS ElasticSearch aws.es.JVMGCOldCollectionTime.byCluster provider.JVMGCOldCollectionTime AWS ElasticSearch aws.es.JVMGCYoungCollectionCount.byCluster provider.JVMGCYoungCollectionCount AWS ElasticSearch aws.es.JVMGCYoungCollectionTime.byCluster provider.JVMGCYoungCollectionTime AWS ElasticSearch aws.es.JVMMemoryPressure.byCluster provider.JVMMemoryPressure AWS ElasticSearch aws.es.KibanaHealthyNodes provider.KibanaHealthyNodes AWS ElasticSearch aws.es.KMSKeyError provider.KMSKeyError AWS ElasticSearch aws.es.KMSKeyInaccessible provider.KMSKeyInaccessible AWS ElasticSearch aws.es.MasterCPUCreditBalance provider.MasterCPUCreditBalance AWS ElasticSearch aws.es.MasterCPUUtilization provider.MasterCPUUtilization AWS ElasticSearch aws.es.MasterJVMMemoryPressure provider.MasterJVMMemoryPressure AWS ElasticSearch aws.es.MasterReachableFromNode provider.MasterReachableFromNode AWS ElasticSearch aws.es.Nodes provider.Nodes AWS ElasticSearch aws.es.ReadIOPS provider.ReadIOPS AWS ElasticSearch aws.es.ReadLatency provider.ReadLatency AWS ElasticSearch aws.es.ReadThroughput provider.ReadThroughput AWS ElasticSearch aws.es.SearchableDocuments provider.SearchableDocuments AWS ElasticSearch aws.es.SearchLatency.byCluster provider.SearchLatency AWS ElasticSearch aws.es.SearchRate.byCluster provider.SearchRate AWS ElasticSearch aws.es.SysMemoryUtilization.byCluster provider.SysMemoryUtilization AWS ElasticSearch aws.es.ThreadpoolBulkQueue.byCluster provider.ThreadpoolBulkQueue AWS ElasticSearch aws.es.ThreadpoolBulkRejected.byCluster provider.ThreadpoolBulkRejected AWS ElasticSearch aws.es.ThreadpoolBulkThreads.byCluster provider.ThreadpoolBulkThreads AWS ElasticSearch aws.es.ThreadpoolForce_mergeQueue.byCluster provider.ThreadpoolForce_mergeQueue AWS ElasticSearch aws.es.ThreadpoolForce_mergeRejected.byCluster provider.ThreadpoolForce_mergeRejected AWS ElasticSearch aws.es.ThreadpoolForce_mergeThreads.byCluster provider.ThreadpoolForce_mergeThreads AWS ElasticSearch aws.es.ThreadpoolIndexQueue.byCluster provider.ThreadpoolIndexQueue AWS ElasticSearch aws.es.ThreadpoolIndexRejected.byCluster provider.ThreadpoolIndexRejected AWS ElasticSearch aws.es.ThreadpoolIndexThreads.byCluster provider.ThreadpoolIndexThreads AWS ElasticSearch aws.es.ThreadpoolSearchQueue.byCluster provider.ThreadpoolSearchQueue AWS ElasticSearch aws.es.ThreadpoolSearchRejected.byCluster provider.ThreadpoolSearchRejected AWS ElasticSearch aws.es.ThreadpoolSearchThreads.byCluster provider.ThreadpoolSearchThreads AWS ElasticSearch aws.es.WriteIOPS provider.WriteIOPS AWS ElasticSearch aws.es.WriteLatency provider.WriteLatency AWS ElasticSearch aws.es.WriteThroughput provider.WriteThroughput AWS ElasticSearch aws.es.CPUUtilization.byNode provider.CPUUtilization AWS ElasticSearch aws.es.FreeStorageSpace.byNode provider.FreeStorageSpace AWS ElasticSearch aws.es.IndexingLatency.byNode provider.IndexingLatency AWS ElasticSearch aws.es.IndexingRate.byNode provider.IndexingRate AWS ElasticSearch aws.es.JVMGCOldCollectionCount.byNode provider.JVMGCOldCollectionCount AWS ElasticSearch aws.es.JVMGCOldCollectionTime.byNode provider.JVMGCOldCollectionTime AWS ElasticSearch aws.es.JVMGCYoungCollectionCount.byNode provider.JVMGCYoungCollectionCount AWS ElasticSearch aws.es.JVMGCYoungCollectionTime.byNode provider.JVMGCYoungCollectionTime AWS ElasticSearch aws.es.JVMMemoryPressure.byNode provider.JVMMemoryPressure AWS ElasticSearch aws.es.SearchLatency.byNode provider.SearchLatency AWS ElasticSearch aws.es.SearchRate.byNode provider.SearchRate AWS ElasticSearch aws.es.SysMemoryUtilization.byNode provider.SysMemoryUtilization AWS ElasticSearch aws.es.ThreadpoolBulkQueue.byNode provider.ThreadpoolBulkQueue AWS ElasticSearch aws.es.ThreadpoolBulkRejected.byNode provider.ThreadpoolBulkRejected AWS ElasticSearch aws.es.ThreadpoolBulkThreads.byNode provider.ThreadpoolBulkThreads AWS ElasticSearch aws.es.ThreadpoolForce_mergeQueue.byNode provider.ThreadpoolForce_mergeQueue AWS ElasticSearch aws.es.ThreadpoolForce_mergeRejected.byNode provider.ThreadpoolForce_mergeRejected AWS ElasticSearch aws.es.ThreadpoolForce_mergeThreads.byNode provider.ThreadpoolForce_mergeThreads AWS ElasticSearch aws.es.ThreadpoolIndexQueue.byNode provider.ThreadpoolIndexQueue AWS ElasticSearch aws.es.ThreadpoolIndexRejected.byNode provider.ThreadpoolIndexRejected AWS ElasticSearch aws.es.ThreadpoolIndexThreads.byNode provider.ThreadpoolIndexThreads AWS ElasticSearch aws.es.ThreadpoolSearchQueue.byNode provider.ThreadpoolSearchQueue AWS ElasticSearch aws.es.ThreadpoolSearchRejected.byNode provider.ThreadpoolSearchRejected AWS ElasticSearch aws.es.ThreadpoolSearchThreads.byNode provider.ThreadpoolSearchThreads AWS ElastiCache aws.elasticache.CPUCreditBalance.byMemcachedCluster provider.cpuCreditBalance AWS ElastiCache aws.elasticache.CPUCreditUsage.byMemcachedCluster provider.cpuCreditUsage AWS ElastiCache aws.elasticache.CPUUtilization.byMemcachedCluster provider.cpuUtilization AWS ElastiCache aws.elasticache.DatabaseMemoryUsagePercentage.byMemcachedCluster provider.databaseMemoryUsagePercentage AWS ElastiCache aws.elasticache.FreeableMemory.byMemcachedCluster provider.freeableMemory AWS ElastiCache aws.elasticache.NetworkBytesIn.byMemcachedCluster provider.networkBytesIn AWS ElastiCache aws.elasticache.NetworkBytesOut.byMemcachedCluster provider.networkBytesOut AWS ElastiCache aws.elasticache.SwapUsage.byMemcachedCluster provider.swapUsage AWS ElastiCache aws.elasticache.BytesReadIntoMemcached provider.bytesReadIntoMemcached AWS ElastiCache aws.elasticache.BytesUsedForCacheItems provider.bytesUsedForCacheItems AWS ElastiCache aws.elasticache.BytesUsedForHash provider.bytesUsedForHash AWS ElastiCache aws.elasticache.BytesWrittenOutFromMemcached provider.bytesWrittenOutFromMemcached AWS ElastiCache aws.elasticache.CasBadval provider.casBadval AWS ElastiCache aws.elasticache.CasHits provider.casHits AWS ElastiCache aws.elasticache.CasMisses provider.casMisses AWS ElastiCache aws.elasticache.CmdConfigGet provider.cmdConfigGet AWS ElastiCache aws.elasticache.CmdConfigSet provider.cmdConfigSet AWS ElastiCache aws.elasticache.CmdFlush provider.cmdFlush AWS ElastiCache aws.elasticache.CmdGet provider.cmdGet AWS ElastiCache aws.elasticache.CmdSet provider.cmdSet AWS ElastiCache aws.elasticache.CmdTouch provider.cmdTouch AWS ElastiCache aws.elasticache.CPUCreditBalance.byMemcachedNode provider.cpuCreditBalance AWS ElastiCache aws.elasticache.CPUCreditUsage.byMemcachedNode provider.cpuCreditUsage AWS ElastiCache aws.elasticache.CPUUtilization.byMemcachedNode provider.cpuUtilization AWS ElastiCache aws.elasticache.CurrConfig provider.currConfig AWS ElastiCache aws.elasticache.CurrConnections.byMemcachedNode provider.currConnections AWS ElastiCache aws.elasticache.CurrItems.byMemcachedNode provider.currItems AWS ElastiCache aws.elasticache.DatabaseMemoryUsagePercentage.byMemcachedNode provider.databaseMemoryUsagePercentage AWS ElastiCache aws.elasticache.DecrHits provider.decrHits AWS ElastiCache aws.elasticache.DecrMisses provider.decrMisses AWS ElastiCache aws.elasticache.DeleteHits provider.deleteHits AWS ElastiCache aws.elasticache.DeleteMisses provider.deleteMisses AWS ElastiCache aws.elasticache.EvictedUnfetched provider.evictedUnfetched AWS ElastiCache aws.elasticache.Evictions.byMemcachedNode provider.evictions AWS ElastiCache aws.elasticache.ExpiredUnfetched provider.expiredUnfetched AWS ElastiCache aws.elasticache.FreeableMemory.byMemcachedNode provider.freeableMemory AWS ElastiCache aws.elasticache.GetHits provider.getHits AWS ElastiCache aws.elasticache.GetMisses provider.getMisses AWS ElastiCache aws.elasticache.IncrHits provider.incrHits AWS ElastiCache aws.elasticache.IncrMisses provider.incrMisses AWS ElastiCache aws.elasticache.NetworkBytesIn.byMemcachedNode provider.networkBytesIn AWS ElastiCache aws.elasticache.NetworkBytesOut.byMemcachedNode provider.networkBytesOut AWS ElastiCache aws.elasticache.NewConnections.byMemcachedNode provider.newConnections AWS ElastiCache aws.elasticache.NewItems provider.newItems AWS ElastiCache aws.elasticache.Reclaimed.byMemcachedNode provider.reclaimed AWS ElastiCache aws.elasticache.SlabsMoved provider.slabsMoved AWS ElastiCache aws.elasticache.SwapUsage.byMemcachedNode provider.swapUsage AWS ElastiCache aws.elasticache.TouchHits provider.touchHits AWS ElastiCache aws.elasticache.TouchMisses provider.touchMisses AWS ElastiCache aws.elasticache.UnusedMemory provider.unusedMemory AWS ElastiCache aws.elasticache.ActiveDefragHits.byRedisCluster provider.activeDefragHits AWS ElastiCache aws.elasticache.CPUCreditBalance.byRedisCluster provider.cpuCreditBalance AWS ElastiCache aws.elasticache.CPUCreditUsage.byRedisCluster provider.cpuCreditUsage AWS ElastiCache aws.elasticache.CPUUtilization.byRedisCluster provider.cpuUtilization AWS ElastiCache aws.elasticache.DatabaseMemoryUsagePercentage.byRedisCluster provider.databaseMemoryUsagePercentage AWS ElastiCache aws.elasticache.EngineCPUUtilization.byRedisCluster provider.engineCpuUtilization AWS ElastiCache aws.elasticache.FreeableMemory.byRedisCluster provider.freeableMemory AWS ElastiCache aws.elasticache.NetworkBytesIn.byRedisCluster provider.networkBytesIn AWS ElastiCache aws.elasticache.NetworkBytesOut.byRedisCluster provider.networkBytesOut AWS ElastiCache aws.elasticache.StreamBasedCmds.byRedisCluster provider.streamBasedCmds AWS ElastiCache aws.elasticache.SwapUsage.byRedisCluster provider.swapUsage AWS ElastiCache aws.elasticache.ActiveDefragHits.byRedisNode provider.activeDefragHits AWS ElastiCache aws.elasticache.BytesUsedForCache provider.bytesUsedForCache AWS ElastiCache aws.elasticache.CacheHitRate provider.cacheHitRate AWS ElastiCache aws.elasticache.CacheHits provider.cacheHits AWS ElastiCache aws.elasticache.CacheMisses provider.cacheMisses AWS ElastiCache aws.elasticache.CPUCreditBalance.byRedisNode provider.cpuCreditBalance AWS ElastiCache aws.elasticache.CPUCreditUsage.byRedisNode provider.cpuCreditUsage AWS ElastiCache aws.elasticache.CPUUtilization.byRedisNode provider.cpuUtilization AWS ElastiCache aws.elasticache.CurrConnections.byRedisNode provider.currConnections AWS ElastiCache aws.elasticache.CurrItems.byRedisNode provider.currItems AWS ElastiCache aws.elasticache.DatabaseMemoryUsagePercentage.byRedisNode provider.databaseMemoryUsagePercentage AWS ElastiCache aws.elasticache.DB0AverageTTL provider.db0AverageTtl AWS ElastiCache aws.elasticache.EngineCPUUtilization.byRedisNode provider.engineCpuUtilization AWS ElastiCache aws.elasticache.EvalBasedCmds provider.evalBasedCmds AWS ElastiCache aws.elasticache.EvalBasedCmdsLatency provider.evalBasedCmdsLatency AWS ElastiCache aws.elasticache.Evictions.byRedisNode provider.evictions AWS ElastiCache aws.elasticache.FreeableMemory.byRedisNode provider.freeableMemory AWS ElastiCache aws.elasticache.GeoSpatialBasedCmds provider.geoSpatialBasedCmds AWS ElastiCache aws.elasticache.GeoSpatialBasedCmdsLatency provider.geoSpatialBasedCmdsLatency AWS ElastiCache aws.elasticache.GetTypeCmds provider.getTypeCmds AWS ElastiCache aws.elasticache.GetTypeCmdsLatency provider.getTypeCmdsLatency AWS ElastiCache aws.elasticache.HashBasedCmds provider.hashBasedCmds AWS ElastiCache aws.elasticache.HashBasedCmdsLatency provider.hashBasedCmdsLatency AWS ElastiCache aws.elasticache.HyperLogLogBasedCmds provider.hyperLogLogBasedCmds AWS ElastiCache aws.elasticache.HyperLogLogBasedCmdsLatency provider.hyperLogLogBasedCmdsLatency AWS ElastiCache aws.elasticache.KeyBasedCmds provider.keyBasedCmds AWS ElastiCache aws.elasticache.KeyBasedCmdsLatency provider.keyBasedCmdsLatency AWS ElastiCache aws.elasticache.ListBasedCmds provider.listBasedCmds AWS ElastiCache aws.elasticache.ListBasedCmdsLatency provider.listBasedCmdsLatency AWS ElastiCache aws.elasticache.MasterLinkHealthStatus provider.masterLinkHealthStatus AWS ElastiCache aws.elasticache.MemoryFragmentationRatio provider.memoryFragmentationRatio AWS ElastiCache aws.elasticache.NetworkBytesIn.byRedisNode provider.networkBytesIn AWS ElastiCache aws.elasticache.NetworkBytesOut.byRedisNode provider.networkBytesOut AWS ElastiCache aws.elasticache.NewConnections.byRedisNode provider.newConnections AWS ElastiCache aws.elasticache.PubSubBasedCmds provider.pubSubBasedCmds AWS ElastiCache aws.elasticache.PubSubBasedCmdsLatency provider.pubSubBasedCmdsLatency AWS ElastiCache aws.elasticache.Reclaimed.byRedisNode provider.reclaimed AWS ElastiCache aws.elasticache.ReplicationBytes provider.replicationBytes AWS ElastiCache aws.elasticache.ReplicationLag provider.replicationLag AWS ElastiCache aws.elasticache.SaveInProgress provider.saveInProgress AWS ElastiCache aws.elasticache.SetBasedCmds provider.setBasedCmds AWS ElastiCache aws.elasticache.SetBasedCmdsLatency provider.setBasedCmdsLatency AWS ElastiCache aws.elasticache.SetTypeCmds provider.setTypeCmds AWS ElastiCache aws.elasticache.SetTypeCmdsLatency provider.setTypeCmdsLatency AWS ElastiCache aws.elasticache.SortedBasedCmdsLatency provider.sortedBasedCmdsLatency AWS ElastiCache aws.elasticache.SortedSetBasedCmds provider.sortedSetBasedCmds AWS ElastiCache aws.elasticache.StreamBasedCmds.byRedisNode provider.streamBasedCmds AWS ElastiCache aws.elasticache.StreamBasedCmdsLatency provider.streamBasedCmdsLatency AWS ElastiCache aws.elasticache.StringBasedCmds provider.stringBasedCmds AWS ElastiCache aws.elasticache.StringBasedCmdsLatency provider.stringBasedCmdsLatency AWS ElastiCache aws.elasticache.SwapUsage.byRedisNode provider.swapUsage AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP10.byEnvironment provider.applicationLatencyP10 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP50.byEnvironment provider.applicationLatencyP50 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP75.byEnvironment provider.applicationLatencyP75 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP85.byEnvironment provider.applicationLatencyP85 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP90.byEnvironment provider.applicationLatencyP90 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP95.byEnvironment provider.applicationLatencyP95 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP99.9.byEnvironment provider.applicationLatencyP99.9 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP99.byEnvironment provider.applicationLatencyP99 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequests2xx.byEnvironment provider.applicationRequests2xx AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequests3xx.byEnvironment provider.applicationRequests3xx AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequests4xx.byEnvironment provider.applicationRequests4xx AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequests5xx.byEnvironment provider.applicationRequests5xx AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequestsTotal.byEnvironment provider.applicationRequestsTotal AWS Elastic Beanstalk aws.elasticbeanstalk.EnvironmentHealth provider.environmentHealth AWS Elastic Beanstalk aws.elasticbeanstalk.InstancesDegraded provider.instancesDegraded AWS Elastic Beanstalk aws.elasticbeanstalk.InstancesInfo provider.instancesInfo AWS Elastic Beanstalk aws.elasticbeanstalk.InstancesNoData provider.instancesNoData AWS Elastic Beanstalk aws.elasticbeanstalk.InstancesOk provider.instancesOk AWS Elastic Beanstalk aws.elasticbeanstalk.InstancesPending provider.instancesPending AWS Elastic Beanstalk aws.elasticbeanstalk.InstancesSevere provider.instancesSevere AWS Elastic Beanstalk aws.elasticbeanstalk.InstancesUnknown provider.instancesUnknown AWS Elastic Beanstalk aws.elasticbeanstalk.InstancesWarning provider.instancesWarning AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP10.byInstance provider.applicationLatencyP10 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP50.byInstance provider.applicationLatencyP50 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP75.byInstance provider.applicationLatencyP75 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP85.byInstance provider.applicationLatencyP85 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP90.byInstance provider.applicationLatencyP90 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP95.byInstance provider.applicationLatencyP95 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP99.9.byInstance provider.applicationLatencyP99.9 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationLatencyP99.byInstance provider.applicationLatencyP99 AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequests2xx.byInstance provider.applicationRequests2xx AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequests3xx.byInstance provider.applicationRequests3xx AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequests4xx.byInstance provider.applicationRequests4xx AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequests5xx.byInstance provider.applicationRequests5xx AWS Elastic Beanstalk aws.elasticbeanstalk.ApplicationRequestsTotal.byInstance provider.applicationRequestsTotal AWS Elastic Beanstalk aws.elasticbeanstalk.CPUIdle provider.cpuIdle AWS Elastic Beanstalk aws.elasticbeanstalk.CPUIowait provider.cpuIowait AWS Elastic Beanstalk aws.elasticbeanstalk.CPUIrq provider.cpuIrq AWS Elastic Beanstalk aws.elasticbeanstalk.CPUNice provider.cpuNice AWS Elastic Beanstalk aws.elasticbeanstalk.CPUSoftirq provider.cpuSoftirq AWS Elastic Beanstalk aws.elasticbeanstalk.CPUSystem provider.cpuSystem AWS Elastic Beanstalk aws.elasticbeanstalk.CPUUser provider.cpuUser AWS Elastic Beanstalk aws.elasticbeanstalk.InstanceHealth provider.instanceHealth AWS Elastic Beanstalk aws.elasticbeanstalk.LoadAverage1min provider.loadAverage1min AWS Elastic Beanstalk aws.elasticbeanstalk.RootFilesystemUtil provider.rootFilesystemUtil AWS ELB aws.elb.BackendConnectionErrors provider.backendConnectionErrors AWS ELB aws.elb.EstimatedALBActiveConnectionCount provider.estimatedAlbActiveConnectionCount AWS ELB aws.elb.EstimatedALBConsumedLCUs provider.estimatedAlbConsumedLcus AWS ELB aws.elb.EstimatedALBNewConnectionCount provider.estimatedAlbNewConnectionCount AWS ELB aws.elb.EstimatedProcessedBytes provider.estimatedProcessedBytes AWS ELB aws.elb.HealthyHostCount provider.healthyHostCount AWS ELB aws.elb.HTTPCode_Backend_2XX provider.httpCodeBackend2XX AWS ELB aws.elb.HTTPCode_Backend_3XX provider.httpCodeBackend3XX AWS ELB aws.elb.HTTPCode_Backend_4XX provider.httpCodeBackend4XX AWS ELB aws.elb.HTTPCode_Backend_5XX provider.httpCodeBackend5XX AWS ELB aws.elb.HTTPCode_ELB_4XX provider.httpCodeElb4XX AWS ELB aws.elb.HTTPCode_ELB_5XX provider.httpCodeElb5XX AWS ELB aws.elb.Latency provider.latency AWS ELB aws.elb.RequestCount provider.requestCount AWS ELB aws.elb.SpilloverCount provider.spilloverCount AWS ELB aws.elb.SurgeQueueLength provider.surgeQueueLength AWS ELB aws.elb.UnhealthyHostCount provider.unhealthyHostCount AWS EMR aws.elasticmapreduce.AppsCompleted provider.appsCompleted AWS EMR aws.elasticmapreduce.AppsFailed provider.appsFailed AWS EMR aws.elasticmapreduce.AppsKilled provider.appsKilled AWS EMR aws.elasticmapreduce.AppsPending provider.appsPending AWS EMR aws.elasticmapreduce.AppsRunning provider.appsRunning AWS EMR aws.elasticmapreduce.AppsSubmitted provider.appsSubmitted AWS EMR aws.elasticmapreduce.CapacityRemainingGB provider.capacityRemainingBytes AWS EMR aws.elasticmapreduce.ContainerAllocated provider.containerAllocated AWS EMR aws.elasticmapreduce.ContainerPending provider.containerPending AWS EMR aws.elasticmapreduce.ContainerPendingRatio provider.containerPendingRatio AWS EMR aws.elasticmapreduce.ContainerReserved provider.containerReserved AWS EMR aws.elasticmapreduce.CoreNodesPending provider.coreNodesPending AWS EMR aws.elasticmapreduce.CoreNodesRunning provider.coreNodesRunning AWS EMR aws.elasticmapreduce.CorruptBlocks provider.corruptBlocks AWS EMR aws.elasticmapreduce.HDFSBytesRead provider.hdfsReadBytes AWS EMR aws.elasticmapreduce.HDFSBytesWritten provider.hdfsWrittenBytes AWS EMR aws.elasticmapreduce.HDFSUtilization provider.hdfsUtilizationPercentage AWS EMR aws.elasticmapreduce.IsIdle provider.isIdle AWS EMR aws.elasticmapreduce.JobsFailed provider.jobsFailed AWS EMR aws.elasticmapreduce.JobsRunning provider.jobsRunning AWS EMR aws.elasticmapreduce.LiveDataNodes provider.liveDataNodesPercentage AWS EMR aws.elasticmapreduce.LiveTaskTrackers provider.liveTaskTrackersPercentage AWS EMR aws.elasticmapreduce.MapSlotsOpen provider.mapSlotsOpen AWS EMR aws.elasticmapreduce.MemoryAllocatedMB provider.memoryAllocatedBytes AWS EMR aws.elasticmapreduce.MemoryAvailableMB provider.memoryAvailableBytes AWS EMR aws.elasticmapreduce.MemoryReservedMB provider.memoryReservedBytes AWS EMR aws.elasticmapreduce.MemoryTotalMB provider.memoryTotalBytes AWS EMR aws.elasticmapreduce.MissingBlocks provider.missingBlocks AWS EMR aws.elasticmapreduce.MRActiveNodes provider.mrActiveNodes AWS EMR aws.elasticmapreduce.MRDecommissionedNodes provider.mrDecommissionedNodes AWS EMR aws.elasticmapreduce.MRLostNodes provider.mrLostNodes AWS EMR aws.elasticmapreduce.MRRebootedNodes provider.mrRebootedNodes AWS EMR aws.elasticmapreduce.MRTotalNodes provider.mrTotalNodes AWS EMR aws.elasticmapreduce.MRUnhealthyNodes provider.mrUnhealthyNodes AWS EMR aws.elasticmapreduce.PendingDeletionBlocks provider.pendingDeletionBlocks AWS EMR aws.elasticmapreduce.ReduceSlotsOpen provider.reduceSlotsOpen AWS EMR aws.elasticmapreduce.RemainingMapTasksPerSlot provider.remainingMapTasksPerSlot AWS EMR aws.elasticmapreduce.S3BytesRead provider.s3ReadBytes AWS EMR aws.elasticmapreduce.S3BytesWritten provider.s3WrittenBytes AWS EMR aws.elasticmapreduce.TaskNodesPending provider.taskNodesPending AWS EMR aws.elasticmapreduce.TaskNodesRunning provider.taskNodesRunning AWS EMR aws.elasticmapreduce.TotalLoad provider.totalLoad AWS EMR aws.elasticmapreduce.UnderReplicatedBlocks provider.underReplicatedBlocks AWS EMR aws.elasticmapreduce.YARNMemoryAvailablePercentage provider.yarnMemoryAvailablePercentage AWS FSx aws.fsx.DataReadBytes provider.dataReadBytes AWS FSx aws.fsx.DataReadOperations provider.dataReadOperations AWS FSx aws.fsx.DataWriteBytes provider.dataWriteBytes AWS FSx aws.fsx.DataWriteOperations provider.dataWriteOperations AWS FSx aws.fsx.FreeStorageCapacity provider.freeStorageCapacity AWS FSx aws.fsx.MetadataOperations provider.metadataOperations AWS Glue aws.glue.glue.ALL.jvm.heap.usage provider.glue.ALL.jvm.heap.usage AWS Glue aws.glue.glue.ALL.jvm.heap.used provider.glue.ALL.jvm.heap.used AWS Glue aws.glue.glue.ALL.s3.filesystem.read_bytes provider.glue.ALL.s3.filesystem.read_bytes AWS Glue aws.glue.glue.ALL.s3.filesystem.write_bytes provider.glue.ALL.s3.filesystem.write_bytes AWS Glue aws.glue.glue.ALL.system.cpuSystemLoad provider.glue.ALL.system.cpuSystemLoad AWS Glue aws.glue.glue.driver.aggregate.bytesRead provider.glue.driver.aggregate.bytesRead AWS Glue aws.glue.glue.driver.aggregate.elapsedTime provider.glue.driver.aggregate.elapsedTime AWS Glue aws.glue.glue.driver.aggregate.numCompletedStages provider.glue.driver.aggregate.numCompletedStages AWS Glue aws.glue.glue.driver.aggregate.numCompletedTasks provider.glue.driver.aggregate.numCompletedTasks AWS Glue aws.glue.glue.driver.aggregate.numFailedTasks provider.glue.driver.aggregate.numFailedTasks AWS Glue aws.glue.glue.driver.aggregate.numKilledTasks provider.glue.driver.aggregate.numKilledTasks AWS Glue aws.glue.glue.driver.aggregate.recordsRead provider.glue.driver.aggregate.recordsRead AWS Glue aws.glue.glue.driver.aggregate.shuffleBytesWritten provider.glue.driver.aggregate.shuffleBytesWritten AWS Glue aws.glue.glue.driver.aggregate.shuffleLocalBytesRead provider.glue.driver.aggregate.shuffleLocalBytesRead AWS Glue aws.glue.glue.driver.BlockManager.disk.diskSpaceUsed_MB provider.glue.driver.BlockManager.disk.diskSpaceUsed_MB AWS Glue aws.glue.glue.driver.ExecutorAllocationManager.executors.numberAllExecutors provider.glue.driver.ExecutorAllocationManager.executors.numberAllExecutors AWS Glue aws.glue.glue.driver.ExecutorAllocationManager.executors.numberMaxNeededExecutors provider.glue.driver.ExecutorAllocationManager.executors.numberMaxNeededExecutors AWS Glue aws.glue.glue.driver.jvm.heap.usage provider.glue.driver.jvm.heap.usage AWS Glue aws.glue.glue.driver.jvm.heap.used provider.glue.driver.jvm.heap.used AWS Glue aws.glue.glue.driver.s3.filesystem.read_bytes provider.glue.driver.s3.filesystem.read_bytes AWS Glue aws.glue.glue.driver.s3.filesystem.write_bytes provider.glue.driver.s3.filesystem.write_bytes AWS Glue aws.glue.glue.driver.system.cpuSystemLoad provider.glue.driver.system.cpuSystemLoad AWS IAM aws.iam.AccessKeysPerUserQuota AccessKeysPerUserQuota AWS IAM aws.iam.AccountAccessKeysPresent AccountAccessKeysPresent AWS IAM aws.iam.AccountMFAEnabled AccountMFAEnabled AWS IAM aws.iam.AccountSigningCertificatesPresent AccountSigningCertificatesPresent AWS IAM aws.iam.AssumeRolePolicySizeQuota AssumeRolePolicySizeQuota AWS IAM aws.iam.AttachedPoliciesPerGroupQuota AttachedPoliciesPerGroupQuota AWS IAM aws.iam.AttachedPoliciesPerRoleQuota AttachedPoliciesPerRoleQuota AWS IAM aws.iam.AttachedPoliciesPerUserQuota AttachedPoliciesPerUserQuota AWS IAM aws.iam.GlobalEndpointTokenVersion GlobalEndpointTokenVersion AWS IAM aws.iam.GroupPolicySizeQuota GroupPolicySizeQuota AWS IAM aws.iam.Groups Groups AWS IAM aws.iam.GroupsPerUserQuota GroupsPerUserQuota AWS IAM aws.iam.GroupsQuota GroupsQuota AWS IAM aws.iam.InstanceProfiles InstanceProfiles AWS IAM aws.iam.InstanceProfilesQuota InstanceProfilesQuota AWS IAM aws.iam.MFADevices MFADevices AWS IAM aws.iam.MFADevicesInUse MFADevicesInUse AWS IAM aws.iam.Policies Policies AWS IAM aws.iam.PoliciesQuota PoliciesQuota AWS IAM aws.iam.PolicySizeQuota PolicySizeQuota AWS IAM aws.iam.PolicyVersionsInUse PolicyVersionsInUse AWS IAM aws.iam.PolicyVersionsInUseQuota PolicyVersionsInUseQuota AWS IAM aws.iam.Providers Providers AWS IAM aws.iam.RolePolicySizeQuota RolePolicySizeQuota AWS IAM aws.iam.Roles Roles AWS IAM aws.iam.RolesQuota RolesQuota AWS IAM aws.iam.ServerCertificates ServerCertificates AWS IAM aws.iam.ServerCertificatesQuota ServerCertificatesQuota AWS IAM aws.iam.SigningCertificatesPerUserQuota SigningCertificatesPerUserQuota AWS IAM aws.iam.UserPolicySizeQuota UserPolicySizeQuota AWS IAM aws.iam.Users Users AWS IAM aws.iam.UsersQuota UsersQuota AWS IAM aws.iam.VersionsPerPolicyQuota VersionsPerPolicyQuota AWS IoT aws.iot.Connect.AuthError provider.connect.AuthError AWS IoT aws.iot.Connect.ClientError provider.connect.ClientError AWS IoT aws.iot.Connect.ServerError provider.connect.ServerError AWS IoT aws.iot.Connect.Success provider.connect.Success AWS IoT aws.iot.Connect.Throttle provider.connect.Throttle AWS IoT aws.iot.DeleteThingShadow.Accepted provider.deleteThingShadow.Accepted AWS IoT aws.iot.GetThingShadow.Accepted provider.getThingShadow.Accepted AWS IoT aws.iot.Ping.Success provider.ping.Success AWS IoT aws.iot.PublishIn.AuthError provider.publishIn.AuthError AWS IoT aws.iot.PublishIn.ClientError provider.publishIn.ClientError AWS IoT aws.iot.PublishIn.ServerError provider.publishIn.ServerError AWS IoT aws.iot.PublishIn.Success provider.publishIn.Success AWS IoT aws.iot.PublishIn.Throttle provider.publishIn.Throttle AWS IoT aws.iot.PublishOut.AuthError provider.publishOut.AuthError AWS IoT aws.iot.PublishOut.ClientError provider.publishOut.ClientError AWS IoT aws.iot.PublishOut.Success provider.publishOut.Success AWS IoT aws.iot.Subscribe.AuthError provider.subscribe.AuthError AWS IoT aws.iot.Subscribe.ClientError provider.subscribe.ClientError AWS IoT aws.iot.Subscribe.ServerError provider.subscribe.ServerError AWS IoT aws.iot.Subscribe.Success provider.subscribe.Success AWS IoT aws.iot.Subscribe.Throttle provider.subscribe.Throttle AWS IoT aws.iot.Unsubscribe.ClientError provider.unsubscribe.ClientError AWS IoT aws.iot.Unsubscribe.ServerError provider.unsubscribe.ServerError AWS IoT aws.iot.Unsubscribe.Success provider.unsubscribe.Success AWS IoT aws.iot.Unsubscribe.Throttle provider.unsubscribe.Throttle AWS IoT aws.iot.UpdateThingShadow.Accepted provider.updateThingShadow.Accepted AWS IoT aws.iot.ParseError provider.parseError AWS IoT aws.iot.RuleMessageThrottled provider.ruleMessageThrottled AWS IoT aws.iot.RuleNotFound provider.RuleNotFound AWS IoT aws.iot.TopicMatch provider.topicMatch AWS IoT aws.iot.Failure provider.failure AWS IoT aws.iot.Success provider.success AWS Kinesis aws.kinesis.GetRecords.Bytes provider.getRecordsBytes AWS Kinesis aws.kinesis.GetRecords.IteratorAgeMilliseconds provider.getRecordsIteratorAgeMilliseconds AWS Kinesis aws.kinesis.GetRecords.Latency provider.getRecordsLatency AWS Kinesis aws.kinesis.GetRecords.Records provider.getRecordsRecords AWS Kinesis aws.kinesis.GetRecords.Success provider.getRecordsSuccess AWS Kinesis aws.kinesis.IncomingBytes.byStream provider.incomingBytes AWS Kinesis aws.kinesis.IncomingRecords.byStream provider.incomingRecords AWS Kinesis aws.kinesis.PutRecord.Bytes provider.putRecordBytes AWS Kinesis aws.kinesis.PutRecord.Latency provider.putRecordLatency AWS Kinesis aws.kinesis.PutRecord.Success provider.putRecordSuccess AWS Kinesis aws.kinesis.PutRecords.Bytes provider.putRecordsBytes AWS Kinesis aws.kinesis.PutRecords.Latency provider.putRecordsLatency AWS Kinesis aws.kinesis.PutRecords.Records provider.putRecordsRecords AWS Kinesis aws.kinesis.PutRecords.Success provider.putRecordsSuccess AWS Kinesis aws.kinesis.ReadProvisionedThroughputExceeded.byStream provider.readProvisionedThroughputExceeded AWS Kinesis aws.kinesis.WriteProvisionedThroughputExceeded.byStream provider.writeProvisionedThroughputExceeded AWS Kinesis aws.kinesis.IncomingBytes.byStreamShard provider.incomingBytes AWS Kinesis aws.kinesis.IncomingRecords.byStreamShard provider.incomingRecords AWS Kinesis aws.kinesis.IteratorAgeMilliseconds provider.iteratorAgeMilliseconds AWS Kinesis aws.kinesis.OutgoingBytes provider.outgoingBytes AWS Kinesis aws.kinesis.OutgoingRecords provider.outgoingRecords AWS Kinesis aws.kinesis.ReadProvisionedThroughputExceeded.byStreamShard provider.readProvisionedThroughputExceeded AWS Kinesis aws.kinesis.WriteProvisionedThroughputExceeded.byStreamShard provider.writeProvisionedThroughputExceeded AWS Kinesis Firehose aws.firehose.DeliveryToElasticsearch.Bytes provider.deliveryToElasticsearchBytes AWS Kinesis Firehose aws.firehose.DeliveryToElasticsearch.Records provider.deliveryToElasticsearchRecords AWS Kinesis Firehose aws.firehose.DeliveryToElasticsearch.Success provider.deliveryToElasticsearchSuccess AWS Kinesis Firehose aws.firehose.DeliveryToRedshift.Bytes provider.deliveryToRedshiftBytes AWS Kinesis Firehose aws.firehose.DeliveryToRedshift.Records provider.deliveryToRedshiftRecords AWS Kinesis Firehose aws.firehose.DeliveryToRedshift.Success provider.deliveryToRedshiftSuccess AWS Kinesis Firehose aws.firehose.DeliveryToS3.Bytes provider.deliveryToS3Bytes AWS Kinesis Firehose aws.firehose.DeliveryToS3.DataFreshness provider.deliveryToS3DataFreshness AWS Kinesis Firehose aws.firehose.DeliveryToS3.Records provider.deliveryToS3Records AWS Kinesis Firehose aws.firehose.DeliveryToS3.Success provider.deliveryToS3Success AWS Kinesis Firehose aws.firehose.IncomingBytes provider.incomingBytes AWS Kinesis Firehose aws.firehose.IncomingRecords provider.incomingRecords AWS Kinesis Firehose aws.firehose.PutRecord.Bytes provider.putRecordBytes AWS Kinesis Firehose aws.firehose.PutRecord.Latency provider.putRecordLatency AWS Kinesis Firehose aws.firehose.PutRecord.Requests provider.putRecordRequests AWS Kinesis Firehose aws.firehose.PutRecordBatch.Bytes provider.putRecordBatchBytes AWS Kinesis Firehose aws.firehose.PutRecordBatch.Latency provider.putRecordBatchLatency AWS Kinesis Firehose aws.firehose.PutRecordBatch.Records provider.putRecordBatchRecords AWS Kinesis Firehose aws.firehose.PutRecordBatch.Requests provider.putRecordBatchRequests AWS Lambda aws.lambda.ConcurrentExecutions.byFunction provider.concurrentExecutions AWS Lambda aws.lambda.DeadLetterErrors.byFunction provider.deadLetterErrors AWS Lambda aws.lambda.Duration.byFunction provider.duration AWS Lambda aws.lambda.Errors.byFunction provider.errors AWS Lambda aws.lambda.Invocations.byFunction provider.invocations AWS Lambda aws.lambda.IteratorAge.byFunction provider.iteratorAge AWS Lambda aws.lambda.ProvisionedConcurrencyInvocations.byFunction provider.provisionedConcurrencyInvocations AWS Lambda aws.lambda.ProvisionedConcurrencySpilloverInvocations.byFunction provider.provisionedConcurrencySpilloverInvocations AWS Lambda aws.lambda.ProvisionedConcurrentExecutions.byFunction provider.provisionedConcurrentExecutions AWS Lambda aws.lambda.Throttles.byFunction provider.throttles AWS Lambda aws.lambda.DeadLetterErrors.byFunctionAlias provider.deadLetterErrors AWS Lambda aws.lambda.Duration.byFunctionAlias provider.duration AWS Lambda aws.lambda.Errors.byFunctionAlias provider.errors AWS Lambda aws.lambda.Invocations.byFunctionAlias provider.invocations AWS Lambda aws.lambda.IteratorAge.byFunctionAlias provider.iteratorAge AWS Lambda aws.lambda.ProvisionedConcurrencyInvocations.byFunctionAlias provider.provisionedConcurrencyInvocations AWS Lambda aws.lambda.ProvisionedConcurrencySpilloverInvocations.byFunctionAlias provider.provisionedConcurrencySpilloverInvocations AWS Lambda aws.lambda.ProvisionedConcurrencyUtilization.byFunctionAlias provider.provisionedConcurrencyUtilization AWS Lambda aws.lambda.ProvisionedConcurrentExecutions.byFunctionAlias provider.provisionedConcurrentExecutions AWS Lambda aws.lambda.Throttles.byFunctionAlias provider.throttles AWS Lambda aws.lambda.ConcurrentExecutions.byRegion provider.concurrentExecutions AWS Lambda aws.lambda.UnreservedConcurrentExecutions provider.unreservedConcurrentExecutions AWS Elemental MediaConvert aws.mediaconvert.8KOutputDuration provider.8KOutputDuration AWS Elemental MediaConvert aws.mediaconvert.AudioOutputDuration provider.audioOutputDuration AWS Elemental MediaConvert aws.mediaconvert.HDOutputDuration provider.hDOutputDuration AWS Elemental MediaConvert aws.mediaconvert.JobsCompletedCount provider.jobsCompletedCount AWS Elemental MediaConvert aws.mediaconvert.JobsErroredCount provider.jobsErroredCount AWS Elemental MediaConvert aws.mediaconvert.SDOutputDuration provider.sDOutputDuration AWS Elemental MediaConvert aws.mediaconvert.StandbyTime provider.standbyTime AWS Elemental MediaConvert aws.mediaconvert.TranscodingTime provider.transcodingTime AWS Elemental MediaConvert aws.mediaconvert.UHDOutputDuration provider.uHDOutputDuration AWS Elemental MediaConvert aws.mediaconvert.Errors provider.errors AWS MediaPackage VOD aws.mediapackage.EgressBytes provider.egressBytes AWS MediaPackage VOD aws.mediapackage.EgressRequestCount provider.egressRequestCount AWS MediaPackage VOD aws.mediapackage.EgressResponseTime provider.egressResponseTime AWS MQ aws.amazonmq.CpuCreditBalance provider.cpuCreditBalance AWS MQ aws.amazonmq.CpuUtilization provider.cpuUtilization AWS MQ aws.amazonmq.CurrentConnectionsCount provider.currentConnectionsCount AWS MQ aws.amazonmq.EstablishedConnectionsCount provider.establishedConnectionsCount AWS MQ aws.amazonmq.HeapUsage provider.heapUsage AWS MQ aws.amazonmq.InactiveDurableTopicSubscribersCount provider.inactiveDurableTopicSubscribersCount AWS MQ aws.amazonmq.JournalFilesForFastRecovery provider.journalFilesForFastRecovery AWS MQ aws.amazonmq.JournalFilesForFullRecovery provider.journalFilesForFullRecovery AWS MQ aws.amazonmq.NetworkIn provider.networkIn AWS MQ aws.amazonmq.NetworkOut provider.networkOut AWS MQ aws.amazonmq.OpenTransactionsCount provider.openTransactionsCount AWS MQ aws.amazonmq.StorePercentUsage provider.storePercentUsage AWS MQ aws.amazonmq.TotalConsumerCount provider.totalConsumerCount AWS MQ aws.amazonmq.TotalMessageCount provider.totalMessageCount AWS MQ aws.amazonmq.TotalProducerCount provider.totalProducerCount AWS MQ aws.amazonmq.ConsumerCount.byTopic provider.consumerCount AWS MQ aws.amazonmq.DequeueCount.byTopic provider.dequeueCount AWS MQ aws.amazonmq.DispatchCount.byTopic provider.dispatchCount AWS MQ aws.amazonmq.EnqueueCount.byTopic provider.enqueueCount AWS MQ aws.amazonmq.EnqueueTime.byTopic provider.enqueueTime AWS MQ aws.amazonmq.ExpiredCount.byTopic provider.expiredCount AWS MQ aws.amazonmq.InFlightCount.byTopic provider.inFlightCount AWS MQ aws.amazonmq.MemoryUsage.byTopic provider.memoryUsage AWS MQ aws.amazonmq.ProducerCount.byTopic provider.producerCount AWS MQ aws.amazonmq.ReceiveCount.byTopic provider.receiveCount AWS MQ aws.amazonmq.ConsumerCount.byQueue provider.consumerCount AWS MQ aws.amazonmq.DequeueCount.byQueue provider.dequeueCount AWS MQ aws.amazonmq.DispatchCount.byQueue provider.dispatchCount AWS MQ aws.amazonmq.EnqueueCount.byQueue provider.enqueueCount AWS MQ aws.amazonmq.EnqueueTime.byQueue provider.enqueueTime AWS MQ aws.amazonmq.ExpiredCount.byQueue provider.expiredCount AWS MQ aws.amazonmq.InFlightCount.byQueue provider.inFlightCount AWS MQ aws.amazonmq.MemoryUsage.byQueue provider.memoryUsage AWS MQ aws.amazonmq.ProducerCount.byQueue provider.producerCount AWS MQ aws.amazonmq.QueueSize provider.queueSize AWS MQ aws.amazonmq.ReceiveCount.byQueue provider.receiveCount AWS MSK aws.kafka.ActiveControllerCount provider.activeControllerCount AWS MSK aws.kafka.GlobalPartitionCount provider.globalPartitionCount AWS MSK aws.kafka.GlobalTopicCount provider.globalTopicCount AWS MSK aws.kafka.OfflinePartitionsCount provider.offlinePartitionsCount AWS MSK aws.kafka.BytesInPerSec.byBroker provider.bytesInPerSec AWS MSK aws.kafka.BytesOutPerSec.byBroker provider.bytesOutPerSec AWS MSK aws.kafka.CpuIdle provider.cpuIdle AWS MSK aws.kafka.CpuSystem provider.cpuSystem AWS MSK aws.kafka.CpuUser provider.cpuUser AWS MSK aws.kafka.FetchConsumerLocalTimeMsMean provider.fetchConsumerLocalTimeMsMean AWS MSK aws.kafka.FetchConsumerRequestQueueTimeMsMean provider.fetchConsumerRequestQueueTimeMsMean AWS MSK aws.kafka.FetchConsumerResponseQueueTimeMsMean provider.fetchConsumerResponseQueueTimeMsMean AWS MSK aws.kafka.FetchConsumerResponseSendTimeMsMean provider.fetchConsumerResponseSendTimeMsMean AWS MSK aws.kafka.FetchConsumerTotalTimeMsMean provider.fetchConsumerTotalTimeMsMean AWS MSK aws.kafka.FetchFollowerLocalTimeMsMean provider.fetchFollowerLocalTimeMsMean AWS MSK aws.kafka.FetchFollowerRequestQueueTimeMsMean provider.fetchFollowerRequestQueueTimeMsMean AWS MSK aws.kafka.FetchFollowerResponseQueueTimeMsMean provider.fetchFollowerResponseQueueTimeMsMean AWS MSK aws.kafka.FetchFollowerResponseSendTimeMsMean provider.fetchFollowerResponseSendTimeMsMean AWS MSK aws.kafka.FetchFollowerTotalTimeMsMean provider.fetchFollowerTotalTimeMsMean AWS MSK aws.kafka.FetchMessageConversionsPerSec.byBroker provider.fetchMessageConversionsPerSec AWS MSK aws.kafka.FetchThrottleByteRate provider.fetchThrottleByteRate AWS MSK aws.kafka.FetchThrottleQueueSize provider.fetchThrottleQueueSize AWS MSK aws.kafka.FetchThrottleTime provider.fetchThrottleTime AWS MSK aws.kafka.KafkaAppLogsDiskUsed provider.kafkaAppLogsDiskUsed AWS MSK aws.kafka.KafkaDataLogsDiskUsed provider.kafkaDataLogsDiskUsed AWS MSK aws.kafka.LeaderCount provider.leaderCount AWS MSK aws.kafka.MemoryBuffered provider.memoryBuffered AWS MSK aws.kafka.MemoryCached provider.memoryCached AWS MSK aws.kafka.MemoryFree provider.memoryFree AWS MSK aws.kafka.MemoryUsed provider.memoryUsed AWS MSK aws.kafka.MessagesInPerSec.byBroker provider.messagesInPerSec AWS MSK aws.kafka.NetworkProcessorAvgIdlePercent provider.networkProcessorAvgIdlePercent AWS MSK aws.kafka.NetworkRxDropped provider.networkRxDropped AWS MSK aws.kafka.NetworkRxErrors provider.networkRxErrors AWS MSK aws.kafka.NetworkRxPackets provider.networkRxPackets AWS MSK aws.kafka.NetworkTxDropped provider.networkTxDropped AWS MSK aws.kafka.NetworkTxErrors provider.networkTxErrors AWS MSK aws.kafka.NetworkTxPackets provider.networkTxPackets AWS MSK aws.kafka.PartitionCount provider.partitionCount AWS MSK aws.kafka.ProduceLocalTimeMsMean provider.produceLocalTimeMsMean AWS MSK aws.kafka.ProduceMessageConversionsPerSec.byBroker provider.produceMessageConversionsPerSec AWS MSK aws.kafka.ProduceMessageConversionsTimeMsMean provider.produceMessageConversionsTimeMsMean AWS MSK aws.kafka.ProduceRequestQueueTimeMsMean provider.produceRequestQueueTimeMsMean AWS MSK aws.kafka.ProduceResponseQueueTimeMsMean provider.produceResponseQueueTimeMsMean AWS MSK aws.kafka.ProduceResponseSendTimeMsMean provider.produceResponseSendTimeMsMean AWS MSK aws.kafka.ProduceThrottleByteRate provider.produceThrottleByteRate AWS MSK aws.kafka.ProduceThrottleQueueSize provider.produceThrottleQueueSize AWS MSK aws.kafka.ProduceThrottleTime provider.produceThrottleTime AWS MSK aws.kafka.ProduceTotalTimeMsMean provider.produceTotalTimeMsMean AWS MSK aws.kafka.RequestBytesMean provider.requestBytesMean AWS MSK aws.kafka.RequestExemptFromThrottleTime provider.requestExemptFromThrottleTime AWS MSK aws.kafka.RequestHandlerAvgIdlePercent provider.requestHandlerAvgIdlePercent AWS MSK aws.kafka.RequestThrottleQueueSize provider.requestThrottleQueueSize AWS MSK aws.kafka.RequestThrottleTime provider.requestThrottleTime AWS MSK aws.kafka.RequestTime provider.requestTime AWS MSK aws.kafka.RootDiskUsed provider.rootDiskUsed AWS MSK aws.kafka.SwapFree provider.swapFree AWS MSK aws.kafka.SwapUsed provider.swapUsed AWS MSK aws.kafka.UnderMinIsrPartitionCount provider.underMinIsrPartitionCount AWS MSK aws.kafka.UnderReplicatedPartitions provider.underReplicatedPartitions AWS MSK aws.kafka.ZooKeeperRequestLatencyMsMean provider.zooKeeperRequestLatencyMsMean AWS MSK aws.kafka.ZooKeeperSessionState provider.zooKeeperSessionState AWS MSK aws.kafka.BytesInPerSec.byTopic provider.bytesInPerSec AWS MSK aws.kafka.BytesOutPerSec.byTopic provider.bytesOutPerSec AWS MSK aws.kafka.FetchMessageConversionsPerSec.byTopic provider.fetchMessageConversionsPerSec AWS MSK aws.kafka.MessagesInPerSec.byTopic provider.messagesInPerSec AWS MSK aws.kafka.ProduceMessageConversionsPerSec.byTopic provider.produceMessageConversionsPerSec AWS Neptune aws.neptune.BackupRetentionPeriodStorageUsed.byInstance provider.backupRetentionPeriodStorageUsed AWS Neptune aws.neptune.ClusterReplicaLag.byInstance provider.clusterReplicaLag AWS Neptune aws.neptune.ClusterReplicaLagMaximum.byInstance provider.clusterReplicaLagMaximum AWS Neptune aws.neptune.ClusterReplicaLagMinimum.byInstance provider.clusterReplicaLagMinimum AWS Neptune aws.neptune.CPUUtilization.byInstance provider.cpuUtilization AWS Neptune aws.neptune.EngineUptime.byInstance provider.engineUptime AWS Neptune aws.neptune.FreeableMemory.byInstance provider.freeableMemory AWS Neptune aws.neptune.GremlinRequestsPerSec.byInstance provider.gremlinRequestsPerSec AWS Neptune aws.neptune.GremlinWebSocketOpenConnections.byInstance provider.gremlinWebSocketOpenConnections AWS Neptune aws.neptune.LoaderRequestsPerSec.byInstance provider.loaderRequestsPerSec AWS Neptune aws.neptune.MainRequestQueuePendingRequests.byInstance provider.mainRequestQueuePendingRequests AWS Neptune aws.neptune.NetworkReceiveThroughput.byInstance provider.networkReceiveThroughput AWS Neptune aws.neptune.NetworkThroughput.byInstance provider.networkThroughput AWS Neptune aws.neptune.NetworkTransmitThroughput.byInstance provider.networkTransmitThroughput AWS Neptune aws.neptune.NumTxCommitted.byInstance provider.numTxCommitted AWS Neptune aws.neptune.NumTxOpened.byInstance provider.numTxOpened AWS Neptune aws.neptune.NumTxRolledBack.byInstance provider.numTxRolledBack AWS Neptune aws.neptune.SnapshotStorageUsed.byInstance provider.snapshotStorageUsed AWS Neptune aws.neptune.SparqlRequestsPerSec.byInstance provider.sparqlRequestsPerSec AWS Neptune aws.neptune.TotalBackupStorageBilled.byInstance provider.totalBackupStorageBilled AWS Neptune aws.neptune.TotalClientErrorsPerSec.byInstance provider.totalClientErrorsPerSec AWS Neptune aws.neptune.TotalRequestsPerSec.byInstance provider.totalRequestsPerSec AWS Neptune aws.neptune.TotalServerErrorsPerSec.byInstance provider.totalServerErrorsPerSec AWS Neptune aws.neptune.VolumeBytesUsed.byInstance provider.volumeBytesUsed AWS Neptune aws.neptune.VolumeReadIOPs.byInstance provider.volumeReadIOPs AWS Neptune aws.neptune.VolumeWriteIOPs.byInstance provider.volumeWriteIOPs AWS Neptune aws.neptune.BackupRetentionPeriodStorageUsed.byCluster provider.backupRetentionPeriodStorageUsed AWS Neptune aws.neptune.ClusterReplicaLag.byCluster provider.clusterReplicaLag AWS Neptune aws.neptune.ClusterReplicaLagMaximum.byCluster provider.clusterReplicaLagMaximum AWS Neptune aws.neptune.ClusterReplicaLagMinimum.byCluster provider.clusterReplicaLagMinimum AWS Neptune aws.neptune.CPUUtilization.byCluster provider.cpuUtilization AWS Neptune aws.neptune.EngineUptime.byCluster provider.engineUptime AWS Neptune aws.neptune.FreeableMemory.byCluster provider.freeableMemory AWS Neptune aws.neptune.GremlinRequestsPerSec.byCluster provider.gremlinRequestsPerSec AWS Neptune aws.neptune.GremlinWebSocketOpenConnections.byCluster provider.gremlinWebSocketOpenConnections AWS Neptune aws.neptune.LoaderRequestsPerSec.byCluster provider.loaderRequestsPerSec AWS Neptune aws.neptune.MainRequestQueuePendingRequests.byCluster provider.mainRequestQueuePendingRequests AWS Neptune aws.neptune.NetworkReceiveThroughput.byCluster provider.networkReceiveThroughput AWS Neptune aws.neptune.NetworkThroughput.byCluster provider.networkThroughput AWS Neptune aws.neptune.NetworkTransmitThroughput.byCluster provider.networkTransmitThroughput AWS Neptune aws.neptune.NumTxCommitted.byCluster provider.numTxCommitted AWS Neptune aws.neptune.NumTxOpened.byCluster provider.numTxOpened AWS Neptune aws.neptune.NumTxRolledBack.byCluster provider.numTxRolledBack AWS Neptune aws.neptune.SnapshotStorageUsed.byCluster provider.snapshotStorageUsed AWS Neptune aws.neptune.SparqlRequestsPerSec.byCluster provider.sparqlRequestsPerSec AWS Neptune aws.neptune.TotalBackupStorageBilled.byCluster provider.totalBackupStorageBilled AWS Neptune aws.neptune.TotalClientErrorsPerSec.byCluster provider.totalClientErrorsPerSec AWS Neptune aws.neptune.TotalRequestsPerSec.byCluster provider.totalRequestsPerSec AWS Neptune aws.neptune.TotalServerErrorsPerSec.byCluster provider.totalServerErrorsPerSec AWS Neptune aws.neptune.VolumeBytesUsed.byCluster provider.volumeBytesUsed AWS Neptune aws.neptune.VolumeReadIOPs.byCluster provider.volumeReadIOPs AWS Neptune aws.neptune.VolumeWriteIOPs.byCluster provider.volumeWriteIOPs AWS Neptune aws.neptune.BackupRetentionPeriodStorageUsed.byClusterByRole provider.backupRetentionPeriodStorageUsed AWS Neptune aws.neptune.ClusterReplicaLag.byClusterByRole provider.clusterReplicaLag AWS Neptune aws.neptune.ClusterReplicaLagMaximum.byClusterByRole provider.clusterReplicaLagMaximum AWS Neptune aws.neptune.ClusterReplicaLagMinimum.byClusterByRole provider.clusterReplicaLagMinimum AWS Neptune aws.neptune.CPUUtilization.byClusterByRole provider.cpuUtilization AWS Neptune aws.neptune.EngineUptime.byClusterByRole provider.engineUptime AWS Neptune aws.neptune.FreeableMemory.byClusterByRole provider.freeableMemory AWS Neptune aws.neptune.GremlinRequestsPerSec.byClusterByRole provider.gremlinRequestsPerSec AWS Neptune aws.neptune.GremlinWebSocketOpenConnections.byClusterByRole provider.gremlinWebSocketOpenConnections AWS Neptune aws.neptune.LoaderRequestsPerSec.byClusterByRole provider.loaderRequestsPerSec AWS Neptune aws.neptune.MainRequestQueuePendingRequests.byClusterByRole provider.mainRequestQueuePendingRequests AWS Neptune aws.neptune.NetworkReceiveThroughput.byClusterByRole provider.networkReceiveThroughput AWS Neptune aws.neptune.NetworkThroughput.byClusterByRole provider.networkThroughput AWS Neptune aws.neptune.NetworkTransmitThroughput.byClusterByRole provider.networkTransmitThroughput AWS Neptune aws.neptune.NumTxCommitted.byClusterByRole provider.numTxCommitted AWS Neptune aws.neptune.NumTxOpened.byClusterByRole provider.numTxOpened AWS Neptune aws.neptune.NumTxRolledBack.byClusterByRole provider.numTxRolledBack AWS Neptune aws.neptune.SnapshotStorageUsed.byClusterByRole provider.snapshotStorageUsed AWS Neptune aws.neptune.SparqlRequestsPerSec.byClusterByRole provider.sparqlRequestsPerSec AWS Neptune aws.neptune.TotalBackupStorageBilled.byClusterByRole provider.totalBackupStorageBilled AWS Neptune aws.neptune.TotalClientErrorsPerSec.byClusterByRole provider.totalClientErrorsPerSec AWS Neptune aws.neptune.TotalRequestsPerSec.byClusterByRole provider.totalRequestsPerSec AWS Neptune aws.neptune.TotalServerErrorsPerSec.byClusterByRole provider.totalServerErrorsPerSec AWS Neptune aws.neptune.VolumeBytesUsed.byClusterByRole provider.volumeBytesUsed AWS Neptune aws.neptune.VolumeReadIOPs.byClusterByRole provider.volumeReadIOPs AWS Neptune aws.neptune.VolumeWriteIOPs.byClusterByRole provider.volumeWriteIOPs AWS Neptune aws.neptune.BackupRetentionPeriodStorageUsed.byDatabaseClass provider.backupRetentionPeriodStorageUsed AWS Neptune aws.neptune.ClusterReplicaLag.byDatabaseClass provider.clusterReplicaLag AWS Neptune aws.neptune.ClusterReplicaLagMaximum.byDatabaseClass provider.clusterReplicaLagMaximum AWS Neptune aws.neptune.ClusterReplicaLagMinimum.byDatabaseClass provider.clusterReplicaLagMinimum AWS Neptune aws.neptune.CPUUtilization.byDatabaseClass provider.cpuUtilization AWS Neptune aws.neptune.EngineUptime.byDatabaseClass provider.engineUptime AWS Neptune aws.neptune.FreeableMemory.byDatabaseClass provider.freeableMemory AWS Neptune aws.neptune.GremlinRequestsPerSec.byDatabaseClass provider.gremlinRequestsPerSec AWS Neptune aws.neptune.GremlinWebSocketOpenConnections.byDatabaseClass provider.gremlinWebSocketOpenConnections AWS Neptune aws.neptune.LoaderRequestsPerSec.byDatabaseClass provider.loaderRequestsPerSec AWS Neptune aws.neptune.MainRequestQueuePendingRequests.byDatabaseClass provider.mainRequestQueuePendingRequests AWS Neptune aws.neptune.NetworkReceiveThroughput.byDatabaseClass provider.networkReceiveThroughput AWS Neptune aws.neptune.NetworkThroughput.byDatabaseClass provider.networkThroughput AWS Neptune aws.neptune.NetworkTransmitThroughput.byDatabaseClass provider.networkTransmitThroughput AWS Neptune aws.neptune.NumTxCommitted.byDatabaseClass provider.numTxCommitted AWS Neptune aws.neptune.NumTxOpened.byDatabaseClass provider.numTxOpened AWS Neptune aws.neptune.NumTxRolledBack.byDatabaseClass provider.numTxRolledBack AWS Neptune aws.neptune.SnapshotStorageUsed.byDatabaseClass provider.snapshotStorageUsed AWS Neptune aws.neptune.SparqlRequestsPerSec.byDatabaseClass provider.sparqlRequestsPerSec AWS Neptune aws.neptune.TotalBackupStorageBilled.byDatabaseClass provider.totalBackupStorageBilled AWS Neptune aws.neptune.TotalClientErrorsPerSec.byDatabaseClass provider.totalClientErrorsPerSec AWS Neptune aws.neptune.TotalRequestsPerSec.byDatabaseClass provider.totalRequestsPerSec AWS Neptune aws.neptune.TotalServerErrorsPerSec.byDatabaseClass provider.totalServerErrorsPerSec AWS Neptune aws.neptune.VolumeBytesUsed.byDatabaseClass provider.volumeBytesUsed AWS Neptune aws.neptune.VolumeReadIOPs.byDatabaseClass provider.volumeReadIOPs AWS Neptune aws.neptune.VolumeWriteIOPs.byDatabaseClass provider.volumeWriteIOPs AWS NLB aws.networkelb.ActiveFlowCount provider.activeFlowCount AWS NLB aws.networkelb.ActiveFlowCount_TLS provider.activeFlowCountTls AWS NLB aws.networkelb.ClientTLSNegotiationErrorCount provider.clientTlsNegotiationErrorCount AWS NLB aws.networkelb.ConsumedLCUs provider.consumedLcus AWS NLB aws.networkelb.NewFlowCount provider.newFlowCount AWS NLB aws.networkelb.NewFlowCount_TLS provider.newFlowCountTls AWS NLB aws.networkelb.ProcessedBytes provider.processedBytes AWS NLB aws.networkelb.ProcessedBytes_TLS provider.processedBytesTls AWS NLB aws.networkelb.TargetTLSNegotiationErrorCount provider.targetTlsNegotiationErrorCount AWS NLB aws.networkelb.TCP_Client_Reset_Count provider.tcpClientResetCount AWS NLB aws.networkelb.TCP_ELB_Reset_Count provider.tcpElbResetCount AWS NLB aws.networkelb.TCP_Target_Reset_Count provider.tcpTargetResetCount AWS NLB aws.networkelb.HealthyHostCount provider.healthyHostCount AWS NLB aws.networkelb.UnHealthyHostCount provider.unHealthyHostCount AWS QLDB aws.qldb.CommandLatency provider.commandLatency AWS QLDB aws.qldb.IndexedStorage provider.indexedStorage AWS QLDB aws.qldb.JournalStorage provider.journalStorage AWS QLDB aws.qldb.OccConflictExceptions provider.occConflictExceptions AWS QLDB aws.qldb.ReadIOs provider.readIOs AWS QLDB aws.qldb.Session4xxExceptions provider.session4xxExceptions AWS QLDB aws.qldb.Session5xxExceptions provider.session5xxExceptions AWS QLDB aws.qldb.SessionRateExceededExceptions provider.sessionRateExceededExceptions AWS QLDB aws.qldb.WriteIOs provider.writeIOs AWS RDS aws.rds.VolumeBytesUsed.byDbCluster provider.volumeUsedBytes AWS RDS aws.rds.VolumeReadIOPs.byDbCluster provider.volumeReadIops AWS RDS aws.rds.VolumeWriteIOPs.byDbCluster provider.volumeWriteIops AWS RDS aws.rds.ActiveTransactions provider.activeTransactions AWS RDS aws.rds.allocatedStorageBytes provider.allocatedStorageBytes AWS RDS aws.rds.AuroraBinlogReplicaLag provider.auroraBinlogReplicaLag AWS RDS aws.rds.AuroraReplicaLag provider.auroraReplicaLag AWS RDS aws.rds.AuroraReplicaLagMaximum provider.auroraReplicaLagMaximum AWS RDS aws.rds.AuroraReplicaLagMinimum provider.auroraReplicaLagMinimum AWS RDS aws.rds.BacktrackWindowActual provider.backtrackWindowActual AWS RDS aws.rds.BacktrackWindowAlert provider.backtrackWindowAlert AWS RDS aws.rds.BinLogDiskUsage provider.binLogDiskUsageBytes AWS RDS aws.rds.BlockedTransactions provider.blockedTransactions AWS RDS aws.rds.BufferCacheHitRatio provider.bufferCacheHitRatio AWS RDS aws.rds.BurstBalance provider.burstBalance AWS RDS aws.rds.CommitLatency provider.commitLatency AWS RDS aws.rds.CommitThroughput provider.commitThroughput AWS RDS aws.rds.CPUCreditBalance provider.cpuCreditBalance AWS RDS aws.rds.CPUCreditUsage provider.cpuCreditUsage AWS RDS aws.rds.CPUUtilization provider.cpuUtilization AWS RDS aws.rds.cpuUtilization.guest provider.cpuUtilization.guest AWS RDS aws.rds.cpuUtilization.idle provider.cpuUtilization.idle AWS RDS aws.rds.cpuUtilization.irq provider.cpuUtilization.irq AWS RDS aws.rds.cpuUtilization.nice provider.cpuUtilization.nice AWS RDS aws.rds.cpuUtilization.steal provider.cpuUtilization.steal AWS RDS aws.rds.cpuUtilization.system provider.cpuUtilization.system AWS RDS aws.rds.cpuUtilization.total provider.cpuUtilization.total AWS RDS aws.rds.cpuUtilization.user provider.cpuUtilization.user AWS RDS aws.rds.cpuUtilization.wait provider.cpuUtilization.wait AWS RDS aws.rds.DatabaseConnections provider.databaseConnections AWS RDS aws.rds.DDLLatency provider.ddlLatency AWS RDS aws.rds.DDLThroughput provider.ddlThroughput AWS RDS aws.rds.Deadlocks provider.deadlocks AWS RDS aws.rds.DeleteLatency provider.deleteLatency AWS RDS aws.rds.DeleteThroughput provider.deleteThroughput AWS RDS aws.rds.diskIo.avgQueueLen provider.diskIo.avgQueueLen AWS RDS aws.rds.diskIo.avgReqSz provider.diskIo.avgReqSz AWS RDS aws.rds.diskIo.await provider.diskIo.await AWS RDS aws.rds.diskIo.readIosps provider.diskIo.readIosps AWS RDS aws.rds.diskIo.readKb provider.diskIo.readKb AWS RDS aws.rds.diskIo.readKbps provider.diskIo.readKbps AWS RDS aws.rds.diskIo.readLatency provider.diskIo.readLatency AWS RDS aws.rds.diskIo.rrqmPs provider.diskIo.rrqmPs AWS RDS aws.rds.diskIo.tps provider.diskIo.tps AWS RDS aws.rds.diskIo.util provider.diskIo.util AWS RDS aws.rds.diskIo.writeIosps provider.diskIo.writeIosps AWS RDS aws.rds.diskIo.writeKb provider.diskIo.writeKb AWS RDS aws.rds.diskIo.writeKbps provider.diskIo.writeKbps AWS RDS aws.rds.diskIo.writeLatency provider.diskIo.writeLatency AWS RDS aws.rds.diskIo.writeThroughput provider.diskIo.writeThroughput AWS RDS aws.rds.diskIo.wrqmPs provider.diskIo.wrqmPs AWS RDS aws.rds.DiskQueueDepth provider.diskQueueDepth AWS RDS aws.rds.disks.availKb provider.disks.availKb AWS RDS aws.rds.disks.availPc provider.disks.availPc AWS RDS aws.rds.disks.rdBytesPs provider.disks.rdBytesPS AWS RDS aws.rds.disks.rdCountPs provider.disks.rdCountPS AWS RDS aws.rds.disks.totalKb provider.disks.totalKb AWS RDS aws.rds.disks.usedKb provider.disks.usedKb AWS RDS aws.rds.disks.usedPc provider.disks.usedPc AWS RDS aws.rds.disks.wrBytesPs provider.disks.wrBytesPS AWS RDS aws.rds.disks.wrCountPs provider.disks.wrCountPS AWS RDS aws.rds.DMLLatency provider.dmlLatency AWS RDS aws.rds.DMLThroughput provider.dmlThroughput AWS RDS aws.rds.EngineUptime provider.engineUptime AWS RDS aws.rds.fileSys.maxFiles provider.fileSys.maxFiles AWS RDS aws.rds.fileSys.total provider.fileSys.total AWS RDS aws.rds.fileSys.used provider.fileSys.used AWS RDS aws.rds.fileSys.usedFiles provider.fileSys.usedFiles AWS RDS aws.rds.fileSys.usedFilesPercent provider.fileSys.usedFilesPercent AWS RDS aws.rds.fileSys.usedPercent provider.fileSys.usedPercent AWS RDS aws.rds.FreeableMemory provider.freeableMemoryBytes AWS RDS aws.rds.FreeLocalStorage provider.freeLocalStorageBytes AWS RDS aws.rds.FreeStorageSpace provider.freeStorageSpaceBytes AWS RDS aws.rds.InsertLatency provider.insertLatency AWS RDS aws.rds.InsertThroughput provider.insertThroughput AWS RDS aws.rds.loadAverageMinute.fifteen provider.loadAverageMinute.fifteen AWS RDS aws.rds.loadAverageMinute.five provider.loadAverageMinute.five AWS RDS aws.rds.loadAverageMinute.one provider.loadAverageMinute.one AWS RDS aws.rds.LoginFailures provider.loginFailures AWS RDS aws.rds.MaximumUsedTransactionIDs provider.maximumUsedTransactionIDs AWS RDS aws.rds.memory.active provider.memory.active AWS RDS aws.rds.memory.buffers provider.memory.buffers AWS RDS aws.rds.memory.cached provider.memory.cached AWS RDS aws.rds.memory.commitLimitKb provider.memory.commitLimitKb AWS RDS aws.rds.memory.commitPeakKb provider.memory.commitPeakKb AWS RDS aws.rds.memory.commitTotKb provider.memory.commitTotKb AWS RDS aws.rds.memory.dirty provider.memory.dirty AWS RDS aws.rds.memory.free provider.memory.free AWS RDS aws.rds.memory.hugePagesFree provider.memory.hugePagesFree AWS RDS aws.rds.memory.hugePagesRsvd provider.memory.hugePagesRsvd AWS RDS aws.rds.memory.hugePagesSize provider.memory.hugePagesSize AWS RDS aws.rds.memory.hugePagesSurp provider.memory.hugePagesSurp AWS RDS aws.rds.memory.hugePagesTotal provider.memory.hugePagesTotal AWS RDS aws.rds.memory.inactive provider.memory.inactive AWS RDS aws.rds.memory.kernNonpagedKb provider.memory.kernNonpagedKb AWS RDS aws.rds.memory.kernPagedKb provider.memory.kernPagedKb AWS RDS aws.rds.memory.kernTotKb provider.memory.kernTotKb AWS RDS aws.rds.memory.mapped provider.memory.mapped AWS RDS aws.rds.memory.pageSize provider.memory.pageSize AWS RDS aws.rds.memory.pageTables provider.memory.pageTables AWS RDS aws.rds.memory.physAvailKb provider.memory.physAvailKb AWS RDS aws.rds.memory.physTotKb provider.memory.physTotKb AWS RDS aws.rds.memory.slab provider.memory.slab AWS RDS aws.rds.memory.sqlServerTotKb provider.memory.sqlServerTotKb AWS RDS aws.rds.memory.sysCacheKb provider.memory.sysCacheKb AWS RDS aws.rds.memory.total provider.memory.total AWS RDS aws.rds.memory.writeback provider.memory.writeback AWS RDS aws.rds.network.rdBytesPs provider.network.rdBytesPS AWS RDS aws.rds.network.rx provider.network.rx AWS RDS aws.rds.network.tx provider.network.tx AWS RDS aws.rds.network.wrBytesPs provider.network.wrBytesPS AWS RDS aws.rds.NetworkReceiveThroughput provider.networkReceiveThroughput AWS RDS aws.rds.NetworkThroughput provider.networkThroughput AWS RDS aws.rds.NetworkTransmitThroughput provider.networkTransmitThroughput AWS RDS aws.rds.OldestReplicationSlotLag provider.oldestReplicationSlotLag AWS RDS aws.rds.process.cpuUsedPc provider.process.cpuUsedPc AWS RDS aws.rds.process.memoryUsedPc provider.process.memoryUsedPc AWS RDS aws.rds.process.memUsedPc provider.process.memUsedPc AWS RDS aws.rds.process.rss provider.process.rss AWS RDS aws.rds.process.virtKb provider.process.virtKb AWS RDS aws.rds.process.workingSetKb provider.process.workingSetKb AWS RDS aws.rds.process.workingSetPrivKb provider.process.workingSetPrivKb AWS RDS aws.rds.process.workingSetShareableKb provider.process.workingSetShareableKb AWS RDS aws.rds.Queries provider.queries AWS RDS aws.rds.RDSToAuroraPostgreSQLReplicaLag provider.rdsToAuroraPostgreSQLReplicaLag AWS RDS aws.rds.ReadIOPS provider.readIops AWS RDS aws.rds.ReadLatency provider.readLatency AWS RDS aws.rds.ReadThroughput provider.readThroughput AWS RDS aws.rds.ReplicaLag provider.replicaLag AWS RDS aws.rds.ReplicationSlotDiskUsage provider.replicationSlotDiskUsageBytes AWS RDS aws.rds.ResultSetCacheHitRatio provider.resultSetCacheHitRatio AWS RDS aws.rds.SelectLatency provider.selectLatency AWS RDS aws.rds.SelectThroughput provider.selectThroughput AWS RDS aws.rds.swap.cached provider.swap.cached AWS RDS aws.rds.swap.free provider.swap.free AWS RDS aws.rds.swap.total provider.swap.total AWS RDS aws.rds.SwapUsage provider.swapUsageBytes AWS RDS aws.rds.system.handles provider.system.handles AWS RDS aws.rds.system.processes provider.system.processes AWS RDS aws.rds.system.threads provider.system.threads AWS RDS aws.rds.tasks.blocked provider.tasks.blocked AWS RDS aws.rds.tasks.running provider.tasks.running AWS RDS aws.rds.tasks.sleeping provider.tasks.sleeping AWS RDS aws.rds.tasks.stopped provider.tasks.stopped AWS RDS aws.rds.tasks.total provider.tasks.total AWS RDS aws.rds.tasks.zombie provider.tasks.zombie AWS RDS aws.rds.TransactionLogsDiskUsage provider.transactionLogsDiskUsageBytes AWS RDS aws.rds.TransactionLogsGeneration provider.transactionLogsGeneration AWS RDS aws.rds.UpdateLatency provider.updateLatency AWS RDS aws.rds.UpdateThroughput provider.updateThroughput AWS RDS aws.rds.WriteIOPS provider.writeIops AWS RDS aws.rds.WriteLatency provider.writeLatency AWS RDS aws.rds.WriteThroughput provider.writeThroughput AWS Redshift aws.redshift.CPUUtilization.byCluster provider.cpuUtilization AWS Redshift aws.redshift.DatabaseConnections.byCluster provider.DatabaseConnections AWS Redshift aws.redshift.HealthStatus.byCluster provider.HealthStatus AWS Redshift aws.redshift.MaintenanceMode.byCluster provider.MaintenanceMode AWS Redshift aws.redshift.NetworkReceiveThroughput.byCluster provider.NetworkReceiveThroughput AWS Redshift aws.redshift.NetworkTransmitThroughput.byCluster provider.NetworkTransmitThroughput AWS Redshift aws.redshift.PercentageDiskSpaceUsed.byCluster provider.PercentageDiskSpaceUsed AWS Redshift aws.redshift.QueriesCompletedPerSecond provider.QueriesCompletedPerSecond AWS Redshift aws.redshift.QueryDuration provider.QueryDuration AWS Redshift aws.redshift.ReadIOPS.byCluster provider.readIops AWS Redshift aws.redshift.ReadLatency.byCluster provider.ReadLatency AWS Redshift aws.redshift.ReadThroughput.byCluster provider.ReadThroughput AWS Redshift aws.redshift.WriteIOPS.byCluster provider.writeIops AWS Redshift aws.redshift.WriteLatency.byCluster provider.WriteLatency AWS Redshift aws.redshift.WriteThroughput.byCluster provider.WriteThroughput AWS Redshift aws.redshift.CPUUtilization.byNode provider.cpuUtilization AWS Redshift aws.redshift.DatabaseConnections.byNode provider.DatabaseConnections AWS Redshift aws.redshift.HealthStatus.byNode provider.HealthStatus AWS Redshift aws.redshift.MaintenanceMode.byNode provider.MaintenanceMode AWS Redshift aws.redshift.NetworkReceiveThroughput.byNode provider.NetworkReceiveThroughput AWS Redshift aws.redshift.NetworkTransmitThroughput.byNode provider.NetworkTransmitThroughput AWS Redshift aws.redshift.PercentageDiskSpaceUsed.byNode provider.PercentageDiskSpaceUsed AWS Redshift aws.redshift.ReadIOPS.byNode provider.readIops AWS Redshift aws.redshift.ReadLatency.byNode provider.ReadLatency AWS Redshift aws.redshift.ReadThroughput.byNode provider.ReadThroughput AWS Redshift aws.redshift.WriteIOPS.byNode provider.writeIops AWS Redshift aws.redshift.WriteLatency.byNode provider.WriteLatency AWS Redshift aws.redshift.WriteThroughput.byNode provider.WriteThroughput AWS Route53 aws.route53.ChildHealthCheckHealthyCount provider.childHealthCheckHealthyCount AWS Route53 aws.route53.ConnectionTime provider.connectionTime AWS Route53 aws.route53.HealthCheckPercentageHealthy provider.healthCheckPercentageHealthy AWS Route53 aws.route53.HealthCheckStatus provider.healthCheckStatus AWS Route53 aws.route53.SSLHandshakeTime provider.sSLHandshakeTime AWS Route53 aws.route53.TimeToFirstByte provider.timeToFirstByte AWS Route53 Resolver aws.route53resolver.InboundQueryVolume provider.inboundQueryVolume AWS Route53 Resolver aws.route53resolver.OutboundQueryAggregatedVolume provider.outboundQueryAggregatedVolume AWS Route53 Resolver aws.route53resolver.OutboundQueryVolume provider.outboundQueryVolume AWS S3 aws.s3.BucketSizeBytes provider.bucketSizeBytes AWS S3 aws.s3.NumberOfObjects provider.numberOfObjects AWS S3 aws.s3.4xxErrors provider.error4xxErrors AWS S3 aws.s3.5xxErrors provider.error5xxErrors AWS S3 aws.s3.AllRequests provider.allRequests AWS S3 aws.s3.BytesDownloaded provider.bytesDownloaded AWS S3 aws.s3.BytesUploaded provider.bytesUploaded AWS S3 aws.s3.DeleteRequests provider.deleteRequests AWS S3 aws.s3.FirstByteLatency provider.firstByteLatency AWS S3 aws.s3.GetRequests provider.getRequests AWS S3 aws.s3.HeadRequests provider.headRequests AWS S3 aws.s3.ListRequests provider.listRequests AWS S3 aws.s3.PostRequests provider.postRequests AWS S3 aws.s3.PutRequests provider.putRequests AWS S3 aws.s3.TotalRequestLatency provider.totalRequestLatency AWS SES aws.ses.Bounce provider.Bounce AWS SES aws.ses.Click provider.Click AWS SES aws.ses.Complaint provider.Complaint AWS SES aws.ses.Delivery provider.Delivery AWS SES aws.ses.Open provider.Open AWS SES aws.ses.Send provider.Send AWS SNS aws.sns.NumberOfMessagesPublished provider.numberOfMessagesPublished AWS SNS aws.sns.NumberOfNotificationsDelivered provider.numberOfNotificationsDelivered AWS SNS aws.sns.NumberOfNotificationsFailed provider.numberOfNotificationsFailed AWS SNS aws.sns.PublishSize provider.publishSize AWS SNS aws.sns.SubscriptionsConfirmed provider.subscriptionsConfirmed AWS SNS aws.sns.SubscriptionsDeleted provider.subscriptionsDeleted AWS SNS aws.sns.SubscriptionsPending provider.subscriptionsPending AWS SQS aws.sqs.ApproximateAgeOfOldestMessage provider.approximateAgeOfOldestMessage AWS SQS aws.sqs.ApproximateNumberOfMessagesDelayed provider.approximateNumberOfMessagesDelayed AWS SQS aws.sqs.ApproximateNumberOfMessagesNotVisible provider.approximateNumberOfMessagesNotVisible AWS SQS aws.sqs.ApproximateNumberOfMessagesVisible provider.approximateNumberOfMessagesVisible AWS SQS aws.sqs.NumberOfEmptyReceives provider.numberOfEmptyReceives AWS SQS aws.sqs.NumberOfMessagesDeleted provider.numberOfMessagesDeleted AWS SQS aws.sqs.NumberOfMessagesReceived provider.numberOfMessagesReceived AWS SQS aws.sqs.NumberOfMessagesSent provider.numberOfMessagesSent AWS SQS aws.sqs.SentMessageSize provider.sentMessageSize AWS Step Functions aws.states.ExecutionsAborted provider.executionsAborted AWS Step Functions aws.states.ExecutionsFailed provider.executionsFailed AWS Step Functions aws.states.ExecutionsStarted provider.executionsStarted AWS Step Functions aws.states.ExecutionsSucceeded provider.executionsSucceeded AWS Step Functions aws.states.ExecutionsTimedOut provider.executionsTimedOut AWS Step Functions aws.states.ExecutionThrottled provider.executionThrottled AWS Step Functions aws.states.ExecutionTime provider.executionTime AWS Step Functions aws.states.ActivitiesFailed provider.activitiesFailed AWS Step Functions aws.states.ActivitiesHeartbeatTimedOut provider.activitiesHeartbeatTimedOut AWS Step Functions aws.states.ActivitiesScheduled provider.activitiesScheduled AWS Step Functions aws.states.ActivitiesStarted provider.activitiesStarted AWS Step Functions aws.states.ActivitiesSucceeded provider.activitiesSucceeded AWS Step Functions aws.states.ActivitiesTimedOut provider.activitiesTimedOut AWS Step Functions aws.states.ActivityRunTime provider.activityRunTime AWS Step Functions aws.states.ActivityScheduleTime provider.activityScheduleTime AWS Step Functions aws.states.ActivityTime provider.activityTime AWS Step Functions aws.states.LambdaFunctionRunTime provider.lambdaFunctionRunTime AWS Step Functions aws.states.LambdaFunctionScheduleTime provider.lambdaFunctionScheduleTime AWS Step Functions aws.states.LambdaFunctionsFailed provider.lambdaFunctionsFailed AWS Step Functions aws.states.LambdaFunctionsScheduled provider.lambdaFunctionsScheduled AWS Step Functions aws.states.LambdaFunctionsStarted provider.lambdaFunctionsStarted AWS Step Functions aws.states.LambdaFunctionsSucceeded provider.lambdaFunctionsSucceeded AWS Step Functions aws.states.LambdaFunctionsTimedOut provider.lambdaFunctionsTimedOut AWS Step Functions aws.states.LambdaFunctionTime provider.lambdaFunctionTime AWS Step Functions aws.states.ServiceIntegrationRunTime provider.serviceIntegrationRunTime AWS Step Functions aws.states.ServiceIntegrationScheduleTime provider.serviceIntegrationScheduleTime AWS Step Functions aws.states.ServiceIntegrationsFailed provider.serviceIntegrationsFailed AWS Step Functions aws.states.ServiceIntegrationsScheduled provider.serviceIntegrationsScheduled AWS Step Functions aws.states.ServiceIntegrationsStarted provider.serviceIntegrationsStarted AWS Step Functions aws.states.ServiceIntegrationsSucceeded provider.serviceIntegrationsSucceeded AWS Step Functions aws.states.ServiceIntegrationsTimedOut provider.serviceIntegrationsTimedOut AWS Step Functions aws.states.ServiceIntegrationTime provider.serviceIntegrationTime AWS Step Functions aws.states.ConsumedCapacity.byService provider.consumedCapacity AWS Step Functions aws.states.ProvisionedBucketSize.byService provider.provisionedBucketSize AWS Step Functions aws.states.ProvisionedRefillRate.byApiUsage provider.provisionedRefillRate AWS Step Functions aws.states.ThrottledEvents.byService provider.throttledEvents AWS Step Functions aws.states.ConsumedCapacity.byApiUsage provider.consumedCapacity AWS Step Functions aws.states.ProvisionedBucketSize.byApiUsage provider.provisionedBucketSize AWS Step Functions aws.states.ProvisionedRefillRate.byService provider.provisionedRefillRate AWS Step Functions aws.states.ThrottledEvents.byApiUsage provider.throttledEvents AWS Transit Gateway aws.transitgateway.BytesDropCountBlackhole provider.bytesDropCountBlackhole AWS Transit Gateway aws.transitgateway.BytesDropCountNoRoute provider.bytesDropCountNoRoute AWS Transit Gateway aws.transitgateway.BytesIn provider.bytesIn AWS Transit Gateway aws.transitgateway.BytesOut provider.bytesOut AWS Transit Gateway aws.transitgateway.PacketDropCountBlackhole provider.packetDropCountBlackhole AWS Transit Gateway aws.transitgateway.PacketDropCountNoRoute provider.packetDropCountNoRoute AWS Transit Gateway aws.transitgateway.PacketsIn provider.packetsIn AWS Transit Gateway aws.transitgateway.PacketsOut provider.packetsOut AWS Trusted Advisor aws.trustedadvisor.currentUsage currentUsage AWS Trusted Advisor aws.trustedadvisor.limitAmount limitAmount AWS Trusted Advisor aws.trustedadvisor.limitUsage serviceLimitUsage AWS VPC aws.natgateway.ActiveConnectionCount provider.activeConnectionCount AWS VPC aws.natgateway.BytesInFromDestination provider.bytesInFromDestination AWS VPC aws.natgateway.BytesInFromSource provider.bytesInFromSource AWS VPC aws.natgateway.BytesOutToDestination provider.bytesOutToDestination AWS VPC aws.natgateway.BytesOutToSource provider.bytesOutToSource AWS VPC aws.natgateway.ConnectionAttemptCount provider.connectionAttemptCount AWS VPC aws.natgateway.ConnectionEstablishedCount provider.connectionEstablishedCount AWS VPC aws.natgateway.ErrorPortAllocation provider.errorPortAllocation AWS VPC aws.natgateway.IdleTimeoutCount provider.idleTimeoutCount AWS VPC aws.natgateway.PacketsDropCount provider.packetsDropCount AWS VPC aws.natgateway.PacketsInFromDestination provider.packetsInFromDestination AWS VPC aws.natgateway.PacketsInFromSource provider.packetsInFromSource AWS VPC aws.natgateway.PacketsOutToDestination provider.packetsOutToDestination AWS VPC aws.natgateway.PacketsOutToSource provider.packetsOutToSource AWS VPC aws.vpc.bytes provider.bytes AWS VPC aws.vpc.packets provider.packets AWS VPC aws.vpn.TunnelDataIn provider.tunnelDataIn AWS VPC aws.vpn.TunnelDataOut provider.tunnelDataOut AWS VPC aws.vpn.TunnelState provider.tunnelState AWS WAF aws.waf.AllowedRequests.byWebACL provider.allowedRequests AWS WAF aws.waf.BlockedRequests.byWebACL provider.blockedRequests AWS WAF aws.waf.CountedRequests.byWebACL provider.countedRequests AWS WAF aws.waf.PassedRequests.byWebACL provider.passedRequests AWS WAF aws.waf.AllowedRequests.byRuleGroup provider.allowedRequests AWS WAF aws.waf.BlockedRequests.byRuleGroup provider.blockedRequests AWS WAF aws.waf.CountedRequests.byRuleGroup provider.countedRequests AWS WAF aws.waf.PassedRequests.byRuleGroup provider.passedRequests AWS WAFV2 aws.wafv2.AllowedRequests.byWebACL provider.allowedRequests AWS WAFV2 aws.wafv2.BlockedRequests.byWebACL provider.blockedRequests AWS WAFV2 aws.wafv2.CountedRequests.byWebACL provider.countedRequests AWS WAFV2 aws.wafv2.PassedRequests.byWebACL provider.passedRequests AWS WAFV2 aws.wafv2.AllowedRequests.byRuleGroup provider.allowedRequests AWS WAFV2 aws.wafv2.BlockedRequests.byRuleGroup provider.blockedRequests AWS WAFV2 aws.wafv2.CountedRequests.byRuleGroup provider.countedRequests AWS WAFV2 aws.wafv2.PassedRequests.byRuleGroup provider.passedRequests",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.05791,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS <em>integrations</em> metrics",
        "sections": "AWS <em>integrations</em> metrics",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " aws.states.Service<em>IntegrationsStarted</em> provider.service<em>IntegrationsStarted</em> AWS Step Functions aws.states.Service<em>Integrations</em>Succeeded provider.service<em>Integrations</em>Succeeded AWS Step Functions aws.states.Service<em>Integrations</em>TimedOut provider.service<em>Integrations</em>TimedOut AWS Step Functions"
      },
      "id": "603e8a26196a670f28a83deb"
    },
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-09-14T07:27:00Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 104.855194,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Azure monitoring <em>integrations</em>",
        "sections": "Introduction to Azure monitoring <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "Our Microsoft Azure <em>integrations</em> allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure <em>integrations</em> are not the same as APM&#x27;s .NET support for Azure. Requirements Check the Azure <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Go agent compatibility and requirements",
        "Golang versions",
        "Operating environments",
        "Integrations",
        "Database and instance-level performance",
        "Connect the agent to other features"
      ],
      "title": "Go agent compatibility and requirements",
      "type": "docs",
      "tags": [
        "Agents",
        "Go agent",
        "Get started"
      ],
      "external_id": "551e784873adba059e7b6112fc75199dce0e3f28",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/go-agent/get-started/go-agent-compatibility-requirements/",
      "published_at": "2021-09-13T18:57:53Z",
      "updated_at": "2021-09-07T15:53:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you don't have one already, create a New Relic account. It's free, forever. Before you install New Relic for Go, ensure your system meets these requirements. Golang versions New Relic supports Golang 1.7 or higher. Operating environments The agent supports Linux, macOS, and Windows. You can also use the Go agent in a Google App Engine (GAE) flexible environment. Integrations The following integration packages extend the base newrelic package to support other frameworks and libraries. If frameworks and databases don't have an integration package, you can still instrument them using the newrelic package primitives. For more information about instrumenting your database using these primitives, see the Go agent's datastore segments documentation on GitHub. The following integration packages must be imported along with the newrelic package, as shown in the nrgin example on GitHub. Project Integration package Purpose aws/aws-sdk-go v3/integrations/nrawssdk-v1 Instrument outbound calls made using Go AWS SDK aws/aws-sdk-go-v2 v3/integrations/nrawssdk-v2 Instrument outbound calls made using Go AWS SDK v2 labstack/echo v3/integrations/nrecho-v3 Instrument inbound requests through version 3 of the Echo framework labstack/echo v3/integrations/nrecho-v4 Instrument inbound requests through version 4 of the Echo framework gin-gonic/gin v3/integrations/nrgin Instrument inbound requests through the Gin framework gorilla/mux v3/integrations/nrgorilla Instrument inbound requests through the Gorilla framework julienschmidt/httprouter v3/integrations/nrhttprouter Instrument inbound requests through the HttpRouter framework aws/aws-lambda-go v3/integrations/nrlambda Instrument AWS Lambda applications sirupsen/logrus v3/integrations/nrlogrus Send agent log messages to Logrus mgutz/logxi v3/integrations/nrlogxi Send agent log messages to Logxi uber-go/zap v3/integrations/nrzap Send agent log messages to Zap pkg/errors v3/integrations/nrpkgerrors Wrap pkg/errors errors to improve stack traces and error class information openzipkin/b3-propagation v3/integrations/nrb3 Add B3 headers to outgoing requests database/sql Use a supported database driver or builtin instrumentation Instrument database calls with SQL jmoiron/sqlx Use a supported database driver or builtin instrumentation Instrument database calls with SQLx go-sql-driver/mysql v3/integrations/nrmysql Instrument database calls to MySQL lib/pq v3/integrations/nrpq Instrument database calls to Postgres using the database/sql library and pq jackc/pgx v3/integrations/nrpgx Instrument database calls to Postgres using the database/sql library and jackc/pgx snowflakedb/gosnowflake v3/integrations/nrsnowflake Instrument database calls to Snowflake go-redis/redis v3/integrations/nrredis-v7 Instrument calls to Redis Version 7 go-redis/redis v3/integrations/nrredis-v8 Instrument calls to Redis Version 8 mattn/go-sqlite3 v3/integrations/nrsqlite3 Instrument database calls to SQLite mongodb/mongo-go-driver v3/integrations/nrmongo Instrument MongoDB calls google.golang.org/grpc v3/integrations/nrgrpc Instrument gRPC servers and clients micro/go-micro v3/integrations/nrmicro Instrument servers, clients, publishers, and subscribers through the Micro framework nats-io/nats.go v3/integrations/nrnats Instrument publishers and subscribers using the NATS client nats-io/stan.go v3/integrations/nrstan Instrument publishers and subscribers using the NATS streaming client graphql-go/graphql v3/integrations/nrgraphqlgo Instrument inbound requests using graphql-go/graphql graph-gophers/graphql-go v3/integrations/nrgraphgophers Instrument inbound requests using graph-gophers/graphql-go Database and instance-level performance New Relic collects instance details for a variety of databases and database drivers. The ability to view specific instances and the types of database information in APM depends on your agent version. The New Relic Go agent version 1.4 or higher supports instance details for all database drivers. Connect the agent to other features The Go agent integrates with other features to give you full-stack observability: Product Integration Infrastructure monitoring When you install the infrastructure and APM agents on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI, and filter your Infrastructure hosts by APM app in the Infrastructure UI. For more information, see APM data in infrastructure monitoring. New Relic dashboards The Go agent sends default events and attributes to New Relic for NRQL queries. You can also record custom events for advanced analysis. Synthetic monitoring Synthetic transaction traces connect requests from synthetic monitors to the underlying APM transaction. Browser monitoring The Go agent has an API for returning the browser agent's JavaScript. After using this API method on each request, you can view browser data in the APM Summary page and quickly switch between the APM and browser data for a particular app.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 95.78525,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " use the Go agent in a Google App Engine (GAE) flexible environment. <em>Integrations</em> The following integration packages extend the base newrelic package to support other frameworks and libraries. If frameworks and databases don&#x27;t have an integration package, you can still instrument them using"
      },
      "id": "603e848a196a677702a83d8a"
    }
  ],
  "/docs/integrations/grafana-integrations/set-configure/configure-new-relic-prometheus-data-source-grafana": [
    {
      "sections": [
        "Grafana support with Prometheus and PromQL",
        "Use existing Grafana dashboards with New Relic",
        "Compatibility and requirements",
        "Support for PromQL",
        "Get data flowing in Grafana",
        "What’s next?"
      ],
      "title": "Grafana support with Prometheus and PromQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Grafana integrations",
        "Get started"
      ],
      "external_id": "52addf26732e0146545ae8dee6540d3bd7cab2ff",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/grafana-integrations/get-started/grafana-support-prometheus-promql/",
      "published_at": "2021-09-13T15:40:34Z",
      "updated_at": "2021-04-22T13:30:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In Grafana, you can configure New Relic as a Prometheus data source. Not only that, within Grafana you can query metrics stored in New Relic using the PromQL query language. Use existing Grafana dashboards with New Relic When you integrate Prometheus metrics with New Relic via Remote Write or the OpenMetrics Integration (2.0+) and configure New Relic as a Prometheus data source in Grafana, you can use existing Grafana dashboards and seamlessly tap into the additional monitoring, reliability, and scale we provide. Compatibility and requirements Before you begin, make sure you’ve finished integrating Prometheus metrics and are running a recent enough version of Grafana. You should have either the Remote Write or the OpenMetrics Integration ( v2.0+) set up before you can configure New Relic Prometheus data sources in Grafana. You can only configure New Relic Prometheus data sources using this method in Grafana versions 6.7.0 or newer. You will need to configure custom headers in the UI, and this isn’t possible with earlier versions. For details, see Configure New Relic as a Prometheus data source for Grafana. Support for PromQL Our Prometheus API emulates Prometheus' query APIs. We support the Prometheus query language (PromQL) through our PromQL-style query mode. We do our best to automatically translate PromQL syntax queries into the closest NRQL approximation. For more information on how this works and differences you may observe between Prometheus and New Relic, see Supported PromQL features. Get data flowing in Grafana To make your New Relic data available in Grafana, you can configure a new or existing Prometheus data source in just a couple of simple steps: In the Grafana UI, add and configure a new data source. Save the new data source and start viewing your data. What’s next? Ready to configure a Grafana data source? Read the how-to documentation for setting up the Prometheus remote write integration or the Prometheus OpenMetrics Integration. Read the how-to documentation for configuring Prometheus data sources in Grafana.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.86167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Grafana</em> support with Prometheus <em>and</em> PromQL",
        "sections": "<em>Grafana</em> support with Prometheus <em>and</em> PromQL",
        "tags": "<em>Grafana</em> <em>integrations</em>",
        "body": " integrating Prometheus metrics and are running a recent enough version of <em>Grafana</em>. You should have either the Remote Write or the OpenMetrics Integration ( v2.0+) <em>set</em> <em>up</em> before you can <em>configure</em> New Relic Prometheus data sources in <em>Grafana</em>. You can only <em>configure</em> New Relic Prometheus data sources using"
      },
      "id": "603e94de64441f9a804e8843"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 47.286446,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "<em>Configure</em> the <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-host <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. <em>Configure</em> the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Istio adapter",
        "Enable Istio adapter",
        "Find your data"
      ],
      "title": "Istio adapter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Istio"
      ],
      "external_id": "05c14aeaf5d07e0edcb4bd4bc4efcb2745627f00",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/istio/istio-adapter/",
      "published_at": "2021-09-13T16:57:41Z",
      "updated_at": "2021-08-02T10:43:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Istio provides a flexible model to enforce authorization policies and collect telemetry for the services in a mesh. New Relic's Istio adapter exports telemetry data from your Istio instance to your New Relic account. Resources for our Istio adapter: Enable Istio adapter To enable our Istio adapter: If you don't have one already, create a New Relic account. It's free, forever. Follow the Istio adapter install instructions. Optional: Instead of using the native Istio trace sampling, you can enable our Infinite Tracing feature. If you use this option, you will typically want to configure Istio to send us all trace data (learn more about sampling). To enable Infinite Tracing: In the New Relic UI, set up a trace observer. Configure our Istio adapter to send data to the trace observer: Set the spansHost value with YOUR_TRACE_OBSERVER_URL when deploying the Helm chart. Find your data To find your data, go to one.newrelic.com and go to Your applications > Explorer. From the Entities screen, search for your service by name. From there, you can explore your metrics using the Data explorer and build dashboards using your metrics. If you're sending distributed tracing data, the distributed tracing feature is available to query and view traces. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 45.45275,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Integrations</em>",
        "body": " to send us all trace data (learn more about sampling). To enable Infinite Tracing: In the New Relic UI, <em>set</em> <em>up</em> a trace observer. <em>Configure</em> our Istio adapter to send data to the trace observer: <em>Set</em> the spansHost value with YOUR_TRACE_OBSERVER_URL when deploying the Helm chart. Find your data To find"
      },
      "id": "603e81b464441fd2a54e8875"
    }
  ],
  "/docs/integrations/host-integrations/get-started/introduction-host-integrations": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.54404,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "HashiCorp Consul monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configuration",
        "Configure the integration",
        "Important",
        "Consul Instance Settings",
        "Labels/Custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Environment variables replacement",
        "Metrics-only with Token and no Fan Out",
        "Multi-instance monitoring",
        "Find and use data",
        "Metric data",
        "Consul datacenter sample metrics",
        "Consul agent sample metrics",
        "Inventory data",
        "Tip",
        "Check the source code"
      ],
      "title": "HashiCorp Consul monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "1f01bb7961a56a4a079f8638f1980102c4745088",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/hashicorp-consul-monitoring-integration/",
      "published_at": "2021-09-14T20:42:08Z",
      "updated_at": "2021-09-14T20:42:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The HashiCorp Consul on-host integration collects and sends inventory and metrics from your Consul environment to New Relic, where you can see the health of your Consul datacenter environment. We collect data on both the datacenter and agent/node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with HashiCorp Consul 1.0 or newer. Before installing the integration, make sure that you meet the following requirements: If Consul is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host running Consul. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. If using ACL, the credentials for the Consul integration must have the following policies: agent:read, node:read, and service:read. Quick start Instrument your Consul environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the HashiCorp Consul integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your Consul environment. Install and activate To install the HashiCorp Consul integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-consul. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp consul-config.yml.sample consul-config.yml Copy Edit the consul-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-consul .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-consul/nri-consul-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-consul-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp consul-config.yml.sample consul-config.yml Copy Edit the consul-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, consul-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, inventory_source. To read all about these common settings, refer to our Configuration Format document. Important If you are still using our legacy configuration/definition files, please refer to this document for help. Specific settings related to Consul are defined using the env section of the configuration file. These settings control the connection to your Consul instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Consul Instance Settings The Consul integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where Consul is running. localhost M/I PORT Port on which Consul is listening. 8500 M/I TOKEN ACL Token if token authentication is enabled. N/A M/I ENABLE_SSL Connect using SSL. false M/I CA_BUNDLE_FILE Alternative Certificate Authority bundle file. N/A M/I CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M/I TRUST_SERVER_CERTIFICATE If set to true, server certificate is NOT verified for SSL. false M/I FAN_OUT If true will attempt to gather metrics from all other nodes in Consul cluster. true M CHECK_LEADERSHIP Check leadership on consul server. This should be disabled on consul in client mode. true M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here or see the example below. Using secrets management. Use this to protect sensitive information, such as passwords that would be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics on. Our default sample config file includes examples of labels; however, as they are not mandatory, you can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-consul env: HOSTNAME: localhost PORT: 8500 interval: 15s labels: environment: production inventory_source: config/consul Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 30 seconds and inventory every 60 seconds: integrations: - name: nri-consul env: METRICS: true HOSTNAME: localhost PORT: 8500 interval: 15s labels: environment: production - name: nri-consul env: INVENTORY: true HOSTNAME: localhost PORT: 8500 interval: 60s labels: environment: production inventory_source: config/consul Copy Environment variables replacement In this configuration we are using the environment variable CONSUL_HOST to populate the HOSTNAME setting of the integration: integrations: - name: nri-consul env: METRICS: \"true\" HOSTNAME: {{CONSUL_HOST}} PORT: 8500 interval: 15s labels: env: production role: load_balancer Copy Metrics-only with Token and no Fan Out In this configuration we usee an ACL token to connect and we are not collecting metrics from other connected consul nodes (FAN_OUT: false): integrations: - name: nri-consul env: METRICS: \"true\" HOSTNAME: localhost PORT: 8500 TOKEN: my_token FAN_OUT: false interval: 15s labels: env: production role: load_balancer Copy Multi-instance monitoring In this configuration we are monitoring multiple Consul servers from the same integration. For the first instance (HOSTNAME: 1st_consul_host) we are collecting metrics and inventory while for the second instance (HOSTNAME: 2nd_consul_host) we will only collect metrics. integrations: - name: nri-consul env: METRICS: \"true\" HOSTNAME: 1st_consul_host PORT: 8500 interval: 15s labels: env: production role: load_balancer - name: nri-consul env: INVENTORY: \"true\" HOSTNAME: 1st_consul_host PORT: 8500 interval: 60s labels: env: production role: load_balancer inventory_source: config/consul - name: nri-consul env: METRICS: \"true\" HOSTNAME: 2nd_consul_host PORT: 8500 interval: 15s labels: env: production role: load_balancer Copy Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: ConsulDatacenterSample ConsulAgentSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The HashiCorp Consul integration collects the following metric data attributes. Consul datacenter sample metrics These attributes are attached to the ConsulDatacenterSample event type: Metric Description consul.catalog.nodes_critical The number of nodes with service status critical from those registered. consul.catalog.nodes_passing The number of nodes with service status passing from those registered. consul.catalog.nodes_up The number of nodes. consul.catalog.nodes_warning The number of nodes with service status warning from those registered. consul.catalog.total_nodes The number of nodes registered in the consul cluster. consul.memberlist.msg.suspect The number of times an agent suspects another as failed while probing during gossip protocol. consul.raft.apply The number of raft transactions occurring. consul.raft.commitTime.avg The average time it takes to commit a new entry to the raft log on the leader. consul.raft.commitTime.count The number of samples of raft.commitTime. consul.raft.commitTime.max The max time it takes to commit a new entry to the raft log on the leader. consul.raft.commitTime.median The median time it takes to commit a new entry to the raft log on the leader. consul.raft.leader.dispatchLog.avg The average time it takes for the leader to write log entries to disk. consul.raft.leader.dispatchLog.count The number of samples of raft.leader.dispatchLog. consul.raft.leader.dispatchLog.max The max time it takes for the leader to write log entries to disk. consul.raft.leader.dispatchLog.median The median time it takes for the leader to write log entries to disk. consul.raft.leader.lastContact.avg The average time elapsed since the leader was last able to check its lease with followers. consul.raft.leader.lastContact.count The number of samples of raft.leader.lastContact. consul.raft.leader.lastContact.max The max time elapsed since the leader was last able to check its lease with followers. consul.raft.leader.lastContact.median The median time elapsed since the leader was last able to check its lease with followers. consul.raft.state.candidate The number of initiated leader elections. consul.raft.state.leader The number of completed leader elections. consul.serf.member.flap The number of times an agent is marked dead and then quickly recovers. Consul agent sample metrics These attributes are attached to the ConsulAgentSample event type: Metric Description agent.aclCacheHit ACL cache hits. agent.aclCacheMiss ACL cache misses. agent.kvStores The number of samples of kvs.apply. agent.kvStoresAvgInMilliseconds The average time it takes to complete an update to the KV store. agent.kvStoresMaxInMilliseconds The max time it takes to complete an update to the KV store. agent.kvStoresMedianInMilliseconds The median time it takes to complete an update to the KV store. agent.peers The number of peers in the peer set. agent.staleQueries Served queries within the allowed stale threshold. agent.txnAvgInMilliseconds The average time it takes to apply a transaction operation. agent.txnMaxInMilliseconds The max time it takes to apply a transaction operation. agent.txnMedianInMilliseconds The median time it takes to apply a transaction operation. agent.txns The number of samples of txn.apply. client.rpcFailed Measure of failed RPC requests. client.rpcLoad Measure of how much an agent is loading Consul servers. client.rpcRateLimited Measure of RPC requests that get rate limited. net.agent.maxLatencyInMilliseconds Maximum latency from this node to all others. net.agent.medianLatencyInMilliseconds Median latency from this node to all others. net.agent.minLatencyInMilliseconds Minimum latency from this node to all others. net.agent.p25LatencyInMilliseconds P25 latency from this node to all others. net.agent.p75LatencyInMilliseconds P75 latency from this node to all others. net.agent.p90LatencyInMilliseconds P90 latency from this node to all others. net.agent.p95LatencyInMilliseconds P95 latency from this node to all others. net.agent.p99LatencyInMilliseconds P99 latency from this node to all others. runtime.allocations Cumulative count of heap objects allocated. runtime.allocationsInBytes The current bytes allocated by the Consul process. runtime.frees Cumulative count of heap objects freed. runtime.gcCycles The number of completed GC cycles. runtime.gcPauseInMilliseconds Cumulative nanoseconds in GC stop-the-world pauses since Consul started. runtime.goroutines The number of running go routines. runtime.heapObjects The number of objects allocated on the heap runtime.virtualAddressSpaceInBytes Total size of the virtual address space reserved by the go runtime. Inventory data The HashiCorp Consul integration captures the configuration parameters and current settings of the Consul Agent nodes. It collects the results of the /v1/agent/self REST API endpoint. It pulls the Config and DebugConfig sections from that response. Tip Note: Nested sections within Config and DebugConfig are not collected. The data is available on the Inventory page, under the config/consul source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.66779,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "HashiCorp Consul monitoring <em>integration</em>",
        "sections": "HashiCorp Consul monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " agent on a <em>host</em> running Consul. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. If using ACL, the credentials for the Consul integration must have the following policies: agent:read, node:read, and service:read. Quick <em>start</em> Instrument your"
      },
      "id": "603ea36864441f081a4e887e"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.53635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". Quick <em>start</em> Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to <em>get</em> <em>started</em>? Click one"
      },
      "id": "6043a676196a67eb76960f7b"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/apache-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80298,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75323,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/cassandra-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98938,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80286,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75314,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/collectd-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98938,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80286,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75314,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/couchbase-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80273,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75302,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80273,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75302,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98883,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.8026,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.7529,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/flex-integration-tool-build-your-own-integration": [
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-09-13T17:41:13Z",
      "updated_at": "2021-07-27T09:41:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.68071,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": " <em>integrations</em>, such as our APM agents. Whatever data you need to bring in, chances are that we have options for <em>your</em> environment. If you prefer to make <em>your</em> <em>own</em> solutions, we also offer tools to get you started. Choose what&#x27;s right for you We offer a wide range of solutions so you can easily collect"
      },
      "id": "603e817f28ccbc4857eba798"
    },
    {
      "sections": [
        "APM best practices guide",
        "Tip",
        "1. Standardize application names",
        "How to do it",
        "2. Add tags to your applications",
        "Caution",
        "3. Create and evaluate alert policies",
        "4. Identify and set up key transactions",
        "5. Track deployment history",
        "6. Review APM reports",
        "7. Review your environment with service maps",
        "8. Keep current",
        "9. Manage user access"
      ],
      "title": "APM best practices guide",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "Best practices guides",
        "Full-stack observability"
      ],
      "external_id": "368a1a5688384d5bebf128604a9b8f190d335524",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/best-practices-guides/full-stack-observability/apm-best-practices-guide/",
      "published_at": "2021-09-14T06:06:59Z",
      "updated_at": "2021-09-14T06:06:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Application monitoring tips you need to know It's one thing to know how to use APM, but it's another thing to know how to use New Relic's application performance monitoring software well. Here are some best practices designed to help you become an APM master—and a key asset to your team! Tip To get a high-level overview of all your applications and services, use the New Relic Explorer in New Relic One. 1. Standardize application names Most New Relic agents provide a default application name, such as \"My Application\" or \"PHP Application,\" if you don't specify one in your New Relic configuration file. You don't want to end up with 20 identically named applications, be sure to select a descriptive identifier for your apps as soon you deploy them. To keep things consistent and easy to navigate, New Relic recommends standardizing your application naming (for example, all apps in Staging append [ Staging] or the like at the end of their names). Ideally, you want your new Java applications to be named automatically to reduce the chances of typographical errors and misnaming. How to do it For Java applications, automatic application naming can come from the following sources: Request attribute Servlet init parameter Filter init parameter Web app context parameter Web app context name (display name) Web app context path Choose the method that fits best your needs and follow these steps. For non-Java applications, there are no automatic naming methods, so refer to the documentation for your APM agent. 2. Add tags to your applications When several different applications use the same account, and each application spans multiple environments (for example, development, test, pre-production, production), it can be hard to find a specific application in your overview dashboard. That's why we recommend adding tags to your apps so that you can segment them into logical groups. The two most common tags that mature APM customers use are application name and environment. So, for example, if you wanted to view the billing application in Test, you could simply filter by \"billing app\" (name tag) and \"test\" (environment tag). Caution In the APM agent configuration settings files, use the labels field to add tags to your data. For example, see this description of the Python labels setting. APM is designed so that apps can roll up into an unlimited number of meaningful tag categories. How to do it Learn about tags. Learn how to add tags. Learn how to query tags. 3. Create and evaluate alert policies When key performance indicators spike or drop, individuals and teams in your organization need to be notified. Alerting in New Relic provides a set of tools including dynamic baselines that allow you to detect problems before they impact your end users. Alert policies can be set up in two primary ways: Static threshold alerts are great when you already know the nature of an application and its normal behaviors aren't likely to change anytime soon. Apdex score, response time, error rate, throughput are some of the static thresholds you can create alert policies on. Dynamic baseline alerts make it easy to determine and set dynamic alert thresholds for applications with varying seasonal patterns and growth trends (which make it difficult to set thresholds that define normal behavior). These alerts use baselines modeled from your application’s historical metric data. Each alert policy can contain as many conditions as you need, and each alert condition includes three components: Type of condition (metric, external service, and so on) Entities that the policy targets (for example, APM apps, browser monitoring apps, or hosts) Thresholds that escalate into alerting situations with increasing severity Once you have your alerting set up, you then want to make sure you're taking advantage of all viable notification channels. After all, what good are alerts if no one knows about them? You can manage alerts by creating specific user groups and by leveraging New Relic's integrated alert channels, including Slack, PagerDuty, webhooks, and email. Be sure to evaluate alert policies on a regular basis to ensure that they are always valid. How to do it See the detailed documentation: To set up dynamic baseline alerts and choose an application, follow standard procedures. You will see a preview of the metric with the predicted baseline You can select a metric for that application and see the corresponding baseline. Then, using the threshold sliders, you can set how closely you want your threshold to follow the baseline prediction. To set up static threshold alerts for your Apdex settings, follow standard procedures. To set up your alert notification channels, follow standard procedures. 4. Identify and set up key transactions Depending on the nature of your application, some transactions may be more important to you than others. New Relic's key transactions feature is designed to help you closely monitor what you consider to be your app's most business-critical transactions, whether that's end-user or app response time, call counts, error rates, or something else. You can also set alert threshold levels for notifications when your key transactions are performing poorly. How to do it In the menu bar, select More > Key transactions, and then select Add more. Then select the app and web transaction or, from the selected transaction, select Track as key transaction. Type a name for the key transaction, and select Track key transaction. Optional: If the agent for the selected app supports custom alerting, use the default values that New Relic automatically fills, or select Edit key alert transaction policy to set the Apdex and alert threshold values. To view the key transactions dashboard details, select View new key transaction. 5. Track deployment history When development teams are pushing new code out as frequently as possible, it can be hard to measure the impact that each deployment is having on performance. One way to stay in tune with how these changes are affecting your application is with deployment reports. These reports list recent deployments and their impact on end-users and app servers' Apdex scores, along with response times, throughput, and errors. You can also view and drill down into the details to catch errors related to recent deployments, or file a ticket and share details with your team. How to do it From the New Relic menu bar, select APM > (selected app) > Events > Deployments. To view performance after a deployment, go to the selected app's Overview dashboard in the Recent events section. A blue vertical bar on a chart indicates a deployment. To view summary information about the deployment, point to the blue bar. 6. Review APM reports From SLA, deployment, and capacity to scalability, host usage reports, and more, APM offers a variety of downloadable reporting tools surfacing historical trends—all great ways to report to senior executive teams or customers. Take a look at the full list of reports and use them to your advantage. How to do it From the APM menu bar, select Applications > (selected app) > Reports. Select the report you'd like to see. If you want to save or export a report to share, select Download this report as .csv, which will create a report with comma-separated values. 7. Review your environment with service maps Use New Relic service maps, a feature included in APM, to understand how apps and services in your architecture connect and talk to each other. Service maps are visual, customizable representations of your application architecture. Maps automatically show you your app's connections and dependencies, including databases and external services. Health indicators and performance metrics show you the current operational status for every part of your architecture. How to do it Go to one.newrelic.com > More > Service maps. To get started, see Introduction to service maps. 8. Keep current With New Relic’s SaaS platform, getting new features is as easy as updating your agent. Most likely your organization already has a set of scripts for deploying application upgrades into your environment. In a similar fashion, you can also automate your New Relic agent deployment to ensure that your systems are up to date. Both Puppet and Chef scripts are great examples of deployment frameworks that make life easier by allowing you to automate your entire deployment and management process. How to do it Regularly review which version of the agent you're using so that you know when an update is needed. If the latest agent release contains a needed fix or added functionality, download it. To deploy the agent automatically (preferred as a method to avoid errors): Use existing deployment scripts, provided they can be adapted to handle the deployment. OR Create and maintain a script that specifically deploys and configures the New Relic agent. Ideally, the script would pull the agent files from a repository where the files are versioned (for rollback purposes). Once the script has been created, shut down the application (unless script handles this). Run the deployment script. Start the application (unless script handles this). If problems arise, run the script to roll back to the previous version. To deploy the agent manually: Back up the current agent directory. Deploy the updated agent into the existing agent directory. Modify configuration files by comparing new files with existing files. In particular, make sure things like license key and custom extensions are copied over to the new configuration. Restart the application. If problems arise, restore the old agent using the backup and restart. 9. Manage user access How you manage your users depends on which user model your users are on: See original user management docs See New Relic One user management docs",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.6017,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "2. Add tags to <em>your</em> applications",
        "tags": "<em>Full</em>-<em>stack</em> <em>observability</em>",
        "body": " great ways to report to senior executive teams or customers. Take a look at the <em>full</em> list of reports and use them to <em>your</em> advantage. How to do it From the APM menu bar, select Applications &gt; (selected app) &gt; Reports. Select the report you&#x27;d like to see. If you want to save or export a report"
      },
      "id": "6044186564441f1f94378ecc"
    },
    {
      "sections": [
        "Infrastructure monitoring best practices guide",
        "1. Install the infrastructure agent across your entire environment",
        "How to do it",
        "Tip",
        "2. Configure the native EC2 integration",
        "3. Activate the integrations",
        "4. Create filter sets",
        "5. Create alert conditions",
        "6. View infrastructure data alongside APM data",
        "7. Access Infrastructure data using the Data explorer",
        "8. Update your agents regularly",
        "Want more user tips?"
      ],
      "title": "Infrastructure monitoring best practices guide",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "Best practices guides",
        "Full-stack observability"
      ],
      "external_id": "931ea7767d73381ca0cb3502ec14f88d66ce5eaf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/best-practices-guides/full-stack-observability/infrastructure-monitoring-best-practices-guide/",
      "published_at": "2021-09-14T06:03:33Z",
      "updated_at": "2021-09-14T06:03:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Want even longer periods of uninterrupted sleep? Here are eight best practices to make dynamic infrastructure and server monitoring even easier with New Relic's infrastructure monitoring. 1. Install the infrastructure agent across your entire environment Our infrastructure monitoring solution was designed to help enterprise customers monitor their large and dynamically changing environments at scale. In order to facilitate this, the UI is completely driven by tags that let you visualize aggregated metrics, events, and inventory for a large number of servers. To really get the most out of infrastructure monitoring, we recommend installing it across your entire environment, preferably even across multiple regions and clusters. This will provide a more accurate picture of the health of your host ecosystem and the impact your infrastructure has on your applications. Want to achieve faster Mean Time To Resolution (MTTR)? Install the infrastructure agent on database servers, web servers, and any other host that supports your applications. When deploying the agent, leverage custom attributes to tag your hosts so that you can use those for filtering the data presented in the UI and for setting alerts. This is in addition to any Amazon EC2 tags you may be using which will auto-import when you enable the EC2 integration. You may also prefer to keep the agent logs separate from the system logs, which you can do through the configuration. How to do it Leverage our install modules for config management tools such as Chef, Puppet and Ansible to easily deploy your agent across all your infrastructure. Read the instructions in the github repo for your config management tool referenced in the link above and define the custom_attributes you want to use to tag your hosts. Set the log_file attribute to your preferred location for the infrastructure agent logs. Tip If you are installing the agent on a single host, the process should only take a few minutes and you can find detailed instructions in our documentation. 2. Configure the native EC2 integration If you have an AWS environment, in addition to installing the infrastructure agent on your EC2 instances to monitor them, we also recommend configuring the EC2 integration so that New Relic can automatically import all the tags and metadata associated with your AWS instances. This allows you to filter down to a part of your infrastructure using the same AWS tags (example, ECTag_Role='Kafka'), and slice-and-dice your data in multiple ways. Additionally, our ‘Alerts’ and ‘Saved Filter Sets’ are completely tag-driven and dynamic, so they automatically add/remove instances matching these tags to give our users the most real-time views that scale with your cloud infrastructure. 3. Activate the integrations Monitoring your infrastructure extends beyond just CPU, memory, and storage utilization. That’s why New Relic has out-of-the-box integrations that allow you to monitor all the services that support your hosts as well. Activate any of our integrations, including AWS Billing, AWS ELB, Amazon S3, MySQL, NGINX, and more, to extend monitoring to your AWS or on-host applications, and access the pre-configured dashboards that appear for each of them. 4. Create filter sets With New Relic, users can create filter sets to organize hosts, cluster roles, and other resources based on criteria that matter the most to users. This allows you to optimize your resources by using a focused view to monitor, detect, and resolve any problems proactively. The attributes for filtering are populated from the auto-imported EC2 tags or custom tags that may be applied to hosts. You can combine as many filters as you want in a filter set, and save them to share with other people in your account. You’ll also be able to see the color-coded health status of each host inside the filter set, so you can quickly identify problematic areas of your infrastructure. Additionally, filter sets can be used in the health map to get an overview of your infrastructure performance at a glance based on the filters that matter to your teams. 5. Create alert conditions With New Relic, you can create alert conditions directly within the context of what you are currently monitoring with New Relic. For example, if you are viewing a filter set comprised of a large number of hosts and notice a problem, you don’t need to create an individual alert condition for every host within. Instead, we recommend initiating the alert condition directly from the chart of the metric you are viewing and creating it based on the filter tags. This will create an alert condition for any hosts that match those tags, allowing our infrastructure monitoring to automatically remove hosts that go offline and add new hosts to the alert condition if they match those tags. Alerts configured once for the appropriate tags will scale correctly across all future hosts. And know that you can also leverage existing alert policies for infrastructure alert conditions. 6. View infrastructure data alongside APM data The integration between APM and infrastructure monitoring lets you see your APM data and infrastructure data side by side, so you can find the root cause of problems more quickly, no matter where they originate. This allows users to view the performance relationship of your hosts and the applications running on them, allowing for quicker diagnosis of the issue and impact on the business’ health. Use health maps to quickly spot any issues or alerts related to the health of your applications and how that connects to the supporting infrastructure. The first boxes starting from the top left are those that require your attention. 7. Access Infrastructure data using the Data explorer Teams that use multiple New Relic capabilities find it useful to create a single dashboard to visually correlate the infrastructure’s health with application, browser and synthetics metrics. That’s where New Relic data exploration features comes in. All the granular metrics and events collected by infrastructure monitoring are stored in New Relic and are accessible to you immediately. Having access to the raw metrics means you can run more custom queries using NRQL, and also create dashboards to share infrastructure metrics with your team. 8. Update your agents regularly New Relic’s software engineering team is constantly pushing out improvements and new features to improve our customers’ overall monitoring experience. In order to take advantage of all the awesomeness they’re delivering, we recommend regularly updating to the latest version of the infrastructure agent. Want more user tips? View training videos at New Relic University. Read the documentation. Check out our Tutorials. Ask a question in the New Relic Explorers Hub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "3. Activate the <em>integrations</em>",
        "tags": "<em>Full</em>-<em>stack</em> <em>observability</em>",
        "body": "-time views that scale with <em>your</em> cloud infrastructure. 3. Activate the <em>integrations</em> Monitoring <em>your</em> infrastructure extends beyond just CPU, memory, and storage utilization. That’s why New Relic has out-of-the-box <em>integrations</em> that allow you to monitor all the services that support <em>your</em> hosts as well"
      },
      "id": "6044151c28ccbc4b4f2c60af"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/go-insights-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9886,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.8025,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/haproxy-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9886,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.8025,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/hashicorp-consul-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9883,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80237,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration": [
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80237,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.74554,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Nagios monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". This gives you full control over the installation and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80225,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75256,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80225,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.74542,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Nagios monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". This gives you full control over the installation and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9878,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.7453,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Nagios monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". This gives you full control over the installation and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/mongodb-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9878,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98752,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.802,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/mysql-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98752,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.802,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.7522,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nfs-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.7522,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nginx-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.987,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80176,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/oracle-database-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.987,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80176,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/perfmon-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9867,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80164,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/port-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9867,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80164,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/postgresql-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98645,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.8015,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/rabbitmq-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98645,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.8015,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/redis-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.8014,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/snmp-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.8014,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75174,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9859,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80127,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/unix-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9859,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80127,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/varnish-cache-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98566,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80115,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.7515,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98566,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80115,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.7515,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/vmware-vsphere-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.98538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.80103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/windows-services-integration": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/infrastructure-release-notes/infrastructure-agent-release-notes/new-relic-infrastructure-agent-1121/",
      "sections": [
        "Infrastructure agent v1.12.1",
        "Notes",
        "Added",
        "Fixed"
      ],
      "published_at": "2021-09-14T05:19:14Z",
      "title": "Infrastructure agent v1.12.1",
      "updated_at": "2021-03-13T03:15:26Z",
      "type": "docs",
      "external_id": "93e606131035b520d2d5f6be4220698349929793",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "Notes A new version of the agent has been released. Follow standard procedures to update your Infrastructure agent. Added Beta version (v0.1.0-beta) of nri-winservices is now packaged with the agent. For more information, see the Windows services integration documentation. Fixed d35cbe7 Fixed the sending of heartbeat samples to New Relic. 6503df0 Inventory is now fully re-sent if the host has been offline for 24 hours or if the agent ID changes. 4ccd9ff Fixed issue where running Docker auto discovery was leaking file descriptors.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 469.28152,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "Notes A new version of the agent has been released. Follow standard procedures to update your Infrastructure agent. Added Beta version (v0.1.0-beta) of nri-winservices is now packaged with the agent. For more information, see the <em>Windows</em> <em>services</em> <em>integration</em> documentation. Fixed d35cbe7 Fixed"
      },
      "id": "6044211fe7b9d21ae05799cb"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.189384,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the Kubernetes <em>integration</em> for <em>Windows</em>",
        "sections": "Install the Kubernetes <em>integration</em> for <em>Windows</em>",
        "tags": "<em>Integrations</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for <em>Windows</em>, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install New Relic Node.js agent in GAE flexible environment",
        "Use native deploy",
        "Build a custom runtime",
        "1. Set up the GAE project and install dependencies",
        "2. Configure your app.yaml",
        "3. Configure and deploy",
        "Optional: Disable health checks",
        "Get New Relic agent troubleshooting logs from GAE"
      ],
      "title": "Install New Relic Node.js agent in GAE flexible environment",
      "type": "docs",
      "tags": [
        "Agents",
        "Nodejs agent",
        "Hosting services"
      ],
      "external_id": "28df1194ff848b2c26e5c966843d2bf5f0adbf9e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/nodejs-agent/hosting-services/install-new-relic-nodejs-agent-gae-flexible-environment/",
      "published_at": "2021-09-14T02:43:28Z",
      "updated_at": "2021-09-14T02:43:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Node.js agent, you can monitor applications that reside in the Google App Engine (GAE) flexible environment. Adding New Relic to your GAE flex app gives you insight into the health and performance of your app and extends GAE with metrics you can view in APM and browser monitoring. This document explains how to add New Relic to your GAE flex app using either of these methods: Google App Engine's \"native mode\" installation with a standard GAE runtime Docker installation using a custom runtime Use native deploy To use Google App Engine's \"native mode\" installation with your Node.js app: Follow standard procedures to install New Relic's Node.js agent, including your license key. Be sure to save the newrelic module to the package.json file. Follow Google App Engine procedures for Node.js to create a new Cloud Platform project, create an App Engine application, and complete other prerequisites for the Google Cloud SDK. Optional: Set environment variables to configure the Node.js agent's GAE app.yaml file. Use the Google Cloud SDK's gcloud command line tool to deploy GAE apps. To deploy your Node.js app to your initialized GAE flexible environment, run the following command: gcloud --project new-relic-apm-nodejs app deploy Copy Google App Engine automatically includes your Node.js app's newrelic.js configuration file in the deployed package. Wait until the deployment completes, then view your GAE flex app data in the APM Summary page. Build a custom runtime See Google's documentation for building custom runtimes. This example describes how to add New Relic to your GAE flex app by building a custom runtime for Docker. You can deploy the app without any special configuration. For more information about deploying and configuring your Node.js app in the GAE flexible environment, see: Google App Engine's documentation for Node.js Google App Engine's tutorial to deploy a Node.js app 1. Set up the GAE project and install dependencies Follow standard procedures to install New Relic's Node.js agent, including your license key. Be sure to save the newrelic module to the package.json file. Follow Google App Engine procedures for Node.js to create a new Cloud Platform project, create an App Engine application, and complete other prerequisites for the Google Cloud SDK. The Google Cloud SDK provides the gcloud command line tool to manage and deploy GAE apps. 2. Configure your app.yaml The app.yaml configuration file is required for a GAE flexible environment app with a custom runtime. At a minimum, make sure it contains: runtime: custom env: flex Copy Optional: Set environment variables. 3. Configure and deploy The Dockerfile defines the Docker image to be built and is required for a GAE flexible environment app. To create the Dockerfile, build the container, and deploy your app, follow the GAE procedures for Node.js. Wait until the deployment completes. To view your GAE flex app data in New Relic, go to the APM Summary page. Optional: Disable health checks Google App Engine sends periodic health check requests to confirm that an instance has been successfully deployed, and to check that a running instance maintains a healthy status. A health check is an HTTP request to the URL /_ah/health. If you create a custom runtime, your app must be able to handle a large number of health check requests. Otherwise, your app data may not display correctly in APM. If you notice performance issues, disable GAE health checks. In your app.yaml, add: health_check: enable_health_check: False Copy Get New Relic agent troubleshooting logs from GAE Use these resources to troubleshoot your GAE flex environment app: To connect to the GAE instance and start a shell in the Docker container running your code, see Debugging an instance. To redirect New Relic Node.js agent logs to Stackdriver in the Cloud Platform Console, change the newrelic.js configuration file to: log_file_name: STDOUT Copy To view the logs, use the Cloud Platform Console's Log Viewer.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 93.74028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "Hosting <em>services</em>"
      },
      "id": "6043d8da28ccbc08242c60a6"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/zookeeper-monitoring-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.9851,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configuration",
        "Enabling your MS SQL Server",
        "Configure the integration",
        "Important",
        "Instance settings",
        "Labels and custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Multi Instance connection",
        "Metrics-only connecting over SSL",
        "Buffer Pool and Partition Reserve metrics optimization",
        "Custom Query",
        "Multiple Custom Queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-15T01:39:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration requires Microsoft SQL Server 2008 R2 SP3 or higher, using mixed authentication mode. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. Windows distribution compatible with our infrastructure agent. The infrastructure agent installed. Requires SQL Server user or domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. Note that SQL Server must be using mixed mode authentication. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files, please refer to this document for help. Instance settings The SQL Server integration collects both Metrics(M) and Inventory(I) information. In the table, use the Applies to column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where MS SQL server is running. 127.0.0.1 M/I PORT Port on which MS SQL server is listening. Note: Port is only required when INSTANCE is not specified. 1433 M/I INSTANCE The MS SQL Server Instance to connect to. Note: Only required when PORT is not specified. Do not use both. N/A M/I USERNAME Username for accessing the MS SQL server. N/A M/I PASSWORD Password for the given user. N/A M/I EXTRA_CONNECTION_URL_ARGS Specify extra connection parameters as attr1=val1&attr2=val2. N/A M/I ENABLE_SSL Use SSL to connect to the MS SQL Server. false M/I TRUST_SERVER_CERTIFICATE if set to true, server certificate is NOT verified for SSL. false M/I CERTIFICATE_LOCATION Location of the SSL Certificate. N/A M/I TIMEOUT Timeout for queries, in seconds. Set 0 for no timeout. 30 M/I ENABLE_BUFFER_METRICS Enable collection of buffer pool metrics. These can be resource intensive for large systems. true M ENABLE_DATABASE_RESERVE_METRICS Enable collection of database partition reserve space. These can be resource intensive for large systems. true M CUSTOM_METRICS_QUERY A SQL query to collect custom metrics. See example below. N/A M CUSTOM_METRICS_CONFIG YAML configuration with one or more SQL queries to collect custom metrics. See example below. false M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly to the configuration file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires infrastructure agent 1.14.0+. For more on this, see more on infrastructure agent passthrough environment variables. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can also decorate your metrics with labels. Labels allow you to add key/value pair attributes to your metrics so that you can query, filter, or group your metrics. Even though our default sample configuration file includes examples of labels, they're optional. You can remove, modify, or add new ones. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost. It uses default connection on port 1433: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production inventory_source: config/mssql Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 15s labels: environment: production - name: nri-mssql env: INVENTORY: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password interval: 60s labels: environment: production inventory_source: config/mssql Copy Multi Instance connection This configuration monitors 2 instances on the same SQL Server (mssql_instance1 and mssql_instance1). When connecting directly to an Instance you will need to remove the PORT setting. The SQL Browser service also needs to be running on the server: integrations: - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance1 interval: 15s labels: environment: production inventory_source: config/mssql - name: nri-mssql env: HOSTNAME: localhost USERNAME: mssql_user PASSWORD: mssql_password INSTANCE: mssql_instance2 interval: 15s labels: environment: production inventory_source: config/mssql Copy Metrics-only connecting over SSL Use to connect to MS SQL using SSL without validation of the certificate: integrations: - name: nri-mssql env: METRICS: true HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_SSL: true TRUST_SERVER_CERTIFICATE: true interval: 15s labels: environment: production Copy Buffer Pool and Partition Reserve metrics optimization If the integration causes some overhead on your SQL server, disabling Buffer Pool and Database Partition Reserve metrics can help to optimize the integration's performance. In this example we also increased the sampling interval to 30 seconds to reduce the sampling frequency: integrations: - name: nri-mssql env: HOSTNAME: localhost PORT: 1433 USERNAME: mssql_user PASSWORD: mssql_password ENABLE_BUFFER_METRICS: false ENABLE_DATABASE_RESERVE_METRICS: false interval: 30s labels: environment: production inventory_source: config/mssql Copy Custom Query You can use custom a custom query to collect additional metrics. Custom metrics will be added to the MssqlCustomQuerySample event sample. Note: If you need to use multiple custom queries, remove this setting and use CUSTOM_METRICS_CONFIG instead. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_QUERY: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id interval: 15s labels: environment: production Copy Multiple Custom Queries If you need multiple custom SQL queries, add them to mssql-custom-query.yml, and reference that file on your configuration. For more examples of custom queries, check our sample file on github. NOTE: CUSTOM_METRICS_CONFIG is only enabled if CUSTOM_METRICS_QUERY is not present. integrations: - name: nri-mssql env: METRICS: true HOSTNAME: mssql_host1 PORT: 1433 USERNAME: mssql1_user PASSWORD: mssql1_password CUSTOM_METRICS_CONFIG: 'C:\\path\\to\\mssql-custom-query.yml' interval: 15s labels: environment: production Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.8009,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration Enabling your MS SQL Server In the Microsoft SQL Server to be monitored, execute the following script to create a new"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Memcached monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Memcached instance settings",
        "Labels/custom attributes",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Memcached sample metrics",
        "Memcached slab sample metrics",
        "Inventory data",
        "Memcached Inventory",
        "Check the source code"
      ],
      "title": "Memcached monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "d9a738c38a1327132b9a3ca7ccca7de27f457a44",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration/",
      "published_at": "2021-09-14T20:52:23Z",
      "updated_at": "2021-09-14T20:52:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Memcached integration collects and sends inventory and metrics from your Memcached instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at both instance and slab levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Memcached 1.4 or higher. If Memcached is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Memcached. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Memcached instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Memcached integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MySQL instance. Install and activate To install the Memcached integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-memcached. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-memcached .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-memcached/nri-memcached-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-memcached-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp memcached-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, memcached-config.yml. The configuration file has common settings used by all of our integrations, such as interval, timeout, and inventory_source. For more on these common settings, see our configuration properties list. If you're still using our legacy configuration/definition files, see the standard configuration format for help. Specific settings related to Memcached are defined using the env section of the configuration file. These settings control the connection to your Memcached instance as well as other security settings and features. Configuration options are below. For a better sense of how this works in practice, see our example config file. Memcached instance settings The Memcached integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column to find which settings can be used for each collection: Setting Description Default Applies To HOST Hostname of the memcached instance. localhost M/I PORT Port memcached is running on. 11211 M/I USERNAME Memcached SASL username. Only required if authentication is enabled. N/A M/I PASSWORD Memcached SASL password. Only required if authentication is enabled. N/A M/I METRICS Set to true to enable Metrics-only collection. false INVENTORY Set to true to enable Inventory-only collection. false The values for these settings can be defined in several ways: Add the value directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here or see example below. Use secrets management to protect sensitive information, such as passwords, from being exposed in plain text in the configuration file. For more information, see how to use secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use them to query, filter, or group your metrics. Our default sample configuration file includes examples of labels, but you can remove, modify, or add new ones of your choice. labels: env: production role: memcached Copy Example configuration Example memcached-config.yml file configuration: Example configuration integrations: - name: nri-memcached env: PORT: \"11211\" HOST: memcached_host # ifauthentication is enabled. USERNAME: cacheuser PASSWORD: password interval: 15s inventory_source: config/memcached Copy For more about the general structure of on-host integration configuration, see our integration configuration overview. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: MemcachedSample MemcachedSlabSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Memcached integration collects the following metric data attributes. Memcached sample metrics These attributes are attached to this MemcachedSample event type: Metric Description avgItemSizeInBytes The average size of an item. bytesReadServerPerSecond Rate of bytes read from the network by this server. bytesUsedServerInBytes Current number of bytes used by this server to store items. bytesWrittenServerPerSecond Rate of bytes written to the network by this server. casHitRatePerSecond. Rate at which keys are compared and swapped and found present. casMissRatePerSecond Rate at which keys are compared and swapped and not found present. casWrongRatePerSecond Rate at which keys are compared and swapped where the original value did not match the supplied value. cmdFlushRatePerSecond Rate of flushall commands. cmdGetRatePerSecond Rate of get commands. cmdSetRatePerSecond Rate of set commands. connectionRateServerPerSecond Rate at which connections to this server are opened. connectionStructuresAllocated Number of connection structures allocated by the server. currentItemsStoredServer Current number of items stored by the server. deleteCmdNoneRemovedPerSecond Rate at which delete commands result in no items being removed. deleteCmdRemovedPerSecond Rate at which delete commands result in items being removed. evictionsPerSecond Rate at which valid items are removed from cache to free memory for new items. executionTime Fraction of user time the CPU spent executing this server process. getHitPercent Percentage of requested keys that are found present since the start of the memcache server. getHitPerSecond Rate at which keys are requested and found present. getMissPerSecond Rate at which keys are requested and not found. limitBytesStorage Number of bytes this server is allowed to use for storage. openConnectionsServer Number of open connections to this server. pointerSize Default size of pointers on the host OS (generally 32 or 64). serverMaxConnectionLimitPerSecond Rate at which the server has reached the max connection limit storingItemsPercentMemory Amount of memory being used by the server for storing items as a percentage of the max allowed. threads Number of threads used by the current Memcached server process. totalItemsStored Total number of items stored by this server since it started. uptimeInMilliseocnds Number of seconds this server has been running. usageRate Fraction of time the CPU spent executing kernel code on behalf of this server process. Memcached slab sample metrics These attributes are attached to the MemcachedSlabSample event type: Metric Description activeItemsBumpedPerSecond Rate at which active items were bumped within HOT or WARM. activeSlabs Total number of slab classes allocated. casBadValPerSecond Rate at which Check-And-Set (CAS) commands failed to modify a value due to a bad CAS ID. casModifiedSlabPerSecond Rate at which CAS commands modified this slab class. chunkSizeInBytes The amount of space each chunk uses. chunksPerPage How many chunks exist within one page. cmdSetRateSlabPerSecond Rate at which set requests stored data in this slab class. decrsModifySlabPerSecond Rate at which decrs commands modified this slab class. deleteRateSlabPerSecond Rate at which delete commands succeeded in this slab class. entriesReclaimedPerSecond Rate at which entries were stored using memory from an expired entry. evictionsBeforeExpirationPerSecond Rate at which items had to be evicted from the Least Recently Used (LRU) before expiring. evictionsBeforeExplicitExpirationPerSecond Rate at which nonzero items which had an explicit expire time set had to be evicted from the LRU before expiring. expiredItemsReclaimedPerSecond Rate at which expired items reclaimed from the LRU which were never touched after being set. freeChunksEnd Number of free chunks at the end of the last allocated page. freedChunks Chunks not yet allocated to items or freed via delete. getHitRateSlabPerSecond Rate at which get requests were serviced by this slab class. incrsModifySlabPerSecond Rate at which incrs commands modified this slab class. itemsCold Number of items presently stored in the COLD LRU. itemsColdPerSecond Rate at which items were moved from HOT or WARM into COLD. itemsDirectReclaimPerSecond Rate at which worker threads had to directly pull LRU tails to find memory for a new item. itemsFreedCrawlerPerSecond Rate at which items freed by the LRU crawler. itemsHot Number of items presently stored in the HOT LRU. itemsOldestInMilliseconds Age of the oldest item in the LRU. itemsRefcountLockedPerSecond Rate at which items found to be refcount locked in the LRU tail. itemsSlabClass Number of items presently stored in this slab class. itemsTimeSinceEvictionInMilliseconds Seconds since the last access for the most recent item evicted from this slab class, shown as milliseconds. itemsWarm Number of items presently stored in the WARM LRU. itemsWarmPerSecond Rate at which items were moved from COLD to WARM. memAllocatedSlabsInBytes Total amount of memory allocated to slab pages. memRequestedSlabInBytes Number of bytes requested to be stored in this slab. outOfMemoryPerSecond Rate at which the underlying slab class was unable to store a new item shown as error. selfHealedSlabPerSecond Rate at which memcache self-healed a slab with a refcount leak. totalChunksSlab Total number of chunks allocated to the slab class. totalPagesSlab Total number of pages allocated to the slab class. touchHitSlabPerSecond Rate of touches serviced by this slab class. usedChunksItems Number of chunks allocated to items. usedChunksPerSecond Rate at which chunks have been allocated to items. validItemsEvictedPerSecond Rate at which valid items evicted from the LRU which were never touched after being set. Inventory data The Memcached integration captures the configuration parameters of the memcached instance. The data is available on the Inventory page, under the config/memcached source. For more about inventory data, see Understand integration data. The integration captures data for the following Memcached configuration parameters: Memcached Inventory Metric Description auth_enabled_sasl Indicates whether SASL authentication is enabled. binding_protocol Sets the default protocol support for client connections. Options: ascii, binary, or auto/. Default: Auto cas_enabled Indicates whether Check-And-Set (CAS) is enabled. chunk_size The amount of space each chunk uses. One item will use one chunk of the appropriate size. detail_enabled Indicates whether stats detail is enabled. domain_socket The path to the UNIX socket to listen on. dump_enabled Indicates whether stats cachedump and lru_crawler metadump is enabled. evictions Indicates whether evictions are enabled. If so, returns an error on memory exhausted instead of evicting. flush_enabled Indicates whether flush_all command is enabled. growth_factor The chunk size growth factor. hash_algorithm The hash table algorithm. hashpower_init An integer multiplier for how large the hash table should be. Normally grows at runtime. hot_lru_pct Percent of slab memory to reserve for HOT LRU. hot_max_factor Set idle age of HOT LRU to COLD age multiplied by this value. idle_timeout Timeout for idle connections. inline_ascii_response Save up to 24 bytes per item. inter The interface to listen on. item_size_max The maximum size for an item. lru_crawler Enable the LRU crawler background thread. lru_crawler_sleep Microseconds to sleep between items. lru_crawler_tocrawl Max items to crawl per slab per run. lru_maintainer_thread Split LRU mode and background threads. lru_segmented Enable segmented LRU mode. maxbytes The maximum number of bytes allowed in the cache. maxconns The maximum number of clients allowed. maxconns_fast Immediately close new connections after limit is reached. num_threads Number of threads to use. num_threads_per_udp Number of threads to use per UDP. oldest The age of the oldest honored object. reqs_per_event Maximum number of requests per event. slab_automove Indicates whether slab page automover is enabled. slab_automove_ratio Ratio limit between young/old slab classes. slab_automove_window Internal algorithm tunable for automove. slab_chunk_max Specifies the maximum size of a slab. Items larger than the set max are split over multiple slabs. slab_reassign Enable or disable slab reassignment. stat_key_prefix The prefix used for stats keys. tail_repair_time Time in seconds for how long to wait before forcefully killing LRU tail item. tcp_backlog The backlog queue limit. tcpport The TCP port to listen on. temp_lru Boolean value, indicates if temporary_ttl uses temp_lru. temporary_ttl Items set with a TTL lower than this value will go into TEMP_LRU and be unevictable until they expire or are deleted or replaced. If TTL is set to zero, TEMP_LRU is disabled. track_sizes Enable dynamic reporters for 'stats sizes' command. udpport The UDP port to listen on. umask Access mask for UNIX socket, in octal. verbosity Set the verbosity level of the logging output. 0 = none, 1 = some, 2 = lots. warm_lru_pct Percent of slab memory to reserve for WARM LRU. warm_max_factor Set idle age of WARM LRU to COLD age multiplied by this value. watcher_logbuf_size Size in kilobytes of per-watcher write buffer. worker_logbuf_size Size in kilobytes of per-worker-thread buffer. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.75125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Memcached monitoring <em>integration</em>",
        "sections": "Memcached monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": "-config.yml.sample memcached-config.yml Copy Edit the memcached-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em>"
      },
      "id": "603ea4c628ccbc0217eba76f"
    }
  ],
  "/docs/integrations/host-integrations/installation/container-auto-discovery-host-integrations": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.83588,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the infrastructure agent. Important If the sample files are not present in your <em>installation</em>, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;windows&#x2F;<em>integrations</em>&#x2F;nri-jmx&#x2F;nri-jmx-amd64"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.87491,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". This gives you full control over the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    },
    {
      "sections": [
        "F5 monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Enable your F5 instance",
        "Tip",
        "Configure the integration",
        "F5 instance settings",
        "Labels/custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Environment variables replacement",
        "Metrics-only with partition filtering",
        "Multi-instance monitoring",
        "Find and use data",
        "Metric data",
        "System sample metrics",
        "Virtual server sample metrics",
        "Pool sample metrics",
        "Pool member sample metrics",
        "Node sample metrics",
        "Inventory data",
        "Pool Inventory",
        "Node inventory",
        "Pool Member Inventory",
        "Virtual Server Inventory",
        "System Inventory",
        "Application Inventory",
        "Check the source code"
      ],
      "title": "F5 monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "86250de7e0529371148dab5e96960893b88288b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration/",
      "published_at": "2021-09-14T18:21:49Z",
      "updated_at": "2021-09-14T18:21:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our F5 BIG-IP integration collects and sends inventory and metrics from your F5 BIG-IP instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at the system, application, pool, pool member, virtual server, and node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with F5 BIG-IP 11.6 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows version compatible with the infrastructure agent. F5 BIG-IP user account with Auditor-level access user privileges and iControl REST API access permissions. Install and activate To install the F5 BIG-IP integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-f5. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-f5 MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-f5/nri-f5-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-f5-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: We recommend you install the integration on a separate server and monitor F5 remotely. Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Enable your F5 instance Create a new F5 BIG-IP user and assign user permissions: Create a user account with, at minimum, Auditor-level access permissions. For instructions on how to do this, see the official F5 documentation. Once the user has been created, assign the user iControl REST user permissions. Tip Administrator-level permissions may be required to collect some system sample metrics or system inventory configuration data. For more information on user permission levels, see the official F5 documentation on user role access descriptions. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes, see monitor services running on Kubernetes. If enabled via Amazon ECS, see monitor services running on ECS. If installed via on-host, edit the config in the integration's YAML config file, f5-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. The options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, and inventory_source, among others. For more on these common settings, see our list of configuration properties document. If you're still using our legacy configuration/definition files, see on-host integrations standard configuration format. Specific settings related to F5 are defined using the env section of the configuration file. These settings control the connection to your F5 instance, as well as other security settings and features. F5 instance settings The F5 integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where F5 is running. localhost M/I PORT Port on which F5 API is listening. 443 M/I USERNAME Username for accessing F5 API. N/A M/I PASSWORD Password for the given user. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. Only required if USE_SSL is true. N/A M/I CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M/I TIMEOUT Timeout for requests, in seconds. 30 M/I PARTITION_FILTER A JSON array of BIG-IP partitions to collect from. See this metrics-only with partition filtering example. [\"Common\"] M MAX_CONCURRENT_REQUESTS Maximum number of requests running concurrently. 10 M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false You can define these setting values in different ways, depending on your preference and need: Add the value directly in the config file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more on using passthrough or see the environment variables replacement example. Use secrets management to protect sensitive information, such as passwords, that would be exposed in plain text in the configuration file. For more information, read more about using secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics. Our default sample config file includes examples of labels. You can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-f5 env: HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production inventory_source: config/f5 Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-f5 env: METRICS: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production - name: nri-f5 env: INVENTORY: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: environment: production inventory_source: config/f5 Copy Environment variables replacement In this configuration, the environment variable F5_HOST populates the HOSTNAME setting of the integration: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Metrics-only with partition filtering This configuration only collects metrics and adds \"MyOtherPartition\" to the list of partitions to be sampled. By default, the integration only samples the \"Common\" partition: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password PARTITION_FILTER: '[\"Common\",\"MyOtherPartition\"]' interval: 15s labels: env: production role: load_balancer Copy Multi-instance monitoring This configuration monitors multiple F5 servers from the same integration. The first instance (HOSTNAME: 1st_f5_host) collects metrics and inventory, while the second instance (HOSTNAME: 2nd_f5_host) only collects metrics. integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer - name: nri-f5 env: INVENTORY: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: env: production role: load_balancer inventory_source: config/f5 - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 2nd_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Third-party services and select one of the F5 BIG-IP integration links. In New Relic Insights, F5 BIG-IP data is attached to the following Insights event types: F5BigIpSystemSample F5BigIpVirtualServerSample F5BigIpPoolSample F5BigIpPoolMemberSample F5BigIpNodeSample For more on how to find and use your data, see Understand integration data. Metric data The F5 BIG-IP integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as system., virtualserver., or pool.. System sample metrics These attributes can be found by querying the F5BigIpSystemSample event types. Metric Description system.cpuIdleTicksPerSecond Amount of CPU ticks that the CPU was idle per second. Requires Administrator-level user permissions to collect. system.cpuIdleUtilization Average percentage of time the CPU is idle. system.cpuInterruptRequestUtilization Average percentage of time the CPU is handling interrupt requests. system.cpuIOWaitUtilization Average percentage of time the CPU is waiting on IO. system.cpuNiceLevelUtilization Average percentage of time the CPU is handling nice level processes. system.cpuSoftInterruptRequestUtilization Average percentage of time the CPU is handling soft interrupt requests. system.cpuStolenUtilization Average percentage of time the CPU is handling reclaimed cycles by the hypervisor. system.cpuSystemTicksPerSecond Amount of CPU ticks used by the kernel processes per second. Requires Administrator-level user permissions to collect. system.cpuSystemUtilization Average percentage of time the CPU is used by the kernel. system.cpuUserTicksPerSecond Amount of CPU ticks used by user processes per second. Requires Administrator-level user permissions to collect. system.cpuUserUtilization Average percentage of time the CPU is used by user processes. system.memoryFreeInBytes Total amount of memory free, in bytes. system.memoryTotalInBytes Total amount of memory, in bytes. Requires Administrator-level user permissions to collect. system.memoryUsedInBytes Total amount of memory used, in bytes. Requires Administrator-level user permissions to collect. system.otherMemoryFreeInBytes Free memory reserved for control plane processes, in bytes. system.otherMemoryTotalInBytes Total memory reserved for control plane processes, in bytes. system.otherMemoryUsedInBytes Used memory reserved for control plane processes, in bytes. system.swapFreeInBytes Swap space free, in bytes. system.swapTotalInBytes Swap space total, in bytes. system.swapUsedInBytes Swap space used, in bytes. system.tmmMemoryFreeInBytes Free memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryTotalInBytes Total memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryUsedInBytes Used memory reserved for Traffic Management Microkernel (TMM), in bytes. Virtual server sample metrics These attributes can be found by querying the F5BigIpVirtualServerSample event types in Insights. Metric Description virtualserver.avaibilityState The BIG-IP defined availability. Options: 0 = Offline 1 = Unknown 2 = Online virtualserver.clientsideConnectionsPerSecond The rate of connections created through the client side of the object per second. virtualserver.cmpEnabled Indicates whether or not Cluster Multiprocessing (CMP) is enabled. virtualserver.cmpEnableMode Shows the Cluster Multiprocessing (CMP) mode indicators. Options: CMP disabled = none, disable, or single. CMP enabled = enable or all. virtualserver.connections The current number of connections from BIG-IP. virtualserver.csMaxConnDur Maximum connection duration from the client side of the object. virtualserver.csMinConnDur Minimum connection duration from the client side of the object. virtualserver.enabled The current enabled state. Options: 0 = Disabled 1 = Enabled virtualserver.ephemeralBytesInPerSecond Total number of bytes in through the ephemeral port per second. virtualserver.ephemeralBytesOutPerSecond Total number of bytes out through the ephemeral port per second. virtualserver.ephemeralConnectionsPerSecond The rate of connection creation through the ephemeral port per second. virtualserver.ephemeralCurrentConnections The current number of connections through the ephemeral port. virtualserver.ephemeralEvictedConnectionsPerSecond The number of connections that are evicted through the ephemeral port per second. virtualserver.ephemeralMaxConnections Maximum number of connections through the ephemeral port. virtualserver.ephemeralPacketsReceivedPerSecond The number of packets in through the ephemeral port per second. virtualserver.ephemeralPacketsSentPerSecond The number of packets out through the ephemeral port per second. virtualserver.ephemeralSlowKilledPerSecond The number of slow connections that are killed through the ephemeral port per second. virtualserver.evictedConnsPerSecond The rate of connections evicted per second. virtualserver.inDataInBytes The amount of data received from the BIG-IP virtual server, in bytes. virtualserver.outDataInBytes The amount of data sent to the BIG-IP virtual server, in bytes. virtualserver.packetsReceived The number of packets received from the BIG-IP virtual server. virtualserver.packetsSent The number of packets sent to the BIG-IP virtual server. virtualserver.requests The number of requests in the last collection interval to BIG-IP. virtualserver.slowKilledPerSecond The number of slow connections killed through the client side of the object per second. virtualserver.statusReason An explanation of the current status. virtualserver.usageRatio The usage ratio for the virtual server. Pool sample metrics These attributes can be found by querying the F5BigIpPoolSample event types in Insights. Metric Description pool.activeMembers The number of active pool members. pool.availabilityState The current availability state. Options: 0 = Offline 1 = Unknown 2 = Online pool.connections The current number of connections. pool.connqAgeEdm The queue age exponential-decaying max. pool.connqAgeEma The queue age exponential-moving average. pool.connqAgeHead The current queue age head. pool.connqAgeMax The queue age all-time max. pool.connqAllAgeEdm The sum of pool member queue age exponential-decaying max. pool.connqAllAgeEma The sum of pool member queue age exponential-moving average. pool.connqAllAgeHead The sum of pool member queue age head. pool.connqAllAgeMax The sum of pool member queue age all-time max. pool.connqAllDepth The sum of pool member depth. pool.connqDepth The queue depth. pool.currentConnections The current connections. pool.enabled The current enabled state, can be user defined. Options: 0 = Disabled 1 = Enabled pool.inDataInBytes The amount of data received from the BIG-IP pool, in bytes. pool.minActiveMembers Pool minimum active members. pool.outDataInBytes The amount of data sent to the BIG-IP pool, in bytes. pool.packetsReceived The number of packets received from the BIG-IP pool. pool.packetsSent The number of packets sent to the BIG-IP pool. pool.requests The total number of requests to the pool. pool.statusReason Textual property explaining the overall health reason. Pool member sample metrics These attributes can be found by querying the F5BigIpPoolMemberSample event types in Insights. Metric Description member.availabilityState The current availability from the BIG-IP system. Options: 0 = Offline 1 = Unknown 2 = Online member.connections The current connections. member.enabled Enabled state of the pool member with regards to the parent pool. Options: 0 = Disabled 1 = Enabled member.inDataInBytes The amount of data received from the BIG-IP pool member, in bytes. member.monitorStatus The status of the monitor. Options: 0 = Down 1 = Unchecked 2 = Any other status member.outDataInBytes The amount of data sent to the BIG-IP pool member, in bytes. member.packetsReceived The number of packets received from the BIG-IP pool member. member.packetsSent The number of packets sent to the BIG-IP pool member. member.requests The current number of requests over the last collection interval. member.sessions The current session count. member.sessionStatus The current session health status. Options: 0 = Disabled 1 = Enabled member.state The current state. Options: 0 = Down 1 = Up member.statusReason Explanation of the current status. Node sample metrics These attributes can be found by querying the F5BigIpNodeSample event types in Insights. Metric Description node.availabilityState The current BIG-IP availability state to the node. Options: 0 = Offline 1 = Unknown 2 = Online node.connections The current number of network connections from BIG-IP. node.connectionsPerSecond The number of connections made per second. node.enabled The current BIG-IP enabled state. Options: 0 = Disabled 1 = Enabled , node.inDataInBytes The amount of data received from the BIG-IP node, in bytes. node.monitorStatus The current health monitor rule status. Options: 0 = Down 1 = Unchecked 2 = Any other status node.outDataInBytes The amount of data sent to the BIG-IP node, in bytes. node.packetsReceived The number of packets received from the BIG-IP node. node.packetsSent The number of packets sent to the BIG-IP node. node.requests The current number of requests over the last collection from BIG-IP. node.sessions The current number of sessions. node.sessionStatus The current status of the session. Options: 0 = Disabled 1 = Enabled node.statusReason BIG-IP reason for the current status. Inventory data The F5 BIG-IP integration also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config/f5 source. For more about inventory data, see Understand integration data. The integration captures data for the following F5 BIG-IP configuration parameters: Pool Inventory Metric Description currentLoadMode Current load balancing mode. description User defined description. kind Kind of pool. maxConnections Current max number of connections seen at one point. monitorRule Current health monitoring rule applied. Node inventory Metric Description address BIG-IP network address to send to the node. fqdn FQDN of node. kind Type of Node in BIG-IP. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP Health Monitor rule. Pool Member Inventory Metric Description kind Type of Pool member. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP health monitor rule. nodeName Name of the node the pool member is using. poolName Name of the pool the pool member belongs. port Port the pool member listens on. Virtual Server Inventory Metric Description applicationService Current application service assigned. destination Destination address picked up by BIG-IP. kind Type of virtual server. maxConnections Current highest number of network connections reported from BIG-IP. name User defined name. pool Pool the virtual server uses for load balancing. System Inventory Metric Description chassisSerialNumber Chassis Serial Number for the current device. Requires Access Administrator-level user permissions to collect. platform Platform of the current device. Requires Access Administrator-level user permissions to collect. product Product Name for the current device. Requires Access Administrator-level user permissions to collect. Application Inventory Metric Description deviceGroup Device group running application service. kind BIG-IP Defined type. name User defined name. poolToUse Server side pool load balancing requests. template Template applied to application including security and monitoring rules. templateModified Indicator of modifications made to out of the box template. trafficGroup Current traffic group to which service is applied. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.77406,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "F5 monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " you install the integration on a separate server and monitor F5 remotely. Advanced: It&#x27;s also possible to install the integration from a tarball file. This gives you full control over the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results"
      },
      "id": "6044e41ce7b9d2f0975799b4"
    }
  ],
  "/docs/integrations/host-integrations/installation/install-infrastructure-host-integrations": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.83572,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the infrastructure agent. Important If the sample files are not present in your <em>installation</em>, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;windows&#x2F;<em>integrations</em>&#x2F;nri-jmx&#x2F;nri-jmx-amd64"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.87482,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". This gives you full control over the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    },
    {
      "sections": [
        "F5 monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Enable your F5 instance",
        "Tip",
        "Configure the integration",
        "F5 instance settings",
        "Labels/custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Environment variables replacement",
        "Metrics-only with partition filtering",
        "Multi-instance monitoring",
        "Find and use data",
        "Metric data",
        "System sample metrics",
        "Virtual server sample metrics",
        "Pool sample metrics",
        "Pool member sample metrics",
        "Node sample metrics",
        "Inventory data",
        "Pool Inventory",
        "Node inventory",
        "Pool Member Inventory",
        "Virtual Server Inventory",
        "System Inventory",
        "Application Inventory",
        "Check the source code"
      ],
      "title": "F5 monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "86250de7e0529371148dab5e96960893b88288b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration/",
      "published_at": "2021-09-14T18:21:49Z",
      "updated_at": "2021-09-14T18:21:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our F5 BIG-IP integration collects and sends inventory and metrics from your F5 BIG-IP instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at the system, application, pool, pool member, virtual server, and node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with F5 BIG-IP 11.6 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows version compatible with the infrastructure agent. F5 BIG-IP user account with Auditor-level access user privileges and iControl REST API access permissions. Install and activate To install the F5 BIG-IP integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-f5. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-f5 MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-f5/nri-f5-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-f5-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: We recommend you install the integration on a separate server and monitor F5 remotely. Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Enable your F5 instance Create a new F5 BIG-IP user and assign user permissions: Create a user account with, at minimum, Auditor-level access permissions. For instructions on how to do this, see the official F5 documentation. Once the user has been created, assign the user iControl REST user permissions. Tip Administrator-level permissions may be required to collect some system sample metrics or system inventory configuration data. For more information on user permission levels, see the official F5 documentation on user role access descriptions. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes, see monitor services running on Kubernetes. If enabled via Amazon ECS, see monitor services running on ECS. If installed via on-host, edit the config in the integration's YAML config file, f5-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. The options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, and inventory_source, among others. For more on these common settings, see our list of configuration properties document. If you're still using our legacy configuration/definition files, see on-host integrations standard configuration format. Specific settings related to F5 are defined using the env section of the configuration file. These settings control the connection to your F5 instance, as well as other security settings and features. F5 instance settings The F5 integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where F5 is running. localhost M/I PORT Port on which F5 API is listening. 443 M/I USERNAME Username for accessing F5 API. N/A M/I PASSWORD Password for the given user. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. Only required if USE_SSL is true. N/A M/I CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M/I TIMEOUT Timeout for requests, in seconds. 30 M/I PARTITION_FILTER A JSON array of BIG-IP partitions to collect from. See this metrics-only with partition filtering example. [\"Common\"] M MAX_CONCURRENT_REQUESTS Maximum number of requests running concurrently. 10 M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false You can define these setting values in different ways, depending on your preference and need: Add the value directly in the config file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more on using passthrough or see the environment variables replacement example. Use secrets management to protect sensitive information, such as passwords, that would be exposed in plain text in the configuration file. For more information, read more about using secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics. Our default sample config file includes examples of labels. You can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-f5 env: HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production inventory_source: config/f5 Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-f5 env: METRICS: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production - name: nri-f5 env: INVENTORY: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: environment: production inventory_source: config/f5 Copy Environment variables replacement In this configuration, the environment variable F5_HOST populates the HOSTNAME setting of the integration: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Metrics-only with partition filtering This configuration only collects metrics and adds \"MyOtherPartition\" to the list of partitions to be sampled. By default, the integration only samples the \"Common\" partition: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password PARTITION_FILTER: '[\"Common\",\"MyOtherPartition\"]' interval: 15s labels: env: production role: load_balancer Copy Multi-instance monitoring This configuration monitors multiple F5 servers from the same integration. The first instance (HOSTNAME: 1st_f5_host) collects metrics and inventory, while the second instance (HOSTNAME: 2nd_f5_host) only collects metrics. integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer - name: nri-f5 env: INVENTORY: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: env: production role: load_balancer inventory_source: config/f5 - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 2nd_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Third-party services and select one of the F5 BIG-IP integration links. In New Relic Insights, F5 BIG-IP data is attached to the following Insights event types: F5BigIpSystemSample F5BigIpVirtualServerSample F5BigIpPoolSample F5BigIpPoolMemberSample F5BigIpNodeSample For more on how to find and use your data, see Understand integration data. Metric data The F5 BIG-IP integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as system., virtualserver., or pool.. System sample metrics These attributes can be found by querying the F5BigIpSystemSample event types. Metric Description system.cpuIdleTicksPerSecond Amount of CPU ticks that the CPU was idle per second. Requires Administrator-level user permissions to collect. system.cpuIdleUtilization Average percentage of time the CPU is idle. system.cpuInterruptRequestUtilization Average percentage of time the CPU is handling interrupt requests. system.cpuIOWaitUtilization Average percentage of time the CPU is waiting on IO. system.cpuNiceLevelUtilization Average percentage of time the CPU is handling nice level processes. system.cpuSoftInterruptRequestUtilization Average percentage of time the CPU is handling soft interrupt requests. system.cpuStolenUtilization Average percentage of time the CPU is handling reclaimed cycles by the hypervisor. system.cpuSystemTicksPerSecond Amount of CPU ticks used by the kernel processes per second. Requires Administrator-level user permissions to collect. system.cpuSystemUtilization Average percentage of time the CPU is used by the kernel. system.cpuUserTicksPerSecond Amount of CPU ticks used by user processes per second. Requires Administrator-level user permissions to collect. system.cpuUserUtilization Average percentage of time the CPU is used by user processes. system.memoryFreeInBytes Total amount of memory free, in bytes. system.memoryTotalInBytes Total amount of memory, in bytes. Requires Administrator-level user permissions to collect. system.memoryUsedInBytes Total amount of memory used, in bytes. Requires Administrator-level user permissions to collect. system.otherMemoryFreeInBytes Free memory reserved for control plane processes, in bytes. system.otherMemoryTotalInBytes Total memory reserved for control plane processes, in bytes. system.otherMemoryUsedInBytes Used memory reserved for control plane processes, in bytes. system.swapFreeInBytes Swap space free, in bytes. system.swapTotalInBytes Swap space total, in bytes. system.swapUsedInBytes Swap space used, in bytes. system.tmmMemoryFreeInBytes Free memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryTotalInBytes Total memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryUsedInBytes Used memory reserved for Traffic Management Microkernel (TMM), in bytes. Virtual server sample metrics These attributes can be found by querying the F5BigIpVirtualServerSample event types in Insights. Metric Description virtualserver.avaibilityState The BIG-IP defined availability. Options: 0 = Offline 1 = Unknown 2 = Online virtualserver.clientsideConnectionsPerSecond The rate of connections created through the client side of the object per second. virtualserver.cmpEnabled Indicates whether or not Cluster Multiprocessing (CMP) is enabled. virtualserver.cmpEnableMode Shows the Cluster Multiprocessing (CMP) mode indicators. Options: CMP disabled = none, disable, or single. CMP enabled = enable or all. virtualserver.connections The current number of connections from BIG-IP. virtualserver.csMaxConnDur Maximum connection duration from the client side of the object. virtualserver.csMinConnDur Minimum connection duration from the client side of the object. virtualserver.enabled The current enabled state. Options: 0 = Disabled 1 = Enabled virtualserver.ephemeralBytesInPerSecond Total number of bytes in through the ephemeral port per second. virtualserver.ephemeralBytesOutPerSecond Total number of bytes out through the ephemeral port per second. virtualserver.ephemeralConnectionsPerSecond The rate of connection creation through the ephemeral port per second. virtualserver.ephemeralCurrentConnections The current number of connections through the ephemeral port. virtualserver.ephemeralEvictedConnectionsPerSecond The number of connections that are evicted through the ephemeral port per second. virtualserver.ephemeralMaxConnections Maximum number of connections through the ephemeral port. virtualserver.ephemeralPacketsReceivedPerSecond The number of packets in through the ephemeral port per second. virtualserver.ephemeralPacketsSentPerSecond The number of packets out through the ephemeral port per second. virtualserver.ephemeralSlowKilledPerSecond The number of slow connections that are killed through the ephemeral port per second. virtualserver.evictedConnsPerSecond The rate of connections evicted per second. virtualserver.inDataInBytes The amount of data received from the BIG-IP virtual server, in bytes. virtualserver.outDataInBytes The amount of data sent to the BIG-IP virtual server, in bytes. virtualserver.packetsReceived The number of packets received from the BIG-IP virtual server. virtualserver.packetsSent The number of packets sent to the BIG-IP virtual server. virtualserver.requests The number of requests in the last collection interval to BIG-IP. virtualserver.slowKilledPerSecond The number of slow connections killed through the client side of the object per second. virtualserver.statusReason An explanation of the current status. virtualserver.usageRatio The usage ratio for the virtual server. Pool sample metrics These attributes can be found by querying the F5BigIpPoolSample event types in Insights. Metric Description pool.activeMembers The number of active pool members. pool.availabilityState The current availability state. Options: 0 = Offline 1 = Unknown 2 = Online pool.connections The current number of connections. pool.connqAgeEdm The queue age exponential-decaying max. pool.connqAgeEma The queue age exponential-moving average. pool.connqAgeHead The current queue age head. pool.connqAgeMax The queue age all-time max. pool.connqAllAgeEdm The sum of pool member queue age exponential-decaying max. pool.connqAllAgeEma The sum of pool member queue age exponential-moving average. pool.connqAllAgeHead The sum of pool member queue age head. pool.connqAllAgeMax The sum of pool member queue age all-time max. pool.connqAllDepth The sum of pool member depth. pool.connqDepth The queue depth. pool.currentConnections The current connections. pool.enabled The current enabled state, can be user defined. Options: 0 = Disabled 1 = Enabled pool.inDataInBytes The amount of data received from the BIG-IP pool, in bytes. pool.minActiveMembers Pool minimum active members. pool.outDataInBytes The amount of data sent to the BIG-IP pool, in bytes. pool.packetsReceived The number of packets received from the BIG-IP pool. pool.packetsSent The number of packets sent to the BIG-IP pool. pool.requests The total number of requests to the pool. pool.statusReason Textual property explaining the overall health reason. Pool member sample metrics These attributes can be found by querying the F5BigIpPoolMemberSample event types in Insights. Metric Description member.availabilityState The current availability from the BIG-IP system. Options: 0 = Offline 1 = Unknown 2 = Online member.connections The current connections. member.enabled Enabled state of the pool member with regards to the parent pool. Options: 0 = Disabled 1 = Enabled member.inDataInBytes The amount of data received from the BIG-IP pool member, in bytes. member.monitorStatus The status of the monitor. Options: 0 = Down 1 = Unchecked 2 = Any other status member.outDataInBytes The amount of data sent to the BIG-IP pool member, in bytes. member.packetsReceived The number of packets received from the BIG-IP pool member. member.packetsSent The number of packets sent to the BIG-IP pool member. member.requests The current number of requests over the last collection interval. member.sessions The current session count. member.sessionStatus The current session health status. Options: 0 = Disabled 1 = Enabled member.state The current state. Options: 0 = Down 1 = Up member.statusReason Explanation of the current status. Node sample metrics These attributes can be found by querying the F5BigIpNodeSample event types in Insights. Metric Description node.availabilityState The current BIG-IP availability state to the node. Options: 0 = Offline 1 = Unknown 2 = Online node.connections The current number of network connections from BIG-IP. node.connectionsPerSecond The number of connections made per second. node.enabled The current BIG-IP enabled state. Options: 0 = Disabled 1 = Enabled , node.inDataInBytes The amount of data received from the BIG-IP node, in bytes. node.monitorStatus The current health monitor rule status. Options: 0 = Down 1 = Unchecked 2 = Any other status node.outDataInBytes The amount of data sent to the BIG-IP node, in bytes. node.packetsReceived The number of packets received from the BIG-IP node. node.packetsSent The number of packets sent to the BIG-IP node. node.requests The current number of requests over the last collection from BIG-IP. node.sessions The current number of sessions. node.sessionStatus The current status of the session. Options: 0 = Disabled 1 = Enabled node.statusReason BIG-IP reason for the current status. Inventory data The F5 BIG-IP integration also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config/f5 source. For more about inventory data, see Understand integration data. The integration captures data for the following F5 BIG-IP configuration parameters: Pool Inventory Metric Description currentLoadMode Current load balancing mode. description User defined description. kind Kind of pool. maxConnections Current max number of connections seen at one point. monitorRule Current health monitoring rule applied. Node inventory Metric Description address BIG-IP network address to send to the node. fqdn FQDN of node. kind Type of Node in BIG-IP. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP Health Monitor rule. Pool Member Inventory Metric Description kind Type of Pool member. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP health monitor rule. nodeName Name of the node the pool member is using. poolName Name of the pool the pool member belongs. port Port the pool member listens on. Virtual Server Inventory Metric Description applicationService Current application service assigned. destination Destination address picked up by BIG-IP. kind Type of virtual server. maxConnections Current highest number of network connections reported from BIG-IP. name User defined name. pool Pool the virtual server uses for load balancing. System Inventory Metric Description chassisSerialNumber Chassis Serial Number for the current device. Requires Access Administrator-level user permissions to collect. platform Platform of the current device. Requires Access Administrator-level user permissions to collect. product Product Name for the current device. Requires Access Administrator-level user permissions to collect. Application Inventory Metric Description deviceGroup Device group running application service. kind BIG-IP Defined type. name User defined name. poolToUse Server side pool load balancing requests. template Template applied to application including security and monitoring rules. templateModified Indicator of modifications made to out of the box template. trafficGroup Current traffic group to which service is applied. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.77399,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "F5 monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " you install the integration on a separate server and monitor F5 remotely. Advanced: It&#x27;s also possible to install the integration from a tarball file. This gives you full control over the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results"
      },
      "id": "6044e41ce7b9d2f0975799b4"
    }
  ],
  "/docs/integrations/host-integrations/installation/secrets-management": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.83572,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the infrastructure agent. Important If the sample files are not present in your <em>installation</em>, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;windows&#x2F;<em>integrations</em>&#x2F;nri-jmx&#x2F;nri-jmx-amd64"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.87482,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". This gives you full control over the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    },
    {
      "sections": [
        "F5 monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Enable your F5 instance",
        "Tip",
        "Configure the integration",
        "F5 instance settings",
        "Labels/custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Environment variables replacement",
        "Metrics-only with partition filtering",
        "Multi-instance monitoring",
        "Find and use data",
        "Metric data",
        "System sample metrics",
        "Virtual server sample metrics",
        "Pool sample metrics",
        "Pool member sample metrics",
        "Node sample metrics",
        "Inventory data",
        "Pool Inventory",
        "Node inventory",
        "Pool Member Inventory",
        "Virtual Server Inventory",
        "System Inventory",
        "Application Inventory",
        "Check the source code"
      ],
      "title": "F5 monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "86250de7e0529371148dab5e96960893b88288b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration/",
      "published_at": "2021-09-14T18:21:49Z",
      "updated_at": "2021-09-14T18:21:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our F5 BIG-IP integration collects and sends inventory and metrics from your F5 BIG-IP instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at the system, application, pool, pool member, virtual server, and node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with F5 BIG-IP 11.6 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows version compatible with the infrastructure agent. F5 BIG-IP user account with Auditor-level access user privileges and iControl REST API access permissions. Install and activate To install the F5 BIG-IP integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-f5. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-f5 MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-f5/nri-f5-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-f5-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: We recommend you install the integration on a separate server and monitor F5 remotely. Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Enable your F5 instance Create a new F5 BIG-IP user and assign user permissions: Create a user account with, at minimum, Auditor-level access permissions. For instructions on how to do this, see the official F5 documentation. Once the user has been created, assign the user iControl REST user permissions. Tip Administrator-level permissions may be required to collect some system sample metrics or system inventory configuration data. For more information on user permission levels, see the official F5 documentation on user role access descriptions. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes, see monitor services running on Kubernetes. If enabled via Amazon ECS, see monitor services running on ECS. If installed via on-host, edit the config in the integration's YAML config file, f5-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. The options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, and inventory_source, among others. For more on these common settings, see our list of configuration properties document. If you're still using our legacy configuration/definition files, see on-host integrations standard configuration format. Specific settings related to F5 are defined using the env section of the configuration file. These settings control the connection to your F5 instance, as well as other security settings and features. F5 instance settings The F5 integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where F5 is running. localhost M/I PORT Port on which F5 API is listening. 443 M/I USERNAME Username for accessing F5 API. N/A M/I PASSWORD Password for the given user. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. Only required if USE_SSL is true. N/A M/I CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M/I TIMEOUT Timeout for requests, in seconds. 30 M/I PARTITION_FILTER A JSON array of BIG-IP partitions to collect from. See this metrics-only with partition filtering example. [\"Common\"] M MAX_CONCURRENT_REQUESTS Maximum number of requests running concurrently. 10 M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false You can define these setting values in different ways, depending on your preference and need: Add the value directly in the config file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more on using passthrough or see the environment variables replacement example. Use secrets management to protect sensitive information, such as passwords, that would be exposed in plain text in the configuration file. For more information, read more about using secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics. Our default sample config file includes examples of labels. You can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-f5 env: HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production inventory_source: config/f5 Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-f5 env: METRICS: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production - name: nri-f5 env: INVENTORY: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: environment: production inventory_source: config/f5 Copy Environment variables replacement In this configuration, the environment variable F5_HOST populates the HOSTNAME setting of the integration: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Metrics-only with partition filtering This configuration only collects metrics and adds \"MyOtherPartition\" to the list of partitions to be sampled. By default, the integration only samples the \"Common\" partition: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password PARTITION_FILTER: '[\"Common\",\"MyOtherPartition\"]' interval: 15s labels: env: production role: load_balancer Copy Multi-instance monitoring This configuration monitors multiple F5 servers from the same integration. The first instance (HOSTNAME: 1st_f5_host) collects metrics and inventory, while the second instance (HOSTNAME: 2nd_f5_host) only collects metrics. integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer - name: nri-f5 env: INVENTORY: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: env: production role: load_balancer inventory_source: config/f5 - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 2nd_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Third-party services and select one of the F5 BIG-IP integration links. In New Relic Insights, F5 BIG-IP data is attached to the following Insights event types: F5BigIpSystemSample F5BigIpVirtualServerSample F5BigIpPoolSample F5BigIpPoolMemberSample F5BigIpNodeSample For more on how to find and use your data, see Understand integration data. Metric data The F5 BIG-IP integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as system., virtualserver., or pool.. System sample metrics These attributes can be found by querying the F5BigIpSystemSample event types. Metric Description system.cpuIdleTicksPerSecond Amount of CPU ticks that the CPU was idle per second. Requires Administrator-level user permissions to collect. system.cpuIdleUtilization Average percentage of time the CPU is idle. system.cpuInterruptRequestUtilization Average percentage of time the CPU is handling interrupt requests. system.cpuIOWaitUtilization Average percentage of time the CPU is waiting on IO. system.cpuNiceLevelUtilization Average percentage of time the CPU is handling nice level processes. system.cpuSoftInterruptRequestUtilization Average percentage of time the CPU is handling soft interrupt requests. system.cpuStolenUtilization Average percentage of time the CPU is handling reclaimed cycles by the hypervisor. system.cpuSystemTicksPerSecond Amount of CPU ticks used by the kernel processes per second. Requires Administrator-level user permissions to collect. system.cpuSystemUtilization Average percentage of time the CPU is used by the kernel. system.cpuUserTicksPerSecond Amount of CPU ticks used by user processes per second. Requires Administrator-level user permissions to collect. system.cpuUserUtilization Average percentage of time the CPU is used by user processes. system.memoryFreeInBytes Total amount of memory free, in bytes. system.memoryTotalInBytes Total amount of memory, in bytes. Requires Administrator-level user permissions to collect. system.memoryUsedInBytes Total amount of memory used, in bytes. Requires Administrator-level user permissions to collect. system.otherMemoryFreeInBytes Free memory reserved for control plane processes, in bytes. system.otherMemoryTotalInBytes Total memory reserved for control plane processes, in bytes. system.otherMemoryUsedInBytes Used memory reserved for control plane processes, in bytes. system.swapFreeInBytes Swap space free, in bytes. system.swapTotalInBytes Swap space total, in bytes. system.swapUsedInBytes Swap space used, in bytes. system.tmmMemoryFreeInBytes Free memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryTotalInBytes Total memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryUsedInBytes Used memory reserved for Traffic Management Microkernel (TMM), in bytes. Virtual server sample metrics These attributes can be found by querying the F5BigIpVirtualServerSample event types in Insights. Metric Description virtualserver.avaibilityState The BIG-IP defined availability. Options: 0 = Offline 1 = Unknown 2 = Online virtualserver.clientsideConnectionsPerSecond The rate of connections created through the client side of the object per second. virtualserver.cmpEnabled Indicates whether or not Cluster Multiprocessing (CMP) is enabled. virtualserver.cmpEnableMode Shows the Cluster Multiprocessing (CMP) mode indicators. Options: CMP disabled = none, disable, or single. CMP enabled = enable or all. virtualserver.connections The current number of connections from BIG-IP. virtualserver.csMaxConnDur Maximum connection duration from the client side of the object. virtualserver.csMinConnDur Minimum connection duration from the client side of the object. virtualserver.enabled The current enabled state. Options: 0 = Disabled 1 = Enabled virtualserver.ephemeralBytesInPerSecond Total number of bytes in through the ephemeral port per second. virtualserver.ephemeralBytesOutPerSecond Total number of bytes out through the ephemeral port per second. virtualserver.ephemeralConnectionsPerSecond The rate of connection creation through the ephemeral port per second. virtualserver.ephemeralCurrentConnections The current number of connections through the ephemeral port. virtualserver.ephemeralEvictedConnectionsPerSecond The number of connections that are evicted through the ephemeral port per second. virtualserver.ephemeralMaxConnections Maximum number of connections through the ephemeral port. virtualserver.ephemeralPacketsReceivedPerSecond The number of packets in through the ephemeral port per second. virtualserver.ephemeralPacketsSentPerSecond The number of packets out through the ephemeral port per second. virtualserver.ephemeralSlowKilledPerSecond The number of slow connections that are killed through the ephemeral port per second. virtualserver.evictedConnsPerSecond The rate of connections evicted per second. virtualserver.inDataInBytes The amount of data received from the BIG-IP virtual server, in bytes. virtualserver.outDataInBytes The amount of data sent to the BIG-IP virtual server, in bytes. virtualserver.packetsReceived The number of packets received from the BIG-IP virtual server. virtualserver.packetsSent The number of packets sent to the BIG-IP virtual server. virtualserver.requests The number of requests in the last collection interval to BIG-IP. virtualserver.slowKilledPerSecond The number of slow connections killed through the client side of the object per second. virtualserver.statusReason An explanation of the current status. virtualserver.usageRatio The usage ratio for the virtual server. Pool sample metrics These attributes can be found by querying the F5BigIpPoolSample event types in Insights. Metric Description pool.activeMembers The number of active pool members. pool.availabilityState The current availability state. Options: 0 = Offline 1 = Unknown 2 = Online pool.connections The current number of connections. pool.connqAgeEdm The queue age exponential-decaying max. pool.connqAgeEma The queue age exponential-moving average. pool.connqAgeHead The current queue age head. pool.connqAgeMax The queue age all-time max. pool.connqAllAgeEdm The sum of pool member queue age exponential-decaying max. pool.connqAllAgeEma The sum of pool member queue age exponential-moving average. pool.connqAllAgeHead The sum of pool member queue age head. pool.connqAllAgeMax The sum of pool member queue age all-time max. pool.connqAllDepth The sum of pool member depth. pool.connqDepth The queue depth. pool.currentConnections The current connections. pool.enabled The current enabled state, can be user defined. Options: 0 = Disabled 1 = Enabled pool.inDataInBytes The amount of data received from the BIG-IP pool, in bytes. pool.minActiveMembers Pool minimum active members. pool.outDataInBytes The amount of data sent to the BIG-IP pool, in bytes. pool.packetsReceived The number of packets received from the BIG-IP pool. pool.packetsSent The number of packets sent to the BIG-IP pool. pool.requests The total number of requests to the pool. pool.statusReason Textual property explaining the overall health reason. Pool member sample metrics These attributes can be found by querying the F5BigIpPoolMemberSample event types in Insights. Metric Description member.availabilityState The current availability from the BIG-IP system. Options: 0 = Offline 1 = Unknown 2 = Online member.connections The current connections. member.enabled Enabled state of the pool member with regards to the parent pool. Options: 0 = Disabled 1 = Enabled member.inDataInBytes The amount of data received from the BIG-IP pool member, in bytes. member.monitorStatus The status of the monitor. Options: 0 = Down 1 = Unchecked 2 = Any other status member.outDataInBytes The amount of data sent to the BIG-IP pool member, in bytes. member.packetsReceived The number of packets received from the BIG-IP pool member. member.packetsSent The number of packets sent to the BIG-IP pool member. member.requests The current number of requests over the last collection interval. member.sessions The current session count. member.sessionStatus The current session health status. Options: 0 = Disabled 1 = Enabled member.state The current state. Options: 0 = Down 1 = Up member.statusReason Explanation of the current status. Node sample metrics These attributes can be found by querying the F5BigIpNodeSample event types in Insights. Metric Description node.availabilityState The current BIG-IP availability state to the node. Options: 0 = Offline 1 = Unknown 2 = Online node.connections The current number of network connections from BIG-IP. node.connectionsPerSecond The number of connections made per second. node.enabled The current BIG-IP enabled state. Options: 0 = Disabled 1 = Enabled , node.inDataInBytes The amount of data received from the BIG-IP node, in bytes. node.monitorStatus The current health monitor rule status. Options: 0 = Down 1 = Unchecked 2 = Any other status node.outDataInBytes The amount of data sent to the BIG-IP node, in bytes. node.packetsReceived The number of packets received from the BIG-IP node. node.packetsSent The number of packets sent to the BIG-IP node. node.requests The current number of requests over the last collection from BIG-IP. node.sessions The current number of sessions. node.sessionStatus The current status of the session. Options: 0 = Disabled 1 = Enabled node.statusReason BIG-IP reason for the current status. Inventory data The F5 BIG-IP integration also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config/f5 source. For more about inventory data, see Understand integration data. The integration captures data for the following F5 BIG-IP configuration parameters: Pool Inventory Metric Description currentLoadMode Current load balancing mode. description User defined description. kind Kind of pool. maxConnections Current max number of connections seen at one point. monitorRule Current health monitoring rule applied. Node inventory Metric Description address BIG-IP network address to send to the node. fqdn FQDN of node. kind Type of Node in BIG-IP. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP Health Monitor rule. Pool Member Inventory Metric Description kind Type of Pool member. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP health monitor rule. nodeName Name of the node the pool member is using. poolName Name of the pool the pool member belongs. port Port the pool member listens on. Virtual Server Inventory Metric Description applicationService Current application service assigned. destination Destination address picked up by BIG-IP. kind Type of virtual server. maxConnections Current highest number of network connections reported from BIG-IP. name User defined name. pool Pool the virtual server uses for load balancing. System Inventory Metric Description chassisSerialNumber Chassis Serial Number for the current device. Requires Access Administrator-level user permissions to collect. platform Platform of the current device. Requires Access Administrator-level user permissions to collect. product Product Name for the current device. Requires Access Administrator-level user permissions to collect. Application Inventory Metric Description deviceGroup Device group running application service. kind BIG-IP Defined type. name User defined name. poolToUse Server side pool load balancing requests. template Template applied to application including security and monitoring rules. templateModified Indicator of modifications made to out of the box template. trafficGroup Current traffic group to which service is applied. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.77399,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "F5 monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " you install the integration on a separate server and monitor F5 remotely. Advanced: It&#x27;s also possible to install the integration from a tarball file. This gives you full control over the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results"
      },
      "id": "6044e41ce7b9d2f0975799b4"
    }
  ],
  "/docs/integrations/host-integrations/installation/update-infrastructure-host-integration-package": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.83557,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the infrastructure agent. Important If the sample files are not present in your <em>installation</em>, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;windows&#x2F;<em>integrations</em>&#x2F;nri-jmx&#x2F;nri-jmx-amd64"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.87474,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". This gives you full control over the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    },
    {
      "sections": [
        "F5 monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Enable your F5 instance",
        "Tip",
        "Configure the integration",
        "F5 instance settings",
        "Labels/custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Environment variables replacement",
        "Metrics-only with partition filtering",
        "Multi-instance monitoring",
        "Find and use data",
        "Metric data",
        "System sample metrics",
        "Virtual server sample metrics",
        "Pool sample metrics",
        "Pool member sample metrics",
        "Node sample metrics",
        "Inventory data",
        "Pool Inventory",
        "Node inventory",
        "Pool Member Inventory",
        "Virtual Server Inventory",
        "System Inventory",
        "Application Inventory",
        "Check the source code"
      ],
      "title": "F5 monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "86250de7e0529371148dab5e96960893b88288b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration/",
      "published_at": "2021-09-14T18:21:49Z",
      "updated_at": "2021-09-14T18:21:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our F5 BIG-IP integration collects and sends inventory and metrics from your F5 BIG-IP instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at the system, application, pool, pool member, virtual server, and node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with F5 BIG-IP 11.6 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows version compatible with the infrastructure agent. F5 BIG-IP user account with Auditor-level access user privileges and iControl REST API access permissions. Install and activate To install the F5 BIG-IP integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-f5. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-f5 MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-f5/nri-f5-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-f5-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: We recommend you install the integration on a separate server and monitor F5 remotely. Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Enable your F5 instance Create a new F5 BIG-IP user and assign user permissions: Create a user account with, at minimum, Auditor-level access permissions. For instructions on how to do this, see the official F5 documentation. Once the user has been created, assign the user iControl REST user permissions. Tip Administrator-level permissions may be required to collect some system sample metrics or system inventory configuration data. For more information on user permission levels, see the official F5 documentation on user role access descriptions. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes, see monitor services running on Kubernetes. If enabled via Amazon ECS, see monitor services running on ECS. If installed via on-host, edit the config in the integration's YAML config file, f5-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. The options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, and inventory_source, among others. For more on these common settings, see our list of configuration properties document. If you're still using our legacy configuration/definition files, see on-host integrations standard configuration format. Specific settings related to F5 are defined using the env section of the configuration file. These settings control the connection to your F5 instance, as well as other security settings and features. F5 instance settings The F5 integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where F5 is running. localhost M/I PORT Port on which F5 API is listening. 443 M/I USERNAME Username for accessing F5 API. N/A M/I PASSWORD Password for the given user. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. Only required if USE_SSL is true. N/A M/I CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M/I TIMEOUT Timeout for requests, in seconds. 30 M/I PARTITION_FILTER A JSON array of BIG-IP partitions to collect from. See this metrics-only with partition filtering example. [\"Common\"] M MAX_CONCURRENT_REQUESTS Maximum number of requests running concurrently. 10 M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false You can define these setting values in different ways, depending on your preference and need: Add the value directly in the config file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more on using passthrough or see the environment variables replacement example. Use secrets management to protect sensitive information, such as passwords, that would be exposed in plain text in the configuration file. For more information, read more about using secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics. Our default sample config file includes examples of labels. You can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-f5 env: HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production inventory_source: config/f5 Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-f5 env: METRICS: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production - name: nri-f5 env: INVENTORY: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: environment: production inventory_source: config/f5 Copy Environment variables replacement In this configuration, the environment variable F5_HOST populates the HOSTNAME setting of the integration: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Metrics-only with partition filtering This configuration only collects metrics and adds \"MyOtherPartition\" to the list of partitions to be sampled. By default, the integration only samples the \"Common\" partition: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password PARTITION_FILTER: '[\"Common\",\"MyOtherPartition\"]' interval: 15s labels: env: production role: load_balancer Copy Multi-instance monitoring This configuration monitors multiple F5 servers from the same integration. The first instance (HOSTNAME: 1st_f5_host) collects metrics and inventory, while the second instance (HOSTNAME: 2nd_f5_host) only collects metrics. integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer - name: nri-f5 env: INVENTORY: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: env: production role: load_balancer inventory_source: config/f5 - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 2nd_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Third-party services and select one of the F5 BIG-IP integration links. In New Relic Insights, F5 BIG-IP data is attached to the following Insights event types: F5BigIpSystemSample F5BigIpVirtualServerSample F5BigIpPoolSample F5BigIpPoolMemberSample F5BigIpNodeSample For more on how to find and use your data, see Understand integration data. Metric data The F5 BIG-IP integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as system., virtualserver., or pool.. System sample metrics These attributes can be found by querying the F5BigIpSystemSample event types. Metric Description system.cpuIdleTicksPerSecond Amount of CPU ticks that the CPU was idle per second. Requires Administrator-level user permissions to collect. system.cpuIdleUtilization Average percentage of time the CPU is idle. system.cpuInterruptRequestUtilization Average percentage of time the CPU is handling interrupt requests. system.cpuIOWaitUtilization Average percentage of time the CPU is waiting on IO. system.cpuNiceLevelUtilization Average percentage of time the CPU is handling nice level processes. system.cpuSoftInterruptRequestUtilization Average percentage of time the CPU is handling soft interrupt requests. system.cpuStolenUtilization Average percentage of time the CPU is handling reclaimed cycles by the hypervisor. system.cpuSystemTicksPerSecond Amount of CPU ticks used by the kernel processes per second. Requires Administrator-level user permissions to collect. system.cpuSystemUtilization Average percentage of time the CPU is used by the kernel. system.cpuUserTicksPerSecond Amount of CPU ticks used by user processes per second. Requires Administrator-level user permissions to collect. system.cpuUserUtilization Average percentage of time the CPU is used by user processes. system.memoryFreeInBytes Total amount of memory free, in bytes. system.memoryTotalInBytes Total amount of memory, in bytes. Requires Administrator-level user permissions to collect. system.memoryUsedInBytes Total amount of memory used, in bytes. Requires Administrator-level user permissions to collect. system.otherMemoryFreeInBytes Free memory reserved for control plane processes, in bytes. system.otherMemoryTotalInBytes Total memory reserved for control plane processes, in bytes. system.otherMemoryUsedInBytes Used memory reserved for control plane processes, in bytes. system.swapFreeInBytes Swap space free, in bytes. system.swapTotalInBytes Swap space total, in bytes. system.swapUsedInBytes Swap space used, in bytes. system.tmmMemoryFreeInBytes Free memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryTotalInBytes Total memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryUsedInBytes Used memory reserved for Traffic Management Microkernel (TMM), in bytes. Virtual server sample metrics These attributes can be found by querying the F5BigIpVirtualServerSample event types in Insights. Metric Description virtualserver.avaibilityState The BIG-IP defined availability. Options: 0 = Offline 1 = Unknown 2 = Online virtualserver.clientsideConnectionsPerSecond The rate of connections created through the client side of the object per second. virtualserver.cmpEnabled Indicates whether or not Cluster Multiprocessing (CMP) is enabled. virtualserver.cmpEnableMode Shows the Cluster Multiprocessing (CMP) mode indicators. Options: CMP disabled = none, disable, or single. CMP enabled = enable or all. virtualserver.connections The current number of connections from BIG-IP. virtualserver.csMaxConnDur Maximum connection duration from the client side of the object. virtualserver.csMinConnDur Minimum connection duration from the client side of the object. virtualserver.enabled The current enabled state. Options: 0 = Disabled 1 = Enabled virtualserver.ephemeralBytesInPerSecond Total number of bytes in through the ephemeral port per second. virtualserver.ephemeralBytesOutPerSecond Total number of bytes out through the ephemeral port per second. virtualserver.ephemeralConnectionsPerSecond The rate of connection creation through the ephemeral port per second. virtualserver.ephemeralCurrentConnections The current number of connections through the ephemeral port. virtualserver.ephemeralEvictedConnectionsPerSecond The number of connections that are evicted through the ephemeral port per second. virtualserver.ephemeralMaxConnections Maximum number of connections through the ephemeral port. virtualserver.ephemeralPacketsReceivedPerSecond The number of packets in through the ephemeral port per second. virtualserver.ephemeralPacketsSentPerSecond The number of packets out through the ephemeral port per second. virtualserver.ephemeralSlowKilledPerSecond The number of slow connections that are killed through the ephemeral port per second. virtualserver.evictedConnsPerSecond The rate of connections evicted per second. virtualserver.inDataInBytes The amount of data received from the BIG-IP virtual server, in bytes. virtualserver.outDataInBytes The amount of data sent to the BIG-IP virtual server, in bytes. virtualserver.packetsReceived The number of packets received from the BIG-IP virtual server. virtualserver.packetsSent The number of packets sent to the BIG-IP virtual server. virtualserver.requests The number of requests in the last collection interval to BIG-IP. virtualserver.slowKilledPerSecond The number of slow connections killed through the client side of the object per second. virtualserver.statusReason An explanation of the current status. virtualserver.usageRatio The usage ratio for the virtual server. Pool sample metrics These attributes can be found by querying the F5BigIpPoolSample event types in Insights. Metric Description pool.activeMembers The number of active pool members. pool.availabilityState The current availability state. Options: 0 = Offline 1 = Unknown 2 = Online pool.connections The current number of connections. pool.connqAgeEdm The queue age exponential-decaying max. pool.connqAgeEma The queue age exponential-moving average. pool.connqAgeHead The current queue age head. pool.connqAgeMax The queue age all-time max. pool.connqAllAgeEdm The sum of pool member queue age exponential-decaying max. pool.connqAllAgeEma The sum of pool member queue age exponential-moving average. pool.connqAllAgeHead The sum of pool member queue age head. pool.connqAllAgeMax The sum of pool member queue age all-time max. pool.connqAllDepth The sum of pool member depth. pool.connqDepth The queue depth. pool.currentConnections The current connections. pool.enabled The current enabled state, can be user defined. Options: 0 = Disabled 1 = Enabled pool.inDataInBytes The amount of data received from the BIG-IP pool, in bytes. pool.minActiveMembers Pool minimum active members. pool.outDataInBytes The amount of data sent to the BIG-IP pool, in bytes. pool.packetsReceived The number of packets received from the BIG-IP pool. pool.packetsSent The number of packets sent to the BIG-IP pool. pool.requests The total number of requests to the pool. pool.statusReason Textual property explaining the overall health reason. Pool member sample metrics These attributes can be found by querying the F5BigIpPoolMemberSample event types in Insights. Metric Description member.availabilityState The current availability from the BIG-IP system. Options: 0 = Offline 1 = Unknown 2 = Online member.connections The current connections. member.enabled Enabled state of the pool member with regards to the parent pool. Options: 0 = Disabled 1 = Enabled member.inDataInBytes The amount of data received from the BIG-IP pool member, in bytes. member.monitorStatus The status of the monitor. Options: 0 = Down 1 = Unchecked 2 = Any other status member.outDataInBytes The amount of data sent to the BIG-IP pool member, in bytes. member.packetsReceived The number of packets received from the BIG-IP pool member. member.packetsSent The number of packets sent to the BIG-IP pool member. member.requests The current number of requests over the last collection interval. member.sessions The current session count. member.sessionStatus The current session health status. Options: 0 = Disabled 1 = Enabled member.state The current state. Options: 0 = Down 1 = Up member.statusReason Explanation of the current status. Node sample metrics These attributes can be found by querying the F5BigIpNodeSample event types in Insights. Metric Description node.availabilityState The current BIG-IP availability state to the node. Options: 0 = Offline 1 = Unknown 2 = Online node.connections The current number of network connections from BIG-IP. node.connectionsPerSecond The number of connections made per second. node.enabled The current BIG-IP enabled state. Options: 0 = Disabled 1 = Enabled , node.inDataInBytes The amount of data received from the BIG-IP node, in bytes. node.monitorStatus The current health monitor rule status. Options: 0 = Down 1 = Unchecked 2 = Any other status node.outDataInBytes The amount of data sent to the BIG-IP node, in bytes. node.packetsReceived The number of packets received from the BIG-IP node. node.packetsSent The number of packets sent to the BIG-IP node. node.requests The current number of requests over the last collection from BIG-IP. node.sessions The current number of sessions. node.sessionStatus The current status of the session. Options: 0 = Disabled 1 = Enabled node.statusReason BIG-IP reason for the current status. Inventory data The F5 BIG-IP integration also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config/f5 source. For more about inventory data, see Understand integration data. The integration captures data for the following F5 BIG-IP configuration parameters: Pool Inventory Metric Description currentLoadMode Current load balancing mode. description User defined description. kind Kind of pool. maxConnections Current max number of connections seen at one point. monitorRule Current health monitoring rule applied. Node inventory Metric Description address BIG-IP network address to send to the node. fqdn FQDN of node. kind Type of Node in BIG-IP. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP Health Monitor rule. Pool Member Inventory Metric Description kind Type of Pool member. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP health monitor rule. nodeName Name of the node the pool member is using. poolName Name of the pool the pool member belongs. port Port the pool member listens on. Virtual Server Inventory Metric Description applicationService Current application service assigned. destination Destination address picked up by BIG-IP. kind Type of virtual server. maxConnections Current highest number of network connections reported from BIG-IP. name User defined name. pool Pool the virtual server uses for load balancing. System Inventory Metric Description chassisSerialNumber Chassis Serial Number for the current device. Requires Access Administrator-level user permissions to collect. platform Platform of the current device. Requires Access Administrator-level user permissions to collect. product Product Name for the current device. Requires Access Administrator-level user permissions to collect. Application Inventory Metric Description deviceGroup Device group running application service. kind BIG-IP Defined type. name User defined name. poolToUse Server side pool load balancing requests. template Template applied to application including security and monitoring rules. templateModified Indicator of modifications made to out of the box template. trafficGroup Current traffic group to which service is applied. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.77393,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "F5 monitoring <em>integration</em>",
        "sections": "Linux <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " you install the integration on a separate server and monitor F5 remotely. Advanced: It&#x27;s also possible to install the integration from a tarball file. This gives you full control over the <em>installation</em> and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results"
      },
      "id": "6044e41ce7b9d2f0975799b4"
    }
  ],
  "/docs/integrations/host-integrations/open-source-host-integrations-list/f5-open-source-integration": [
    {
      "sections": [
        "F5 monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Enable your F5 instance",
        "Tip",
        "Configure the integration",
        "F5 instance settings",
        "Labels/custom attributes",
        "Example configurations",
        "Basic configuration",
        "Basic configuration with different metric/inventory intervals",
        "Environment variables replacement",
        "Metrics-only with partition filtering",
        "Multi-instance monitoring",
        "Find and use data",
        "Metric data",
        "System sample metrics",
        "Virtual server sample metrics",
        "Pool sample metrics",
        "Pool member sample metrics",
        "Node sample metrics",
        "Inventory data",
        "Pool Inventory",
        "Node inventory",
        "Pool Member Inventory",
        "Virtual Server Inventory",
        "System Inventory",
        "Application Inventory",
        "Check the source code"
      ],
      "title": "F5 monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "86250de7e0529371148dab5e96960893b88288b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration/",
      "published_at": "2021-09-14T18:21:49Z",
      "updated_at": "2021-09-14T18:21:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our F5 BIG-IP integration collects and sends inventory and metrics from your F5 BIG-IP instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at the system, application, pool, pool member, virtual server, and node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with F5 BIG-IP 11.6 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows version compatible with the infrastructure agent. F5 BIG-IP user account with Auditor-level access user privileges and iControl REST API access permissions. Install and activate To install the F5 BIG-IP integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-f5. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-f5 MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-f5/nri-f5-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-f5-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: We recommend you install the integration on a separate server and monitor F5 remotely. Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Enable your F5 instance Create a new F5 BIG-IP user and assign user permissions: Create a user account with, at minimum, Auditor-level access permissions. For instructions on how to do this, see the official F5 documentation. Once the user has been created, assign the user iControl REST user permissions. Tip Administrator-level permissions may be required to collect some system sample metrics or system inventory configuration data. For more information on user permission levels, see the official F5 documentation on user role access descriptions. Configure the integration There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes, see monitor services running on Kubernetes. If enabled via Amazon ECS, see monitor services running on ECS. If installed via on-host, edit the config in the integration's YAML config file, f5-config.yml. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. The options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations, such as interval, timeout, and inventory_source, among others. For more on these common settings, see our list of configuration properties document. If you're still using our legacy configuration/definition files, see on-host integrations standard configuration format. Specific settings related to F5 are defined using the env section of the configuration file. These settings control the connection to your F5 instance, as well as other security settings and features. F5 instance settings The F5 integration collects both metrics(M) and inventory(I) information. In the table, use the Applies To column for the settings available to each collection: Setting Description Default Applies to HOSTNAME Hostname or IP where F5 is running. localhost M/I PORT Port on which F5 API is listening. 443 M/I USERNAME Username for accessing F5 API. N/A M/I PASSWORD Password for the given user. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. Only required if USE_SSL is true. N/A M/I CA_BUNDLE_DIR Alternative Certificate Authority bundle directory. N/A M/I TIMEOUT Timeout for requests, in seconds. 30 M/I PARTITION_FILTER A JSON array of BIG-IP partitions to collect from. See this metrics-only with partition filtering example. [\"Common\"] M MAX_CONCURRENT_REQUESTS Maximum number of requests running concurrently. 10 M METRICS Set to true to enable metrics-only collection. false INVENTORY Set to true to enable inventory-only collection. false You can define these setting values in different ways, depending on your preference and need: Add the value directly in the config file. This is the most common way. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more on using passthrough or see the environment variables replacement example. Use secrets management to protect sensitive information, such as passwords, that would be exposed in plain text in the configuration file. For more information, read more about using secrets management. Labels/custom attributes You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter, or group your metrics. Our default sample config file includes examples of labels. You can remove, modify, or add new ones of your choice. labels: env: production role: load_balancer Copy Example configurations Basic configuration This is the basic configuration used to collect metrics and inventory from your localhost: integrations: - name: nri-f5 env: HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production inventory_source: config/f5 Copy Basic configuration with different metric/inventory intervals This configuration collects metrics every 15 seconds and inventory every 60 seconds: integrations: - name: nri-f5 env: METRICS: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: environment: production - name: nri-f5 env: INVENTORY: true HOSTNAME: localhost PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: environment: production inventory_source: config/f5 Copy Environment variables replacement In this configuration, the environment variable F5_HOST populates the HOSTNAME setting of the integration: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Metrics-only with partition filtering This configuration only collects metrics and adds \"MyOtherPartition\" to the list of partitions to be sampled. By default, the integration only samples the \"Common\" partition: integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: {{F5_HOST}} PORT: 443 USERNAME: f5_user PASSWORD: f5_password PARTITION_FILTER: '[\"Common\",\"MyOtherPartition\"]' interval: 15s labels: env: production role: load_balancer Copy Multi-instance monitoring This configuration monitors multiple F5 servers from the same integration. The first instance (HOSTNAME: 1st_f5_host) collects metrics and inventory, while the second instance (HOSTNAME: 2nd_f5_host) only collects metrics. integrations: - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer - name: nri-f5 env: INVENTORY: \"true\" HOSTNAME: 1st_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 60s labels: env: production role: load_balancer inventory_source: config/f5 - name: nri-f5 env: METRICS: \"true\" HOSTNAME: 2nd_f5_host PORT: 443 USERNAME: f5_user PASSWORD: f5_password interval: 15s labels: env: production role: load_balancer Copy Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Third-party services and select one of the F5 BIG-IP integration links. In New Relic Insights, F5 BIG-IP data is attached to the following Insights event types: F5BigIpSystemSample F5BigIpVirtualServerSample F5BigIpPoolSample F5BigIpPoolMemberSample F5BigIpNodeSample For more on how to find and use your data, see Understand integration data. Metric data The F5 BIG-IP integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as system., virtualserver., or pool.. System sample metrics These attributes can be found by querying the F5BigIpSystemSample event types. Metric Description system.cpuIdleTicksPerSecond Amount of CPU ticks that the CPU was idle per second. Requires Administrator-level user permissions to collect. system.cpuIdleUtilization Average percentage of time the CPU is idle. system.cpuInterruptRequestUtilization Average percentage of time the CPU is handling interrupt requests. system.cpuIOWaitUtilization Average percentage of time the CPU is waiting on IO. system.cpuNiceLevelUtilization Average percentage of time the CPU is handling nice level processes. system.cpuSoftInterruptRequestUtilization Average percentage of time the CPU is handling soft interrupt requests. system.cpuStolenUtilization Average percentage of time the CPU is handling reclaimed cycles by the hypervisor. system.cpuSystemTicksPerSecond Amount of CPU ticks used by the kernel processes per second. Requires Administrator-level user permissions to collect. system.cpuSystemUtilization Average percentage of time the CPU is used by the kernel. system.cpuUserTicksPerSecond Amount of CPU ticks used by user processes per second. Requires Administrator-level user permissions to collect. system.cpuUserUtilization Average percentage of time the CPU is used by user processes. system.memoryFreeInBytes Total amount of memory free, in bytes. system.memoryTotalInBytes Total amount of memory, in bytes. Requires Administrator-level user permissions to collect. system.memoryUsedInBytes Total amount of memory used, in bytes. Requires Administrator-level user permissions to collect. system.otherMemoryFreeInBytes Free memory reserved for control plane processes, in bytes. system.otherMemoryTotalInBytes Total memory reserved for control plane processes, in bytes. system.otherMemoryUsedInBytes Used memory reserved for control plane processes, in bytes. system.swapFreeInBytes Swap space free, in bytes. system.swapTotalInBytes Swap space total, in bytes. system.swapUsedInBytes Swap space used, in bytes. system.tmmMemoryFreeInBytes Free memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryTotalInBytes Total memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryUsedInBytes Used memory reserved for Traffic Management Microkernel (TMM), in bytes. Virtual server sample metrics These attributes can be found by querying the F5BigIpVirtualServerSample event types in Insights. Metric Description virtualserver.avaibilityState The BIG-IP defined availability. Options: 0 = Offline 1 = Unknown 2 = Online virtualserver.clientsideConnectionsPerSecond The rate of connections created through the client side of the object per second. virtualserver.cmpEnabled Indicates whether or not Cluster Multiprocessing (CMP) is enabled. virtualserver.cmpEnableMode Shows the Cluster Multiprocessing (CMP) mode indicators. Options: CMP disabled = none, disable, or single. CMP enabled = enable or all. virtualserver.connections The current number of connections from BIG-IP. virtualserver.csMaxConnDur Maximum connection duration from the client side of the object. virtualserver.csMinConnDur Minimum connection duration from the client side of the object. virtualserver.enabled The current enabled state. Options: 0 = Disabled 1 = Enabled virtualserver.ephemeralBytesInPerSecond Total number of bytes in through the ephemeral port per second. virtualserver.ephemeralBytesOutPerSecond Total number of bytes out through the ephemeral port per second. virtualserver.ephemeralConnectionsPerSecond The rate of connection creation through the ephemeral port per second. virtualserver.ephemeralCurrentConnections The current number of connections through the ephemeral port. virtualserver.ephemeralEvictedConnectionsPerSecond The number of connections that are evicted through the ephemeral port per second. virtualserver.ephemeralMaxConnections Maximum number of connections through the ephemeral port. virtualserver.ephemeralPacketsReceivedPerSecond The number of packets in through the ephemeral port per second. virtualserver.ephemeralPacketsSentPerSecond The number of packets out through the ephemeral port per second. virtualserver.ephemeralSlowKilledPerSecond The number of slow connections that are killed through the ephemeral port per second. virtualserver.evictedConnsPerSecond The rate of connections evicted per second. virtualserver.inDataInBytes The amount of data received from the BIG-IP virtual server, in bytes. virtualserver.outDataInBytes The amount of data sent to the BIG-IP virtual server, in bytes. virtualserver.packetsReceived The number of packets received from the BIG-IP virtual server. virtualserver.packetsSent The number of packets sent to the BIG-IP virtual server. virtualserver.requests The number of requests in the last collection interval to BIG-IP. virtualserver.slowKilledPerSecond The number of slow connections killed through the client side of the object per second. virtualserver.statusReason An explanation of the current status. virtualserver.usageRatio The usage ratio for the virtual server. Pool sample metrics These attributes can be found by querying the F5BigIpPoolSample event types in Insights. Metric Description pool.activeMembers The number of active pool members. pool.availabilityState The current availability state. Options: 0 = Offline 1 = Unknown 2 = Online pool.connections The current number of connections. pool.connqAgeEdm The queue age exponential-decaying max. pool.connqAgeEma The queue age exponential-moving average. pool.connqAgeHead The current queue age head. pool.connqAgeMax The queue age all-time max. pool.connqAllAgeEdm The sum of pool member queue age exponential-decaying max. pool.connqAllAgeEma The sum of pool member queue age exponential-moving average. pool.connqAllAgeHead The sum of pool member queue age head. pool.connqAllAgeMax The sum of pool member queue age all-time max. pool.connqAllDepth The sum of pool member depth. pool.connqDepth The queue depth. pool.currentConnections The current connections. pool.enabled The current enabled state, can be user defined. Options: 0 = Disabled 1 = Enabled pool.inDataInBytes The amount of data received from the BIG-IP pool, in bytes. pool.minActiveMembers Pool minimum active members. pool.outDataInBytes The amount of data sent to the BIG-IP pool, in bytes. pool.packetsReceived The number of packets received from the BIG-IP pool. pool.packetsSent The number of packets sent to the BIG-IP pool. pool.requests The total number of requests to the pool. pool.statusReason Textual property explaining the overall health reason. Pool member sample metrics These attributes can be found by querying the F5BigIpPoolMemberSample event types in Insights. Metric Description member.availabilityState The current availability from the BIG-IP system. Options: 0 = Offline 1 = Unknown 2 = Online member.connections The current connections. member.enabled Enabled state of the pool member with regards to the parent pool. Options: 0 = Disabled 1 = Enabled member.inDataInBytes The amount of data received from the BIG-IP pool member, in bytes. member.monitorStatus The status of the monitor. Options: 0 = Down 1 = Unchecked 2 = Any other status member.outDataInBytes The amount of data sent to the BIG-IP pool member, in bytes. member.packetsReceived The number of packets received from the BIG-IP pool member. member.packetsSent The number of packets sent to the BIG-IP pool member. member.requests The current number of requests over the last collection interval. member.sessions The current session count. member.sessionStatus The current session health status. Options: 0 = Disabled 1 = Enabled member.state The current state. Options: 0 = Down 1 = Up member.statusReason Explanation of the current status. Node sample metrics These attributes can be found by querying the F5BigIpNodeSample event types in Insights. Metric Description node.availabilityState The current BIG-IP availability state to the node. Options: 0 = Offline 1 = Unknown 2 = Online node.connections The current number of network connections from BIG-IP. node.connectionsPerSecond The number of connections made per second. node.enabled The current BIG-IP enabled state. Options: 0 = Disabled 1 = Enabled , node.inDataInBytes The amount of data received from the BIG-IP node, in bytes. node.monitorStatus The current health monitor rule status. Options: 0 = Down 1 = Unchecked 2 = Any other status node.outDataInBytes The amount of data sent to the BIG-IP node, in bytes. node.packetsReceived The number of packets received from the BIG-IP node. node.packetsSent The number of packets sent to the BIG-IP node. node.requests The current number of requests over the last collection from BIG-IP. node.sessions The current number of sessions. node.sessionStatus The current status of the session. Options: 0 = Disabled 1 = Enabled node.statusReason BIG-IP reason for the current status. Inventory data The F5 BIG-IP integration also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config/f5 source. For more about inventory data, see Understand integration data. The integration captures data for the following F5 BIG-IP configuration parameters: Pool Inventory Metric Description currentLoadMode Current load balancing mode. description User defined description. kind Kind of pool. maxConnections Current max number of connections seen at one point. monitorRule Current health monitoring rule applied. Node inventory Metric Description address BIG-IP network address to send to the node. fqdn FQDN of node. kind Type of Node in BIG-IP. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP Health Monitor rule. Pool Member Inventory Metric Description kind Type of Pool member. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP health monitor rule. nodeName Name of the node the pool member is using. poolName Name of the pool the pool member belongs. port Port the pool member listens on. Virtual Server Inventory Metric Description applicationService Current application service assigned. destination Destination address picked up by BIG-IP. kind Type of virtual server. maxConnections Current highest number of network connections reported from BIG-IP. name User defined name. pool Pool the virtual server uses for load balancing. System Inventory Metric Description chassisSerialNumber Chassis Serial Number for the current device. Requires Access Administrator-level user permissions to collect. platform Platform of the current device. Requires Access Administrator-level user permissions to collect. product Product Name for the current device. Requires Access Administrator-level user permissions to collect. Application Inventory Metric Description deviceGroup Device group running application service. kind BIG-IP Defined type. name User defined name. poolToUse Server side pool load balancing requests. template Template applied to application including security and monitoring rules. templateModified Indicator of modifications made to out of the box template. trafficGroup Current traffic group to which service is applied. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 371.11002,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>F5</em> monitoring <em>integration</em>",
        "sections": "<em>F5</em> monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": ". Inventory data The <em>F5</em> BIG-IP <em>integration</em> also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config&#x2F;<em>f5</em> <em>source</em>. For more about inventory data, see Understand <em>integration</em> data"
      },
      "id": "6044e41ce7b9d2f0975799b4"
    },
    {
      "sections": [
        "On-host integrations metrics",
        "BETA FEATURE",
        "New Relic Integrations Metrics"
      ],
      "title": "On-host integrations metrics",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2021-09-14T20:50:16Z",
      "updated_at": "2021-09-14T20:50:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. New Relic Integrations Metrics The following table contains the metrics we collect for our infrastructure integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosShared locks.globalTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.metadataAcquireExclusive locks.metadataAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireExclusive locks.oplogAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentExclusive locks.oplogAcquireIntentExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentSha",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 326.42963,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em> metrics",
        "sections": "On-host <em>integrations</em> metrics",
        "body": " transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes <em>F5</em> <em>f5</em>.node.availabilityState"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "Set up network flow data monitoring",
        "Prerequisites",
        "Network security prerequisites",
        "Supported types of network flow data",
        "Important",
        "Scaling Network Flow Collection",
        "Set up network flow data monitoring in New Relic One",
        "Tip"
      ],
      "title": "Set up network flow data monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Network Performance Monitoring",
        "Installation",
        "Setup"
      ],
      "external_id": "626c9bebce36e550d5793d8ef932e6d654c23e47",
      "image": "",
      "url": "https://docs.newrelic.com/docs/network-performance-monitoring/setup-performance-monitoring/network-flow-monitoring/",
      "published_at": "2021-09-14T18:19:01Z",
      "updated_at": "2021-09-14T18:19:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Set up your network devices so they send network data to New Relic One. Prerequisites A New Relic account. Don't have one? Sign up for free! No credit card required. A New Relic account ID. Read how to find your account ID. A New Relic license key. Docker installed in your local machine. SSH access to the Docker host, with the ability to launch new containers. Access to Layer 2/3 network devices that can generate and send network flow data, and also add and modify network flow targets on the device. Here's how to configure network flow data collection in some devices: NetFlow data Palo Alto - PAN-OS Fortinet Fortigate Cisco - NX-OS Cisco - IOS Cisco - Meraki sFlow data F5 - BIG-IP jFlow data Juniper - Junos Network security prerequisites Direction Source Destination Ports Protocol Outbound Docker host Kentik's docker image GitHub repository 80, 443 UDP, TCP Outbound Docker host EU Logs endpoint https://log-api.eu.newrelic.com/log/v1 Copy US Logs endpoint https://log-api.newrelic.com/log/v1 Copy 80, 443 UDP, TCP Outbound Docker host EU Events endpoint http://insights-collector.eu01.nr-data.net/ Copy US Events endpoint http://insights-collector.newrelic.com/ Copy 80, 443 UDP, TCP Inbound Network flow data device Docker host 9995 UDP Supported types of network flow data Kentik's integration supports four types of network flow data. When running the ktranslate image, you can specify which type you want to monitor using the -nf.source option. Important The ktranslate image only supports monitoring one type of network flow data type at a time. If you want to monitor several types, each will require a container. IPFIX and NetflowV9 can be sent to the same container, but we recommend running a separate container as a best practice. To check the equivalence among the network flow data type and the value you need to specify when running the image, see the following table: Network flow data type -nf.source value IPFIX ipfix NetFlow version 5 netflow5 NetFlow version 9 netflow9 sFlow sflow Important For Juniper Networks' jFlow, use the netflow5 value. Scaling Network Flow Collection When planning your strategy for collecting network flows at scale, New Relic recommends 1 CPU per 2000 flows-per-second (120,000 flows-per-minute). Deciding whether to run more small containers to distribute load or fewer large containers to consolidate management is a matter of personal preference. Set up network flow data monitoring in New Relic One In your local machine, from a Linux host with Docker installed, download the ktranslate image from dockerhub by running bash Copy $ docker pull kentik/ktranslate:v2 Copy the snmp-base.yaml file to the local $HOME directory of your Docker user, and discard the container by running bash Copy $ cd . $ id=$(docker create kentik/ktranslate:v2) $ docker cp $id:/etc/ktranslate/snmp-base.yaml . $ docker rm -v $id In the snmp-base.yaml file, add your network flow devices inside the devices variable with the following structure: devices: flowDevice: device_name: edge-router device_ip: 10.10.1.254 flow_only: true user_tags: owning_team: net_eng environment: production Copy Tip If you're already monitoring SNMP data devices that send network flow data, you don't need to add them in your snmp-base.yaml file. Run ktranslate to listen for network flows from devices by entering the following commands: Important Add your New Relic license key and your account ID in the $NR_LICENSE_KEY and $NR_ACCOUNT_ID variables respectively. bash Copy $ docker run -d --name ktranslate-sflow --restart unless-stopped --net=host \\ > -v `pwd`/snmp-base.yaml:/snmp-base.yaml \\ > -e NEW_RELIC_API_KEY=$NR_LICENSE_KEY \\ > kentik/ktranslate:v2 \\ > -snmp /snmp-base.yaml \\ > -nr_account_id=$NR_ACCOUNT_ID \\ > -metrics=jchf \\ > -log_level=info \\ > -tee_logs=true \\ > -flow_only=true \\ > -nf.source=sflow \\ > nr1.flow $ ## If your account is located in Europe, you need to add the following option before the nr1.flow line $ ## -nr_region=EU \\ To get better visibility into your network, set up SNMP data monitoring. Visualize your network performance data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.6641,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up network <em>flow</em> data monitoring",
        "sections": "Set up network <em>flow</em> data monitoring",
        "tags": "<em>Integrations</em>",
        "body": " - PAN-OS Fortinet Fortigate Cisco - NX-OS Cisco - IOS Cisco - Meraki sFlow data <em>F5</em> - BIG-IP jFlow data Juniper - Junos Network security prerequisites Direction <em>Source</em> Destination Ports Protocol Outbound Docker host Kentik&#x27;s docker image GitHub repository 80, 443 UDP, TCP Outbound Docker host EU Logs"
      },
      "id": "612724e128ccbc4ac9f2612a"
    }
  ],
  "/docs/integrations/host-integrations/open-source-host-integrations-list/memcached-open-source-integration": [
    {
      "sections": [
        "New Relic guided install overview",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes",
        "Troubleshoot common problems",
        "MySQL: Incorrect user permissions",
        "NGINX: No status URL"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "2058522f6cb1e82dbbe111a176c22ec4aa515ae5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-09-19T01:39:59Z",
      "updated_at": "2021-08-20T13:37:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU datacenter, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to what’s being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration Troubleshoot common problems As we identify areas where the guided install fails, we'll document them here and provide some troubleshooting guidance. MySQL: Incorrect user permissions To monitor MySQL health data, you need a valid username and password with specific permissions. These commands will create a user and grant the required permissions: Create a user newrelic@localhost with a specific password. sudo mysql -e \"CREATE USER 'newrelic'@'localhost' IDENTIFIED BY 'YOUR_SELECTED_PASSWORD';\" Copy Give replication privileges to newrelic@localhost with a maximum of 5 connections. sudo mysql -e \"GRANT REPLICATION CLIENT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Give select privileges to newrelic@localhost with a maximum of 5 connections. sudo mysql -e \"GRANT SELECT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Once done, your next guided install attempt should work. NGINX: No status URL To monitor your NGINX server, you'll need to configure a valid status URL. status_url: The URL set up to provide the metrics using the status module. If the default value of 127.0.0.1 is incorrect, substitute the address/FQDN/URL for your system. Example: status_url: http://127.0.0.1/status You can read more about the status_url in these NGINX docs: For NGINX Open Source: HTTP stub status module For NGINX Plus: HTTP status module and HTTP API module There are different ways to set status_url, depending on how NGINX was installed: If enabled via Kubernetes: See Monitor services running on Kubernetes. If enabled via Amazon ECS: See Monitor services running on ECS. If installed on-host: Edit the config in the integration's YAML config file, nginx-config.yml.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 471.82428,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "On-host <em>integration</em> (OHI) recipes",
        "body": " install -n hashicorp-consul-<em>open</em>-<em>source</em>-<em>integration</em> <em>Memcached</em> newrelic install -n <em>memcached</em>-<em>open</em>-<em>source</em>-<em>integration</em> Microsoft SQL Server (Windows only) newrelic install -n mssql-server-<em>integration</em>-installer MongoDB newrelic install -n mongodb-<em>open</em>-<em>source</em>-<em>integration</em> MySQL newrelic install -n mysql"
      },
      "id": "604130a7e7b9d299cb2a07c0"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "APM agents",
        "How OpenTelemetry works with New Relic",
        "Important"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-09-14T07:27:00Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. APM agents In general, APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemtry community to export your data to New Relic. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 201.80301,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> telemetry <em>integrations</em>",
        "body": " instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into <em>Open</em>Telemetry or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Google Memorystore for Memcached",
        "BETA FEATURE",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Memcache MemcacheNode data"
      ],
      "title": "Google Memorystore for Memcached",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "faf3c02773c3bb62ebd194ec0e6227e83ce910ea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached/",
      "published_at": "2021-09-14T20:39:54Z",
      "updated_at": "2021-09-14T20:39:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. We offer a cloud integration for reporting your GCP Memcache data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to connect your GCP service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Memcache integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select the integration. Data is attached to the following event types: Entity Event Type Provider MemcacheNode GcpMemcacheMemcacheNodeSample GcpMemcacheMemcacheNode For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Memcache data for MemcacheNode. Memcache MemcacheNode data Metric Unit Description node.ActiveConnections Count Connections active in this Memcached node. node.CacheMemory Bytes Bytes alloted for Memcached in this node, grouped by whether that memory is used or not. node.cpu.UsageTime Seconds CPU usage time by Memcached process grouped by user and kernel mode. node.cpu.Utilization Percent CPU usage percent by Memcached node. node.Eviction Count Count of items evicted by this Memcached node. node.HitRatio Percent Hit ratio, expressed as a percentage of the total cache requests excluding set operations. Values are numbers between 0.0 and 1.0, charts display the values as a percentage between 0% and 100%. node.Items Count Items stored in this Memcached node. node.Operation Count Count of Memcached operations grouped by command and response_type (e.g. hit, miss). node.ReceivedBytes Bytes Bytes received by this Memcached node. node.SentBytes Bytes Bytes sent by this Memcached node. node.Uptime Seconds Time in seconds the node has been running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.8555,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Google Memorystore for <em>Memcached</em>",
        "sections": "Google Memorystore for <em>Memcached</em>",
        "tags": "<em>Integrations</em>",
        "body": "BETA FEATURE This feature is currently in beta. We offer a cloud <em>integration</em> for reporting your GCP Memcache data to our platform. Here we explain how to activate the <em>integration</em> and what data it collects. Activate the <em>integration</em> To enable the <em>integration</em> follow standard procedures to connect your"
      },
      "id": "604508a864441f1644378ef0"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/not-seeing-host-integration-data": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 234.24039,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.47464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Nagios monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". This gives you full control over the installation and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    },
    {
      "sections": [
        "RabbitMQ monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Important",
        "Configure the integration",
        "RabbitMQ Instance Settings",
        "Labels/Custom Attributes",
        "Example configuration",
        "BASIC CONFIGURATION WITH SSL",
        "METRICS ONLY",
        "INVENTORY ONLY",
        "EVENTS ONLY",
        "Find and use data",
        "Metric data",
        "RabbitMQ vhost sample event",
        "RabbitMQ node sample event",
        "RabbitMQ exchange sample event",
        "RabbitMQ queue sample event",
        "System metadata",
        "Inventory data",
        "Caution",
        "Troubleshooting",
        "Error getting local node name: exec: \\\"rabbitmqctl\\\": executable file not found in $PATH",
        "Check the source code",
        "For more help"
      ],
      "title": "RabbitMQ monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "b321df4ac10040b755a7cf7445eca43688eee9d2",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/rabbitmq-monitoring-integration/",
      "published_at": "2021-09-14T20:43:01Z",
      "updated_at": "2021-09-14T20:43:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic RabbitMQ on-host integration reports metrics and configuration data from your RabbitMQ service, including important metrics relating to the cluster, vhosts, queues, exchanges, and consumers. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with: RabbitMQ version 3.0 or higher for Metric data. RabbitMQ version 3.7 or higher for Inventory data. Before installing the integration, make sure that you meet the following requirements: RabbitMQ Management Plugin configured. RabbitMQ command line tool, rabbitmqctl, in the PATH of the root user. If RabbitMQ is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host running RabbitMQ. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your RabbitMQ service quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the RabbitMQ integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your RabbitMQ service. Install and activate To install the RabbitMQ integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-rabbitmq. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp rabbitmq-config.yml.sample rabbitmq-config.yml Copy Edit the rabbitmq-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-rabbitmq .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-rabbitmq/nri-rabbitmq-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-rabbitmq-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp rabbitmq-config.yml.sample rabbitmq-config.yml Copy Edit the rabbitmq-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Important This integration can monitor only one RabbitMQ server instance per host. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, rabbitmq-config.yml. For an example configuration, see Example config file. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to RabbitMQ are defined using the env section of the configuration file. These settings control the connection to your RabbitMQ instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Cluster environments In cluster environments, only one node should use METRICS=false and INVENTORY=false ; the rest should use INVENTORY=true and METRICS=false. If you're running a cluster environment in Kubernetes, you need to deploy RabbitMQ as a StatefulSet, and configure the agent to query all the metrics from the RabbitMQ pod. Set the autodiscovery matching condition in the config file to this value: discovery: command: exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: podName: rabbitmq-0 Copy RabbitMQ Instance Settings The RabbitMQ integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To HOSTNAME Hostname or IP of the RabbitMQ management plugin. localhost M/I PORT Port number of the RabbitMQ management plugin. 15672 M/I USERNAME User that is connecting to RabbitMQ management plugin. N/A M/I PASSWORD Password to connect to RabbitMQ management plugin. N/A M/I MANAGEMENT_PATH_PREFIX RabbitMQ Management Prefix. N/A M/I USE_SSL Option to connect using SSL. false M/I CA_BUNDLE_DIR Location of SSL certificate on the host. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. N/A M/I NODE_NAME_OVERRIDE Overrides the local node name instead of retrieving it from RabbitMQ. N/A M/I CONFIG_PATH Absolute path to the RabbitMQ configuration file. N/A M/I QUEUES Queue names to collect in the format of a JSON array of strings. If a queue name exactly matches (case sensitive) one of these, it will be collected. Example: queues: '[\"myQueue1\",\"myQueue2\"]' Copy N/A I QUEUES_REGEXES Queue names to collect in the format of a JSON array of REGEX strings. If a queue name matches one of these, it will be collected. Example: queues_regexes: '[\"queue[0-9]+\",\".*\"]' Copy N/A M/I EXCHANGES Exchange names to collect in the format of a JSON array of strings. If an exchange name exactly matches (case sensitive) one of these, it will be collected. N/A M/I EXCHANGES_REGEXES Exchanges names to collect in the format of a JSON array of REGEX strings. If an exchange name matches one of these it will be collected. N/A M/I VHOSTS Vhost names to collect in the format of a JSON array of strings. If a vhost name exactly matches (case sensitive) one of these, it will be included. This also affects only collecting entities belonging to vhosts specified. N/A M/I VHOSTS_REGEXES Vhost names to collect in the format of a JSON array of REGEX strings. If a vhost name matches one of these the vhost and any entities belonging to this vhost will be collected. N/A M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false EVENTS Set to true to enable Events only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: rabbitmq Copy Example configuration Here's an example configuration file: BASIC CONFIGURATION WITH SSL integrations: - name: nri-rabbitmq env: HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin CA_BUNDLE_DIR: /path/to/bundle/dir/ NODE_NAME_OVERRIDE: local_node_name CONFIG_PATH: /path/to/config/file/rabbitmq.conf USE_SSL: true QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy METRICS ONLY integrations: - name: nri-rabbitmq env: METRICS: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy INVENTORY ONLY integrations: - name: nri-rabbitmq env: INVENTORY: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name CONFIG_PATH: /path/to/config/file/rabbitmq.conf QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy EVENTS ONLY integrations: - name: nri-rabbitmq env: EVENTS: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: RabbitmqVhostSample RabbitmqNodeSample RabbitmqExchangeSample RabbitmqQueueSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The RabbitMQ integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as queue. or node.. RabbitMQ vhost sample event These attributes are attached to the RabbitmqVhostSample event type: Name Description vhost.connectionsBlocked Number of current connections in the state blocked. vhost.connectionsBlocking Number of current connections in the state blocking. vhost.connectionsClosed Number of current connections in the state closed. vhost.connectionsClosing Number of current connections in the state closing. vhost.connectionsFlow Number of current connections in the state flow. vhost.connectionsOpening Number of current connections in the state opening. vhost.connectionsRunning Number of current connections in the state running. vhost.connectionsStarting Number of current connections in the state starting. vhost.connectionsTotal Number of current connections to a given rabbitmq vhost. vhost.connectionsTuning Number of current connections in the state tuning. RabbitMQ node sample event These attributes are attached to the RabbitmqNodeSample event type: Name Description node.averageErlangProcessesWaiting Average number of Erlang processes waiting to run. In RabbitMQ this is seen as run_queue. node.diskAlarm Node disk alarm (0 or 1). 0 shows that the alarm is not tripped and 1 shows that the alarm is tripped. In RabbitMQ this is seen as disk_free_alarm. node.diskSpaceFreeInBytes Current free disk space in bytes. In RabbitMQ this is seen as disk_free. node.fileDescriptorsTotalUsed The total count of file descriptors used. In RabbitMQ this is seen as fd_used. node.fileDescriptorsUsedSockets Number of file descriptors used as sockets. In RabbitMQ this is seen as sockets_used. node.hostMemoryAlarm Host memory alarm (0 or 1). 0 shows that the alarm is not tripped and 1 shows that the alarm is tripped. In RabbitMQ this is seen as mem_alarm. node.totalMemoryUsedInBytes Memory used in bytes. In RabbitMQ this is seen as mem_used. node.paritionsSeen Number of network partitions seen per node. In RabbitMQ this is seen as partitions. node.running Node running (0 or 1). 0 shows that the node is not running and 1 shows that the node is running. In RabbitMQ this is seen as running. RabbitMQ exchange sample event These attributes are attached to the RabbitmqExchangeSample event type: Name Description exchange.bindings Number of bindings for a specific exchange. exchange.messagesDeliveredAcknowledged Count of messages delivered to clients and acknowledged per exchange. exchange.messagesDeliveredAcknowledgedPerSecond Rate of messages delivered to clients and acknowledged per exchange per second. exchange.messagesConfirmed Count of messages confirmed per exchange. exchange.messagesConfirmedPerSecond Rate of messages confirmed per exchange per second. exchange.messagesRedelivered Count of subset of messages in deliver_get which had the redelivered flag set per queue. exchange.messagesRedeliveredPerSecond Rate of subset of messages in deliver_get which had the redelivered flag set per queue per second. exchange.messagesReturnedUnroutable Count of messages returned to publisher as unroutable per exchange. exchange.messagesReturnedUnroutablePerSecond Rate of messages returned to publisher as unroutable per exchange per second. exchange.messagesPublished Count of messages published per exchange. exchange.messagesPublishedPerSecond Rate of messages published per exchange queue per second. exchange.messagesPublishedPerChannel Count of messages published from a channel into this exchange. In RabbitMQ this is seen as message_stats.publish_in. exchange.messagesPublishedPerChannelPerSecond Rate of messages published from a channel into this exchange per sec. In RabbitMQ this is seen as message_stats.publish_in_details.rate. exchange.messagesPublishedQueue Count of messages published from this exchange into a queue. In RabbitMQ this is seen as message_stats.publish_out. exchange.messagesPublishedQueuePerSecond Rate of messages published from this exchange into a queue per second. In RabbitMQ this is seen as message_stats.publish_out_details.rate. RabbitMQ queue sample event These attributes are attached to the RabbitmqQueueSample event type: Name Description queue.bindings Number of bindings for a specific queue. queue.countActiveConsumersReceiveMessages Number of active consumers that can immediately receive any messages sent to the queue. In RabbitMQ this is seen as active_consumers. queue.consumers Number of consumers per queue. In RabbitMQ this is seen as consumers. queue.consumerMessageUtilizationPerSecond The ratio of time that a queue's consumers can take new messages (utilization per second). In RabbitMQ this is seen as consumer_utilisation. This metric is only available on RabbitMQ version 3.3 or higher. queue.erlangBytesConsumedInBytes Bytes consumed by the Erlang process associated with the queue. In RabbitMQ this is seen as memory. queue.messagesReadyDeliveryClients Count of messages ready to be delivered to clients. In RabbitMQ this is seen as message_stats.messages_ready. queue.messagesReadyDeliveryClientsPerSecond Rate of messages ready to be delivered to clients per second. In RabbitMQ this is seen as message_stats.messages_ready_details.rate. queue.messagesReadyUnacknowledged Count of messages per queue delivered to clients but not yet acknowledged. In RabbitMQ this is seen as message_stats.deliver_no_ack. queue.messagesReadyUnacknowledgedPerSecond Rate of messages per queue delivered to clients but not yet acknowledged per second. In RabbitMQ this is seen as message_stats.deliver_no_ack_details.rate. queue.messagesAcknowledged Count of messages delivered to clients and acknowledged per queue. In RabbitMQ this is seen as message_stats.ack. queue.messagesAcknowledgedPerSecond Rate of messages delivered to clients and acknowledged per second per queue. In RabbitMQ this is seen as message_stats.ack_details.rate. queue.messagesDeliveredAckMode Count of messages delivered in acknowledgment mode to consumers per queue. In RabbitMQ this is seen as message_stats.deliver. queue.messagesDeliveredAckModePerSecond Rate of messages delivered in acknowledgment mode to consumers per queue per second. In RabbitMQ this is seen as message_stats.deliver_details.rate. queue.messagesPublished Count of messages published per queue. In RabbitMQ this is seen as message_stats.publish. queue.messagesPublishedPerSecond Rate of messages published per second per queue. In RabbitMQ this is seen as message_stats.publish_details.rate. queue.messagesRedeliverGet Count of subset of messages in acknowledgment mode which had the redelivered flag set per queue. In RabbitMQ this is seen as message_stats.redeliver. queue.messagesRedeliverGetPerSecond Rate of subset of messages in acknowledgment mode which had the redelivered flag set per queue per second. In RabbitMQ this is seen as message_stats.redeliver_details.rate. queue.sumMessagesDelivered Sum of messages delivered in acknowledgment mode to consumers, in no-acknowledgment mode to consumers, in acknowledgment mode in response to basic.get, and in no-acknowledgment mode in response to basic.get. per queue. In RabbitMQ this is seen as message_stats.deliver_get. queue.sumMessagesDeliveredPerSecond Rate per second of the sum of messages delivered in acknowledgment mode to consumers, in no-acknowledgment mode to consumers, in acknowledgment mode in response to basic.get, and in no-acknowledgment mode in response to basic.get per queue. In RabbitMQ this is seen as message_stats.deliver_get_details.rate. queue.totalMessages Count of the total messages in the queue. In RabbitMQ this is seen as messages. queue.totalMessagesPerSecond Rate of total messages in queue. In RabbitMQ this is seen as messages_details.rate. System metadata Other metadata includes: Name Description version.rabbitmq The version of the RabbitMQ server. Example: 3.6.7. version.management The version of the RabbitMQ management plugin. For example: 3.6.7. Inventory data The integration captures the configuration parameters of RabbitMQ in the /etc/rabbitmq/rabbitmq.conf file. Inventory data is only captured in RabbitMQ version 3.7 or higher; the inventory data will appear on the Infrastructure Inventory page, under the config/rabbitmq source. Caution Be aware that any sensitive information that you put into the rabbit.conf file will appear on the Infrastructure Inventory page. This includes items such as the following from AWS: cluster_formation.aws.secret_key, cluster_formation.aws.access_key_id Copy Troubleshooting Troubleshooting tips: Error getting local node name: exec: \\\"rabbitmqctl\\\": executable file not found in $PATH If you receive this error, it means that the RabbitMQ command line tool, rabbitmqctl, is not in the PATH of the root user. To correct this issue, execute the following command: find -name \"rabbitmqctl\" export PATH=\"$PATH:<LOCATION_TO_RABBITMQCTL> Copy Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.34183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "RabbitMQ monitoring <em>integration</em>",
        "sections": "RabbitMQ monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the rabbitmq-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results"
      },
      "id": "6045095928ccbcddbf2c60d4"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/pass-infrastructure-agent-parameters-host-integration": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 234.24019,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.47456,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Nagios monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". This gives you full control over the installation and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    },
    {
      "sections": [
        "RabbitMQ monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Important",
        "Configure the integration",
        "RabbitMQ Instance Settings",
        "Labels/Custom Attributes",
        "Example configuration",
        "BASIC CONFIGURATION WITH SSL",
        "METRICS ONLY",
        "INVENTORY ONLY",
        "EVENTS ONLY",
        "Find and use data",
        "Metric data",
        "RabbitMQ vhost sample event",
        "RabbitMQ node sample event",
        "RabbitMQ exchange sample event",
        "RabbitMQ queue sample event",
        "System metadata",
        "Inventory data",
        "Caution",
        "Troubleshooting",
        "Error getting local node name: exec: \\\"rabbitmqctl\\\": executable file not found in $PATH",
        "Check the source code",
        "For more help"
      ],
      "title": "RabbitMQ monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "b321df4ac10040b755a7cf7445eca43688eee9d2",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/rabbitmq-monitoring-integration/",
      "published_at": "2021-09-14T20:43:01Z",
      "updated_at": "2021-09-14T20:43:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic RabbitMQ on-host integration reports metrics and configuration data from your RabbitMQ service, including important metrics relating to the cluster, vhosts, queues, exchanges, and consumers. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with: RabbitMQ version 3.0 or higher for Metric data. RabbitMQ version 3.7 or higher for Inventory data. Before installing the integration, make sure that you meet the following requirements: RabbitMQ Management Plugin configured. RabbitMQ command line tool, rabbitmqctl, in the PATH of the root user. If RabbitMQ is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host running RabbitMQ. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your RabbitMQ service quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the RabbitMQ integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your RabbitMQ service. Install and activate To install the RabbitMQ integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-rabbitmq. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp rabbitmq-config.yml.sample rabbitmq-config.yml Copy Edit the rabbitmq-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-rabbitmq .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-rabbitmq/nri-rabbitmq-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-rabbitmq-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp rabbitmq-config.yml.sample rabbitmq-config.yml Copy Edit the rabbitmq-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Important This integration can monitor only one RabbitMQ server instance per host. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, rabbitmq-config.yml. For an example configuration, see Example config file. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to RabbitMQ are defined using the env section of the configuration file. These settings control the connection to your RabbitMQ instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Cluster environments In cluster environments, only one node should use METRICS=false and INVENTORY=false ; the rest should use INVENTORY=true and METRICS=false. If you're running a cluster environment in Kubernetes, you need to deploy RabbitMQ as a StatefulSet, and configure the agent to query all the metrics from the RabbitMQ pod. Set the autodiscovery matching condition in the config file to this value: discovery: command: exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: podName: rabbitmq-0 Copy RabbitMQ Instance Settings The RabbitMQ integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To HOSTNAME Hostname or IP of the RabbitMQ management plugin. localhost M/I PORT Port number of the RabbitMQ management plugin. 15672 M/I USERNAME User that is connecting to RabbitMQ management plugin. N/A M/I PASSWORD Password to connect to RabbitMQ management plugin. N/A M/I MANAGEMENT_PATH_PREFIX RabbitMQ Management Prefix. N/A M/I USE_SSL Option to connect using SSL. false M/I CA_BUNDLE_DIR Location of SSL certificate on the host. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. N/A M/I NODE_NAME_OVERRIDE Overrides the local node name instead of retrieving it from RabbitMQ. N/A M/I CONFIG_PATH Absolute path to the RabbitMQ configuration file. N/A M/I QUEUES Queue names to collect in the format of a JSON array of strings. If a queue name exactly matches (case sensitive) one of these, it will be collected. Example: queues: '[\"myQueue1\",\"myQueue2\"]' Copy N/A I QUEUES_REGEXES Queue names to collect in the format of a JSON array of REGEX strings. If a queue name matches one of these, it will be collected. Example: queues_regexes: '[\"queue[0-9]+\",\".*\"]' Copy N/A M/I EXCHANGES Exchange names to collect in the format of a JSON array of strings. If an exchange name exactly matches (case sensitive) one of these, it will be collected. N/A M/I EXCHANGES_REGEXES Exchanges names to collect in the format of a JSON array of REGEX strings. If an exchange name matches one of these it will be collected. N/A M/I VHOSTS Vhost names to collect in the format of a JSON array of strings. If a vhost name exactly matches (case sensitive) one of these, it will be included. This also affects only collecting entities belonging to vhosts specified. N/A M/I VHOSTS_REGEXES Vhost names to collect in the format of a JSON array of REGEX strings. If a vhost name matches one of these the vhost and any entities belonging to this vhost will be collected. N/A M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false EVENTS Set to true to enable Events only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: rabbitmq Copy Example configuration Here's an example configuration file: BASIC CONFIGURATION WITH SSL integrations: - name: nri-rabbitmq env: HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin CA_BUNDLE_DIR: /path/to/bundle/dir/ NODE_NAME_OVERRIDE: local_node_name CONFIG_PATH: /path/to/config/file/rabbitmq.conf USE_SSL: true QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy METRICS ONLY integrations: - name: nri-rabbitmq env: METRICS: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy INVENTORY ONLY integrations: - name: nri-rabbitmq env: INVENTORY: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name CONFIG_PATH: /path/to/config/file/rabbitmq.conf QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy EVENTS ONLY integrations: - name: nri-rabbitmq env: EVENTS: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: RabbitmqVhostSample RabbitmqNodeSample RabbitmqExchangeSample RabbitmqQueueSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The RabbitMQ integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as queue. or node.. RabbitMQ vhost sample event These attributes are attached to the RabbitmqVhostSample event type: Name Description vhost.connectionsBlocked Number of current connections in the state blocked. vhost.connectionsBlocking Number of current connections in the state blocking. vhost.connectionsClosed Number of current connections in the state closed. vhost.connectionsClosing Number of current connections in the state closing. vhost.connectionsFlow Number of current connections in the state flow. vhost.connectionsOpening Number of current connections in the state opening. vhost.connectionsRunning Number of current connections in the state running. vhost.connectionsStarting Number of current connections in the state starting. vhost.connectionsTotal Number of current connections to a given rabbitmq vhost. vhost.connectionsTuning Number of current connections in the state tuning. RabbitMQ node sample event These attributes are attached to the RabbitmqNodeSample event type: Name Description node.averageErlangProcessesWaiting Average number of Erlang processes waiting to run. In RabbitMQ this is seen as run_queue. node.diskAlarm Node disk alarm (0 or 1). 0 shows that the alarm is not tripped and 1 shows that the alarm is tripped. In RabbitMQ this is seen as disk_free_alarm. node.diskSpaceFreeInBytes Current free disk space in bytes. In RabbitMQ this is seen as disk_free. node.fileDescriptorsTotalUsed The total count of file descriptors used. In RabbitMQ this is seen as fd_used. node.fileDescriptorsUsedSockets Number of file descriptors used as sockets. In RabbitMQ this is seen as sockets_used. node.hostMemoryAlarm Host memory alarm (0 or 1). 0 shows that the alarm is not tripped and 1 shows that the alarm is tripped. In RabbitMQ this is seen as mem_alarm. node.totalMemoryUsedInBytes Memory used in bytes. In RabbitMQ this is seen as mem_used. node.paritionsSeen Number of network partitions seen per node. In RabbitMQ this is seen as partitions. node.running Node running (0 or 1). 0 shows that the node is not running and 1 shows that the node is running. In RabbitMQ this is seen as running. RabbitMQ exchange sample event These attributes are attached to the RabbitmqExchangeSample event type: Name Description exchange.bindings Number of bindings for a specific exchange. exchange.messagesDeliveredAcknowledged Count of messages delivered to clients and acknowledged per exchange. exchange.messagesDeliveredAcknowledgedPerSecond Rate of messages delivered to clients and acknowledged per exchange per second. exchange.messagesConfirmed Count of messages confirmed per exchange. exchange.messagesConfirmedPerSecond Rate of messages confirmed per exchange per second. exchange.messagesRedelivered Count of subset of messages in deliver_get which had the redelivered flag set per queue. exchange.messagesRedeliveredPerSecond Rate of subset of messages in deliver_get which had the redelivered flag set per queue per second. exchange.messagesReturnedUnroutable Count of messages returned to publisher as unroutable per exchange. exchange.messagesReturnedUnroutablePerSecond Rate of messages returned to publisher as unroutable per exchange per second. exchange.messagesPublished Count of messages published per exchange. exchange.messagesPublishedPerSecond Rate of messages published per exchange queue per second. exchange.messagesPublishedPerChannel Count of messages published from a channel into this exchange. In RabbitMQ this is seen as message_stats.publish_in. exchange.messagesPublishedPerChannelPerSecond Rate of messages published from a channel into this exchange per sec. In RabbitMQ this is seen as message_stats.publish_in_details.rate. exchange.messagesPublishedQueue Count of messages published from this exchange into a queue. In RabbitMQ this is seen as message_stats.publish_out. exchange.messagesPublishedQueuePerSecond Rate of messages published from this exchange into a queue per second. In RabbitMQ this is seen as message_stats.publish_out_details.rate. RabbitMQ queue sample event These attributes are attached to the RabbitmqQueueSample event type: Name Description queue.bindings Number of bindings for a specific queue. queue.countActiveConsumersReceiveMessages Number of active consumers that can immediately receive any messages sent to the queue. In RabbitMQ this is seen as active_consumers. queue.consumers Number of consumers per queue. In RabbitMQ this is seen as consumers. queue.consumerMessageUtilizationPerSecond The ratio of time that a queue's consumers can take new messages (utilization per second). In RabbitMQ this is seen as consumer_utilisation. This metric is only available on RabbitMQ version 3.3 or higher. queue.erlangBytesConsumedInBytes Bytes consumed by the Erlang process associated with the queue. In RabbitMQ this is seen as memory. queue.messagesReadyDeliveryClients Count of messages ready to be delivered to clients. In RabbitMQ this is seen as message_stats.messages_ready. queue.messagesReadyDeliveryClientsPerSecond Rate of messages ready to be delivered to clients per second. In RabbitMQ this is seen as message_stats.messages_ready_details.rate. queue.messagesReadyUnacknowledged Count of messages per queue delivered to clients but not yet acknowledged. In RabbitMQ this is seen as message_stats.deliver_no_ack. queue.messagesReadyUnacknowledgedPerSecond Rate of messages per queue delivered to clients but not yet acknowledged per second. In RabbitMQ this is seen as message_stats.deliver_no_ack_details.rate. queue.messagesAcknowledged Count of messages delivered to clients and acknowledged per queue. In RabbitMQ this is seen as message_stats.ack. queue.messagesAcknowledgedPerSecond Rate of messages delivered to clients and acknowledged per second per queue. In RabbitMQ this is seen as message_stats.ack_details.rate. queue.messagesDeliveredAckMode Count of messages delivered in acknowledgment mode to consumers per queue. In RabbitMQ this is seen as message_stats.deliver. queue.messagesDeliveredAckModePerSecond Rate of messages delivered in acknowledgment mode to consumers per queue per second. In RabbitMQ this is seen as message_stats.deliver_details.rate. queue.messagesPublished Count of messages published per queue. In RabbitMQ this is seen as message_stats.publish. queue.messagesPublishedPerSecond Rate of messages published per second per queue. In RabbitMQ this is seen as message_stats.publish_details.rate. queue.messagesRedeliverGet Count of subset of messages in acknowledgment mode which had the redelivered flag set per queue. In RabbitMQ this is seen as message_stats.redeliver. queue.messagesRedeliverGetPerSecond Rate of subset of messages in acknowledgment mode which had the redelivered flag set per queue per second. In RabbitMQ this is seen as message_stats.redeliver_details.rate. queue.sumMessagesDelivered Sum of messages delivered in acknowledgment mode to consumers, in no-acknowledgment mode to consumers, in acknowledgment mode in response to basic.get, and in no-acknowledgment mode in response to basic.get. per queue. In RabbitMQ this is seen as message_stats.deliver_get. queue.sumMessagesDeliveredPerSecond Rate per second of the sum of messages delivered in acknowledgment mode to consumers, in no-acknowledgment mode to consumers, in acknowledgment mode in response to basic.get, and in no-acknowledgment mode in response to basic.get per queue. In RabbitMQ this is seen as message_stats.deliver_get_details.rate. queue.totalMessages Count of the total messages in the queue. In RabbitMQ this is seen as messages. queue.totalMessagesPerSecond Rate of total messages in queue. In RabbitMQ this is seen as messages_details.rate. System metadata Other metadata includes: Name Description version.rabbitmq The version of the RabbitMQ server. Example: 3.6.7. version.management The version of the RabbitMQ management plugin. For example: 3.6.7. Inventory data The integration captures the configuration parameters of RabbitMQ in the /etc/rabbitmq/rabbitmq.conf file. Inventory data is only captured in RabbitMQ version 3.7 or higher; the inventory data will appear on the Infrastructure Inventory page, under the config/rabbitmq source. Caution Be aware that any sensitive information that you put into the rabbit.conf file will appear on the Infrastructure Inventory page. This includes items such as the following from AWS: cluster_formation.aws.secret_key, cluster_formation.aws.access_key_id Copy Troubleshooting Troubleshooting tips: Error getting local node name: exec: \\\"rabbitmqctl\\\": executable file not found in $PATH If you receive this error, it means that the RabbitMQ command line tool, rabbitmqctl, is not in the PATH of the root user. To correct this issue, execute the following command: find -name \"rabbitmqctl\" export PATH=\"$PATH:<LOCATION_TO_RABBITMQCTL> Copy Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.34177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "RabbitMQ monitoring <em>integration</em>",
        "sections": "RabbitMQ monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the rabbitmq-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results"
      },
      "id": "6045095928ccbcddbf2c60d4"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/run-integrations-manually": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 234.24019,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Nagios monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Nagios instance settings",
        "Service checks config file",
        "Labels/custom attributes",
        "Permissions",
        "Linux permissions",
        "Windows permissions",
        "Example configurations",
        "Example nagios-config.yml configuration",
        "Example nagios-service-checks.yml configuration",
        "Find and use data",
        "Metric data",
        "Nagios service check sample metrics",
        "Troubleshooting",
        "Config parsing failed error",
        "Solution:",
        "Cause:",
        "Check the source code"
      ],
      "title": "Nagios monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "98062fcf3378e6a1b075d73961c457be1f2b3e16",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration/",
      "published_at": "2021-09-14T20:51:21Z",
      "updated_at": "2021-09-14T20:51:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Nagios integration lets you use your service checks directly, without the need to run a Nagios instance. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with any existing service that conform to the Nagios Plugin API. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows OS version compatible with New Relic's infrastructure agent. Quick start Instrument your Nagios instance quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these buttons to try it out. Guided install EU Guided install Install and activate To install the Nagios integration: Linux installation Follow the instructions for installing an integration, using the file name nri-nagios. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp nagios-config.yml.sample nagios-config.yml Copy Create a copy of the sample service checks file by running: sudo cp nagios-service-checks.yml.sample nagios-service-checks.yml Copy Edit the nagios-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-nagios .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-nagios/nri-nagios-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-nagios-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp nagios-config.yml.sample nagios-config.yml Copy Edit the nagios-config.yml file as described in the configuration settings. In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a nagios-service-checks.yml file that describes the service checks to be run by the integration. For an example configuration, see the example service checks file. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For example configurations, see the nagios-config.yml and nagios-service-checks.yml examples. Our configuration files have common settings used in all of our integrations, such as interval, timeout, andinventory_source, among others. For more on these common settings, see this list of configuration properties. Specific settings related to Nagios are defined using the env section of the configuration file. These settings control the connection to your Nagios instance as well as other security settings and features. Nagios instance settings Setting Description Default SERVICE_CHECKS_CONFIG This points to a yaml file containing definitions of the service checks that will be run by the integration. Required. N/A CONCURRENCY The number of service checks to be run concurrently. 1 OUTPUT_TABLE_NAME The name of the table where the service check results are saved. NagiosServiceCheckSample Service checks config file The service_checks_config yaml file contains the top-level array service_checks. Each service check must contain both a name and a command. Key Description name The naming convention is not specific, and allows for easy recognition in the Infrastructure UI. command The command is an array of strings, with the first position containing the path to the executable and the remaining positions containing the arguments to the executable. labels A collection of key: value pairs which help to identify and group service checks in Insights. parse_output Attempts to parse the output of service checks that conform to the Nagios Plugin API spec. Default: false. These setting values can be defined in several ways: Add the values directly in the config file. Replace the values from environment variables using the {{}} notation. This requires Infrastructure agent v1.14.0+. Read more here. Use secrets management to protect sensible information, such as passwords, so that it's not exposed in plain text in the configuration file. For more information, see secrets management. Labels/custom attributes Environment variables can be used to control configuration settings, such as your license key, and are then passed to the Infrastructure agent. For instructions on how to use the passthrough feature, see configure the Infrastructure agent. You can also decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics. You can use these labels to query, filter, or group your metrics. Our default sample config file includes examples with labels, you can remove, modify, or add new ones of your choice. labels: env: production role: nagios Copy Permissions Non-configurable commands are run by the infrastructure agent, which itself is run by the root user. For the integration to run properly, ensure that the permissions on the yaml file are appropriately restrictive as indicated below: Linux permissions Set the user permissions flag to 0600, restricting read and write privileges to the file owner. If permissions do not meet this requirement, an error will be logged and the integration will fail to run. Windows permissions By default, the agent and any commands in the yaml file run as an Administrator. As the integration is unable to check permissions, it is up to the user to appropriately restrict permissions for the file. Example configurations Example file configurations: Example nagios-config.yml configuration integrations: - name: nri-nagios env: CONCURRENCY: \"1\" SERVICE_CHECKS_CONFIG: /etc/newrelic-infra/integrations.d/nagios-service-checks.yml interval: 15s Copy Example nagios-service-checks.yml configuration service_checks: - name: check_users command: [\"/usr/local/nagios/libexec/check_users\", \"-w\", \"5\", \"-c\", \"10\"] parse_output: true labels: env: staging key1: val1 - name: check_yum command: [\"/usr/local/nagios/libexec/check_yum\"] Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Nagios integration links. Nagios data is attached to the NagiosServiceCheckSample event type. For more on how to find and use your data, see Understand integration data. Metric data The Nagios integration collects the following metric data attributes. Nagios service check sample metrics These attributes can be found by querying the NagiosServiceCheckSample event types in Insights. Metric Description serviceCheck.command The command used to run the service check. serviceCheck.error The standard error (stderr) output of the service check. serviceCheck.longServiceOutput The portion of the message that is parsed by Nagios as $LONGSERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.message The standard output (stdout) of the service check. serviceCheck.name The descriptive name of the service check being performed. serviceCheck.serviceOutput The portion of the message that is parsed by Nagios as $SERVICEOUTPUT$. Only enabled if parse_output is set. serviceCheck.status The return code of the service check. Options: 0 = Ok 1 = Warning 2 = Critical 3 = Unknown * Any additional metrics defined and reported by the service check. Only enabled if parse_output is set. Troubleshooting Troubleshooting tips: Config parsing failed error The following error appears in the log file: Config parsing failed: service checks file permissions are not restrictive enough. Required file permissions are 0600. See documentation for details Copy Solution: Set the user permissions flag to 0600, restricting read and write privileges to the file owner. Cause: If the file is not owned by the root user or the file can be written to by a user other than the root user, the integration will allow users to run arbitrary commands as though they are a root user. If permissions do not meet the requirement, an error will be logged and the integration will fail to run. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.47456,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Nagios monitoring <em>integration</em>",
        "sections": "Nagios monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". This gives you full control over the installation and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the Infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can"
      },
      "id": "603eb881e7b9d2728a2a07b5"
    },
    {
      "sections": [
        "RabbitMQ monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Important",
        "Configure the integration",
        "RabbitMQ Instance Settings",
        "Labels/Custom Attributes",
        "Example configuration",
        "BASIC CONFIGURATION WITH SSL",
        "METRICS ONLY",
        "INVENTORY ONLY",
        "EVENTS ONLY",
        "Find and use data",
        "Metric data",
        "RabbitMQ vhost sample event",
        "RabbitMQ node sample event",
        "RabbitMQ exchange sample event",
        "RabbitMQ queue sample event",
        "System metadata",
        "Inventory data",
        "Caution",
        "Troubleshooting",
        "Error getting local node name: exec: \\\"rabbitmqctl\\\": executable file not found in $PATH",
        "Check the source code",
        "For more help"
      ],
      "title": "RabbitMQ monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "b321df4ac10040b755a7cf7445eca43688eee9d2",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/rabbitmq-monitoring-integration/",
      "published_at": "2021-09-14T20:43:01Z",
      "updated_at": "2021-09-14T20:43:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic RabbitMQ on-host integration reports metrics and configuration data from your RabbitMQ service, including important metrics relating to the cluster, vhosts, queues, exchanges, and consumers. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with: RabbitMQ version 3.0 or higher for Metric data. RabbitMQ version 3.7 or higher for Inventory data. Before installing the integration, make sure that you meet the following requirements: RabbitMQ Management Plugin configured. RabbitMQ command line tool, rabbitmqctl, in the PATH of the root user. If RabbitMQ is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host running RabbitMQ. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your RabbitMQ service quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the RabbitMQ integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your RabbitMQ service. Install and activate To install the RabbitMQ integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-rabbitmq. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp rabbitmq-config.yml.sample rabbitmq-config.yml Copy Edit the rabbitmq-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-rabbitmq .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-rabbitmq/nri-rabbitmq-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-rabbitmq-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp rabbitmq-config.yml.sample rabbitmq-config.yml Copy Edit the rabbitmq-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Important This integration can monitor only one RabbitMQ server instance per host. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, rabbitmq-config.yml. For an example configuration, see Example config file. The configuration file has common settings applicable to all integrations like interval, timeout, inventory_source. To read all about these common settings refer to our Configuration Format document. Important If you are still using our Legacy configuration/definition files please refer to this document for help. Specific settings related to RabbitMQ are defined using the env section of the configuration file. These settings control the connection to your RabbitMQ instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Cluster environments In cluster environments, only one node should use METRICS=false and INVENTORY=false ; the rest should use INVENTORY=true and METRICS=false. If you're running a cluster environment in Kubernetes, you need to deploy RabbitMQ as a StatefulSet, and configure the agent to query all the metrics from the RabbitMQ pod. Set the autodiscovery matching condition in the config file to this value: discovery: command: exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: podName: rabbitmq-0 Copy RabbitMQ Instance Settings The RabbitMQ integration collects both Metrics(M) and Inventory(I) information. Check the Applies To column below to find which settings can be used for each specific collection: Setting Description Default Applies To HOSTNAME Hostname or IP of the RabbitMQ management plugin. localhost M/I PORT Port number of the RabbitMQ management plugin. 15672 M/I USERNAME User that is connecting to RabbitMQ management plugin. N/A M/I PASSWORD Password to connect to RabbitMQ management plugin. N/A M/I MANAGEMENT_PATH_PREFIX RabbitMQ Management Prefix. N/A M/I USE_SSL Option to connect using SSL. false M/I CA_BUNDLE_DIR Location of SSL certificate on the host. N/A M/I CA_BUNDLE_FILE Location of SSL certificate on the host. N/A M/I NODE_NAME_OVERRIDE Overrides the local node name instead of retrieving it from RabbitMQ. N/A M/I CONFIG_PATH Absolute path to the RabbitMQ configuration file. N/A M/I QUEUES Queue names to collect in the format of a JSON array of strings. If a queue name exactly matches (case sensitive) one of these, it will be collected. Example: queues: '[\"myQueue1\",\"myQueue2\"]' Copy N/A I QUEUES_REGEXES Queue names to collect in the format of a JSON array of REGEX strings. If a queue name matches one of these, it will be collected. Example: queues_regexes: '[\"queue[0-9]+\",\".*\"]' Copy N/A M/I EXCHANGES Exchange names to collect in the format of a JSON array of strings. If an exchange name exactly matches (case sensitive) one of these, it will be collected. N/A M/I EXCHANGES_REGEXES Exchanges names to collect in the format of a JSON array of REGEX strings. If an exchange name matches one of these it will be collected. N/A M/I VHOSTS Vhost names to collect in the format of a JSON array of strings. If a vhost name exactly matches (case sensitive) one of these, it will be included. This also affects only collecting entities belonging to vhosts specified. N/A M/I VHOSTS_REGEXES Vhost names to collect in the format of a JSON array of REGEX strings. If a vhost name matches one of these the vhost and any entities belonging to this vhost will be collected. N/A M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false EVENTS Set to true to enable Events only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0+. Read more here. Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see Secrets management. Labels/Custom Attributes Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see Configure the Infrastructure agent. You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice. labels: env: production role: rabbitmq Copy Example configuration Here's an example configuration file: BASIC CONFIGURATION WITH SSL integrations: - name: nri-rabbitmq env: HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin CA_BUNDLE_DIR: /path/to/bundle/dir/ NODE_NAME_OVERRIDE: local_node_name CONFIG_PATH: /path/to/config/file/rabbitmq.conf USE_SSL: true QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy METRICS ONLY integrations: - name: nri-rabbitmq env: METRICS: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy INVENTORY ONLY integrations: - name: nri-rabbitmq env: INVENTORY: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name CONFIG_PATH: /path/to/config/file/rabbitmq.conf QUEUES: '[\"myQueue1\",\"myQueue2\"]' QUEUES_REGEXES: '[\"queue[0-9]+\",\".*\"]' EXCHANGES: '[\"exchange1\",\"exchange2\"]' EXCHANGES_REGEXES: '[\"exchange[0-9]+\",\".*\"]' VHOSTS: '[\"vhost1\",\"vhost2\"]' VHOSTS_REGEXES: '[\"vhost[0-9]+\",\".*\"]' interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy EVENTS ONLY integrations: - name: nri-rabbitmq env: EVENTS: true HOSTNAME: localhost PORT: 15672 USERNAME: admin PASSWORD: admin NODE_NAME_OVERRIDE: local_node_name interval: 15s labels: env: production role: rabbitmq inventory_source: config/rabbitmq Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to these event types: RabbitmqVhostSample RabbitmqNodeSample RabbitmqExchangeSample RabbitmqQueueSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The RabbitMQ integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as queue. or node.. RabbitMQ vhost sample event These attributes are attached to the RabbitmqVhostSample event type: Name Description vhost.connectionsBlocked Number of current connections in the state blocked. vhost.connectionsBlocking Number of current connections in the state blocking. vhost.connectionsClosed Number of current connections in the state closed. vhost.connectionsClosing Number of current connections in the state closing. vhost.connectionsFlow Number of current connections in the state flow. vhost.connectionsOpening Number of current connections in the state opening. vhost.connectionsRunning Number of current connections in the state running. vhost.connectionsStarting Number of current connections in the state starting. vhost.connectionsTotal Number of current connections to a given rabbitmq vhost. vhost.connectionsTuning Number of current connections in the state tuning. RabbitMQ node sample event These attributes are attached to the RabbitmqNodeSample event type: Name Description node.averageErlangProcessesWaiting Average number of Erlang processes waiting to run. In RabbitMQ this is seen as run_queue. node.diskAlarm Node disk alarm (0 or 1). 0 shows that the alarm is not tripped and 1 shows that the alarm is tripped. In RabbitMQ this is seen as disk_free_alarm. node.diskSpaceFreeInBytes Current free disk space in bytes. In RabbitMQ this is seen as disk_free. node.fileDescriptorsTotalUsed The total count of file descriptors used. In RabbitMQ this is seen as fd_used. node.fileDescriptorsUsedSockets Number of file descriptors used as sockets. In RabbitMQ this is seen as sockets_used. node.hostMemoryAlarm Host memory alarm (0 or 1). 0 shows that the alarm is not tripped and 1 shows that the alarm is tripped. In RabbitMQ this is seen as mem_alarm. node.totalMemoryUsedInBytes Memory used in bytes. In RabbitMQ this is seen as mem_used. node.paritionsSeen Number of network partitions seen per node. In RabbitMQ this is seen as partitions. node.running Node running (0 or 1). 0 shows that the node is not running and 1 shows that the node is running. In RabbitMQ this is seen as running. RabbitMQ exchange sample event These attributes are attached to the RabbitmqExchangeSample event type: Name Description exchange.bindings Number of bindings for a specific exchange. exchange.messagesDeliveredAcknowledged Count of messages delivered to clients and acknowledged per exchange. exchange.messagesDeliveredAcknowledgedPerSecond Rate of messages delivered to clients and acknowledged per exchange per second. exchange.messagesConfirmed Count of messages confirmed per exchange. exchange.messagesConfirmedPerSecond Rate of messages confirmed per exchange per second. exchange.messagesRedelivered Count of subset of messages in deliver_get which had the redelivered flag set per queue. exchange.messagesRedeliveredPerSecond Rate of subset of messages in deliver_get which had the redelivered flag set per queue per second. exchange.messagesReturnedUnroutable Count of messages returned to publisher as unroutable per exchange. exchange.messagesReturnedUnroutablePerSecond Rate of messages returned to publisher as unroutable per exchange per second. exchange.messagesPublished Count of messages published per exchange. exchange.messagesPublishedPerSecond Rate of messages published per exchange queue per second. exchange.messagesPublishedPerChannel Count of messages published from a channel into this exchange. In RabbitMQ this is seen as message_stats.publish_in. exchange.messagesPublishedPerChannelPerSecond Rate of messages published from a channel into this exchange per sec. In RabbitMQ this is seen as message_stats.publish_in_details.rate. exchange.messagesPublishedQueue Count of messages published from this exchange into a queue. In RabbitMQ this is seen as message_stats.publish_out. exchange.messagesPublishedQueuePerSecond Rate of messages published from this exchange into a queue per second. In RabbitMQ this is seen as message_stats.publish_out_details.rate. RabbitMQ queue sample event These attributes are attached to the RabbitmqQueueSample event type: Name Description queue.bindings Number of bindings for a specific queue. queue.countActiveConsumersReceiveMessages Number of active consumers that can immediately receive any messages sent to the queue. In RabbitMQ this is seen as active_consumers. queue.consumers Number of consumers per queue. In RabbitMQ this is seen as consumers. queue.consumerMessageUtilizationPerSecond The ratio of time that a queue's consumers can take new messages (utilization per second). In RabbitMQ this is seen as consumer_utilisation. This metric is only available on RabbitMQ version 3.3 or higher. queue.erlangBytesConsumedInBytes Bytes consumed by the Erlang process associated with the queue. In RabbitMQ this is seen as memory. queue.messagesReadyDeliveryClients Count of messages ready to be delivered to clients. In RabbitMQ this is seen as message_stats.messages_ready. queue.messagesReadyDeliveryClientsPerSecond Rate of messages ready to be delivered to clients per second. In RabbitMQ this is seen as message_stats.messages_ready_details.rate. queue.messagesReadyUnacknowledged Count of messages per queue delivered to clients but not yet acknowledged. In RabbitMQ this is seen as message_stats.deliver_no_ack. queue.messagesReadyUnacknowledgedPerSecond Rate of messages per queue delivered to clients but not yet acknowledged per second. In RabbitMQ this is seen as message_stats.deliver_no_ack_details.rate. queue.messagesAcknowledged Count of messages delivered to clients and acknowledged per queue. In RabbitMQ this is seen as message_stats.ack. queue.messagesAcknowledgedPerSecond Rate of messages delivered to clients and acknowledged per second per queue. In RabbitMQ this is seen as message_stats.ack_details.rate. queue.messagesDeliveredAckMode Count of messages delivered in acknowledgment mode to consumers per queue. In RabbitMQ this is seen as message_stats.deliver. queue.messagesDeliveredAckModePerSecond Rate of messages delivered in acknowledgment mode to consumers per queue per second. In RabbitMQ this is seen as message_stats.deliver_details.rate. queue.messagesPublished Count of messages published per queue. In RabbitMQ this is seen as message_stats.publish. queue.messagesPublishedPerSecond Rate of messages published per second per queue. In RabbitMQ this is seen as message_stats.publish_details.rate. queue.messagesRedeliverGet Count of subset of messages in acknowledgment mode which had the redelivered flag set per queue. In RabbitMQ this is seen as message_stats.redeliver. queue.messagesRedeliverGetPerSecond Rate of subset of messages in acknowledgment mode which had the redelivered flag set per queue per second. In RabbitMQ this is seen as message_stats.redeliver_details.rate. queue.sumMessagesDelivered Sum of messages delivered in acknowledgment mode to consumers, in no-acknowledgment mode to consumers, in acknowledgment mode in response to basic.get, and in no-acknowledgment mode in response to basic.get. per queue. In RabbitMQ this is seen as message_stats.deliver_get. queue.sumMessagesDeliveredPerSecond Rate per second of the sum of messages delivered in acknowledgment mode to consumers, in no-acknowledgment mode to consumers, in acknowledgment mode in response to basic.get, and in no-acknowledgment mode in response to basic.get per queue. In RabbitMQ this is seen as message_stats.deliver_get_details.rate. queue.totalMessages Count of the total messages in the queue. In RabbitMQ this is seen as messages. queue.totalMessagesPerSecond Rate of total messages in queue. In RabbitMQ this is seen as messages_details.rate. System metadata Other metadata includes: Name Description version.rabbitmq The version of the RabbitMQ server. Example: 3.6.7. version.management The version of the RabbitMQ management plugin. For example: 3.6.7. Inventory data The integration captures the configuration parameters of RabbitMQ in the /etc/rabbitmq/rabbitmq.conf file. Inventory data is only captured in RabbitMQ version 3.7 or higher; the inventory data will appear on the Infrastructure Inventory page, under the config/rabbitmq source. Caution Be aware that any sensitive information that you put into the rabbit.conf file will appear on the Infrastructure Inventory page. This includes items such as the following from AWS: cluster_formation.aws.secret_key, cluster_formation.aws.access_key_id Copy Troubleshooting Troubleshooting tips: Error getting local node name: exec: \\\"rabbitmqctl\\\": executable file not found in $PATH If you receive this error, it means that the RabbitMQ command line tool, rabbitmqctl, is not in the PATH of the root user. To correct this issue, execute the following command: find -name \"rabbitmqctl\" export PATH=\"$PATH:<LOCATION_TO_RABBITMQCTL> Copy Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.34177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "RabbitMQ monitoring <em>integration</em>",
        "sections": "RabbitMQ monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " the rabbitmq-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results"
      },
      "id": "6045095928ccbcddbf2c60d4"
    }
  ],
  "/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.54074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and <em>use</em> your <em>data</em>, see <em>Understand</em> integration <em>data</em>. Metric <em>data</em> The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can <em>use</em> this metadata in NRQL queries"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Remote monitoring in on-host integrations",
        "Important",
        "Effects of activating remote_monitoring",
        "Alert verification",
        "New entity attributes",
        "Changes in recorded metrics",
        "Unrecorded attributes",
        "Updated hostname"
      ],
      "title": "Remote monitoring in on-host integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "1cfea4c65b855ce9ac5078d2a36ba11b63a6101b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations/",
      "published_at": "2021-09-13T17:08:25Z",
      "updated_at": "2021-03-16T06:05:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has data you can monitor. Integrations can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an integration will be considered a local entity, and the data related to it will be attached to the host entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis integrations, remote monitoring (and multi-tenancy) is enabled by activating the configuration parameter remote_monitoring. Important If your Apache, Cassandra, MySQL, NGINX, or Redis service is located in the same host as the agent, when you activate remote monitoring the resulting entity will be considered as remote, regardless of its actual location. This may affect alerts, alter attributes, and have other effects, as explained here. Effects of activating remote_monitoring By enabling remote_monitoring, the integration becomes a different entity which is no longer attached to the infrastructure agent. As a result, the following items may be affected: Alert verification Enabling remote monitoring can affect your configured alerts in case they are using any of the values that are affected by this new feature. We strongly recommend checking your existing alerts to make sure they keep on working as expected. New entity attributes These attributes are modified in the resulting entity: Display name: New entity unique key (instead of using the display name) Entity GUID: New entity GUID Entity ID: New entity ID Entity key: New entity unique key (instead of using the display name) External key: Using integration entity name (instead of using the agent display) Changes in recorded metrics When remote monitoring is enabled, we will add the hostname and port values to all metrics. If the nricluster name or nriservice are defined in the integration configuration file, they will also be decorated. Unrecorded attributes Since the integration is now an independent entity which is not attached to the agent, the following agent attributes are not collected: agentName agentVersion coreCount criticalViolationCount fullHostname instanceType kernelVersion linuxDistribution entityType operatingSystem processorCount systemMemoryBytes warningViolationCount Your custom attributes Updated hostname For the ApacheSample, RedisSample, CassandraSample, and NginxSample integration metrics, we will use the integration configuration hostname instead of the short hostname from the agent. When the integration hostname is a loopback address, the agent will replace it in order to guarantee uniqueness.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.13126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Remote monitoring in <em>on</em>-<em>host</em> <em>integrations</em>",
        "sections": "Remote monitoring in <em>on</em>-<em>host</em> <em>integrations</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " will be considered a local entity, and the <em>data</em> related to it will be attached to the <em>host</em> entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis <em>integrations</em>, remote monitoring (and multi-tenancy"
      },
      "id": "603ec000e7b9d216732a07ef"
    },
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-09-13T17:13:45Z",
      "updated_at": "2021-08-08T13:47:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration condition.{conditionName}={conditionValue} Status of the current observed node condition. The reported conditions can vary depending on your Kubernetes flavor and installed operators. Examples of common conditions are: Ready, DiskPressure, MemoryPressure, PIDPressure and NetworkUnavailable. Condition values can be 1 (true), 0 (false), or -1 (unknown). cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem unschedulable Status of node schedulability of new pods. Its value can be 0 (false) or 1 (true) label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podIP IP address of the pod. If it doesn't have an IP, it'll be empty podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.24103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": " Relic integration you <em>use</em>, including the Kubernetes integration: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a Kubernetes (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message to the notification"
      },
      "id": "603eb9a4196a678bfca83dbb"
    }
  ],
  "/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations": [
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "JMX Instance Settings",
        "Labels and custom attributes",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-09-19T01:54:14Z",
      "updated_at": "2021-09-19T01:54:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: A New Relic account. Don't have one? Sign up for free! No credit card required. If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files The configuration file has common settings applicable to all integrations, such as interval, timeout, or inventory_source. To read all about these common settings refer to our configuration format document. Important If you're still using our legacy configuration/definition files, refer to the standard configuration doc for help. Specific settings related to JMX are defined using the env section of the configuration file. These settings control the connection to your JMX instance as well as other security settings and features. The list of valid settings is described in the next section of this document. Config options are below. For an example configuration, see the example config file. JMX Instance Settings The JMX integration collects both metrics (M) and inventory (I) information. Check the Applies To column below to find which settings can be used for each specific collection. For an example, see the host connection file example. Setting Description Default Applies To JMX_HOST The host JMX is running on. localhost M/I JMX_PORT The port JMX is running on. 9999 M/I JMX_URI_PATH The path portion of the JMX Service URI. This is useful for nonstandard service uris. N/A M/I JMX_USER The username for the JMX connection. N/A M/I JMX_PASS The password for the JMX connection. N/A M/I JMX_REMOTE (JBoss specific) Whether or not to use the JMX remote URL connection format. Connection defaults to JBoss Domain-mode if true. false M/I JMX_REMOTE_JBOSS_STANDLONE (JBoss specific) Whether or not to use the JBoss standalone connection format. Only relevant if jmx_remote is set. false M/I CONNECTION_URL Full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" N/A M/I COLLECTION_FILES A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. N/A M/I KEY_STORE The filepath of the keystore containing the JMX client's SSL certificate. N/A M/I KEY_STORE_PASSWORD The password for the SSL key store. N/A M/I LOCAL_ENTITY Collect all metrics on the local entity. Only use when monitoring localhost. false M/I TIMEOUT The timeout for individual JMX queries, in milliseconds. 10000 M/I TRUST_STORE The filepath of the keystore containing the JMX server's SSL certificate. N/A M/I TRUST_STORE_PASSWORD The password for the trust store. N/A M/I METRIC_LIMIT Number of metrics that can be collected per entity. If this limit is exceeded the entity will not be reported. A limit of 0 implies no limit. 200 M/I METRICS Set to true to enable Metrics only collection. false INVENTORY Set to true to enable Inventory only collection. false The values for these settings can be defined in several ways: Adding the value directly in the config file. This is the most common way. Replacing the values from environment variables using the {{}} notation. This requires infrastructure agent v1.14.0 or higher. Read more here or see the example below. Using secrets management. Use this to protect sensible information (such as passwords) from being exposed in plain text on the configuration file. For more information, see secrets management. Labels and custom attributes You can further decorate your metrics using labels. Labels allow you to add key/value pair attributes to your metrics, which you can then use to query, filter or group your metrics on. Our default sample config file includes examples of labels. These aren't mandatory, and you can remove, modify, or add new ones of your choice. labels: env: production role: jmx Copy Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integrations: - name: nri-jmx env: COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" JMX_HOST: jmx-host.localnet JMX_PASS: admin JMX_PORT: 9999 JMX_USER: admin interval: 15s labels: env: production Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.54074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and <em>use</em> your <em>data</em>, see <em>Understand</em> integration <em>data</em>. Metric <em>data</em> The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can <em>use</em> this metadata in NRQL queries"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-09-13T17:06:48Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.62576,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>On</em>-<em>host</em> <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "<em>On</em>-<em>host</em> <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-<em>host</em> <em>integrations</em> can help you customize your integration, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-<em>host</em> <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-<em>host</em> integration. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-09-13T17:13:45Z",
      "updated_at": "2021-08-08T13:47:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration condition.{conditionName}={conditionValue} Status of the current observed node condition. The reported conditions can vary depending on your Kubernetes flavor and installed operators. Examples of common conditions are: Ready, DiskPressure, MemoryPressure, PIDPressure and NetworkUnavailable. Condition values can be 1 (true), 0 (false), or -1 (unknown). cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem unschedulable Status of node schedulability of new pods. Its value can be 0 (false) or 1 (true) label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podIP IP address of the pod. If it doesn't have an IP, it'll be empty podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.24103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": " Relic integration you <em>use</em>, including the Kubernetes integration: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a Kubernetes (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message to the notification"
      },
      "id": "603eb9a4196a678bfca83dbb"
    }
  ],
  "/docs/integrations/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard": [
    {
      "sections": [
        "Configure polling frequency and data collection for cloud integrations",
        "Overview of settings",
        "Caution",
        "Change polling frequency",
        "Specify data to be fetched",
        "Data collection",
        "Filters",
        "Potential impact on alerts and charts"
      ],
      "title": "Configure polling frequency and data collection for cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "b900b7545f9032201c212449be114e10176bf789",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations/",
      "published_at": "2021-09-13T17:19:27Z",
      "updated_at": "2021-07-27T15:37:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our cloud integrations get data from cloud provider APIs. In New Relic, you can change some of the data collection-related settings for your cloud integrations. Read on to see what changes you can make and the reasons for making them. Overview of settings New Relic cloud integrations get data from cloud providers' APIs. Data is generally collected from monitoring APIs such as AWS CloudWatch, Azure Monitor, and GCP Stackdriver, and inventory metadata is collected from the specific services' APIs. You can use the account status dashboard to see how your cloud integrations are handling data from a cloud service provider. If you want to report more or less data from your cloud integrations, or if you need to control the use of the cloud providers' APIs to prevent reaching rate and throttling limits in your cloud account, you can change the configuration settings to modify the amount of data they report. The two main controls are: Change polling frequency Change what data is reported Examples of business reasons for wanting to change your polling frequency include: Billing: If you need to manage your AWS CloudWatch bill, you may want to decrease the polling frequency. Before you do this, make sure that any alert conditions set for your cloud integrations are not affected by this reduction. New services: If you are deploying a new service or configuration and you want to collect data more often, you may want to increase the polling frequency temporarily. Caution Changing the configuration settings for your integrations may impact alert conditions and chart trends. Change polling frequency The polling frequency configuration determines how often New Relic reports data from your cloud provider for each service. By default, the polling frequency is set to the maximum frequency that is available for each service. To change the polling frequency for a cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Use the dropdowns next to Data polling interval every to select how frequently you want New Relic to capture your cloud integration data. Specify data to be fetched You can specify which information you want captured for your cloud integration by enabling the collection of additional data and by applying multiple filters to each integration. To change this settings for your cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Under Data collections and filters, turn the toggles you want On. For filters, select or enter the values that you want included in your reported data. Data collection For some cloud integrations, an additional number of calls to the cloud provider APIs are needed in order to collect data. For example, to fetch tags for AWS Elastic Map Reduce clusters, an additional call to the service API is required. To better control the amount of API calls that are sent to your cloud account for these integrations, you can specify when you need these types of data to be collected. Different data collection toggles are available, depending on the integration. Toggle Description Collect tags Some integrations require additional API calls to the cloud provider to report tags. Tag collection is enabled by default. Switch this to Off if you don't want the integration to collect your cloud resource tags and thus reduce the volume of API calls. Collect extended inventory Some integrations can collect extended inventory metadata about your cloud resources by making additional API calls to the cloud provider. The metadata included within the extended inventory for each cloud integration is described in the integration documentation. Extended inventory collection is disabled by default. Switch this to On if you want to monitor extended inventory. This will increase the volume of API calls. Collect shards data Available for AWS Kinesis Streams integration. By default, we don't report shard metrics. Switch this to On if you want to monitor shard metrics in addition to data stream metrics. Collect Lambda@Edge data Available for AWS CloudFront integration. By default, we don't report Lambda@Edge data. Switch this to On if you're using Lambda@Edge in AWS CloudFront and want to get Lambda execution location metadata. Collect node data Available for AWS Elasticsearch integration. By default, we don't report Elasticsearch node metrics. Switch this to On if you want to monitor node metrics in addition to cluster metrics. Collect NAT Gateway data and Collect VPN data Available for AWS VPC integration. By default, we don't report NAT Gateway nor VPN metrics. Switch these to On if you want to monitor NAT Gateway and VPN metrics and inventory, in addition to other VPC related entities inventory. Collect IP addresses Available for AWS EC2 integration. By default, we collect EC2 instance metadata that includes public and private IP addresses, and network interface details. Switch this to Off if you don't want New Relic to store and display these IP data. Filters When a filter is On, you specify the data that you want to be collected; for example, if the Limit to AWS region is On, the regions that you select will be the ones that data will be collected for. There are different filters available, depending on the integration: Filter Description Region Select the regions that include the resources that you want to monitor. Queue prefixes Available for AWS SQS integration. Enter each name or prefix for the queues that you want to monitor. Filter values are case-sensitive. Load balancer prefixes Available for AWS ALB integration. Enter each name or prefix for the application load balancers that you want to monitor. Filter values are case-sensitive. Stage name prefixes Available for AWS API Gateway integration. Enter each name or prefix for the stages that you want to monitor. Filter values are case-sensitive. Tag key Enter one tag key that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag value filter. Tag value Enter one tag value that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag key. Resource group Select the resource groups that are associated with the resources that you want to monitor. Potential impact on alerts and charts If you change an integration's configuration, it can impact alert conditions and charts. Here are some things to consider: If you change this setting... It may have this impact... Any configuration setting When you change the configuration settings, the data that New Relic displays in infrastructure charts, on the inventory page, and in the events feed changes as well. Any filters When you create alert conditions after you set filters, make sure that your alerts are not triggered by resources that you filtered out. Filter for regions If you filter for specific regions, it may lower the amount of data reported to New Relic, which could trigger an alert. If you create an alert condition for a specific region and then filter that region out, the region would no longer report data and would never trigger the alert. Polling frequency When you create an alert, make sure that you define the threshold for a time period that is longer than the polling frequency. Tags and extended inventory If you turn on tags and/or extended inventory, New Relic makes more API calls to the cloud provider, which could increase your cloud provider API usage bill.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "sections": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Our <em>cloud</em> <em>integrations</em> get data from <em>cloud</em> provider APIs. In New Relic, you can change some of the data collection-related settings for your <em>cloud</em> <em>integrations</em>. Read on to see what changes you can make and the reasons for making them. Overview of settings New Relic <em>cloud</em> <em>integrations</em> get data from"
      },
      "id": "603e8eef64441fcc7e4e8853"
    },
    {
      "sections": [
        "Metric data gaps with cloud integrations",
        "Problem",
        "Solution",
        "Amazon (AWS)",
        "Microsoft Azure",
        "Google Cloud Platform (GCP)",
        "Tip",
        "Cause"
      ],
      "title": "Metric data gaps with cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "ee3473d0cbc9059b6b36503949d08d1e76c7fc06",
      "image": "https://docs.newrelic.com/static/dfa79b9e3086b81f216d306ba0afe557/c1b63/screen-metric-gap.png",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations/",
      "published_at": "2021-09-13T17:19:26Z",
      "updated_at": "2021-03-29T21:18:28Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You've set up your AWS, Azure, or GCP integration and are monitoring your metrics. However, you notice gaps in your metric data charts. This screenshot shows a metric data chart with gaps. Solution Here’s a list of metrics which might show gaps in your metric data. If possible, avoid setting up alerts for these metrics because we know they can generate false positives. Amazon (AWS) Integration Provider Event Type Metric SNS SnsTopic QueueSample provider.subscriptionsConfirmed SnsTopic QueueSample provider.subscriptionsPending SnsTopic QueueSample provider.subscriptionsDeleted EFS EfsFileSystem BlockDeviceSample provider.lastKnownSizeInBytes ECS EcsCluster ComputeSample provider.registeredContainerInstancesCount EcsCluster ComputeSample provider.activeServicesCount EcsCluster ComputeSample provider.pendingTasksCount EcsCluster ComputeSample provider.runningTasksCount EcsService ComputeSample provider.pendingCount EcsService ComputeSample provider.runningCount EcsService ComputeSample provider.desiredCount DynamoDB DynamoDbTable DatastoreSample provider.itemCount DynamoDbTable DatastoreSample provider.tableSizeBytes AutoScaling AutoScalingInstance AutoScalingInstanceSample healthStatus Billing BillingBudget FinanceSample provider.actualAmount Billingbudget FinanceSample provider.forecastedAmount BillingBudget FinanceSample provider.limitAmount Microsoft Azure Integration Provider Event Type Metric SQL AzureSqlDatabase AzureSqlDatabaseSample databaseSizeCurrentBytes AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google Cloud Platform (GCP) Tip We're currently reviewing the GCP metrics that can cause data gaps. Tip This list isn't complete. We're currently reviewing the full list of metrics that can cause data gaps. Cause Some metrics aren’t present in the usual cloud provider APIs (CloudWatch, Stackdriver, Azure Monitor) and are fetched from the service APIs instead. Each cloud service provider has a unique service API that processes data and interacts with the service. For example, if a metric isn’t present in AWS CloudWatch, New Relic will fetch the metric from the AWS ECS service API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.61795,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "sections": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google <em>Cloud</em> Platform (GCP) Tip We&#x27;re currently reviewing the GCP metrics that can cause data gaps. Tip This list isn&#x27;t complete. We&#x27;re currently"
      },
      "id": "603e821e196a67a042a83df3"
    },
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-09-13T16:54:31Z",
      "updated_at": "2021-08-27T04:53:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are infrastructure monitoring integrations that can be run directly on your host or server: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including NGINX, MySQL, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.72188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "New Relic offers various <em>integrations</em> for reporting data to our platform. One category of <em>integrations</em> is our <em>Infrastructure</em> <em>integrations</em>. Types of <em>infrastructure</em> <em>integrations</em> New Relic has two main categories of <em>infrastructure</em> <em>integrations</em>: <em>cloud</em> and on-host. <em>Cloud</em> <em>integrations</em> <em>Cloud</em> <em>integrations</em>"
      },
      "id": "60450a39e7b9d2de845799cd"
    }
  ],
  "/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations": [
    {
      "sections": [
        "Metric data gaps with cloud integrations",
        "Problem",
        "Solution",
        "Amazon (AWS)",
        "Microsoft Azure",
        "Google Cloud Platform (GCP)",
        "Tip",
        "Cause"
      ],
      "title": "Metric data gaps with cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "ee3473d0cbc9059b6b36503949d08d1e76c7fc06",
      "image": "https://docs.newrelic.com/static/dfa79b9e3086b81f216d306ba0afe557/c1b63/screen-metric-gap.png",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations/",
      "published_at": "2021-09-13T17:19:26Z",
      "updated_at": "2021-03-29T21:18:28Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You've set up your AWS, Azure, or GCP integration and are monitoring your metrics. However, you notice gaps in your metric data charts. This screenshot shows a metric data chart with gaps. Solution Here’s a list of metrics which might show gaps in your metric data. If possible, avoid setting up alerts for these metrics because we know they can generate false positives. Amazon (AWS) Integration Provider Event Type Metric SNS SnsTopic QueueSample provider.subscriptionsConfirmed SnsTopic QueueSample provider.subscriptionsPending SnsTopic QueueSample provider.subscriptionsDeleted EFS EfsFileSystem BlockDeviceSample provider.lastKnownSizeInBytes ECS EcsCluster ComputeSample provider.registeredContainerInstancesCount EcsCluster ComputeSample provider.activeServicesCount EcsCluster ComputeSample provider.pendingTasksCount EcsCluster ComputeSample provider.runningTasksCount EcsService ComputeSample provider.pendingCount EcsService ComputeSample provider.runningCount EcsService ComputeSample provider.desiredCount DynamoDB DynamoDbTable DatastoreSample provider.itemCount DynamoDbTable DatastoreSample provider.tableSizeBytes AutoScaling AutoScalingInstance AutoScalingInstanceSample healthStatus Billing BillingBudget FinanceSample provider.actualAmount Billingbudget FinanceSample provider.forecastedAmount BillingBudget FinanceSample provider.limitAmount Microsoft Azure Integration Provider Event Type Metric SQL AzureSqlDatabase AzureSqlDatabaseSample databaseSizeCurrentBytes AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google Cloud Platform (GCP) Tip We're currently reviewing the GCP metrics that can cause data gaps. Tip This list isn't complete. We're currently reviewing the full list of metrics that can cause data gaps. Cause Some metrics aren’t present in the usual cloud provider APIs (CloudWatch, Stackdriver, Azure Monitor) and are fetched from the service APIs instead. Each cloud service provider has a unique service API that processes data and interacts with the service. For example, if a metric isn’t present in AWS CloudWatch, New Relic will fetch the metric from the AWS ECS service API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.61795,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "sections": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google <em>Cloud</em> Platform (GCP) Tip We&#x27;re currently reviewing the GCP metrics that can cause data gaps. Tip This list isn&#x27;t complete. We&#x27;re currently"
      },
      "id": "603e821e196a67a042a83df3"
    },
    {
      "sections": [
        "Cloud integrations: Account status dashboard",
        "Why it matters",
        "Understand dashboard data",
        "Find account status dashboard"
      ],
      "title": "Cloud integrations: Account status dashboard",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "3457b96bec1ab5f0252c061f4ef82bc15f9b61e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard/",
      "published_at": "2021-09-13T17:18:13Z",
      "updated_at": "2021-03-16T06:05:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Each cloud account you link to New Relic infrastructure monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that cloud service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use cloud APIs to collect metrics and inventory data. The dashboards show counts and frequencies for those API calls. You may want to monitor these for two reasons: API calls can cost money, and your API usage may be throttled at certain usage levels. To decrease usage, you can configure integration settings to reduce the number of components you monitor or how frequently they're monitored. Troubleshoot missing data or other data issues. The dashboard displays information that affects how integrations report data, including: Errors that affect New Relic collecting data, like permission errors or quota exhaustion errors. Changes to integration configuration, like polling interval changes, or tag-collection being disabled or enabled. Understand dashboard data The specific data displayed on the account status dashboard will differ by cloud service provider. Common charts include: Data updates: Shows updates to metric data or inventory data (updates shown as a 1 value). Account changes: Actions affecting how the integration works. For example: renaming or unlinking a cloud account, changing polling intervals, and other configuration options. Data freshness: Timestamp of the last data point collected for each integration. Fetching errors: These indicate issues with collecting data. (This may be a problem on the cloud-provider side.) Options for better understanding chart data include: Mouse over a chart’s icon to see a chart description (if available). View the chart's underlying NRQL query. Find account status dashboard To find the account status dashboard for a cloud service provider: From one.newrelic.com > Infrastructure, and select a cloud service provider (for example, AWS). Select Account status dashboard for the cloud account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.11265,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "sections": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Each <em>cloud</em> account you link to New Relic <em>infrastructure</em> monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that <em>cloud</em> service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use"
      },
      "id": "603eb14764441f1cda4e8874"
    },
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-09-13T16:54:31Z",
      "updated_at": "2021-08-27T04:53:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are infrastructure monitoring integrations that can be run directly on your host or server: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including NGINX, MySQL, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.72188,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "New Relic offers various <em>integrations</em> for reporting data to our platform. One category of <em>integrations</em> is our <em>Infrastructure</em> <em>integrations</em>. Types of <em>infrastructure</em> <em>integrations</em> New Relic has two main categories of <em>infrastructure</em> <em>integrations</em>: <em>cloud</em> and on-host. <em>Cloud</em> <em>integrations</em> <em>Cloud</em> <em>integrations</em>"
      },
      "id": "60450a39e7b9d2de845799cd"
    }
  ],
  "/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations": [
    {
      "sections": [
        "Configure polling frequency and data collection for cloud integrations",
        "Overview of settings",
        "Caution",
        "Change polling frequency",
        "Specify data to be fetched",
        "Data collection",
        "Filters",
        "Potential impact on alerts and charts"
      ],
      "title": "Configure polling frequency and data collection for cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "b900b7545f9032201c212449be114e10176bf789",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations/",
      "published_at": "2021-09-13T17:19:27Z",
      "updated_at": "2021-07-27T15:37:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our cloud integrations get data from cloud provider APIs. In New Relic, you can change some of the data collection-related settings for your cloud integrations. Read on to see what changes you can make and the reasons for making them. Overview of settings New Relic cloud integrations get data from cloud providers' APIs. Data is generally collected from monitoring APIs such as AWS CloudWatch, Azure Monitor, and GCP Stackdriver, and inventory metadata is collected from the specific services' APIs. You can use the account status dashboard to see how your cloud integrations are handling data from a cloud service provider. If you want to report more or less data from your cloud integrations, or if you need to control the use of the cloud providers' APIs to prevent reaching rate and throttling limits in your cloud account, you can change the configuration settings to modify the amount of data they report. The two main controls are: Change polling frequency Change what data is reported Examples of business reasons for wanting to change your polling frequency include: Billing: If you need to manage your AWS CloudWatch bill, you may want to decrease the polling frequency. Before you do this, make sure that any alert conditions set for your cloud integrations are not affected by this reduction. New services: If you are deploying a new service or configuration and you want to collect data more often, you may want to increase the polling frequency temporarily. Caution Changing the configuration settings for your integrations may impact alert conditions and chart trends. Change polling frequency The polling frequency configuration determines how often New Relic reports data from your cloud provider for each service. By default, the polling frequency is set to the maximum frequency that is available for each service. To change the polling frequency for a cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Use the dropdowns next to Data polling interval every to select how frequently you want New Relic to capture your cloud integration data. Specify data to be fetched You can specify which information you want captured for your cloud integration by enabling the collection of additional data and by applying multiple filters to each integration. To change this settings for your cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Under Data collections and filters, turn the toggles you want On. For filters, select or enter the values that you want included in your reported data. Data collection For some cloud integrations, an additional number of calls to the cloud provider APIs are needed in order to collect data. For example, to fetch tags for AWS Elastic Map Reduce clusters, an additional call to the service API is required. To better control the amount of API calls that are sent to your cloud account for these integrations, you can specify when you need these types of data to be collected. Different data collection toggles are available, depending on the integration. Toggle Description Collect tags Some integrations require additional API calls to the cloud provider to report tags. Tag collection is enabled by default. Switch this to Off if you don't want the integration to collect your cloud resource tags and thus reduce the volume of API calls. Collect extended inventory Some integrations can collect extended inventory metadata about your cloud resources by making additional API calls to the cloud provider. The metadata included within the extended inventory for each cloud integration is described in the integration documentation. Extended inventory collection is disabled by default. Switch this to On if you want to monitor extended inventory. This will increase the volume of API calls. Collect shards data Available for AWS Kinesis Streams integration. By default, we don't report shard metrics. Switch this to On if you want to monitor shard metrics in addition to data stream metrics. Collect Lambda@Edge data Available for AWS CloudFront integration. By default, we don't report Lambda@Edge data. Switch this to On if you're using Lambda@Edge in AWS CloudFront and want to get Lambda execution location metadata. Collect node data Available for AWS Elasticsearch integration. By default, we don't report Elasticsearch node metrics. Switch this to On if you want to monitor node metrics in addition to cluster metrics. Collect NAT Gateway data and Collect VPN data Available for AWS VPC integration. By default, we don't report NAT Gateway nor VPN metrics. Switch these to On if you want to monitor NAT Gateway and VPN metrics and inventory, in addition to other VPC related entities inventory. Collect IP addresses Available for AWS EC2 integration. By default, we collect EC2 instance metadata that includes public and private IP addresses, and network interface details. Switch this to Off if you don't want New Relic to store and display these IP data. Filters When a filter is On, you specify the data that you want to be collected; for example, if the Limit to AWS region is On, the regions that you select will be the ones that data will be collected for. There are different filters available, depending on the integration: Filter Description Region Select the regions that include the resources that you want to monitor. Queue prefixes Available for AWS SQS integration. Enter each name or prefix for the queues that you want to monitor. Filter values are case-sensitive. Load balancer prefixes Available for AWS ALB integration. Enter each name or prefix for the application load balancers that you want to monitor. Filter values are case-sensitive. Stage name prefixes Available for AWS API Gateway integration. Enter each name or prefix for the stages that you want to monitor. Filter values are case-sensitive. Tag key Enter one tag key that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag value filter. Tag value Enter one tag value that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag key. Resource group Select the resource groups that are associated with the resources that you want to monitor. Potential impact on alerts and charts If you change an integration's configuration, it can impact alert conditions and charts. Here are some things to consider: If you change this setting... It may have this impact... Any configuration setting When you change the configuration settings, the data that New Relic displays in infrastructure charts, on the inventory page, and in the events feed changes as well. Any filters When you create alert conditions after you set filters, make sure that your alerts are not triggered by resources that you filtered out. Filter for regions If you filter for specific regions, it may lower the amount of data reported to New Relic, which could trigger an alert. If you create an alert condition for a specific region and then filter that region out, the region would no longer report data and would never trigger the alert. Polling frequency When you create an alert, make sure that you define the threshold for a time period that is longer than the polling frequency. Tags and extended inventory If you turn on tags and/or extended inventory, New Relic makes more API calls to the cloud provider, which could increase your cloud provider API usage bill.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "sections": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Our <em>cloud</em> <em>integrations</em> get data from <em>cloud</em> provider APIs. In New Relic, you can change some of the data collection-related settings for your <em>cloud</em> <em>integrations</em>. Read on to see what changes you can make and the reasons for making them. Overview of settings New Relic <em>cloud</em> <em>integrations</em> get data from"
      },
      "id": "603e8eef64441fcc7e4e8853"
    },
    {
      "sections": [
        "Cloud integrations: Account status dashboard",
        "Why it matters",
        "Understand dashboard data",
        "Find account status dashboard"
      ],
      "title": "Cloud integrations: Account status dashboard",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "3457b96bec1ab5f0252c061f4ef82bc15f9b61e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard/",
      "published_at": "2021-09-13T17:18:13Z",
      "updated_at": "2021-03-16T06:05:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Each cloud account you link to New Relic infrastructure monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that cloud service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use cloud APIs to collect metrics and inventory data. The dashboards show counts and frequencies for those API calls. You may want to monitor these for two reasons: API calls can cost money, and your API usage may be throttled at certain usage levels. To decrease usage, you can configure integration settings to reduce the number of components you monitor or how frequently they're monitored. Troubleshoot missing data or other data issues. The dashboard displays information that affects how integrations report data, including: Errors that affect New Relic collecting data, like permission errors or quota exhaustion errors. Changes to integration configuration, like polling interval changes, or tag-collection being disabled or enabled. Understand dashboard data The specific data displayed on the account status dashboard will differ by cloud service provider. Common charts include: Data updates: Shows updates to metric data or inventory data (updates shown as a 1 value). Account changes: Actions affecting how the integration works. For example: renaming or unlinking a cloud account, changing polling intervals, and other configuration options. Data freshness: Timestamp of the last data point collected for each integration. Fetching errors: These indicate issues with collecting data. (This may be a problem on the cloud-provider side.) Options for better understanding chart data include: Mouse over a chart’s icon to see a chart description (if available). View the chart's underlying NRQL query. Find account status dashboard To find the account status dashboard for a cloud service provider: From one.newrelic.com > Infrastructure, and select a cloud service provider (for example, AWS). Select Account status dashboard for the cloud account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.11265,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "sections": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Each <em>cloud</em> account you link to New Relic <em>infrastructure</em> monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that <em>cloud</em> service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use"
      },
      "id": "603eb14764441f1cda4e8874"
    },
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-09-13T16:54:31Z",
      "updated_at": "2021-08-27T04:53:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are infrastructure monitoring integrations that can be run directly on your host or server: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including NGINX, MySQL, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.72186,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "New Relic offers various <em>integrations</em> for reporting data to our platform. One category of <em>integrations</em> is our <em>Infrastructure</em> <em>integrations</em>. Types of <em>infrastructure</em> <em>integrations</em> New Relic has two main categories of <em>infrastructure</em> <em>integrations</em>: <em>cloud</em> and on-host. <em>Cloud</em> <em>integrations</em> <em>Cloud</em> <em>integrations</em>"
      },
      "id": "60450a39e7b9d2de845799cd"
    }
  ],
  "/docs/integrations/infrastructure-integrations/get-started/infrastructure-integration-dashboards-charts": [
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-09-13T16:54:31Z",
      "updated_at": "2021-08-27T04:53:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are infrastructure monitoring integrations that can be run directly on your host or server: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including NGINX, MySQL, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 143.09517,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "New Relic offers various <em>integrations</em> for reporting data to our platform. One category of <em>integrations</em> is our <em>Infrastructure</em> <em>integrations</em>. Types of <em>infrastructure</em> <em>integrations</em> New Relic has two main categories of <em>infrastructure</em> <em>integrations</em>: cloud and on-host. Cloud <em>integrations</em> Cloud <em>integrations</em>"
      },
      "id": "60450a39e7b9d2de845799cd"
    },
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-09-13T17:20:26Z",
      "updated_at": "2021-07-21T20:24:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's a NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's a NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's a NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in a dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying. Here are some tips for using the different types of integration data: Metric data tips Tips for finding and using integration metric data: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data when querying: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to this data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.196815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; <em>Infrastructure</em> &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "Modern and cloud services",
        "1. Identify applications, cloud services, infrastructure, and technologies",
        "2. Deploy Infrastructure",
        "Tip",
        "3. Configure cloud integrations",
        "4. Track data on your dashboards",
        "AWS EC2 monitoring integration dashboard",
        "Azure VMs monitoring integration dashboard",
        "GCP Compute Engine monitoring integration dashboard",
        "Example modern and cloud services dashboard",
        "5. Add alerts for cloud-based metrics",
        "6. Set up additional monitoring",
        "7. CI/CD Pipeline integration"
      ],
      "title": "Modern and cloud services ",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "New Relic solutions",
        "Cloud adoption"
      ],
      "external_id": "824d892c1faf24fb7eb5bf6e205571446261f699",
      "image": "https://docs.newrelic.com/static/425a652fcbd53bbbea3454793047bd01/8c557/ModernAndCloudServiceDashboardNew.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/new-relic-solutions/cloud-adoption/modern-cloud-services/",
      "published_at": "2021-09-14T05:54:14Z",
      "updated_at": "2021-09-14T05:54:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you've just completed your cloud migration, have been using cloud based services for awhile, or have always been in the cloud, you may find yourself deploying or running technologies and services that are new and modern. These modern technologies could be container solutions such as Docker, Kubernetes, and Amazon AWS ECS or Fargate for example. Or they could be serverless services such as AWS Lambda, Microsoft Azure, or Google Cloud Platform Functions, cloud based databases, or any number of cloud services that abstract the service away from an operations-maintained infrastructure. In these situations you still want to monitor, query, and alert on the performance and usage metrics for both modern technologies and cloud-based services, allowing for faster deployments, the ability to adopt new services, better business decisions, and to expand horizons. This doc demonstrates how to use the New Relic One platform to monitor your modern technologies and cloud services. 1. Identify applications, cloud services, infrastructure, and technologies Determine the components you need to monitor by answering the following the questions: What cloud-based applications do I have? What are the underlying cloud-based services, technologies, and infrastructure supporting those applications? When you have a full understanding of your architecture, you reduce the possibility of missing dependencies during your migration. 2. Deploy Infrastructure After reviewing the requirements for New Relic Infrastructure, install the Infrastructure agent on the hosts you identified so you can start to monitor your cloud services. Tip If you use Ansible, Chef, or Puppet for automation, you can use those tools to deploy Infrastructure agents to your hosts. 3. Configure cloud integrations Once your applications are migrated to the cloud and you start to integrate new cloud services, you can use New Relic to monitor and report data about your cloud services, offering you a comprehensive view of your entire architecture in one place. To get started configuring cloud service integrations, link your cloud service provider account with New Relic, depending on whether you use Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). 4. Track data on your dashboards New Relic Infrastructure integrations auto-populate dashboards with metrics from cloud providers like AWS, Azure, and GCP so you can track the data that is critical to your cloud adoption success. Tip If you adopt a hybrid cloud of multiple cloud providers, New Relic can provide a holistic perspective that is agnostic to cloud providers. AWS EC2 monitoring integration dashboard In this default dashboard for the AWS EC2 monitoring integration, New Relic captures metrics for EC2 instances per region, instance state, and instance type. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Amazon Web Services: View AWS EC2 data on the default dashboard for the AWS EC2 monitoring integration. Azure VMs monitoring integration dashboard The default Azure virtual machine integration dashboard shows data for VM sizes, VMs per region, and VMs per resource group. one.newrelic.com > Infrastructure > Integrations > Microsoft Azure: View Azure virtual machine data on the default dashboard for the Azure VMs monitoring integration. GCP Compute Engine monitoring integration dashboard In this default dashboard for the Google Cloud Platform, New Relic captures metrics for instances per zone, instance status, firewall dropped packets, reserved cores, and disk throttled operations. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Google Cloud Platform: View GCP Compute Engine data on the default dashboard for the GCP Compute Engine monitoring integration. Example modern and cloud services dashboard In this example dashboard, three different cloud vendors, modern technologies, cloud services, infrastructure instance locations, and DevOps widgets are combined for an overall view. one.newrelic.com > Dashboards: Here is an example of a dashboard with data about vendors, technologies, services, instances, and other important details. 5. Add alerts for cloud-based metrics When monitoring cloud-based services, it is essential to keep track of all the changes happening with the system by alerting on them. Integrations with New Relic Infrastructure allow you to create alerts on the metrics that are the most important to you. Here is an example of a baseline alert that will notify you based on the number of requests received on all ALB systems for the AWS Elastic Load Balancing (ALB) monitoring integration: alerts.newrelic.com > Alert policies > (selected policy) > Alert conditions: Create a NRQL baseline alert to monitor the number of requests received your ALB systems. 6. Set up additional monitoring For full-stack visibility into every aspect of your app, we recommend deploying other types of New Relic monitoring: Use APM to report application-tier performance metrics. Use browser monitoring to report front-end web metrics. Use mobile monitoring to report front-end mobile app metrics. Use synthetic monitoring to monitor websites, critical business transactions, and API endpoints. 7. CI/CD Pipeline integration It's important to track deployments and how the impact of the code and infrastructure changes you make affect customer experience. APM's deployment markers feature allows you to record deployments for each application. A deployment marker is an event indicating that a deployment happened, and it's paired with metadata available from your SCM system (such metadata typically includes the user, revision, change log, etc.). APM displays a vertical line, or marker, on charts and graphs at the deployment event's timestamp. When you hover over the line, APM displays the associated metadata for that deployment. Tracking deployments is an invaluable way to determine the root cause of immediate, long-term, or gradual degradations in your application. Tip Recommendation: Make POST requests to the New Relic REST API as the final step of a successful CI/CD deployment as described in the API documentation. The following tools have integrations or plugins available to help automate CI/CD deployments: Chef (see newrelic_deployment) Jenkins Ansible",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.45308,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "1. Identify applications, cloud services, <em>infrastructure</em>, and technologies",
        "body": " applications are migrated to the cloud and you <em>start</em> to integrate new cloud services, you can use New Relic to monitor and report data about your cloud services, offering you a comprehensive view of your entire architecture in one place. To <em>get</em> <em>started</em> configuring cloud service <em>integrations</em>, link"
      },
      "id": "60440f13e7b9d20f275799ca"
    }
  ],
  "/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations": [
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-09-13T17:20:26Z",
      "updated_at": "2021-07-21T20:24:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's a NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's a NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's a NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in a dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying. Here are some tips for using the different types of integration data: Metric data tips Tips for finding and using integration metric data: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data when querying: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to this data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.196815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; <em>Infrastructure</em> &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "Modern and cloud services",
        "1. Identify applications, cloud services, infrastructure, and technologies",
        "2. Deploy Infrastructure",
        "Tip",
        "3. Configure cloud integrations",
        "4. Track data on your dashboards",
        "AWS EC2 monitoring integration dashboard",
        "Azure VMs monitoring integration dashboard",
        "GCP Compute Engine monitoring integration dashboard",
        "Example modern and cloud services dashboard",
        "5. Add alerts for cloud-based metrics",
        "6. Set up additional monitoring",
        "7. CI/CD Pipeline integration"
      ],
      "title": "Modern and cloud services ",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "New Relic solutions",
        "Cloud adoption"
      ],
      "external_id": "824d892c1faf24fb7eb5bf6e205571446261f699",
      "image": "https://docs.newrelic.com/static/425a652fcbd53bbbea3454793047bd01/8c557/ModernAndCloudServiceDashboardNew.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/new-relic-solutions/cloud-adoption/modern-cloud-services/",
      "published_at": "2021-09-14T05:54:14Z",
      "updated_at": "2021-09-14T05:54:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you've just completed your cloud migration, have been using cloud based services for awhile, or have always been in the cloud, you may find yourself deploying or running technologies and services that are new and modern. These modern technologies could be container solutions such as Docker, Kubernetes, and Amazon AWS ECS or Fargate for example. Or they could be serverless services such as AWS Lambda, Microsoft Azure, or Google Cloud Platform Functions, cloud based databases, or any number of cloud services that abstract the service away from an operations-maintained infrastructure. In these situations you still want to monitor, query, and alert on the performance and usage metrics for both modern technologies and cloud-based services, allowing for faster deployments, the ability to adopt new services, better business decisions, and to expand horizons. This doc demonstrates how to use the New Relic One platform to monitor your modern technologies and cloud services. 1. Identify applications, cloud services, infrastructure, and technologies Determine the components you need to monitor by answering the following the questions: What cloud-based applications do I have? What are the underlying cloud-based services, technologies, and infrastructure supporting those applications? When you have a full understanding of your architecture, you reduce the possibility of missing dependencies during your migration. 2. Deploy Infrastructure After reviewing the requirements for New Relic Infrastructure, install the Infrastructure agent on the hosts you identified so you can start to monitor your cloud services. Tip If you use Ansible, Chef, or Puppet for automation, you can use those tools to deploy Infrastructure agents to your hosts. 3. Configure cloud integrations Once your applications are migrated to the cloud and you start to integrate new cloud services, you can use New Relic to monitor and report data about your cloud services, offering you a comprehensive view of your entire architecture in one place. To get started configuring cloud service integrations, link your cloud service provider account with New Relic, depending on whether you use Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). 4. Track data on your dashboards New Relic Infrastructure integrations auto-populate dashboards with metrics from cloud providers like AWS, Azure, and GCP so you can track the data that is critical to your cloud adoption success. Tip If you adopt a hybrid cloud of multiple cloud providers, New Relic can provide a holistic perspective that is agnostic to cloud providers. AWS EC2 monitoring integration dashboard In this default dashboard for the AWS EC2 monitoring integration, New Relic captures metrics for EC2 instances per region, instance state, and instance type. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Amazon Web Services: View AWS EC2 data on the default dashboard for the AWS EC2 monitoring integration. Azure VMs monitoring integration dashboard The default Azure virtual machine integration dashboard shows data for VM sizes, VMs per region, and VMs per resource group. one.newrelic.com > Infrastructure > Integrations > Microsoft Azure: View Azure virtual machine data on the default dashboard for the Azure VMs monitoring integration. GCP Compute Engine monitoring integration dashboard In this default dashboard for the Google Cloud Platform, New Relic captures metrics for instances per zone, instance status, firewall dropped packets, reserved cores, and disk throttled operations. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Google Cloud Platform: View GCP Compute Engine data on the default dashboard for the GCP Compute Engine monitoring integration. Example modern and cloud services dashboard In this example dashboard, three different cloud vendors, modern technologies, cloud services, infrastructure instance locations, and DevOps widgets are combined for an overall view. one.newrelic.com > Dashboards: Here is an example of a dashboard with data about vendors, technologies, services, instances, and other important details. 5. Add alerts for cloud-based metrics When monitoring cloud-based services, it is essential to keep track of all the changes happening with the system by alerting on them. Integrations with New Relic Infrastructure allow you to create alerts on the metrics that are the most important to you. Here is an example of a baseline alert that will notify you based on the number of requests received on all ALB systems for the AWS Elastic Load Balancing (ALB) monitoring integration: alerts.newrelic.com > Alert policies > (selected policy) > Alert conditions: Create a NRQL baseline alert to monitor the number of requests received your ALB systems. 6. Set up additional monitoring For full-stack visibility into every aspect of your app, we recommend deploying other types of New Relic monitoring: Use APM to report application-tier performance metrics. Use browser monitoring to report front-end web metrics. Use mobile monitoring to report front-end mobile app metrics. Use synthetic monitoring to monitor websites, critical business transactions, and API endpoints. 7. CI/CD Pipeline integration It's important to track deployments and how the impact of the code and infrastructure changes you make affect customer experience. APM's deployment markers feature allows you to record deployments for each application. A deployment marker is an event indicating that a deployment happened, and it's paired with metadata available from your SCM system (such metadata typically includes the user, revision, change log, etc.). APM displays a vertical line, or marker, on charts and graphs at the deployment event's timestamp. When you hover over the line, APM displays the associated metadata for that deployment. Tracking deployments is an invaluable way to determine the root cause of immediate, long-term, or gradual degradations in your application. Tip Recommendation: Make POST requests to the New Relic REST API as the final step of a successful CI/CD deployment as described in the API documentation. The following tools have integrations or plugins available to help automate CI/CD deployments: Chef (see newrelic_deployment) Jenkins Ansible",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.45303,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "1. Identify applications, cloud services, <em>infrastructure</em>, and technologies",
        "body": " applications are migrated to the cloud and you <em>start</em> to integrate new cloud services, you can use New Relic to monitor and report data about your cloud services, offering you a comprehensive view of your entire architecture in one place. To <em>get</em> <em>started</em> configuring cloud service <em>integrations</em>, link"
      },
      "id": "60440f13e7b9d20f275799ca"
    },
    {
      "sections": [
        "Understand and use data from infrastructure integrations",
        "Explore your infrastructure integration's data",
        "Types of integration data",
        "Create alert conditions"
      ],
      "title": "Understand and use data from infrastructure integrations ",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "305bd846fb6232dff32dceaacc5d1f0a9f400969",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/understand-use-data-infrastructure-integrations/",
      "published_at": "2021-09-13T17:20:26Z",
      "updated_at": "2021-03-13T03:28:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic infrastructure integrations, you can monitor the performance of popular services, including AWS, Azure, Google Cloud Platform, Kubernetes, Redis, MySQL, and more. Here are some tips on how to find, understand, and use data reported from infrastructure integrations. Explore your infrastructure integration's data The best way to understand infrastructure integrations's data and see what you can do with it is to enable an integration and explore the data in the New Relic UI. Some recommendations for exploring: View dashboards: You can find your dashboards in New Relic One. For details, see Integration dashboards. Query data: You can run custom queries and charts of your integration data. For more information, see Query New Relic data. Create alert conditions: See Alert conditions. Learn more about what metrics and inventory data an integration reports: See an integration's documentation: cloud integrations and on-host integrations. Types of integration data New Relic infrastructure integrations are separated into two main categories: Cloud integrations: Integrations for cloud platform services, including AWS, Azure, and GCP. On-host integrations: \"On-host\" refers to core services integrations that you can install directly on a host. Examples: MySQL, NGINX, Kubernetes, Redis. An infrastructure integration can generate four types of data: Metrics: Numeric measurement data. Examples: message counts, error counts, and CPU used percentage. Metric data appears in an integration's charts. For details about what metrics are reported and how to query them, see the documentation for a specific integration. For details about data structure, see Data types. Inventory: Information about the state and configuration of a service or host. Examples of inventory data: configuration settings, the name of the host the service is on, the AWS region, the port being used. Inventory data appears in the Inventory UI page. Inventory data also appears in integration dashboards. Changes to inventory data generates event data. Events: Events represent important activity on a system. Examples of event data: an admin logging in; a package install or uninstall; a service starting; a table being created. Most events represent changes to inventory data. Attributes: Some integrations will generate other non-metric attributes (key-value pairs) that can be queried in New Relic. Create alert conditions To create an alert condition for integration data in infrastructure, Go to one.newrelic.com > Infrastructure, choose an integration, and then select an available alert option. For more information, see Infrastructure and alerts. For on-host integrations, you can also create alert conditions using NRQL queries. (NRQL alerts are not supported or recommended for cloud integrations, due to issues related to data latency.)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.87315,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand and use data from <em>infrastructure</em> <em>integrations</em> ",
        "sections": "Understand and use data from <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "With New Relic <em>infrastructure</em> <em>integrations</em>, you can monitor the performance of popular services, including AWS, Azure, Google Cloud Platform, Kubernetes, Redis, MySQL, and more. Here are some tips on how to find, understand, and use data reported from <em>infrastructure</em> <em>integrations</em>. Explore your"
      },
      "id": "60450a3928ccbc2d9b2c60c8"
    }
  ],
  "/docs/integrations/infrastructure-integrations/get-started/understand-use-data-infrastructure-integrations": [
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-09-13T16:54:31Z",
      "updated_at": "2021-08-27T04:53:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are infrastructure monitoring integrations that can be run directly on your host or server: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including NGINX, MySQL, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 143.09515,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "New Relic offers various <em>integrations</em> for reporting data to our platform. One category of <em>integrations</em> is our <em>Infrastructure</em> <em>integrations</em>. Types of <em>infrastructure</em> <em>integrations</em> New Relic has two main categories of <em>infrastructure</em> <em>integrations</em>: cloud and on-host. Cloud <em>integrations</em> Cloud <em>integrations</em>"
      },
      "id": "60450a39e7b9d2de845799cd"
    },
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-09-13T17:20:26Z",
      "updated_at": "2021-07-21T20:24:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's a NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's a NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's a NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in a dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying. Here are some tips for using the different types of integration data: Metric data tips Tips for finding and using integration metric data: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data when querying: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to this data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.196815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; <em>Infrastructure</em> &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "Modern and cloud services",
        "1. Identify applications, cloud services, infrastructure, and technologies",
        "2. Deploy Infrastructure",
        "Tip",
        "3. Configure cloud integrations",
        "4. Track data on your dashboards",
        "AWS EC2 monitoring integration dashboard",
        "Azure VMs monitoring integration dashboard",
        "GCP Compute Engine monitoring integration dashboard",
        "Example modern and cloud services dashboard",
        "5. Add alerts for cloud-based metrics",
        "6. Set up additional monitoring",
        "7. CI/CD Pipeline integration"
      ],
      "title": "Modern and cloud services ",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "New Relic solutions",
        "Cloud adoption"
      ],
      "external_id": "824d892c1faf24fb7eb5bf6e205571446261f699",
      "image": "https://docs.newrelic.com/static/425a652fcbd53bbbea3454793047bd01/8c557/ModernAndCloudServiceDashboardNew.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/new-relic-solutions/cloud-adoption/modern-cloud-services/",
      "published_at": "2021-09-14T05:54:14Z",
      "updated_at": "2021-09-14T05:54:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you've just completed your cloud migration, have been using cloud based services for awhile, or have always been in the cloud, you may find yourself deploying or running technologies and services that are new and modern. These modern technologies could be container solutions such as Docker, Kubernetes, and Amazon AWS ECS or Fargate for example. Or they could be serverless services such as AWS Lambda, Microsoft Azure, or Google Cloud Platform Functions, cloud based databases, or any number of cloud services that abstract the service away from an operations-maintained infrastructure. In these situations you still want to monitor, query, and alert on the performance and usage metrics for both modern technologies and cloud-based services, allowing for faster deployments, the ability to adopt new services, better business decisions, and to expand horizons. This doc demonstrates how to use the New Relic One platform to monitor your modern technologies and cloud services. 1. Identify applications, cloud services, infrastructure, and technologies Determine the components you need to monitor by answering the following the questions: What cloud-based applications do I have? What are the underlying cloud-based services, technologies, and infrastructure supporting those applications? When you have a full understanding of your architecture, you reduce the possibility of missing dependencies during your migration. 2. Deploy Infrastructure After reviewing the requirements for New Relic Infrastructure, install the Infrastructure agent on the hosts you identified so you can start to monitor your cloud services. Tip If you use Ansible, Chef, or Puppet for automation, you can use those tools to deploy Infrastructure agents to your hosts. 3. Configure cloud integrations Once your applications are migrated to the cloud and you start to integrate new cloud services, you can use New Relic to monitor and report data about your cloud services, offering you a comprehensive view of your entire architecture in one place. To get started configuring cloud service integrations, link your cloud service provider account with New Relic, depending on whether you use Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). 4. Track data on your dashboards New Relic Infrastructure integrations auto-populate dashboards with metrics from cloud providers like AWS, Azure, and GCP so you can track the data that is critical to your cloud adoption success. Tip If you adopt a hybrid cloud of multiple cloud providers, New Relic can provide a holistic perspective that is agnostic to cloud providers. AWS EC2 monitoring integration dashboard In this default dashboard for the AWS EC2 monitoring integration, New Relic captures metrics for EC2 instances per region, instance state, and instance type. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Amazon Web Services: View AWS EC2 data on the default dashboard for the AWS EC2 monitoring integration. Azure VMs monitoring integration dashboard The default Azure virtual machine integration dashboard shows data for VM sizes, VMs per region, and VMs per resource group. one.newrelic.com > Infrastructure > Integrations > Microsoft Azure: View Azure virtual machine data on the default dashboard for the Azure VMs monitoring integration. GCP Compute Engine monitoring integration dashboard In this default dashboard for the Google Cloud Platform, New Relic captures metrics for instances per zone, instance status, firewall dropped packets, reserved cores, and disk throttled operations. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Google Cloud Platform: View GCP Compute Engine data on the default dashboard for the GCP Compute Engine monitoring integration. Example modern and cloud services dashboard In this example dashboard, three different cloud vendors, modern technologies, cloud services, infrastructure instance locations, and DevOps widgets are combined for an overall view. one.newrelic.com > Dashboards: Here is an example of a dashboard with data about vendors, technologies, services, instances, and other important details. 5. Add alerts for cloud-based metrics When monitoring cloud-based services, it is essential to keep track of all the changes happening with the system by alerting on them. Integrations with New Relic Infrastructure allow you to create alerts on the metrics that are the most important to you. Here is an example of a baseline alert that will notify you based on the number of requests received on all ALB systems for the AWS Elastic Load Balancing (ALB) monitoring integration: alerts.newrelic.com > Alert policies > (selected policy) > Alert conditions: Create a NRQL baseline alert to monitor the number of requests received your ALB systems. 6. Set up additional monitoring For full-stack visibility into every aspect of your app, we recommend deploying other types of New Relic monitoring: Use APM to report application-tier performance metrics. Use browser monitoring to report front-end web metrics. Use mobile monitoring to report front-end mobile app metrics. Use synthetic monitoring to monitor websites, critical business transactions, and API endpoints. 7. CI/CD Pipeline integration It's important to track deployments and how the impact of the code and infrastructure changes you make affect customer experience. APM's deployment markers feature allows you to record deployments for each application. A deployment marker is an event indicating that a deployment happened, and it's paired with metadata available from your SCM system (such metadata typically includes the user, revision, change log, etc.). APM displays a vertical line, or marker, on charts and graphs at the deployment event's timestamp. When you hover over the line, APM displays the associated metadata for that deployment. Tracking deployments is an invaluable way to determine the root cause of immediate, long-term, or gradual degradations in your application. Tip Recommendation: Make POST requests to the New Relic REST API as the final step of a successful CI/CD deployment as described in the API documentation. The following tools have integrations or plugins available to help automate CI/CD deployments: Chef (see newrelic_deployment) Jenkins Ansible",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.45303,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "1. Identify applications, cloud services, <em>infrastructure</em>, and technologies",
        "body": " applications are migrated to the cloud and you <em>start</em> to integrate new cloud services, you can use New Relic to monitor and report data about your cloud services, offering you a comprehensive view of your entire architecture in one place. To <em>get</em> <em>started</em> configuring cloud service <em>integrations</em>, link"
      },
      "id": "60440f13e7b9d20f275799ca"
    }
  ],
  "/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards": [
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-09-13T16:54:31Z",
      "updated_at": "2021-08-27T04:53:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are infrastructure monitoring integrations that can be run directly on your host or server: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including NGINX, MySQL, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 143.09515,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "New Relic offers various <em>integrations</em> for reporting data to our platform. One category of <em>integrations</em> is our <em>Infrastructure</em> <em>integrations</em>. Types of <em>infrastructure</em> <em>integrations</em> New Relic has two main categories of <em>infrastructure</em> <em>integrations</em>: cloud and on-host. Cloud <em>integrations</em> Cloud <em>integrations</em>"
      },
      "id": "60450a39e7b9d2de845799cd"
    },
    {
      "sections": [
        "Modern and cloud services",
        "1. Identify applications, cloud services, infrastructure, and technologies",
        "2. Deploy Infrastructure",
        "Tip",
        "3. Configure cloud integrations",
        "4. Track data on your dashboards",
        "AWS EC2 monitoring integration dashboard",
        "Azure VMs monitoring integration dashboard",
        "GCP Compute Engine monitoring integration dashboard",
        "Example modern and cloud services dashboard",
        "5. Add alerts for cloud-based metrics",
        "6. Set up additional monitoring",
        "7. CI/CD Pipeline integration"
      ],
      "title": "Modern and cloud services ",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "New Relic solutions",
        "Cloud adoption"
      ],
      "external_id": "824d892c1faf24fb7eb5bf6e205571446261f699",
      "image": "https://docs.newrelic.com/static/425a652fcbd53bbbea3454793047bd01/8c557/ModernAndCloudServiceDashboardNew.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/new-relic-solutions/cloud-adoption/modern-cloud-services/",
      "published_at": "2021-09-14T05:54:14Z",
      "updated_at": "2021-09-14T05:54:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you've just completed your cloud migration, have been using cloud based services for awhile, or have always been in the cloud, you may find yourself deploying or running technologies and services that are new and modern. These modern technologies could be container solutions such as Docker, Kubernetes, and Amazon AWS ECS or Fargate for example. Or they could be serverless services such as AWS Lambda, Microsoft Azure, or Google Cloud Platform Functions, cloud based databases, or any number of cloud services that abstract the service away from an operations-maintained infrastructure. In these situations you still want to monitor, query, and alert on the performance and usage metrics for both modern technologies and cloud-based services, allowing for faster deployments, the ability to adopt new services, better business decisions, and to expand horizons. This doc demonstrates how to use the New Relic One platform to monitor your modern technologies and cloud services. 1. Identify applications, cloud services, infrastructure, and technologies Determine the components you need to monitor by answering the following the questions: What cloud-based applications do I have? What are the underlying cloud-based services, technologies, and infrastructure supporting those applications? When you have a full understanding of your architecture, you reduce the possibility of missing dependencies during your migration. 2. Deploy Infrastructure After reviewing the requirements for New Relic Infrastructure, install the Infrastructure agent on the hosts you identified so you can start to monitor your cloud services. Tip If you use Ansible, Chef, or Puppet for automation, you can use those tools to deploy Infrastructure agents to your hosts. 3. Configure cloud integrations Once your applications are migrated to the cloud and you start to integrate new cloud services, you can use New Relic to monitor and report data about your cloud services, offering you a comprehensive view of your entire architecture in one place. To get started configuring cloud service integrations, link your cloud service provider account with New Relic, depending on whether you use Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). 4. Track data on your dashboards New Relic Infrastructure integrations auto-populate dashboards with metrics from cloud providers like AWS, Azure, and GCP so you can track the data that is critical to your cloud adoption success. Tip If you adopt a hybrid cloud of multiple cloud providers, New Relic can provide a holistic perspective that is agnostic to cloud providers. AWS EC2 monitoring integration dashboard In this default dashboard for the AWS EC2 monitoring integration, New Relic captures metrics for EC2 instances per region, instance state, and instance type. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Amazon Web Services: View AWS EC2 data on the default dashboard for the AWS EC2 monitoring integration. Azure VMs monitoring integration dashboard The default Azure virtual machine integration dashboard shows data for VM sizes, VMs per region, and VMs per resource group. one.newrelic.com > Infrastructure > Integrations > Microsoft Azure: View Azure virtual machine data on the default dashboard for the Azure VMs monitoring integration. GCP Compute Engine monitoring integration dashboard In this default dashboard for the Google Cloud Platform, New Relic captures metrics for instances per zone, instance status, firewall dropped packets, reserved cores, and disk throttled operations. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Google Cloud Platform: View GCP Compute Engine data on the default dashboard for the GCP Compute Engine monitoring integration. Example modern and cloud services dashboard In this example dashboard, three different cloud vendors, modern technologies, cloud services, infrastructure instance locations, and DevOps widgets are combined for an overall view. one.newrelic.com > Dashboards: Here is an example of a dashboard with data about vendors, technologies, services, instances, and other important details. 5. Add alerts for cloud-based metrics When monitoring cloud-based services, it is essential to keep track of all the changes happening with the system by alerting on them. Integrations with New Relic Infrastructure allow you to create alerts on the metrics that are the most important to you. Here is an example of a baseline alert that will notify you based on the number of requests received on all ALB systems for the AWS Elastic Load Balancing (ALB) monitoring integration: alerts.newrelic.com > Alert policies > (selected policy) > Alert conditions: Create a NRQL baseline alert to monitor the number of requests received your ALB systems. 6. Set up additional monitoring For full-stack visibility into every aspect of your app, we recommend deploying other types of New Relic monitoring: Use APM to report application-tier performance metrics. Use browser monitoring to report front-end web metrics. Use mobile monitoring to report front-end mobile app metrics. Use synthetic monitoring to monitor websites, critical business transactions, and API endpoints. 7. CI/CD Pipeline integration It's important to track deployments and how the impact of the code and infrastructure changes you make affect customer experience. APM's deployment markers feature allows you to record deployments for each application. A deployment marker is an event indicating that a deployment happened, and it's paired with metadata available from your SCM system (such metadata typically includes the user, revision, change log, etc.). APM displays a vertical line, or marker, on charts and graphs at the deployment event's timestamp. When you hover over the line, APM displays the associated metadata for that deployment. Tracking deployments is an invaluable way to determine the root cause of immediate, long-term, or gradual degradations in your application. Tip Recommendation: Make POST requests to the New Relic REST API as the final step of a successful CI/CD deployment as described in the API documentation. The following tools have integrations or plugins available to help automate CI/CD deployments: Chef (see newrelic_deployment) Jenkins Ansible",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.45299,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "1. Identify applications, cloud services, <em>infrastructure</em>, and technologies",
        "body": " applications are migrated to the cloud and you <em>start</em> to integrate new cloud services, you can use New Relic to monitor and report data about your cloud services, offering you a comprehensive view of your entire architecture in one place. To <em>get</em> <em>started</em> configuring cloud service <em>integrations</em>, link"
      },
      "id": "60440f13e7b9d20f275799ca"
    },
    {
      "sections": [
        "Understand and use data from infrastructure integrations",
        "Explore your infrastructure integration's data",
        "Types of integration data",
        "Create alert conditions"
      ],
      "title": "Understand and use data from infrastructure integrations ",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "305bd846fb6232dff32dceaacc5d1f0a9f400969",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/understand-use-data-infrastructure-integrations/",
      "published_at": "2021-09-13T17:20:26Z",
      "updated_at": "2021-03-13T03:28:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic infrastructure integrations, you can monitor the performance of popular services, including AWS, Azure, Google Cloud Platform, Kubernetes, Redis, MySQL, and more. Here are some tips on how to find, understand, and use data reported from infrastructure integrations. Explore your infrastructure integration's data The best way to understand infrastructure integrations's data and see what you can do with it is to enable an integration and explore the data in the New Relic UI. Some recommendations for exploring: View dashboards: You can find your dashboards in New Relic One. For details, see Integration dashboards. Query data: You can run custom queries and charts of your integration data. For more information, see Query New Relic data. Create alert conditions: See Alert conditions. Learn more about what metrics and inventory data an integration reports: See an integration's documentation: cloud integrations and on-host integrations. Types of integration data New Relic infrastructure integrations are separated into two main categories: Cloud integrations: Integrations for cloud platform services, including AWS, Azure, and GCP. On-host integrations: \"On-host\" refers to core services integrations that you can install directly on a host. Examples: MySQL, NGINX, Kubernetes, Redis. An infrastructure integration can generate four types of data: Metrics: Numeric measurement data. Examples: message counts, error counts, and CPU used percentage. Metric data appears in an integration's charts. For details about what metrics are reported and how to query them, see the documentation for a specific integration. For details about data structure, see Data types. Inventory: Information about the state and configuration of a service or host. Examples of inventory data: configuration settings, the name of the host the service is on, the AWS region, the port being used. Inventory data appears in the Inventory UI page. Inventory data also appears in integration dashboards. Changes to inventory data generates event data. Events: Events represent important activity on a system. Examples of event data: an admin logging in; a package install or uninstall; a service starting; a table being created. Most events represent changes to inventory data. Attributes: Some integrations will generate other non-metric attributes (key-value pairs) that can be queried in New Relic. Create alert conditions To create an alert condition for integration data in infrastructure, Go to one.newrelic.com > Infrastructure, choose an integration, and then select an available alert option. For more information, see Infrastructure and alerts. For on-host integrations, you can also create alert conditions using NRQL queries. (NRQL alerts are not supported or recommended for cloud integrations, due to issues related to data latency.)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.873146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Understand and use data from <em>infrastructure</em> <em>integrations</em> ",
        "sections": "Understand and use data from <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "With New Relic <em>infrastructure</em> <em>integrations</em>, you can monitor the performance of popular services, including AWS, Azure, Google Cloud Platform, Kubernetes, Redis, MySQL, and more. Here are some tips on how to find, understand, and use data reported from <em>infrastructure</em> <em>integrations</em>. Explore your"
      },
      "id": "60450a3928ccbc2d9b2c60c8"
    }
  ],
  "/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-09-13T16:54:31Z",
      "updated_at": "2021-09-07T23:26:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Kubernetes cluster VMware Tanzu Compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: A New Relic account. Don't have one? Sign up for free. No credit card required. Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For detailed instructions about how to install our integration using Helm, see Manual install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.17705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.02492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " <em>get</em> daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the <em>Kubernetes</em> <em>integration</em> for Windows: The Windows agent only sends"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.35167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " installer. <em>Start</em> the installer This page describes in more depth how to install and configure the New Relic <em>integration</em> without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the <em>Kubernetes</em>"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    }
  ],
  "/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements": [
    {
      "sections": [
        "Introduction to the Kubernetes integration",
        "Get started: Install the Kubernetes integration",
        "Tip",
        "Why it matters",
        "Navigate all your Kubernetes events",
        "Bring your cluster logs to New Relic",
        "Check the source code"
      ],
      "title": "Introduction to the Kubernetes integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "c641d1367f1f8fd2b589a2707112759becae609b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration/",
      "published_at": "2021-09-17T01:54:52Z",
      "updated_at": "2021-09-05T01:49:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. With our cluster explorer, you can cut through layers of complexity to see how your cluster is performing, from the heights of the control plane down to applications running on a single pod. one.newrelic.com > Kubernetes cluster explorer: The cluster explorer is our powerful, fully visual answer to the challenges associated with running Kubernetes at a large scale. You can see the power of the Kubernetes integration in the cluster explorer, where the full picture of a cluster is made available on a single screen: nodes and pods are visualized according to their health and performance, with pending and alerting nodes in the innermost circles. Predefined alert conditions help you troubleshoot issues right from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes integration We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you with installations in managed services or platforms, but you'll need to review a few preliminary notes before getting started. Our automated installer will generate either a helm command or a set of plain manifests for you to install. Our automated installer: Allows users to select the cluster name and namespace for the installation. Allows users to selectively enable or disable bundling of Kube-state-metrics, a dependency of the Kubernetes integration. Allows users to seamlessly install our other products related to Kubernetes such as: Kubernetes events monitoring In-cluster prometheus services monitoring Service instrumentation without code changes using Pixie Automatically fills the required properties with the license keys the integration needs to work. Read the install docs Start the installer Tip If your New Relic account is in the EU region, access the automated installer from one.eu.newrelic.com. Why it matters Governing the complexity of Kubernetes can be challenging; there's so much going on at any given moment, with containers being created and deleted in a matter of minutes, applications crashing, and resources being consumed unexpectedly. Our integration helps you navigate Kubernetes abstractions across on-premises, cloud, and hybrid deployments. In New Relic, you can build your own charts and query all your Kubernetes data, which our integration collects by instrumenting the container orchestration layer. This gives you additional insight into nodes, namespaces, deployments, replica sets, pods, and containers. one.newrelic.com > Dashboards: Using the query builder you can turn any query on Kubernetes data to clear visuals. With the Kubernetes integration you can also: Link your APM data to Kubernetes to measure the performance of your web and mobile applications, with metrics such as request rate, throughput, error rate, and availability. Monitor services running on Kubernetes, such as Apache, NGINX, Cassandra, and many more (see our tutorial for monitoring Redis on Kubernetes). Create new alert policies and alert conditions based on your Kubernetes data, or extend the predefined alert conditions. These features are in addition to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your Kubernetes events The Kubernetes events integration, which is installed separately, watches for events happening in your Kubernetes clusters and sends those events to New Relic. Events data is then visualized in the cluster explorer. To set it up, check the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into application logs and infrastructure data. Bring your cluster logs to New Relic Our Kubernetes plugin for log monitoring can collect all your cluster's logs and send them to our platform, so that you can set up new alerts and charts. To set it up, check the Log data box in step 3 of our install wizard, or follow the instructions. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or you can create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 153.5709,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Kubernetes</em> <em>integration</em>",
        "sections": "<em>Get</em> <em>started</em>: Install the <em>Kubernetes</em> <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " from the <em>start</em>. Clicking each node reveals its status and how each app is performing. <em>Get</em> <em>started</em>: Install the <em>Kubernetes</em> <em>integration</em> We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you"
      },
      "id": "6043a212196a678d86960f46"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.02492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " <em>get</em> daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the <em>Kubernetes</em> <em>integration</em> for Windows: The Windows agent only sends"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.35167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " installer. <em>Start</em> the installer This page describes in more depth how to install and configure the New Relic <em>integration</em> without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the <em>Kubernetes</em>"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.28293,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> <em>installation</em> for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.64714,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.14055,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/configure-kubernetes-proxy": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.28293,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> <em>installation</em> for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.64714,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.14055,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/install-fargate-integration": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/whats-new/2021/06/monitoring-amazon-eks-on-aws-fargate/",
      "sections": [
        "Monitor Amazon EKS on AWS Fargate integration with our public beta"
      ],
      "published_at": "2021-09-14T10:21:00Z",
      "title": "Monitor Amazon EKS on AWS Fargate integration with our public beta",
      "updated_at": "2021-07-07T14:12:51Z",
      "type": "docs",
      "external_id": "a7899b6302aa05b943053c2230e2787799a0fef4",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We are introducing New Relic's integration for Amazon EKS on AWS Fargate public beta. If you are interested in checking out the beta, please follow the steps found in the documentation. Our EKS Fargate integration supports any Fargate setup, whether the cluster is only composed of Fargate nodes or if it also coexists with EC2 nodes. The integration is also compatible with the New Relic One Kubernetes cluster explorer, providing a holistic view of a Kubernetes cluster and rapid troubleshooting. We've also improved the Kubernetes dashboard to list Fargate nodes and distinguish between standard and Fargate serverless nodes. Check it out today.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 783.704,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor Amazon <em>EKS</em> on AWS <em>Fargate</em> integration with our public beta",
        "sections": "Monitor Amazon <em>EKS</em> on AWS <em>Fargate</em> integration with our public beta",
        "body": "We are introducing New Relic&#x27;s integration for Amazon <em>EKS</em> on AWS <em>Fargate</em> public beta. If you are interested in checking out the beta, please follow the steps found in the documentation. Our <em>EKS</em> <em>Fargate</em> integration supports any <em>Fargate</em> setup, whether the cluster is only composed of <em>Fargate</em> nodes"
      },
      "id": "60e5b66364441f2e58c42399"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 705.3212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Amazon <em>EKS</em> <em>Fargate</em>",
        "body": " the version of kubectl provided by AWS. Amazon <em>EKS</em> <em>Fargate</em> Installation on <em>EKS</em> <em>Fargate</em> clusters requires dedicated steps, which are detailed in our <em>fargate</em> installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-09-13T16:54:31Z",
      "updated_at": "2021-09-07T23:26:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Kubernetes cluster VMware Tanzu Compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: A New Relic account. Don't have one? Sign up for free. No credit card required. Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For detailed instructions about how to install our integration using Helm, see Manual install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 376.84637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ", depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster <em>EKS</em> (EC2 nodes or <em>Fargate</em>) Compatible with version 1.11 or higher Kubernetes"
      },
      "id": "603e92dc64441f3a974e8891"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.28285,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> <em>installation</em> for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.14052,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Kubernetes integration: Recommended alert policy",
        "Recommended alert conditions",
        "Node allocatable CPU utilization % is too high",
        "Node allocatable memory utilization % is too high",
        "Node pods allocatable utilization % is too high"
      ],
      "title": "Kubernetes integration: Recommended alert policy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a1420c0b9ac0dedab27b91360da8d0f48930f419",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-recommended-alert-policy/",
      "published_at": "2021-09-13T17:10:38Z",
      "updated_at": "2021-08-26T21:58:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When deploying the Kubernetes integration for the first time, we deploy a default set of alert conditions to your account, the predefined alert policy, a basis for alert conditions on your Kubernetes cluster. While we try to tackle the most common use cases across all the environments, there's a number of additional alerts you can set up to extend the default policy. What follows are some recommendations. Recommended alert conditions Node allocatable CPU utilization % is too high Setting Value Event type K8sNodeSample SELECT value (cpuUsedCores/cpuLimitCores)*100 Warning threshold > 90% for at least 5 minutes Critical threshold > 95% for at least 5 mins Node allocatable memory utilization % is too high Setting Value Event type K8sNodeSample SELECT value (memoryUsedBytes/memoryLimitBytes)/100 Warning threshold > 85% for at least 5 minutes Critical threshold > 95% for at least 5 mins You need to set a memory limit in your container specs for memoryLimitBytes to report data. Node pods allocatable utilization % is too high Setting Value Event type K8sPodSample SELECT value isScheduled Warning threshold Critical threshold isScheduled = 0 for at least 7 minutes",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.7814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: Recommended alert policy",
        "sections": "<em>Kubernetes</em> <em>integration</em>: Recommended alert policy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "When deploying the <em>Kubernetes</em> <em>integration</em> for the first time, we deploy a default set of alert conditions to your account, the predefined alert policy, a basis for alert conditions on your <em>Kubernetes</em> cluster. While we try to tackle the most common use cases across all the environments, there&#x27;s"
      },
      "id": "603e80dce7b9d21ed62a07ce"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.647,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.14047,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Kubernetes integration: Recommended alert policy",
        "Recommended alert conditions",
        "Node allocatable CPU utilization % is too high",
        "Node allocatable memory utilization % is too high",
        "Node pods allocatable utilization % is too high"
      ],
      "title": "Kubernetes integration: Recommended alert policy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a1420c0b9ac0dedab27b91360da8d0f48930f419",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-recommended-alert-policy/",
      "published_at": "2021-09-13T17:10:38Z",
      "updated_at": "2021-08-26T21:58:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When deploying the Kubernetes integration for the first time, we deploy a default set of alert conditions to your account, the predefined alert policy, a basis for alert conditions on your Kubernetes cluster. While we try to tackle the most common use cases across all the environments, there's a number of additional alerts you can set up to extend the default policy. What follows are some recommendations. Recommended alert conditions Node allocatable CPU utilization % is too high Setting Value Event type K8sNodeSample SELECT value (cpuUsedCores/cpuLimitCores)*100 Warning threshold > 90% for at least 5 minutes Critical threshold > 95% for at least 5 mins Node allocatable memory utilization % is too high Setting Value Event type K8sNodeSample SELECT value (memoryUsedBytes/memoryLimitBytes)/100 Warning threshold > 85% for at least 5 minutes Critical threshold > 95% for at least 5 mins You need to set a memory limit in your container specs for memoryLimitBytes to report data. Node pods allocatable utilization % is too high Setting Value Event type K8sPodSample SELECT value isScheduled Warning threshold Critical threshold isScheduled = 0 for at least 7 minutes",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.7814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: Recommended alert policy",
        "sections": "<em>Kubernetes</em> <em>integration</em>: Recommended alert policy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "When deploying the <em>Kubernetes</em> <em>integration</em> for the first time, we deploy a default set of alert conditions to your account, the predefined alert policy, a basis for alert conditions on your <em>Kubernetes</em> cluster. While we try to tackle the most common use cases across all the environments, there&#x27;s"
      },
      "id": "603e80dce7b9d21ed62a07ce"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.28278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> <em>installation</em> for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.647,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Kubernetes integration: Recommended alert policy",
        "Recommended alert conditions",
        "Node allocatable CPU utilization % is too high",
        "Node allocatable memory utilization % is too high",
        "Node pods allocatable utilization % is too high"
      ],
      "title": "Kubernetes integration: Recommended alert policy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a1420c0b9ac0dedab27b91360da8d0f48930f419",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-recommended-alert-policy/",
      "published_at": "2021-09-13T17:10:38Z",
      "updated_at": "2021-08-26T21:58:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When deploying the Kubernetes integration for the first time, we deploy a default set of alert conditions to your account, the predefined alert policy, a basis for alert conditions on your Kubernetes cluster. While we try to tackle the most common use cases across all the environments, there's a number of additional alerts you can set up to extend the default policy. What follows are some recommendations. Recommended alert conditions Node allocatable CPU utilization % is too high Setting Value Event type K8sNodeSample SELECT value (cpuUsedCores/cpuLimitCores)*100 Warning threshold > 90% for at least 5 minutes Critical threshold > 95% for at least 5 mins Node allocatable memory utilization % is too high Setting Value Event type K8sNodeSample SELECT value (memoryUsedBytes/memoryLimitBytes)/100 Warning threshold > 85% for at least 5 minutes Critical threshold > 95% for at least 5 mins You need to set a memory limit in your container specs for memoryLimitBytes to report data. Node pods allocatable utilization % is too high Setting Value Event type K8sPodSample SELECT value isScheduled Warning threshold Critical threshold isScheduled = 0 for at least 7 minutes",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.7814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: Recommended alert policy",
        "sections": "<em>Kubernetes</em> <em>integration</em>: Recommended alert policy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "When deploying the <em>Kubernetes</em> <em>integration</em> for the first time, we deploy a default set of alert conditions to your account, the predefined alert policy, a basis for alert conditions on your <em>Kubernetes</em> cluster. While we try to tackle the most common use cases across all the environments, there&#x27;s"
      },
      "id": "603e80dce7b9d21ed62a07ce"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/kubernetes-integration-recommended-alert-policy": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.28268,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> <em>installation</em> for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.64691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.14044,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.02469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.35146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.79553,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> <em>events</em>, Prometheus OpenMetrics, and New Relic log monitoring. Looking"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/kubernetes-events/kubernetes-integration-predefined-alert-policy": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.28268,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> <em>installation</em> for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.64691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.14044,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes": [
    {
      "sections": [
        "Link your applications to Kubernetes",
        "Tip",
        "Compatibility and requirements",
        "Kubernetes requirements",
        "Network requirements",
        "APM agent compatibility",
        "Openshift requirements",
        "Important",
        "Configure the injection of metadata",
        "Default configuration",
        "Custom configuration",
        "Manage custom certificates",
        "Validate the injection of metadata",
        "Disable the injection of metadata",
        "Troubleshooting"
      ],
      "title": "Link your applications to Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "2ae58989813695b48f4924529d6fd6ea17e5f6c5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/",
      "published_at": "2021-09-13T16:52:31Z",
      "updated_at": "2021-05-28T06:30:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can surface Kubernetes metadata and link it to your APM agents as distributed traces to explore performance issues and troubleshoot transaction errors. For more information, see this New Relic blog post. You can quickly start monitoring Kubernetes clusters using Auto-telemetry with Pixie, which is currently a beta release. This Pixie integration into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here's the code to link APM and infrastructure data and the code to automatically manage certificates. Compatibility and requirements Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements: Kubernetes requirements Network requirements APM agent compatibility OpenShift requirements Kubernetes requirements To link your applications and Kubernetes, your cluster must have the MutatingAdmissionWebhook controller enabled, which requires Kubernetes 1.9 or higher. To verify that your cluster is compatible, run the following command: kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 admissionregistration.k8s.io/v1beta1 Copy If you see a different result, follow the Kubernetes documentation to enable admission control in your cluster. Network requirements For Kubernetes to speak to our MutatingAdmissionWebhook, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster. This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc). Tip Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute. APM agent compatibility The following New Relic agents collect Kubernetes metadata: Go 2.3.0 or higher Java 4.10.0 or higher Node.js 5.3.0 or higher Python 4.14.0 or higher Ruby 6.1.0 or higher .NET 8.17.438 or higher Openshift requirements To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires Openshift 3.9 or higher. During the process, install a resource that requires admin permissions to the cluster. Run this to log in as admin: oc login -u system:admin Copy Check that webhooks are correctly configured. If they are not, update the master-config.yaml file. admissionConfig: pluginConfig: MutatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission ValidatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission location: \"\" Copy Important Add kubeConfigFile: /dev/null to address some issues in Openshift. Enable certificate signing by editing the YAML file and updating your configuration: kubernetesMasterConfig: controllerArguments: cluster-signing-cert-file: - \"/etc/origin/master/ca.crt\" cluster-signing-key-file: - \"/etc/origin/master/ca.key\" Copy Restart the Openshift services in the master node. Configure the injection of metadata By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see Validate the injection of metadata). This default configuration also uses the Kubernetes certificates API to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates. Default configuration To proceed with the default injection of metadata, follow these steps: Download the YAML file: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-latest.yaml Copy Custom configuration You can limit the injection of metadata only to specific namespaces by using labels. To enable this feature, edit your YAML file by finding and uncommenting the following lines: # namespaceSelector: # matchLabels: # newrelic-metadata-injection: enabled Copy With this option, injection is only applied to those namespaces that have the newrelic-metadata-injection label set to enabled: kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled Copy Manage custom certificates To use custom certificates you need a specific YAML file: Download the YAML file without automatic certificate management: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-custom-certs-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-custom-certs-latest.yaml Copy Once you have the correct YAML file, you can proceed with the custom certificate management option. You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format. If you have them in the standard certificate format (X.509), install openssl, and run the following: openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem Copy If your certificate/key pair are in another format, see the Digicert knowledgebase for more help. Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands: kubectl create secret tls newrelic-metadata-injection-secret \\ --key=PEM_ENCODED_SERVER_KEY \\ --cert=PEM_ENCODED_CERTIFICATE \\ --dry-run -o yaml | kubectl -n default apply -f - caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d '\\n') kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p \"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]\" Copy Important Certificates signed by Kubernetes have an expiration of one year. For more information, see the Kubernetes source code in GitHub. Validate the injection of metadata In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables. Create a dummy pod containing Busybox by running: kubectl create -f https://git.io/vPieo Copy Check if New Relic environment variables were injected: kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0 NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox Copy Disable the injection of metadata To disable/uninstall the injection of metadata, use the following commands: Delete the Kubernetes objects using the yaml file: kubectl delete -f k8s-metadata-injection-latest.yaml Copy Delete the TLS secret containing the certificate/key pair: kubectl delete secret/newrelic-metadata-injection-secret Copy Troubleshooting Follow these troubleshooting tips as needed. No Kubernetes metadata in APM or distributed tracing transactions Problem The creation of the secret by the k8s-webhook-cert-manager job used to fail due to the kubectl version used by the image when running in Kubernetes version 1.19.x, The new version 1.3.2 fixes this issue, therefore it is enough to run again the job using an update version of the image to fix the issue. Solution Update the image k8s-webhook-cert-manager (to a version >= 1.3.2) and re-run the job. The secret will be correctly created and the k8s-metadata-injection pod will be able to start. Note that the new version of the manifest and of the nri-bundle are already updated with the correct version of the image. Problem In OpenShift version 4.x, the CA that is used in order to patch the mutatingwebhookconfiguration resource is not the one used when signing the certificates. This is a known issue currently tracked here. In the logs of the Pod nri-metadata-injection, you'll see the following error message: TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate Copy Workaround Manually update the certificate stored in the mutatingwebhookconfiguration object. The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret csr-signer in the namespace openshift-kube-controller-manager. Problem There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing. Solution Verify that the environment variables are being correctly injected by following the instructions described in the Validate your installation step. If they are not present, get the name of the metadata injection pod by running: kubectl get pods | grep newrelic-metadata-injection-deployment kubectl logs -f pod/podname Copy In another terminal, create a new pod (for example, see Validate your installation), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like: {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.107Z\",\"caller\":\"server/main.go:139\",\"msg\":\"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\\\" from 10.11.49.2:32836\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.110Z\",\"caller\":\"server/webhook.go:168\",\"msg\":\"received admission review\",\"kind\":\"/v1, Kind=Pod\",\"namespace\":\"default\",\"name\":\"\",\"pod\":\"busybox1\",\"UID\":\"6577519b-7a61-11ea-965e-0e46d1c9335c\",\"operation\":\"CREATE\",\"userinfo\":{\"username\":\"admin\",\"uid\":\"admin\",\"groups\":[\"system:masters\",\"system:authenticated\"]}} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:182\",\"msg\":\"admission response created\",\"response\":\"[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env\\\",\\\"value\\\":[{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\\\",\\\"value\\\":\\\"adn_kops\\\"}]},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"spec.nodeName\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\\\",\\\"value\\\":\\\"busybox\\\"}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\\\",\\\"value\\\":\\\"busybox\\\"}}]\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:257\",\"msg\":\"writing response\"} Copy If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication. To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like: failed calling webhook \"metadata-injection.newrelic.com\": ERROR_REASON Copy To get the apiserver logs: Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running. kubectl proxy --port=8001 Copy Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox. kubectl create -f https://git.io/vPieo Copy Retrieve the apiserver logs. curl localhost:8001/logs/kube-apiserver.log > apiserver.log Copy Delete the busybox container. kubectl delete -f https://git.io/vPieo Copy Inspect the logs for errors. grep -E 'failed calling webhook' apiserver.log Copy Remember that one of the requirements for the metadata injection is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster. If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered. Ensure the metadata injection setup job ran successfully by inspecting the output of: kubectl get job newrelic-metadata-setup Copy If the job is not completed, investigate the logs of the setup job: kubectl logs job/newrelic-metadata-setup Copy Ensure the CertificateSigningRequest is approved and issued by running: kubectl get csr newrelic-metadata-injection-svc.default Copy Ensure the TLS secret is present by running: kubectl get secret newrelic-metadata-injection-secret Copy Ensure the CA bundle is present in the mutating webhook configuration: kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json Copy Ensure the TargetPort of the Service resource matches the Port of the Deployment's container: kubectl describe service/newrelic-metadata-injection-svc kubectl describe deployment/newrelic-metadata-injection-deployment Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.45067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "sections": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is currently a beta release. This Pixie <em>integration</em> into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our <em>Kubernetes</em> metadata injection project is open source. Here&#x27;s the code to <em>link</em> APM and infrastructure data and the code to automatically"
      },
      "id": "603ebb94196a674fd1a83df3"
    },
    {
      "sections": [
        "Tutorial: Monitor Redis running on Kubernetes",
        "What you need",
        "Step 1: Set up an example Redis application",
        "Step 2: Enable monitoring of Redis instances"
      ],
      "title": "Tutorial: Monitor Redis running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "30d0c7b52a792c21a50f98931d05a0665ff19fa1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes/",
      "published_at": "2021-09-13T17:12:35Z",
      "updated_at": "2021-03-16T04:18:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have a service running on Kubernetes, and it's a service we support, you can enable monitoring of that service by adding a configuration section for that integration to the Kubernetes integration's config. This tutorial shows how to enable monitoring for a Redis service running on the Kubernetes PHP Guestbook. For the general procedure, see Monitor a Kubernetes-running service. What you need See the general requirements for this feature, including supported services. The kubectl command-line tool must be configured to communicate with your cluster. If you don't have a cluster, you can create one using Minikube. Step 1: Set up an example Redis application This tutorial builds on the Kubernetes tutorial Deploying a PHP Guestbook application with Redis. Skip the Kubernetes tutorial and run the following command to set up the application needed for our tutorial: kubectl create -f https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml Copy If you'd like to first complete the Kubernetes tutorial, follow their tutorial instructions but do not follow the instructions in the Cleaning up section. Step 2: Enable monitoring of Redis instances The PHP Guestbook application has three Redis instances: one master and two slave instances. Each instance is tagged with a label where app=redis. For this example, we're using our Redis monitoring integration. It can monitor both master and slave instances of Redis, so we don’t have to distinguish between them. In the Kubernetes integration's YAML config file (newrelic-infrastructure-k8s-latest.yaml), you need to update the nri-integration-cfg section. From the list of integration configs, get the Redis integration YAML and add it to the Kubernetes config. The Redis YAML is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label \"app=redis\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: redis integrations: - name: nri-redis env: # using the discovered IP as the hostname address HOSTNAME: ${discovery.ip} PORT: 6379 KEYS: '{\"0\":[\"<KEY_1>\"],\"1\":[\"<KEY_2>\"]}' REMOTE_MONITORING: true labels: env: production Copy Deploy the updated service: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy You should be able to see the following in the logs for the pod newrelic-infra: time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check starting\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check finished with success\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations Copy If there are no errors, you should see Redis data in the Infrastructure UI. To find the Redis dashboards, go to one.newrelic.com > Infrastructure > Third party services, and select the Redis dashboard. For the general procedure of how to monitor services running on Kubernetes, including more detail about how configuration works, see Monitor a Kubernetes-running service.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.7674,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "sections": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-<em>integration</em>-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label &quot;<em>app</em>=redis&quot; # https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>integrations</em>&#x2F;host-<em>integrations</em>&#x2F;installation&#x2F;container-auto-discovery discovery"
      },
      "id": "603e7e8264441f332a4e8879"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.35443,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    }
  ],
  "/docs/integrations/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes": [
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "4c67f6272bda36eda4ad7883e89697a203aa2153",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2021-09-13T17:11:31Z",
      "updated_at": "2021-05-16T04:41:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.13.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.83191,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor <em>services</em> running on <em>Kubernetes</em>",
        "sections": "Monitor <em>services</em> in our <em>Kubernetes</em> <em>integration</em> installed with Helm",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "With New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> you can monitor both <em>Kubernetes</em> and the <em>services</em> running on it, such as Cassandra, Redis, MySQL, and other supported <em>services</em>. Get started Our <em>Kubernetes</em> <em>integration</em> comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "6044e50c196a676012960f35"
    },
    {
      "sections": [
        "Link your applications to Kubernetes",
        "Tip",
        "Compatibility and requirements",
        "Kubernetes requirements",
        "Network requirements",
        "APM agent compatibility",
        "Openshift requirements",
        "Important",
        "Configure the injection of metadata",
        "Default configuration",
        "Custom configuration",
        "Manage custom certificates",
        "Validate the injection of metadata",
        "Disable the injection of metadata",
        "Troubleshooting"
      ],
      "title": "Link your applications to Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "2ae58989813695b48f4924529d6fd6ea17e5f6c5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/",
      "published_at": "2021-09-13T16:52:31Z",
      "updated_at": "2021-05-28T06:30:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can surface Kubernetes metadata and link it to your APM agents as distributed traces to explore performance issues and troubleshoot transaction errors. For more information, see this New Relic blog post. You can quickly start monitoring Kubernetes clusters using Auto-telemetry with Pixie, which is currently a beta release. This Pixie integration into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here's the code to link APM and infrastructure data and the code to automatically manage certificates. Compatibility and requirements Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements: Kubernetes requirements Network requirements APM agent compatibility OpenShift requirements Kubernetes requirements To link your applications and Kubernetes, your cluster must have the MutatingAdmissionWebhook controller enabled, which requires Kubernetes 1.9 or higher. To verify that your cluster is compatible, run the following command: kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 admissionregistration.k8s.io/v1beta1 Copy If you see a different result, follow the Kubernetes documentation to enable admission control in your cluster. Network requirements For Kubernetes to speak to our MutatingAdmissionWebhook, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster. This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc). Tip Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute. APM agent compatibility The following New Relic agents collect Kubernetes metadata: Go 2.3.0 or higher Java 4.10.0 or higher Node.js 5.3.0 or higher Python 4.14.0 or higher Ruby 6.1.0 or higher .NET 8.17.438 or higher Openshift requirements To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires Openshift 3.9 or higher. During the process, install a resource that requires admin permissions to the cluster. Run this to log in as admin: oc login -u system:admin Copy Check that webhooks are correctly configured. If they are not, update the master-config.yaml file. admissionConfig: pluginConfig: MutatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission ValidatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission location: \"\" Copy Important Add kubeConfigFile: /dev/null to address some issues in Openshift. Enable certificate signing by editing the YAML file and updating your configuration: kubernetesMasterConfig: controllerArguments: cluster-signing-cert-file: - \"/etc/origin/master/ca.crt\" cluster-signing-key-file: - \"/etc/origin/master/ca.key\" Copy Restart the Openshift services in the master node. Configure the injection of metadata By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see Validate the injection of metadata). This default configuration also uses the Kubernetes certificates API to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates. Default configuration To proceed with the default injection of metadata, follow these steps: Download the YAML file: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-latest.yaml Copy Custom configuration You can limit the injection of metadata only to specific namespaces by using labels. To enable this feature, edit your YAML file by finding and uncommenting the following lines: # namespaceSelector: # matchLabels: # newrelic-metadata-injection: enabled Copy With this option, injection is only applied to those namespaces that have the newrelic-metadata-injection label set to enabled: kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled Copy Manage custom certificates To use custom certificates you need a specific YAML file: Download the YAML file without automatic certificate management: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-custom-certs-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-custom-certs-latest.yaml Copy Once you have the correct YAML file, you can proceed with the custom certificate management option. You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format. If you have them in the standard certificate format (X.509), install openssl, and run the following: openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem Copy If your certificate/key pair are in another format, see the Digicert knowledgebase for more help. Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands: kubectl create secret tls newrelic-metadata-injection-secret \\ --key=PEM_ENCODED_SERVER_KEY \\ --cert=PEM_ENCODED_CERTIFICATE \\ --dry-run -o yaml | kubectl -n default apply -f - caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d '\\n') kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p \"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]\" Copy Important Certificates signed by Kubernetes have an expiration of one year. For more information, see the Kubernetes source code in GitHub. Validate the injection of metadata In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables. Create a dummy pod containing Busybox by running: kubectl create -f https://git.io/vPieo Copy Check if New Relic environment variables were injected: kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0 NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox Copy Disable the injection of metadata To disable/uninstall the injection of metadata, use the following commands: Delete the Kubernetes objects using the yaml file: kubectl delete -f k8s-metadata-injection-latest.yaml Copy Delete the TLS secret containing the certificate/key pair: kubectl delete secret/newrelic-metadata-injection-secret Copy Troubleshooting Follow these troubleshooting tips as needed. No Kubernetes metadata in APM or distributed tracing transactions Problem The creation of the secret by the k8s-webhook-cert-manager job used to fail due to the kubectl version used by the image when running in Kubernetes version 1.19.x, The new version 1.3.2 fixes this issue, therefore it is enough to run again the job using an update version of the image to fix the issue. Solution Update the image k8s-webhook-cert-manager (to a version >= 1.3.2) and re-run the job. The secret will be correctly created and the k8s-metadata-injection pod will be able to start. Note that the new version of the manifest and of the nri-bundle are already updated with the correct version of the image. Problem In OpenShift version 4.x, the CA that is used in order to patch the mutatingwebhookconfiguration resource is not the one used when signing the certificates. This is a known issue currently tracked here. In the logs of the Pod nri-metadata-injection, you'll see the following error message: TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate Copy Workaround Manually update the certificate stored in the mutatingwebhookconfiguration object. The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret csr-signer in the namespace openshift-kube-controller-manager. Problem There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing. Solution Verify that the environment variables are being correctly injected by following the instructions described in the Validate your installation step. If they are not present, get the name of the metadata injection pod by running: kubectl get pods | grep newrelic-metadata-injection-deployment kubectl logs -f pod/podname Copy In another terminal, create a new pod (for example, see Validate your installation), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like: {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.107Z\",\"caller\":\"server/main.go:139\",\"msg\":\"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\\\" from 10.11.49.2:32836\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.110Z\",\"caller\":\"server/webhook.go:168\",\"msg\":\"received admission review\",\"kind\":\"/v1, Kind=Pod\",\"namespace\":\"default\",\"name\":\"\",\"pod\":\"busybox1\",\"UID\":\"6577519b-7a61-11ea-965e-0e46d1c9335c\",\"operation\":\"CREATE\",\"userinfo\":{\"username\":\"admin\",\"uid\":\"admin\",\"groups\":[\"system:masters\",\"system:authenticated\"]}} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:182\",\"msg\":\"admission response created\",\"response\":\"[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env\\\",\\\"value\\\":[{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\\\",\\\"value\\\":\\\"adn_kops\\\"}]},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"spec.nodeName\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\\\",\\\"value\\\":\\\"busybox\\\"}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\\\",\\\"value\\\":\\\"busybox\\\"}}]\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:257\",\"msg\":\"writing response\"} Copy If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication. To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like: failed calling webhook \"metadata-injection.newrelic.com\": ERROR_REASON Copy To get the apiserver logs: Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running. kubectl proxy --port=8001 Copy Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox. kubectl create -f https://git.io/vPieo Copy Retrieve the apiserver logs. curl localhost:8001/logs/kube-apiserver.log > apiserver.log Copy Delete the busybox container. kubectl delete -f https://git.io/vPieo Copy Inspect the logs for errors. grep -E 'failed calling webhook' apiserver.log Copy Remember that one of the requirements for the metadata injection is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster. If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered. Ensure the metadata injection setup job ran successfully by inspecting the output of: kubectl get job newrelic-metadata-setup Copy If the job is not completed, investigate the logs of the setup job: kubectl logs job/newrelic-metadata-setup Copy Ensure the CertificateSigningRequest is approved and issued by running: kubectl get csr newrelic-metadata-injection-svc.default Copy Ensure the TLS secret is present by running: kubectl get secret newrelic-metadata-injection-secret Copy Ensure the CA bundle is present in the mutating webhook configuration: kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json Copy Ensure the TargetPort of the Service resource matches the Port of the Deployment's container: kubectl describe service/newrelic-metadata-injection-svc kubectl describe deployment/newrelic-metadata-injection-deployment Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 224.45067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "sections": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is currently a beta release. This Pixie <em>integration</em> into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our <em>Kubernetes</em> metadata injection project is open source. Here&#x27;s the code to <em>link</em> APM and infrastructure data and the code to automatically"
      },
      "id": "603ebb94196a674fd1a83df3"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.35443,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    }
  ],
  "/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes": [
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "4c67f6272bda36eda4ad7883e89697a203aa2153",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2021-09-13T17:11:31Z",
      "updated_at": "2021-05-16T04:41:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.13.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.8319,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor <em>services</em> running on <em>Kubernetes</em>",
        "sections": "Monitor <em>services</em> in our <em>Kubernetes</em> <em>integration</em> installed with Helm",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "With New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> you can monitor both <em>Kubernetes</em> and the <em>services</em> running on it, such as Cassandra, Redis, MySQL, and other supported <em>services</em>. Get started Our <em>Kubernetes</em> <em>integration</em> comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "6044e50c196a676012960f35"
    },
    {
      "sections": [
        "Tutorial: Monitor Redis running on Kubernetes",
        "What you need",
        "Step 1: Set up an example Redis application",
        "Step 2: Enable monitoring of Redis instances"
      ],
      "title": "Tutorial: Monitor Redis running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "30d0c7b52a792c21a50f98931d05a0665ff19fa1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes/",
      "published_at": "2021-09-13T17:12:35Z",
      "updated_at": "2021-03-16T04:18:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have a service running on Kubernetes, and it's a service we support, you can enable monitoring of that service by adding a configuration section for that integration to the Kubernetes integration's config. This tutorial shows how to enable monitoring for a Redis service running on the Kubernetes PHP Guestbook. For the general procedure, see Monitor a Kubernetes-running service. What you need See the general requirements for this feature, including supported services. The kubectl command-line tool must be configured to communicate with your cluster. If you don't have a cluster, you can create one using Minikube. Step 1: Set up an example Redis application This tutorial builds on the Kubernetes tutorial Deploying a PHP Guestbook application with Redis. Skip the Kubernetes tutorial and run the following command to set up the application needed for our tutorial: kubectl create -f https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml Copy If you'd like to first complete the Kubernetes tutorial, follow their tutorial instructions but do not follow the instructions in the Cleaning up section. Step 2: Enable monitoring of Redis instances The PHP Guestbook application has three Redis instances: one master and two slave instances. Each instance is tagged with a label where app=redis. For this example, we're using our Redis monitoring integration. It can monitor both master and slave instances of Redis, so we don’t have to distinguish between them. In the Kubernetes integration's YAML config file (newrelic-infrastructure-k8s-latest.yaml), you need to update the nri-integration-cfg section. From the list of integration configs, get the Redis integration YAML and add it to the Kubernetes config. The Redis YAML is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label \"app=redis\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: redis integrations: - name: nri-redis env: # using the discovered IP as the hostname address HOSTNAME: ${discovery.ip} PORT: 6379 KEYS: '{\"0\":[\"<KEY_1>\"],\"1\":[\"<KEY_2>\"]}' REMOTE_MONITORING: true labels: env: production Copy Deploy the updated service: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy You should be able to see the following in the logs for the pod newrelic-infra: time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check starting\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check finished with success\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations Copy If there are no errors, you should see Redis data in the Infrastructure UI. To find the Redis dashboards, go to one.newrelic.com > Infrastructure > Third party services, and select the Redis dashboard. For the general procedure of how to monitor services running on Kubernetes, including more detail about how configuration works, see Monitor a Kubernetes-running service.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.7674,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "sections": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-<em>integration</em>-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label &quot;<em>app</em>=redis&quot; # https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>integrations</em>&#x2F;host-<em>integrations</em>&#x2F;installation&#x2F;container-auto-discovery discovery"
      },
      "id": "603e7e8264441f332a4e8879"
    },
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.35437,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/certificate-signed-unknown-authority": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.02457,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.35135,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Get logs and version",
        "Get verbose logs for installations using a manifest file",
        "Caution",
        "Get verbose logs for installations using Helm",
        "Get the infrastructure agent version",
        "Get kube-state-metrics version",
        "Get logs from pods connecting to kube-state-metrics",
        "Retrieve kube-state-metrics service configuration",
        "Get logs from pod running on a master node",
        "For more help"
      ],
      "title": "Get logs and version",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "d1c70664aab784f3d4b336ada95041373edd03dc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version/",
      "published_at": "2021-09-13T18:46:35Z",
      "updated_at": "2021-08-08T14:05:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you are doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using a manifest file: Enable verbose logging: In the deployment file, set the value of NRIA_VERBOSE to 1. Apply the modified configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Leave on verbose mode for a few minutes, or until you feel enough activity has occurred. Disable verbose mode: Set the NRIA_VERBOSE value back to 0. Apply the restored configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Get a list of nodes in the environment: kubectl get nodes --all-namespaces Copy Get a list of infrastructure and kube-state-metrics pods: kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics' Copy Get logs from the pods connecting to kube-state-metrics. Retrieve kube-state-metrics service configuration. Get verbose logs for installations using Helm For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using Helm: Enable verbose logging: helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=true newrelic/nri-bundle Copy Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: helm upgrade --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=false newrelic/nri-bundle Copy Follow the steps to restore your configuration from step 5 in the section, Get verbose logs for installations using a manifest file. Get the infrastructure agent version For the Kubernetes integration, the infrastructure agent is distributed as a Docker image that contains the infrastructure agent and the Kubernetes integration. The Docker image is tagged with a version, and the infrastructure agent also has its own version. When the agent is successfully sending information to New Relic, you can retrieve the versions of the infrastructure agent for Kubernetes (the Docker image) you are running in your clusters by using the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName = 'newrelic-infra' facet clusterName, containerImage Copy If the agent is not reporting any data: Get the version(s) of the New Relic integration for Kubernetes that you are running in a cluster using kubectl: kubectl get pods --all-namespaces -l name=newrelic-infra -o jsonpath=\"{.items..spec..containers..image}\" Copy Look for output similar to this: newrelic/infrastructure-k8s:1.0.0 Copy Get kube-state-metrics version To retrieve the version of kube-state-metrics running on your clusters, run the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName LIKE '%kube-state-metrics%' facet clusterName, containerImage Copy Get logs from pods connecting to kube-state-metrics To get the logs from pods connecting to kube-state-metrics: Get the nodes that kube-state-metrics is running on: kubectl get pods --all-namespaces -o wide | grep kube-state-metrics Copy Look for output similar to this: kube-system kube-state-metrics-5c6f5cb9b5-pclhh 2/2 Running 4 4d 172.17.0.3 minikube Copy Get the New Relic pods that are running on the same node as kube-state-metrics: kubectl describe node minikube | grep newrelic-infra Copy Look for output similar to this: default newrelic-infra-5wcv6 100m (5%) 0 (0%) 100Mi (5%) 100Mi (5%) Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-5wcv6 Copy Retrieve kube-state-metrics service configuration To retrieve the configuration: Run: kubectl get pods --all-namespaces | grep \"kube-state-metrics\" Copy Look for a response similar to this: kube-system kube-state-metrics-5c6f5cb9b5-5wf9m 2/2 Running 8 6d Copy Review the namespace in the first column. kubectl describe service kube-state-metrics -n <namespace> Copy Get logs from pod running on a master node To get the logs from a pod running on a master node: Get the nodes that are labelled as master: kubectl get nodes -l node-role.kubernetes.io/master=\"\" Copy Or, kubectl get nodes -l kubernetes.io/role=\"master\" Copy Look for output similar to this: NAME STATUS ROLES AGE VERSION ip-10-42-24-4.ec2.internal Ready master 42d v1.14.8 Copy Get the New Relic pods that are running on one of the nodes returned in the previous step: kubectl get pods --field-selector spec.nodeName=ip-10-42-24-4.ec2.internal -l name=newrelic-infra --all-namespaces Copy Look for output similar to this: newrelic-infra-whvzt Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-whvzt Copy For troubleshooting help, see Not seeing data or Error messages. For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.040695,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the <em>Kubernetes</em> <em>integration</em>, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed"
      },
      "id": "603eb9dc28ccbc174deba7a0"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.0245,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.3513,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-09-14T20:44:08Z",
      "updated_at": "2021-09-08T01:28:30Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Looking to install our New Relic One integration with Pixie for fine-grained telemetry data? See our Auto-telemetry with Pixie install instructions to get deeper insight into your Kubernetes clusters and workloads with just one install command. No language agents required. Want to try out our Kubernetes integration? Create a New Relic account for free! No credit card required. Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you do not wish to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Alternatively, we also offer fully manual instructions for deploying our integration using Helm. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integraiton to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path comapred to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f <mark>previous-manifest-file.yml</mark>, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f <mark>manifest-file.yml</mark>. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.862305,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> events, Prometheus OpenMetrics, and New Relic log monitoring. Looking"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-error-messages": [
    {
      "sections": [
        "Install the Kubernetes integration for Windows",
        "LIMITED RELEASE",
        "Compatibility and requirements",
        "Important",
        "Example: Get Kubernetes for Windows from a BusyBox container.",
        "Install",
        "Limitations",
        "Known issues with the Windows Kubelet"
      ],
      "title": "Install the Kubernetes integration for Windows",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "8b179894cbfc76f448c158f36d7ce1843f108dc0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows/",
      "published_at": "2021-09-14T20:44:07Z",
      "updated_at": "2021-09-14T20:44:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "LIMITED RELEASE This feature is a limited release. To run our Kubernetes installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the integration. Compatibility and requirements Before you install New Relic's Kubernetes integration, review the compatibility and requirements. Important When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration supports Windows versions 1809 and 1909. To check your Windows version: Open a command window. Run the following command: Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseIdcmd.exe Copy Example: Get Kubernetes for Windows from a BusyBox container. $ kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\" /v ReleaseId Copy HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion ReleaseId REG_SZ 1809 Copy Install You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004): Install kube-state-metrics and run it in your cluster using this snippet: curl -L -o kube-state-metrics-1.9.5.zip https://github.com/kubernetes/kube-state-metrics/archive/v1.9.5.zip && unzip kube-state-metrics-1.9.5.zip && kubectl apply -f kube-state-metrics-1.9.5/examples/standard Copy Create a values.yml file with the follow data to be used by Helm: global: licenseKey: <YOUR_LICENSE_KEY> cluster: <YOUR_CLUSTER_NAME> enableLinux: false enableWindows: true windowsOsList: - version: 1809 imageTag: 2.2.0-windows-1809-alpha buildNumber: 10.0.17763 - version: 2004 imageTag: 2.2.0-windows-2004-alpha buildNumber: 10.0.19041 Copy Install the integration with: helm install <YOUR_INSTALL_NAME> newrelic/newrelic-infrastructure -f values.yml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy The Helm chart will create one DeamonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node. Limitations The following limitations apply to the Kubernetes integration for Windows: The Windows agent only sends the Kubernetes samples (K8sNodeSample, K8sPodSample, etc.) SystemSample, StorageSample, NetworkSample, and ProcessSample are not generated. Some Kubernetes metrics are missing because the Windows kubelet doesn’t have them: Node: fsInodes: not sent fsInodesFree: not sent fsInodesUsed: not sent memoryMajorPageFaultsPerSecond: always returns zero as a value memoryPageFaults: always returns zero as a value memoryRssBytes: always returns zero as a value runtimeInodes: not sent runtimeInodesFree: not sent runtimeInodesUsed: not sent Pod: net.errorsPerSecond: not sent net.rxBytesPerSecond: not sent net.txBytesPerSecond: not sent Container: containerID: not sent containerImageID: not sent memoryUsedBytes: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use memoryWorkingSetBytes instead. Volume: fsUsedBytes: zero, so fsUsedPercent is zero Known issues with the Windows Kubelet There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data: Issue 90554: This issue makes the Kubelet return 500 errors when the integration makes a request to the /stats/summary endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": error calling kubelet endpoint. Got status code: 500 Copy Issue 87730: This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the TIMEOUT config option. You can see if you're being affected by this problem by enabling verbose logs and looking for messages of the type: error querying Kubelet. Get \"https://<KUBELET_IP>/stats/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.0245,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> for Windows",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "LIMITED RELEASE This feature is a limited release. To run our <em>Kubernetes</em> installation for Windows, follow the steps below to: Check the compatibility and requirements. Update the manifest file. Learn about the limitations of the <em>integration</em>. Compatibility and requirements Before you install New"
      },
      "id": "603e814028ccbc1ce0eba780"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-09-14T07:25:39Z",
      "updated_at": "2021-09-14T07:25:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.3513,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Get logs and version",
        "Get verbose logs for installations using a manifest file",
        "Caution",
        "Get verbose logs for installations using Helm",
        "Get the infrastructure agent version",
        "Get kube-state-metrics version",
        "Get logs from pods connecting to kube-state-metrics",
        "Retrieve kube-state-metrics service configuration",
        "Get logs from pod running on a master node",
        "For more help"
      ],
      "title": "Get logs and version",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "d1c70664aab784f3d4b336ada95041373edd03dc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version/",
      "published_at": "2021-09-13T18:46:35Z",
      "updated_at": "2021-08-08T14:05:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you are doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using a manifest file: Enable verbose logging: In the deployment file, set the value of NRIA_VERBOSE to 1. Apply the modified configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Leave on verbose mode for a few minutes, or until you feel enough activity has occurred. Disable verbose mode: Set the NRIA_VERBOSE value back to 0. Apply the restored configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Get a list of nodes in the environment: kubectl get nodes --all-namespaces Copy Get a list of infrastructure and kube-state-metrics pods: kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics' Copy Get logs from the pods connecting to kube-state-metrics. Retrieve kube-state-metrics service configuration. Get verbose logs for installations using Helm For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using Helm: Enable verbose logging: helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=true newrelic/nri-bundle Copy Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: helm upgrade --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=false newrelic/nri-bundle Copy Follow the steps to restore your configuration from step 5 in the section, Get verbose logs for installations using a manifest file. Get the infrastructure agent version For the Kubernetes integration, the infrastructure agent is distributed as a Docker image that contains the infrastructure agent and the Kubernetes integration. The Docker image is tagged with a version, and the infrastructure agent also has its own version. When the agent is successfully sending information to New Relic, you can retrieve the versions of the infrastructure agent for Kubernetes (the Docker image) you are running in your clusters by using the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName = 'newrelic-infra' facet clusterName, containerImage Copy If the agent is not reporting any data: Get the version(s) of the New Relic integration for Kubernetes that you are running in a cluster using kubectl: kubectl get pods --all-namespaces -l name=newrelic-infra -o jsonpath=\"{.items..spec..containers..image}\" Copy Look for output similar to this: newrelic/infrastructure-k8s:1.0.0 Copy Get kube-state-metrics version To retrieve the version of kube-state-metrics running on your clusters, run the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName LIKE '%kube-state-metrics%' facet clusterName, containerImage Copy Get logs from pods connecting to kube-state-metrics To get the logs from pods connecting to kube-state-metrics: Get the nodes that kube-state-metrics is running on: kubectl get pods --all-namespaces -o wide | grep kube-state-metrics Copy Look for output similar to this: kube-system kube-state-metrics-5c6f5cb9b5-pclhh 2/2 Running 4 4d 172.17.0.3 minikube Copy Get the New Relic pods that are running on the same node as kube-state-metrics: kubectl describe node minikube | grep newrelic-infra Copy Look for output similar to this: default newrelic-infra-5wcv6 100m (5%) 0 (0%) 100Mi (5%) 100Mi (5%) Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-5wcv6 Copy Retrieve kube-state-metrics service configuration To retrieve the configuration: Run: kubectl get pods --all-namespaces | grep \"kube-state-metrics\" Copy Look for a response similar to this: kube-system kube-state-metrics-5c6f5cb9b5-5wf9m 2/2 Running 8 6d Copy Review the namespace in the first column. kubectl describe service kube-state-metrics -n <namespace> Copy Get logs from pod running on a master node To get the logs from a pod running on a master node: Get the nodes that are labelled as master: kubectl get nodes -l node-role.kubernetes.io/master=\"\" Copy Or, kubectl get nodes -l kubernetes.io/role=\"master\" Copy Look for output similar to this: NAME STATUS ROLES AGE VERSION ip-10-42-24-4.ec2.internal Ready master 42d v1.14.8 Copy Get the New Relic pods that are running on one of the nodes returned in the previous step: kubectl get pods --field-selector spec.nodeName=ip-10-42-24-4.ec2.internal -l name=newrelic-infra --all-namespaces Copy Look for output similar to this: newrelic-infra-whvzt Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-whvzt Copy For troubleshooting help, see Not seeing data or Error messages. For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.04069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the <em>Kubernetes</em> <em>integration</em>, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed"
      },
      "id": "603eb9dc28ccbc174deba7a0"
    }
  ]
}