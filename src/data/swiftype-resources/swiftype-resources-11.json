{
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-load-balancing-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01006,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-pubsub-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-router-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-run-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-spanner-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-sql-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Run monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Run Revision data"
      ],
      "title": "Google Cloud Run monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "1dc9fef515b9aa784c6313448ac6ec5808e4781c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-run-monitoring-integration/",
      "published_at": "2021-07-09T18:11:36Z",
      "updated_at": "2021-03-16T05:44:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP Run data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Run integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Revision GcpRunRevisionSample GcpRunRevision For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Run data for Revision. Run Revision data Metric Unit Description container.BillableInstanceTime Seconds Billable time aggregated from all container instances of the revision. For a given container instance, billable time occurs when the container instance is starting or at least one request is being processed. Billable time is rounded up to the nearest 100 milliseconds. Examples: If a revision with 2 container instances has been continuously serving traffic in the last minute, the value is 2s/s with the default \"rate\" aligner. If a single request lasting 30ms was received by a revision in the past minute, it is rounded up to 100ms and averaged to 1.7ms/s over the minute with the default \"rate\" aligner. container.cpu.AllocationTime Seconds Container CPU allocation of the revision in seconds. container.memory.AllocationTime Other Container memory allocation of the revision in Gigabytes-seconds. Request Count Number of requests reaching the revision. Excludes requests that are not reaching your container instances (e.g. unauthorized requests or when maximum number of instances is reached). RequestLatencies Milliseconds Distribution of request latency in milliseconds reaching the revision.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.00995,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Run monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Run monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> Run data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e811328ccbc2883eba789"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration": [
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    },
    {
      "sections": [
        "Google Cloud Run monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Run Revision data"
      ],
      "title": "Google Cloud Run monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "1dc9fef515b9aa784c6313448ac6ec5808e4781c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-run-monitoring-integration/",
      "published_at": "2021-07-09T18:11:36Z",
      "updated_at": "2021-03-16T05:44:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP Run data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Run integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Revision GcpRunRevisionSample GcpRunRevision For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Run data for Revision. Run Revision data Metric Unit Description container.BillableInstanceTime Seconds Billable time aggregated from all container instances of the revision. For a given container instance, billable time occurs when the container instance is starting or at least one request is being processed. Billable time is rounded up to the nearest 100 milliseconds. Examples: If a revision with 2 container instances has been continuously serving traffic in the last minute, the value is 2s/s with the default \"rate\" aligner. If a single request lasting 30ms was received by a revision in the past minute, it is rounded up to 100ms and averaged to 1.7ms/s over the minute with the default \"rate\" aligner. container.cpu.AllocationTime Seconds Container CPU allocation of the revision in seconds. container.memory.AllocationTime Other Container memory allocation of the revision in Gigabytes-seconds. Request Count Number of requests reaching the revision. Excludes requests that are not reaching your container instances (e.g. unauthorized requests or when maximum number of instances is reached). RequestLatencies Milliseconds Distribution of request latency in milliseconds reaching the revision.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.00995,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Run monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Run monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> Run data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e811328ccbc2883eba789"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-datastore-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-direct-interconnect-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-kubernetes-engine-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01021,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration": [
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-07-09T17:44:12Z",
      "updated_at": "2021-05-15T19:35:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.63623,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-07-09T17:34:12Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.01004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    },
    {
      "sections": [
        "Google Cloud Run monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Run Revision data"
      ],
      "title": "Google Cloud Run monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "1dc9fef515b9aa784c6313448ac6ec5808e4781c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-run-monitoring-integration/",
      "published_at": "2021-07-09T18:11:36Z",
      "updated_at": "2021-03-16T05:44:59Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP Run data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP Run integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Revision GcpRunRevisionSample GcpRunRevision For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP Run data for Revision. Run Revision data Metric Unit Description container.BillableInstanceTime Seconds Billable time aggregated from all container instances of the revision. For a given container instance, billable time occurs when the container instance is starting or at least one request is being processed. Billable time is rounded up to the nearest 100 milliseconds. Examples: If a revision with 2 container instances has been continuously serving traffic in the last minute, the value is 2s/s with the default \"rate\" aligner. If a single request lasting 30ms was received by a revision in the past minute, it is rounded up to 100ms and averaged to 1.7ms/s over the minute with the default \"rate\" aligner. container.cpu.AllocationTime Seconds Container CPU allocation of the revision in seconds. container.memory.AllocationTime Other Container memory allocation of the revision in Gigabytes-seconds. Request Count Number of requests reaching the revision. Excludes requests that are not reaching your container instances (e.g. unauthorized requests or when maximum number of instances is reached). RequestLatencies Milliseconds Distribution of request latency in milliseconds reaching the revision.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.00995,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Run monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Run monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> Run data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e811328ccbc2883eba789"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-05-28T05:06:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Role Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list serviceusage.services.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Role Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.45726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-07-09T19:10:54Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.8182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "GCP integration metrics",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-07-09T19:10:04Z",
      "updated_at": "2021-03-16T05:48:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.62085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "<em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization"
      },
      "id": "603e8a5264441f524a4e8840"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-05-28T05:06:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Role Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list serviceusage.services.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Role Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.45726,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-07-09T19:10:54Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.8182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-07-09T19:10:01Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.80779,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles": [
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-07-09T19:10:54Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.8182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-07-09T19:10:01Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.80779,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "GCP integration metrics",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-07-09T19:10:04Z",
      "updated_at": "2021-03-16T05:48:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.62085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "<em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization"
      },
      "id": "603e8a5264441f524a4e8840"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-05-28T05:06:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Role Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list serviceusage.services.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Role Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.45724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-07-09T19:10:01Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.80779,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "GCP integration metrics",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-07-09T19:10:04Z",
      "updated_at": "2021-03-16T05:48:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.62085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "<em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization"
      },
      "id": "603e8a5264441f524a4e8840"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/getting-started/polling-intervals-gcp-integrations": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-05-28T05:06:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Role Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list serviceusage.services.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Role Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.45724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-07-09T19:10:54Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.8182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-07-09T19:10:01Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.80779,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/troubleshooting/gcp-integration-api-authentication-errors": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-07-09T19:10:02Z",
      "updated_at": "2021-05-28T05:06:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Role Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list serviceusage.services.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Role Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 133.15497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-07-09T19:10:01Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.94272,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To start receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-07-09T19:10:54Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.72608,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    }
  ],
  "/docs/integrations/grafana-integrations/get-started/grafana-support-prometheus-promql": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if you’re looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You won’t have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and there’ll be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free — we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relic’s Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.86902,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Prometheus OpenMetrics <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " to <em>get</em> <em>started</em>, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics <em>integrations</em> gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.36888,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes <em>integration</em>: compatibility and requirements",
        "sections": "Kubernetes <em>integration</em>: compatibility and requirements",
        "tags": "<em>Get</em> <em>started</em>"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 99.671364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick <em>start</em>: Use our guided install",
        "tags": "<em>Get</em> <em>started</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided install The quickest way to <em>get</em> <em>started</em> with our infrastructure monitoring agent is through our guided install. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided install EU Guided install"
      },
      "id": "603e79bd64441f99814e8888"
    }
  ],
  "/docs/integrations/grafana-integrations/set-configure/configure-new-relic-prometheus-data-source-grafana": [
    {
      "sections": [
        "Grafana support with Prometheus and PromQL",
        "Use existing Grafana dashboards with New Relic",
        "Compatibility and requirements",
        "Support for PromQL",
        "Get data flowing in Grafana",
        "What’s next?"
      ],
      "title": "Grafana support with Prometheus and PromQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Grafana integrations",
        "Get started"
      ],
      "external_id": "52addf26732e0146545ae8dee6540d3bd7cab2ff",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/grafana-integrations/get-started/grafana-support-prometheus-promql/",
      "published_at": "2021-07-09T17:47:20Z",
      "updated_at": "2021-04-22T13:30:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In Grafana, you can configure New Relic as a Prometheus data source. Not only that, within Grafana you can query metrics stored in New Relic using the PromQL query language. Use existing Grafana dashboards with New Relic When you integrate Prometheus metrics with New Relic via Remote Write or the OpenMetrics Integration (2.0+) and configure New Relic as a Prometheus data source in Grafana, you can use existing Grafana dashboards and seamlessly tap into the additional monitoring, reliability, and scale we provide. Compatibility and requirements Before you begin, make sure you’ve finished integrating Prometheus metrics and are running a recent enough version of Grafana. You should have either the Remote Write or the OpenMetrics Integration ( v2.0+) set up before you can configure New Relic Prometheus data sources in Grafana. You can only configure New Relic Prometheus data sources using this method in Grafana versions 6.7.0 or newer. You will need to configure custom headers in the UI, and this isn’t possible with earlier versions. For details, see Configure New Relic as a Prometheus data source for Grafana. Support for PromQL Our Prometheus API emulates Prometheus' query APIs. We support the Prometheus query language (PromQL) through our PromQL-style query mode. We do our best to automatically translate PromQL syntax queries into the closest NRQL approximation. For more information on how this works and differences you may observe between Prometheus and New Relic, see Supported PromQL features. Get data flowing in Grafana To make your New Relic data available in Grafana, you can configure a new or existing Prometheus data source in just a couple of simple steps: In the Grafana UI, add and configure a new data source. Save the new data source and start viewing your data. What’s next? Ready to configure a Grafana data source? Read the how-to documentation for setting up the Prometheus remote write integration or the Prometheus OpenMetrics Integration. Read the how-to documentation for configuring Prometheus data sources in Grafana.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 113.202934,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Grafana</em> support with Prometheus <em>and</em> PromQL",
        "sections": "<em>Grafana</em> support with Prometheus <em>and</em> PromQL",
        "tags": "<em>Grafana</em> <em>integrations</em>",
        "body": " integrating Prometheus metrics and are running a recent enough version of <em>Grafana</em>. You should have either the Remote Write or the OpenMetrics Integration ( v2.0+) <em>set</em> <em>up</em> before you can <em>configure</em> New Relic Prometheus data sources in <em>Grafana</em>. You can only <em>configure</em> New Relic Prometheus data sources using"
      },
      "id": "603e94de64441f9a804e8843"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 47.62259,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, <em>configure</em> this to 1ms. To never expire metrics, <em>set</em> it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "OpenCensus exporter",
        "Enable exporter",
        "Find your data",
        "Resources"
      ],
      "title": "OpenCensus exporter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenCensus"
      ],
      "external_id": "032a667b90f9a608db4467c364ccbe39d69c48e3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opencensus/opencensus-exporter/",
      "published_at": "2021-07-09T14:13:03Z",
      "updated_at": "2021-03-30T12:39:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenCensus is a set of libraries for various languages that allow you to collect application metrics and distributed traces, then transfer the data to a backend of your choice. New Relic's OpenCensus exporters send telemetry data from your OpenCensus-instrumented applications to your New Relic account. Enable exporter To enable a New Relic OpenCensus exporter: Follow the install procedures: Go language exporter Python language exporter Optional: Instead of using the native OpenCensus trace sampling, you can enable our Infinite Tracing feature. If you do this, you will typically want to configure OpenCensus to send us all trace data (learn more about sampling). To enable Infinite Tracing: In the New Relic UI, set up a trace observer. Configure the exporter to send data to the trace observer: Go exporter: Set the SpansURLOverride field on the Config object with YOUR_TRACE_OBSERVER_URL when creating the Exporter. Python exporter: Pass the host parameter to the trace exporter using YOUR_TRACE_OBSERVER_HOST. Find your data To find your data, go to one.newrelic.com and go to Explorer. From the Entities screen, search for your service by name. From there, you can explore your metrics using the Data explorer and build dashboards using your metrics. If you're also sending distributed tracing data, the distributed tracing feature is available to query and view traces. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL. Resources Our OpenCensus exporter resources include: Go language exporter Python language exporter OpenCensus metric exporter specs",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 42.225643,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Integrations</em>",
        "body": " to <em>configure</em> OpenCensus to send us all trace data (learn more about sampling). To enable Infinite Tracing: In the New Relic UI, <em>set</em> <em>up</em> a trace observer. <em>Configure</em> the exporter to send data to the trace observer: Go exporter: <em>Set</em> the SpansURLOverride field on the Config object with YOUR_TRACE_OBSERVER_URL"
      },
      "id": "603e95ab196a6708e6a83dcb"
    }
  ],
  "/docs/integrations/host-integrations/get-started/introduction-host-integrations": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.09048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": "Our StatsD integration lets you easily <em>get</em> StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.87503,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick <em>start</em>: Use our guided install",
        "tags": "<em>Get</em> <em>started</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided install The quickest way to <em>get</em> <em>started</em> with our infrastructure monitoring agent is through our guided install. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided install EU Guided install"
      },
      "id": "603e79bd64441f99814e8888"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 157.73856,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/apache-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6772,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/cassandra-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6769,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1811,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/collectd-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6769,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1811,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/couchbase-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6769,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1811,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18097,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03986,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18097,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03986,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/flex-integration-tool-build-your-own-integration": [
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Tip",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-07-09T10:15:31Z",
      "updated_at": "2021-03-16T07:30:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.67343,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": " <em>integrations</em>, such as our APM agents. Whatever data you need to bring in, chances are that we have options for <em>your</em> environment. If you prefer to make <em>your</em> <em>own</em> solutions, we also offer tools to get you started. Tip To use <em>integrations</em> and infrastructure monitoring, as well as the rest of our"
      },
      "id": "603e817f28ccbc4857eba798"
    },
    {
      "sections": [
        "Browser monitoring best practices guide",
        "1. Use browser SPA agent",
        "How to do it",
        "2. Use APM auto-instrumentation",
        "Tip",
        "3. Customize Apdex",
        "4. Create alerts",
        "5. Use baseline alerts",
        "6. Add alerts to workflows",
        "7. Create dashboards",
        "8. Group your data",
        "9. Get the right data",
        "10. Break down performance data",
        "Want more user tips?"
      ],
      "title": "Browser monitoring best practices guide",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "Best practices guides",
        "Full-stack observability"
      ],
      "external_id": "65e00c29691e851341248bcadf7302e955718ca8",
      "image": "https://docs.newrelic.com/static/bcbac83ecd75ee65ad36bbf8299ad89f/c1b63/best01_spapro.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/best-practices-guides/full-stack-observability/browser-monitoring-best-practices-guide/",
      "published_at": "2021-07-09T09:17:17Z",
      "updated_at": "2021-07-09T09:17:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Browser monitoring tips & tricks that every user should know Get more out of browser monitoring, with deeper visibility into your websites and your users. Here are 10 best practices to help you find problems faster and deliver a better experience to your customers with real user monitoring. 1. Use browser SPA agent Use the browser SPA agent for deeper visibility Modern websites are complex, with dynamic content and sophisticated logic. See deeper into your user interactions, enable the single-page application (SPA) agent for browser and gain visibility into your user actions and the underlying webpage events behind them. This framework-agnostic agent is not only for single-page application frameworks like React, Angular, Ember, or Backbone, but also for custom frameworks and any other pages with dynamic content. Advanced timing analytics provide more granularity into Navigation Timing Specification API sub-timings beyond page rendering or DOM processing, with detailed performance data filtering useful for understanding all page lifecycles. How to do it From the browser app list, select an app, then select Settings > Application Settings. Select Pro + SPA. Click Save application settings Change your browser monitoring settings to switch to SPA. 2. Use APM auto-instrumentation Use APM automatic instrumentation over copy/paste If you’re also a APM customer, we recommend enabling automatic instrumentation where possible, as this will automatically inject the browser monitoring JavaScript agent into your frontend for you. Not only will the browser agent remain automatically up to date with this approach, using these products together helps unify frontend to backend visibility. For example, you’d be able to link frontend AJAX calls to their corresponding backend transaction, and to align your frontend and backend data together in an Insights dashboard. Tip Depending on your backend framework or CDN strategy, a copy/paste approach may be the better strategy. Just remember that it’ll require periodic updating. How to do it Select Settings > Application Settings. Select Enable via New Relic APM. Select Pro + SPA agent. Click Save application settings. one.newrelic.com > Browser > Settings > Application Settings 3. Customize Apdex Customize your Apdex threshold New Relic uses Apdex, an industry-standard metric, to measure users’ satisfaction with the response time of your applications. Apdex converts many measurements into one number on a uniform scale of 0 to 1 (0 = no users satisfied, 1 = all users satisfied). Apdex T is the central value for Apdex—it is the response time below which a transaction is considered “Satisfactory.” You can define Apdex T values for each application, with separate values for app server and end-user browser performance. (Note that many modern websites are bottlenecked primarily in the frontend browser, accounting for 90% or more of pageload times.) While a suggested T-value threshold is 5 seconds, we encourage you to customize your Apdex T value to fit the needs of your users and application. For example, an ecommerce site may want to have a lower T value to reduce bounce rates from potential buyers frustrated by a slow experience, compared to an internal employee application. However, the goal is to continually lower T-values over time to improve your digital customer experience. How to do it From the browser app list, select an app, then select Settings > Application settings. Set the Apdex T value (in seconds) for this application. Click Save application settings. one.newrelic.com > Browser > Settings > Application 4. Create alerts Create and evaluate alert policies What happens if your Apdex score exceeds your threshold? You want to be alerted whenever this happens, so that you can investigate what is causing the issue before you lose any customers. New Relic provides unified alerting across all our products, including browser monitoring, so that you’ll always be in the know. We recommend setting up alerts to monitor your Apdex score, along with these sample alerts to get you started: Apdex score: Alert if score is below 0.8 for 5 minutes. (If the Apdex score is below 0.8, that means 20% or more of your users are not “satisfied” with their experience on your website.) Page load time: Alert if median page load time is above 10 seconds for 5 minutes. (If the median page load time begins to spike, that suggest that something may be wrong with your web page causing it to significantly slow down. This complements alerting on your Apdex score.) JS errors: Alert if error rate is above 5% for 5 minutes. (If your frontend error rate starts spiking, particularly after a deployment, you may have introduced bad JavaScript into your frontend that should be fixed.) How to do it From Alerts & AI, select Go to Alerts, and click on Policies on the left column. Select (+) New alert policy to create a new alert policy, and give it a meaningful name. Click on Create a condition to create your first condition, then select Browser for your product and the Metric for the condition (which should be the default). Then click Next, select entities. Click the checkbox for the applications you want to alert on, and click Next, define thresholds. Pick the metrics of interest and their thresholds that determine when to trigger an alert. one.newrelic.com > Alerts & AI > Policies > New alert policy 5. Use baseline alerts Alert on anomalous behaviors and events While some metrics can be easily tracked against specific thresholds, other types of data can be more cyclical or have variable ranges for what’s considered healthy. Traffic throughput is a good example of this; it can have significantly cycles, but large traffic drops or spikes may be important indicators of a breakage preventing user traffic or a DDoS attack spiking traffic. Baseline alerts can be helpful for creating an expected \"band\" of normal activity to create more signal to noise in your alerting. How to do it From Alerts & AI, select Go to Alerts, and click on Policies on the left column. Select (+) New alert policy to create a new alert policy, and give it a meaningful name. Click on Create a condition to create your first condition, select Browser for your product and the Metric Baseline for the condition (which should be the default). Then click Next, select entities. Click the checkbox for the applications you want to alert on, and click Next, define thresholds. Pick the Page view throughput and use the slider to define the normal range you want to trigger the alert. one.newrelic.com > Alerts & AI > Policies > New alert policy 6. Add alerts to workflows Integrate your alerts into your workflows With the different alerting policies that you’re setting up, you’ll want to make sure to take advantage of the different alert notification channels available so that they’re integrated into team workflows. After all, what good are alerts if no one knows about them? You can route alerts through Slack, PagerDuty, webhooks, email, and more. You also have the opportunity to align alert notifications with your response processes, such as integrating with ChatOps or linking runbooks to your alerts. Be sure to evaluate alert policies on a regular basis to ensure that they are always valid. How to do it From Alerts & AI, click on Notification channels. Set up different notification channels, which can then be used within different alerting policies. 7. Create dashboards Create your own dashboards using New Relic Insights Browser automatically provides a curated experience to quickly triage issues, but you can also create bespoke dashboards with your frontend data with the query builder. Build unique real-time overviews of what’s going on in your frontend, and see how it fits with the rest of your technology stack. For example, you can: Create a browser-specific breakdown of percentiles, durations, and page views: SELECT average(duration),percentile(duration,50,75,95) as 'Percentile', count(*) as 'Pageviews' FROM PageView WHERE userAgentName='Chrome' SINCE 1 week ago Copy Break down route change performance into percentiles: SELECT count(*) as 'Route Change', average(duration) as 'Avg PageLoad',percentile(duration,50,75) as '%' ,average(jsDuration) as 'JS Duration' FROM BrowserInteraction FACET targetUrl where category = 'Route change' SINCE 1 day ago Copy Compare frontend performance to backend performance in APM: SELECT average(duration) as 'Avg Frontend Duration', average(backendDuration) FROM PageView SINCE 2 minutes ago UNTIL 10 seconds ago TIMESERIES Copy These individual widgets can be added together to create a team dashboard that’s shared across different engineering teams and the company. You can bring together web and mobile application data, backend data, infrastructure data, synthetic monitoring data, and more, all on a single custom dashboard across New Relic One. one.newrelic.com > Dashboards How to do it Go to one.newrelic.com, then click Query your data to access the query builder. In the NRQL tab of the query builder, paste the query into the NRQL> query bar and click Run. Type in a widget title, and add to either an existing or a new dashboard. Go to Dashboards to see your new dashboard. For more information, see our docs on data querying and dashboards. 8. Group your data Group your data into meaningful categories Different websites have different URL architectures, which can vary depending on content structure, technology framework, or SEO strategy. For example: website.com/product/widget-name website.com/gallery?product=109832 website.com/gallery/housewares/lamps/widget-name website.com/product#widget-name URL structure possibilities are endless, so browser includes an automatic grouping algorithm to set categories for that data into different URL groups. We recommend using URL grouping allowed lists to customize how your data is grouped together, which makes the corresponding performance information more useful and aligned to your website architecture. If the data generated by this default grouping is too high level, creating a URL grouping will disaggregate the data and give you greater granularity to make it more useful. For example, you could group by: Different page types: product pages vs. search pages Different forms, APIs, or user groupings Mobile vs. non-mobile Authenticated vs. non-authenticated Different content delivery networks (CDNs) How to do it In the browser monitoring UI, select Settings > Segment allow lists. In the Allow listed segments section, click on the + icon. Type in the URL segments you want to appear in groupings on the page views and AJAX pages. one.newrelic.com > Browser > Settings > Segment allow lists. 9. Get the right data Make sure you get data only from the right sources Browser data is generated from wherever your JavaScript agent is instrumented, though copies of your agent might become duplicated, such as through development, staging, or other environments, resulting in additional data being included from these sources that you would not want mixed with your production data. You can use domain conditions to allow or deny data from the different website domains you want monitored. How to do it In the browser monitoring UI, select Settings > Domain conditions. If there are no domain conditions in place, select Enable domain conditions. If conditions exist, select Next, Choose your setting. Select Deny only or Allow only to identify the data you want collected from the domains you want monitored. Then select Next, Create conditions. Enter the domain string conditions that you want to deny or allow data collection (maximum 10 conditions). Review and confirm your domain condition settings. one.newrelic.com > Browser > Settings > Domain conditions. 10. Break down performance data Break down performance across your users, business, and more By leveraging New Relic APIs, you can add vital context to your performance data as it relates to your technology, users, and your business. All our monitoring tools send their data to New Relic One, which enables plenty of customization and extensibility in your dashboarding. Just be sure to standardize naming for custom events and attributes across different data sources (such as between browser and mobile) for omnichannel engagement. Here are some examples of what you can measure: Build id: Pass in a build number to A/B test differences between versions User id: Manage your VIP customers and track their experience Cart value: Understand how much revenue is at risk when errors occur during checkout Content type: Track the type of content your users are viewing Video playback: See how users are consuming your media content The possibilities are endless. And as you can see, this type of reporting creates context around how your application performance impacts the rest of the business. How to do it Use our APIs to pass custom data about people, things, money, and more into New Relic One. Visit one.newrelic.com, and query on the custom data that is now available. For more information, see our docs on data querying and dashboards. one.newrelic.com > Dashboards > (selected dashboard). Want more user tips? View training videos at New Relic University. Read the Browser documentation. Check out our Tutorials page. Ask a question in the New Relic Community Forum.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 155.20047,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "2. Use APM auto-<em>instrumentation</em>",
        "tags": "<em>Full</em>-<em>stack</em> <em>observability</em>",
        "body": " alert policies on a regular basis to ensure that they are always valid. How to do it From Alerts &amp; AI, click on Notification channels. Set up different notification channels, which can then be used within different alerting policies. 7. Create dashboards Create <em>your</em> <em>own</em> dashboards using New Relic"
      },
      "id": "60441b4a28ccbc584a2c6095"
    },
    {
      "sections": [
        "Browser monitoring best practices in Java",
        "Ensure you are using the Apache Jasper compiler.",
        "Place meta tags immediately after the initial head tag.",
        "Avoid if ... else statements in the head section.",
        "Avoid expressions with the less than sign in the head section."
      ],
      "title": "Browser monitoring best practices in Java",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "Best practices guides",
        "Full-stack observability"
      ],
      "external_id": "536c56c90f1dcd9dee65692a229ffde9762f177e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/best-practices-guides/full-stack-observability/browser-monitoring-best-practices-java/",
      "published_at": "2021-07-09T09:17:17Z",
      "updated_at": "2021-07-09T09:17:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use browser monitoring with APM for Java to measure end-user load times. This document explains best practices for setting up browser monitoring. For information on how to set up browser monitoring for your Java app, see Browser monitoring and the Java agent. Ensure you are using the Apache Jasper compiler. The New Relic Java agent only auto-instruments pages compiled with the Apache Jasper compiler. The following application servers use the Jasper compiler by default: Tomcat Jetty Glassfish JBoss 4 Place meta tags immediately after the initial head tag. Recommendation: Place all <meta> tags immediately after the initial <head> tag. Some meta tags have requirements on how close they need to be to the beginning of an HTML document. The character encoding must be declared within the first 512 bytes of the HTML document. If the New Relic header is placed before the character encoding, it is possible for the character encoding to then be outside that 512 byte limit. Placing the character encoding meta tag immediately after the <head> tag will ensure that the New Relic header is positioned after the character encoding. <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <%= com.newrelic.api.agent.NewRelic.getBrowserTimingHeader() %> . . . </head> Copy The X-UA-Compatible meta tag should be within the meta tags immediately after the <head> tag. This tag allows page authors to set the document mode used for rending the page in Internet Explorer. This tag should be placed before any script tags. If the New Relic header is positioned before the X-UA-Compatible meta tag, the page might display incorrectly in Internet Explorer. <!DOCTYPE html> <html> <head> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=9\"> <%= com.newrelic.api.agent.NewRelic.getBrowserTimingHeader() %> . . . </head> Copy Avoid if ... else statements in the head section. The auto instrumentation script does not recognize if ... else statements within a JSP page. If you have an if ... else block before your first non-meta, non-title tag, the New Relic header might be placed in the incorrect position in the page. For example, this code could potentially cause the auto instrumentation script to be inserted before the meta tags: if (expression) { <nonmeta tag> } <meta tag> <meta tag> Copy Avoid expressions with the less than sign in the head section. The New Relic Java agent looks for the open angle bracket < to mark the beginning of an HTML tag. This means if you have an expression using a less than sign, then you will either need to change your expression to use a greater than sign or use manual instrumentation. For example: <head> <% for (i = 0; i < variable; i++) . . . %> Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 155.20047,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>stack</em> <em>observability</em>",
        "body": "You can use browser monitoring with APM for Java to measure end-user load times. This document explains best practices for setting up browser monitoring. For information on how to set up browser monitoring for <em>your</em> Java app, see Browser monitoring and the Java agent. Ensure you are using the Apache"
      },
      "id": "60441afc64441f7f65378f02"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/go-insights-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03983,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/haproxy-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03983,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/hashicorp-consul-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18073,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18073,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7",
        "Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-07-09T17:49:49Z",
      "updated_at": "2021-07-02T16:08:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. In the runcmd section, add the yum command to install the integration to the appropriate launch configuration. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Also edit the runcmd section to include the yum command to install nri-nginx: runcmd: - [ yum, install, newrelic-infra, -y ] - [ yum, install, nri-nginx, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.35062,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03972,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7",
        "Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-07-09T17:49:49Z",
      "updated_at": "2021-07-02T16:08:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. In the runcmd section, add the yum command to install the integration to the appropriate launch configuration. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Also edit the runcmd section to include the yum command to install nri-nginx: runcmd: - [ yum, install, newrelic-infra, -y ] - [ yum, install, nri-nginx, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.35056,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1806,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03972,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6757,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03967,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/mongodb-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6757,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03967,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6757,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03967,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/mysql-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6754,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18036,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03961,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6754,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18036,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03961,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nfs-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6754,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18036,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03961,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nginx-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67517,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18024,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/oracle-database-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67517,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18024,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/perfmon-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1801,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03952,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/port-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1801,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03952,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/postgresql-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1801,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03952,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/rabbitmq-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03946,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/redis-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.18,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03946,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/snmp-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67444,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.17987,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03943,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2": [
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.17987,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03943,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7",
        "Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-07-09T17:49:49Z",
      "updated_at": "2021-07-02T16:08:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/amazonlinux/2/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ yum, install, newrelic-infra, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. In the runcmd section, add the yum command to install the integration to the appropriate launch configuration. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Also edit the runcmd section to include the yum command to install nri-nginx: runcmd: - [ yum, install, newrelic-infra, -y ] - [ yum, install, nri-nginx, -y ] - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra.service ] - [ systemctl, start, --no-block, newrelic-infra.service ] Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 225.35028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/unix-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67444,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.17987,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03943,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/varnish-cache-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6742,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.17975,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6742,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.17975,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/vmware-vsphere-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.6742,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.17975,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/windows-services-integration": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/infrastructure-release-notes/infrastructure-agent-release-notes/new-relic-infrastructure-agent-1121/",
      "sections": [
        "Infrastructure agent v1.12.1",
        "Notes",
        "Added",
        "Fixed"
      ],
      "published_at": "2021-07-10T06:21:20Z",
      "title": "Infrastructure agent v1.12.1",
      "updated_at": "2021-03-13T03:15:26Z",
      "type": "docs",
      "external_id": "93e606131035b520d2d5f6be4220698349929793",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "Notes A new version of the agent has been released. Follow standard procedures to update your Infrastructure agent. Added Beta version (v0.1.0-beta) of nri-winservices is now packaged with the agent. For more information, see the Windows services integration documentation. Fixed d35cbe7 Fixed the sending of heartbeat samples to New Relic. 6503df0 Inventory is now fully re-sent if the host has been offline for 24 hours or if the agent ID changes. 4ccd9ff Fixed issue where running Docker auto discovery was leaking file descriptors.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 478.56232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "Notes A new version of the agent has been released. Follow standard procedures to update your Infrastructure agent. Added Beta version (v0.1.0-beta) of nri-winservices is now packaged with the agent. For more information, see the <em>Windows</em> <em>services</em> <em>integration</em> documentation. Fixed d35cbe7 Fixed"
      },
      "id": "6044211fe7b9d21ae05799cb"
    },
    {
      "sections": [
        "Link your applications to Kubernetes",
        "Tip",
        "Compatibility and requirements",
        "Kubernetes requirements",
        "Network requirements",
        "APM agent compatibility",
        "Openshift requirements",
        "Important",
        "Configure the injection of metadata",
        "Default configuration",
        "Custom configuration",
        "Manage custom certificates",
        "Validate the injection of metadata",
        "Disable the injection of metadata",
        "Troubleshooting"
      ],
      "title": "Link your applications to Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "2ae58989813695b48f4924529d6fd6ea17e5f6c5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/",
      "published_at": "2021-07-09T17:32:03Z",
      "updated_at": "2021-05-28T06:30:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can surface Kubernetes metadata and link it to your APM agents as distributed traces to explore performance issues and troubleshoot transaction errors. For more information, see this New Relic blog post. You can quickly start monitoring Kubernetes clusters using Auto-telemetry with Pixie, which is currently a beta release. This Pixie integration into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here's the code to link APM and infrastructure data and the code to automatically manage certificates. Compatibility and requirements Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements: Kubernetes requirements Network requirements APM agent compatibility OpenShift requirements Kubernetes requirements To link your applications and Kubernetes, your cluster must have the MutatingAdmissionWebhook controller enabled, which requires Kubernetes 1.9 or higher. To verify that your cluster is compatible, run the following command: kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 admissionregistration.k8s.io/v1beta1 Copy If you see a different result, follow the Kubernetes documentation to enable admission control in your cluster. Network requirements For Kubernetes to speak to our MutatingAdmissionWebhook, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster. This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc). Tip Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute. APM agent compatibility The following New Relic agents collect Kubernetes metadata: Go 2.3.0 or higher Java 4.10.0 or higher Node.js 5.3.0 or higher Python 4.14.0 or higher Ruby 6.1.0 or higher .NET 8.17.438 or higher Openshift requirements To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires Openshift 3.9 or higher. During the process, install a resource that requires admin permissions to the cluster. Run this to log in as admin: oc login -u system:admin Copy Check that webhooks are correctly configured. If they are not, update the master-config.yaml file. admissionConfig: pluginConfig: MutatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission ValidatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission location: \"\" Copy Important Add kubeConfigFile: /dev/null to address some issues in Openshift. Enable certificate signing by editing the YAML file and updating your configuration: kubernetesMasterConfig: controllerArguments: cluster-signing-cert-file: - \"/etc/origin/master/ca.crt\" cluster-signing-key-file: - \"/etc/origin/master/ca.key\" Copy Restart the Openshift services in the master node. Configure the injection of metadata By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see Validate the injection of metadata). This default configuration also uses the Kubernetes certificates API to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates. Default configuration To proceed with the default injection of metadata, follow these steps: Download the YAML file: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-latest.yaml Copy Custom configuration You can limit the injection of metadata only to specific namespaces by using labels. To enable this feature, edit your YAML file by finding and uncommenting the following lines: # namespaceSelector: # matchLabels: # newrelic-metadata-injection: enabled Copy With this option, injection is only applied to those namespaces that have the newrelic-metadata-injection label set to enabled: kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled Copy Manage custom certificates To use custom certificates you need a specific YAML file: Download the YAML file without automatic certificate management: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-custom-certs-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-custom-certs-latest.yaml Copy Once you have the correct YAML file, you can proceed with the custom certificate management option. You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format. If you have them in the standard certificate format (X.509), install openssl, and run the following: openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem Copy If your certificate/key pair are in another format, see the Digicert knowledgebase for more help. Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands: kubectl create secret tls newrelic-metadata-injection-secret \\ --key=PEM_ENCODED_SERVER_KEY \\ --cert=PEM_ENCODED_CERTIFICATE \\ --dry-run -o yaml | kubectl -n default apply -f - caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d '\\n') kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p \"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]\" Copy Important Certificates signed by Kubernetes have an expiration of one year. For more information, see the Kubernetes source code in GitHub. Validate the injection of metadata In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables. Create a dummy pod containing Busybox by running: kubectl create -f https://git.io/vPieo Copy Check if New Relic environment variables were injected: kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0 NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox Copy Disable the injection of metadata To disable/uninstall the injection of metadata, use the following commands: Delete the Kubernetes objects using the yaml file: kubectl delete -f k8s-metadata-injection-latest.yaml Copy Delete the TLS secret containing the certificate/key pair: kubectl delete secret/newrelic-metadata-injection-secret Copy Troubleshooting Follow these troubleshooting tips as needed. No Kubernetes metadata in APM or distributed tracing transactions Problem The creation of the secret by the k8s-webhook-cert-manager job used to fail due to the kubectl version used by the image when running in Kubernetes version 1.19.x, The new version 1.3.2 fixes this issue, therefore it is enough to run again the job using an update version of the image to fix the issue. Solution Update the image k8s-webhook-cert-manager (to a version >= 1.3.2) and re-run the job. The secret will be correctly created and the k8s-metadata-injection pod will be able to start. Note that the new version of the manifest and of the nri-bundle are already updated with the correct version of the image. Problem In OpenShift version 4.x, the CA that is used in order to patch the mutatingwebhookconfiguration resource is not the one used when signing the certificates. This is a known issue currently tracked here. In the logs of the Pod nri-metadata-injection, you'll see the following error message: TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate Copy Workaround Manually update the certificate stored in the mutatingwebhookconfiguration object. The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret csr-signer in the namespace openshift-kube-controller-manager. Problem There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing. Solution Verify that the environment variables are being correctly injected by following the instructions described in the Validate your installation step. If they are not present, get the name of the metadata injection pod by running: kubectl get pods | grep newrelic-metadata-injection-deployment kubectl logs -f pod/podname Copy In another terminal, create a new pod (for example, see Validate your installation), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like: {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.107Z\",\"caller\":\"server/main.go:139\",\"msg\":\"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\\\" from 10.11.49.2:32836\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.110Z\",\"caller\":\"server/webhook.go:168\",\"msg\":\"received admission review\",\"kind\":\"/v1, Kind=Pod\",\"namespace\":\"default\",\"name\":\"\",\"pod\":\"busybox1\",\"UID\":\"6577519b-7a61-11ea-965e-0e46d1c9335c\",\"operation\":\"CREATE\",\"userinfo\":{\"username\":\"admin\",\"uid\":\"admin\",\"groups\":[\"system:masters\",\"system:authenticated\"]}} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:182\",\"msg\":\"admission response created\",\"response\":\"[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env\\\",\\\"value\\\":[{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\\\",\\\"value\\\":\\\"adn_kops\\\"}]},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"spec.nodeName\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\\\",\\\"value\\\":\\\"busybox\\\"}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\\\",\\\"value\\\":\\\"busybox\\\"}}]\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:257\",\"msg\":\"writing response\"} Copy If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication. To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like: failed calling webhook \"metadata-injection.newrelic.com\": ERROR_REASON Copy To get the apiserver logs: Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running. kubectl proxy --port=8001 Copy Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox. kubectl create -f https://git.io/vPieo Copy Retrieve the apiserver logs. curl localhost:8001/logs/kube-apiserver.log > apiserver.log Copy Delete the busybox container. kubectl delete -f https://git.io/vPieo Copy Inspect the logs for errors. grep -E 'failed calling webhook' apiserver.log Copy Remember that one of the requirements for the metadata injection is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster. If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered. Ensure the metadata injection setup job ran successfully by inspecting the output of: kubectl get job newrelic-metadata-setup Copy If the job is not completed, investigate the logs of the setup job: kubectl logs job/newrelic-metadata-setup Copy Ensure the CertificateSigningRequest is approved and issued by running: kubectl get csr newrelic-metadata-injection-svc.default Copy Ensure the TLS secret is present by running: kubectl get secret newrelic-metadata-injection-secret Copy Ensure the CA bundle is present in the mutating webhook configuration: kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json Copy Ensure the TargetPort of the Service resource matches the Port of the Deployment's container: kubectl describe service/newrelic-metadata-injection-svc kubectl describe deployment/newrelic-metadata-injection-deployment Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 97.280235,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Integrations</em>",
        "body": " is currently a beta release. This Pixie <em>integration</em> into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here&#x27;s the code to link APM and infrastructure data and the code to automatically"
      },
      "id": "603ebb94196a674fd1a83df3"
    },
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "4c67f6272bda36eda4ad7883e89697a203aa2153",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2021-07-09T19:24:57Z",
      "updated_at": "2021-05-16T04:41:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.13.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 94.36908,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor <em>services</em> running on Kubernetes",
        "sections": "Monitor <em>services</em> in our Kubernetes <em>integration</em> installed with Helm",
        "tags": "<em>Integrations</em>",
        "body": "With New Relic&#x27;s Kubernetes <em>integration</em> you can monitor both Kubernetes and the <em>services</em> running on it, such as Cassandra, Redis, MySQL, and other supported <em>services</em>. Get started Our Kubernetes <em>integration</em> comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache"
      },
      "id": "6044e50c196a676012960f35"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/zookeeper-monitoring-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.67395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold <em>list</em> of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.17963,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.03932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/installation/container-auto-discovery-host-integrations": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.0885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.71689,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Prepare for the <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". Kubernetes See Monitor service running on Kubernetes. Linux <em>installation</em> Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration file"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.05069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the infrastructure agent",
        "sections": "Other <em>installation</em> scenarios",
        "tags": "<em>Install</em> the infrastructure agent",
        "body": " and processes of the <em>host</em> where it&#x27;s enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-<em>host</em> <em>integrations</em>, and logs. If you want to collect data about core services running on your <em>host</em>, you need to install"
      },
      "id": "603e79bd64441f99814e8888"
    }
  ],
  "/docs/integrations/host-integrations/installation/install-infrastructure-host-integrations": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.0885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.71689,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Prepare for the <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". Kubernetes See Monitor service running on Kubernetes. Linux <em>installation</em> Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration file"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.05069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the infrastructure agent",
        "sections": "Other <em>installation</em> scenarios",
        "tags": "<em>Install</em> the infrastructure agent",
        "body": " and processes of the <em>host</em> where it&#x27;s enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-<em>host</em> <em>integrations</em>, and logs. If you want to collect data about core services running on your <em>host</em>, you need to install"
      },
      "id": "603e79bd64441f99814e8888"
    }
  ],
  "/docs/integrations/host-integrations/installation/secrets-management": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.0885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.71689,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Prepare for the <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". Kubernetes See Monitor service running on Kubernetes. Linux <em>installation</em> Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration file"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.05069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the infrastructure agent",
        "sections": "Other <em>installation</em> scenarios",
        "tags": "<em>Install</em> the infrastructure agent",
        "body": " and processes of the <em>host</em> where it&#x27;s enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-<em>host</em> <em>integrations</em>, and logs. If you want to collect data about core services running on your <em>host</em>, you need to install"
      },
      "id": "603e79bd64441f99814e8888"
    }
  ],
  "/docs/integrations/host-integrations/installation/update-infrastructure-host-integration-package": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.08836,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.71681,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Prepare for the <em>installation</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". Kubernetes See Monitor service running on Kubernetes. Linux <em>installation</em> Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration file"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.05063,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the infrastructure agent",
        "sections": "Other <em>installation</em> scenarios",
        "tags": "<em>Install</em> the infrastructure agent",
        "body": " and processes of the <em>host</em> where it&#x27;s enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-<em>host</em> <em>integrations</em>, and logs. If you want to collect data about core services running on your <em>host</em>, you need to install"
      },
      "id": "603e79bd64441f99814e8888"
    }
  ],
  "/docs/integrations/host-integrations/open-source-host-integrations-list/f5-open-source-integration": [
    {
      "sections": [
        "Request queue server configuration examples",
        "Apache",
        "Nginx",
        "F5 load balancers",
        "Network timing"
      ],
      "title": "Request queue server configuration examples",
      "type": "docs",
      "tags": [
        "APM",
        "APM UI pages",
        "Features"
      ],
      "external_id": "c7a069b8875af411530a34aaef67155d20d7fb19",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apm/applications-menu/features/request-queue-server-configuration-examples/",
      "published_at": "2021-07-09T08:23:36Z",
      "updated_at": "2021-07-09T08:23:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to report request queuing, New Relic agents depend on an HTTP header set by the front-end web server (such as Apache or Nginx) or load balancer (such as HAProxy or F5). These examples use the X-Request-Start header, since it is has broader support across platforms. If this does not work with your server configuration for request queuing, try using the X-Queue-Start header. The syntax should otherwise be the same. Apache Apache's mod_headers module includes a %t variable that is formatted correctly. To enable request queue reporting, add this code to your Apache config: RequestHeader set X-Request-Start \"%t\" Copy Nginx If you are using Nginx version 1.2.6 or higher and the latest version of the Ruby, Python, or PHP agent, Nginx can easily be configured to report queue time. (For Nginx versions 1.2.6 or lower, you must recompile Nginx with a module or patch.) Configuring with Nginx 1.2.6 or higher uses the ${msec} variable, which is a number in seconds with milliseconds resolution. For more information, see http://nginx.org/en/docs/http/ngx_http_core_module.html#variables. Add the appropriate information to your Nginx config: Nginx configuration Values General Nginx use proxy_set_header X-Request-Start \"t=${msec}\"; Copy Passenger Version 5 or higher: >passenger_set_header X-REQUEST-START \"t=${msec}\"; Copy Older versions: passenger_set_cgi_param X_REQUEST_START \"t=${msec}\"; Copy fastcgi fastcgi_param HTTP_X_REQUEST_START \"t=${msec}\"; Copy uWSGI uwsgi_param HTTP_X_REQUEST_START \"t=${msec}\"; Copy F5 load balancers For F5 load balancers, use this configuration snippet: when HTTP_REQUEST_SEND { # TCL 8.4 so we have to calculate the time in millisecond resolution # Calculation from: https://groups.google.com/forum/? fromgroups=#!topic/comp.lang.tcl/tV9H6TDv0t8 set secs [clock seconds] set ms [clock clicks -milliseconds] set base [expr { $secs * 1000 }] set fract [expr { $ms - $base }] if { $fract >= 1000 } { set diff [expr { $fract / 1000 }] incr secs $diff incr fract [expr { -1000 * $diff }] } set micros [format \"%d%03d000\" $secs $fract] # Want this header inserted as if coming from the client clientside { HTTP::header insert X-Request-Start \"t=${micros}\" } } Copy Network timing Even with request queuing configured, the front-end server's setup can still affect network time in your browser data. This is because the front-end server does not add the queuing time header until after it actually accepts and processes the request. The queuing time headers can never account for backlog in the listener socket used to accept requests. For example, if the front-end server's configuration results in a backlog of requests that queue in the listener socket, page load timing will show an increase in network time.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 286.4377,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>F5</em> load balancers",
        "tags": "<em>Features</em>",
        "body": "In order to report request queuing, New Relic agents depend on an HTTP header set by the front-end web server (such as Apache or Nginx) or load balancer (such as HAProxy or <em>F5</em>). These examples use the X-Request-Start header, since it is has broader support across platforms. If this does not work"
      },
      "id": "603eb84a28ccbc1734eba7a5"
    },
    {
      "sections": [
        "Browser injection: Health check conflict",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Browser injection: Health check conflict",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Troubleshooting"
      ],
      "external_id": "25cc8e642062cad92ff38bd4ebfa5ac3fc5d98b3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/net-agent/troubleshooting/browser-injection-health-check-conflict/",
      "published_at": "2021-07-09T11:13:00Z",
      "updated_at": "2021-07-09T11:12:59Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem As a health check, your load balancer (such as an F5 load balancer) periodically pings a specified webpage. In some cases when automatic browser injection is enabled, the browser monitoring JavaScript snippet added to the page header causes that check to fail. Depending on how the load balancer is set up, this can cause traffic to be routed to other application instances. Solution To prevent this, add the requestPathsExcluded sub-element to your .NET agent's configuration. This is a reference to the virtual directory of the path in your application and not the full URL of the path you want to exclude. For example, if your load balancer is set to ping a webpage in https://www.mywebsite.com/healthmonitor/, insert /healthmonitor/ as the path regex value: // If you use both the Exclude and Attribute elements // the Exclude element must be listed first. <browserMonitoring autoInstrument=\"true\"> <requestPathsExcluded> <path regex=\"/healthmonitor/\"/> </requestPathsExcluded> </browserMonitoring> Copy Cause The browser agent is a JavaScript snippet injected into the header of a webpage. Sometimes it can prevent the health check monitor from seeing the information it needs to validate that the site is healthy. Excluding the path where the health check page is located will prevent the agent from injecting the snippet into the headers of specific pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 258.85455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "Problem As a health check, your load balancer (such as an <em>F5</em> load balancer) periodically pings a specified webpage. In some cases when automatic browser injection is enabled, the browser monitoring JavaScript snippet added to the page header causes that check to fail. Depending on how the load"
      },
      "id": "603ec0c1196a67ab2fa83da9"
    },
    {
      "sections": [
        "F5 monitoring integration",
        "Compatibility and requirements",
        "F5 BIG-IP users and privileges",
        "Tip",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "System sample metrics",
        "Virtual server sample metrics",
        "Pool sample metrics",
        "Pool member sample metrics",
        "Node sample metrics",
        "Inventory data",
        "Pool Inventory",
        "Node inventory",
        "Pool Member Inventory",
        "Virtual Server Inventory",
        "System Inventory",
        "Application Inventory",
        "Check the source code"
      ],
      "title": "F5 monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "86250de7e0529371148dab5e96960893b88288b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration/",
      "published_at": "2021-07-09T17:49:08Z",
      "updated_at": "2021-05-15T23:51:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our F5 BIG-IP integration collects and sends inventory and metrics from your F5 BIG-IP instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at the system, application, pool, pool member, virtual server, and node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with F5 BIG-IP 11.6 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows version compatible with the infrastructure agent. F5 BIG-IP user account with Access Auditor-level user privileges and iControl REST API access permissions. F5 BIG-IP users and privileges To create a new user and assign user permissions: Create a user account with, at minimum, Access Auditor-level permissions. For instructions, see the F5 official documentation. Once the user has been created, assign the user iControl REST user permissions. Tip Administrator-level permissions may be required to collect some system sample metrics or system inventory configuration data. For more information on user permission levels, see User role access descriptions. Tip For detailed information on iControl users and permission, download and review the iControl REST User Guide. Install and activate To install the F5 BIG-IP integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-f5. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-f5 MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-f5/nri-f5-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-f5-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: We recommend you install the integration on a separate server and monitor F5 remotely. Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For an example of the configuration file, see the example config file. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The f5-config.yml file accepts the following commands: all_data: collects both inventory and metrics for the BIG-IP instance. inventory: collects only the inventory (configuration) data for the BIG-IP instance. metrics: collects only the metrics data for the BIG-IP instance. Arguments The f5-config.yml commands accept the following arguments: username: The username for the F5 BIG-IP connection. This field is required. password: The password for the F5 BIG-IP connection. This field is required. hostname: The hostname for the F5 BIG-IP connection. Default: f5-host. port: The port on which F5 BIG-IP instance is running. Default: 443. timeout: The number of seconds to wait before a request times out. Default: 30. ca_bundle_file: Alternative certificate authority bundle file. ca_bundle_dir: Alternative certificate authority bundle directory. partition_filter: An array of the partitions to collect from, in JSON. Default: '[\"Common\"]'. Example configuration Example f5-config.yml file configuration: Example configuration integration_name: com.newrelic.f5 instances: - name: nri-f5 # command can be all_data, metrics, or inventory command: all_data arguments: # Username of the F5 instance username: admin # Password of the F5 instance password: admin # Hostname of the F5 instance hostname: f5-host # Port of the F5 instance port: 443 # CA certificate file ca_bundle_file: /etc/ca_certificate.crt # A JSON array of BIG-IP partitions to collect from. # The partition name should have no leading slash. # Defaults to '[\"Common\"]' partition_filter: '[\"Common\",\"MyOtherPartition\"]' # The number of seconds to wait before a request times out # Defaults to 30 timeout: 10 Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Third-party services and select one of the F5 BIG-IP integration links. In New Relic Insights, F5 BIG-IP data is attached to the following Insights event types: F5BigIpSystemSample F5BigIpVirtualServerSample F5BigIpPoolSample F5BigIpPoolMemberSample F5BigIpNodeSample For more on how to find and use your data, see Understand integration data. Metric data The F5 BIG-IP integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as system., virtualserver., or pool.. System sample metrics These attributes can be found by querying the F5BigIpSystemSample event types. Metric Description system.cpuIdleTicksPerSecond Amount of CPU ticks that the CPU was idle per second. Requires Administrator-level user permissions to collect. system.cpuIdleUtilization Average percentage of time the CPU is idle. system.cpuInterruptRequestUtilization Average percentage of time the CPU is handling interrupt requests. system.cpuIOWaitUtilization Average percentage of time the CPU is waiting on IO. system.cpuNiceLevelUtilization Average percentage of time the CPU is handling nice level processes. system.cpuSoftInterruptRequestUtilization Average percentage of time the CPU is handling soft interrupt requests. system.cpuStolenUtilization Average percentage of time the CPU is handling reclaimed cycles by the hypervisor. system.cpuSystemTicksPerSecond Amount of CPU ticks used by the kernel processes per second. Requires Administrator-level user permissions to collect. system.cpuSystemUtilization Average percentage of time the CPU is used by the kernel. system.cpuUserTicksPerSecond Amount of CPU ticks used by user processes per second. Requires Administrator-level user permissions to collect. system.cpuUserUtilization Average percentage of time the CPU is used by user processes. system.memoryFreeInBytes Total amount of memory free, in bytes. system.memoryTotalInBytes Total amount of memory, in bytes. Requires Administrator-level user permissions to collect. system.memoryUsedInBytes Total amount of memory used, in bytes. Requires Administrator-level user permissions to collect. system.otherMemoryFreeInBytes Free memory reserved for control plane processes, in bytes. system.otherMemoryTotalInBytes Total memory reserved for control plane processes, in bytes. system.otherMemoryUsedInBytes Used memory reserved for control plane processes, in bytes. system.swapFreeInBytes Swap space free, in bytes. system.swapTotalInBytes Swap space total, in bytes. system.swapUsedInBytes Swap space used, in bytes. system.tmmMemoryFreeInBytes Free memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryTotalInBytes Total memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryUsedInBytes Used memory reserved for Traffic Management Microkernel (TMM), in bytes. Virtual server sample metrics These attributes can be found by querying the F5BigIpVirtualServerSample event types in Insights. Metric Description virtualserver.avaibilityState The BIG-IP defined availability. Options: 0 = Offline 1 = Unknown 2 = Online virtualserver.clientsideConnectionsPerSecond The rate of connections created through the client side of the object per second. virtualserver.cmpEnabled Indicates whether or not Cluster Multiprocessing (CMP) is enabled. virtualserver.cmpEnableMode Shows the Cluster Multiprocessing (CMP) mode indicators. Options: CMP disabled = none, disable, or single. CMP enabled = enable or all. virtualserver.connections The current number of connections from BIG-IP. virtualserver.csMaxConnDur Maximum connection duration from the client side of the object. virtualserver.csMinConnDur Minimum connection duration from the client side of the object. virtualserver.enabled The current enabled state. Options: 0 = Disabled 1 = Enabled virtualserver.ephemeralBytesInPerSecond Total number of bytes in through the ephemeral port per second. virtualserver.ephemeralBytesOutPerSecond Total number of bytes out through the ephemeral port per second. virtualserver.ephemeralConnectionsPerSecond The rate of connection creation through the ephemeral port per second. virtualserver.ephemeralCurrentConnections The current number of connections through the ephemeral port. virtualserver.ephemeralEvictedConnectionsPerSecond The number of connections that are evicted through the ephemeral port per second. virtualserver.ephemeralMaxConnections Maximum number of connections through the ephemeral port. virtualserver.ephemeralPacketsReceivedPerSecond The number of packets in through the ephemeral port per second. virtualserver.ephemeralPacketsSentPerSecond The number of packets out through the ephemeral port per second. virtualserver.ephemeralSlowKilledPerSecond The number of slow connections that are killed through the ephemeral port per second. virtualserver.evictedConnsPerSecond The rate of connections evicted per second. virtualserver.inDataInBytes The amount of data received from the BIG-IP virtual server, in bytes. virtualserver.outDataInBytes The amount of data sent to the BIG-IP virtual server, in bytes. virtualserver.packetsReceived The number of packets received from the BIG-IP virtual server. virtualserver.packetsSent The number of packets sent to the BIG-IP virtual server. virtualserver.requests The number of requests in the last collection interval to BIG-IP. virtualserver.slowKilledPerSecond The number of slow connections killed through the client side of the object per second. virtualserver.statusReason An explanation of the current status. virtualserver.usageRatio The usage ratio for the virtual server. Pool sample metrics These attributes can be found by querying the F5BigIpPoolSample event types in Insights. Metric Description pool.activeMembers The number of active pool members. pool.availabilityState The current availability state. Options: 0 = Offline 1 = Unknown 2 = Online pool.connections The current number of connections. pool.connqAgeEdm The queue age exponential-decaying max. pool.connqAgeEma The queue age exponential-moving average. pool.connqAgeHead The current queue age head. pool.connqAgeMax The queue age all-time max. pool.connqAllAgeEdm The sum of pool member queue age exponential-decaying max. pool.connqAllAgeEma The sum of pool member queue age exponential-moving average. pool.connqAllAgeHead The sum of pool member queue age head. pool.connqAllAgeMax The sum of pool member queue age all-time max. pool.connqAllDepth The sum of pool member depth. pool.connqDepth The queue depth. pool.currentConnections The current connections. pool.enabled The current enabled state, can be user defined. Options: 0 = Disabled 1 = Enabled pool.inDataInBytes The amount of data received from the BIG-IP pool, in bytes. pool.minActiveMembers Pool minimum active members. pool.outDataInBytes The amount of data sent to the BIG-IP pool, in bytes. pool.packetsReceived The number of packets received from the BIG-IP pool. pool.packetsSent The number of packets sent to the BIG-IP pool. pool.requests The total number of requests to the pool. pool.statusReason Textual property explaining the overall health reason. Pool member sample metrics These attributes can be found by querying the F5BigIpPoolMemberSample event types in Insights. Metric Description member.availabilityState The current availability from the BIG-IP system. Options: 0 = Offline 1 = Unknown 2 = Online member.connections The current connections. member.enabled Enabled state of the pool member with regards to the parent pool. Options: 0 = Disabled 1 = Enabled member.inDataInBytes The amount of data received from the BIG-IP pool member, in bytes. member.monitorStatus The status of the monitor. Options: 0 = Down 1 = Unchecked 2 = Any other status member.outDataInBytes The amount of data sent to the BIG-IP pool member, in bytes. member.packetsReceived The number of packets received from the BIG-IP pool member. member.packetsSent The number of packets sent to the BIG-IP pool member. member.requests The current number of requests over the last collection interval. member.sessions The current session count. member.sessionStatus The current session health status. Options: 0 = Disabled 1 = Enabled member.state The current state. Options: 0 = Down 1 = Up member.statusReason Explanation of the current status. Node sample metrics These attributes can be found by querying the F5BigIpNodeSample event types in Insights. Metric Description node.availabilityState The current BIG-IP availability state to the node. Options: 0 = Offline 1 = Unknown 2 = Online node.connections The current number of network connections from BIG-IP. node.connectionsPerSecond The number of connections made per second. node.enabled The current BIG-IP enabled state. Options: 0 = Disabled 1 = Enabled , node.inDataInBytes The amount of data received from the BIG-IP node, in bytes. node.monitorStatus The current health monitor rule status. Options: 0 = Down 1 = Unchecked 2 = Any other status node.outDataInBytes The amount of data sent to the BIG-IP node, in bytes. node.packetsReceived The number of packets received from the BIG-IP node. node.packetsSent The number of packets sent to the BIG-IP node. node.requests The current number of requests over the last collection from BIG-IP. node.sessions The current number of sessions. node.sessionStatus The current status of the session. Options: 0 = Disabled 1 = Enabled node.statusReason BIG-IP reason for the current status. Inventory data The F5 BIG-IP integration also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config/f5 source. For more about inventory data, see Understand integration data. The integration captures data for the following F5 BIG-IP configuration parameters: Pool Inventory Metric Description currentLoadMode Current load balancing mode. description User defined description. kind Kind of pool. maxConnections Current max number of connections seen at one point. monitorRule Current health monitoring rule applied. Node inventory Metric Description address BIG-IP network address to send to the node. fqdn FQDN of node. kind Type of Node in BIG-IP. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP Health Monitor rule. Pool Member Inventory Metric Description kind Type of Pool member. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP health monitor rule. nodeName Name of the node the pool member is using. poolName Name of the pool the pool member belongs. port Port the pool member listens on. Virtual Server Inventory Metric Description applicationService Current application service assigned. destination Destination address picked up by BIG-IP. kind Type of virtual server. maxConnections Current highest number of network connections reported from BIG-IP. name User defined name. pool Pool the virtual server uses for load balancing. System Inventory Metric Description chassisSerialNumber Chassis Serial Number for the current device. Requires Access Administrator-level user permissions to collect. platform Platform of the current device. Requires Access Administrator-level user permissions to collect. product Product Name for the current device. Requires Access Administrator-level user permissions to collect. Application Inventory Metric Description deviceGroup Device group running application service. kind BIG-IP Defined type. name User defined name. poolToUse Server side pool load balancing requests. template Template applied to application including security and monitoring rules. templateModified Indicator of modifications made to out of the box template. trafficGroup Current traffic group to which service is applied. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 234.82469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>F5</em> monitoring <em>integration</em>",
        "sections": "<em>F5</em> monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": " for the current status. Inventory data The <em>F5</em> BIG-IP <em>integration</em> also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config&#x2F;<em>f5</em> <em>source</em>. For more about inventory data, see"
      },
      "id": "6044e41ce7b9d2f0975799b4"
    }
  ],
  "/docs/integrations/host-integrations/open-source-host-integrations-list/memcached-open-source-integration": [
    {
      "sections": [
        "New Relic guided install overview",
        "Supported APM agents",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes",
        "Troubleshoot common problems",
        "MySQL: Incorrect user permissions",
        "NGINX: No status URL"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "2058522f6cb1e82dbbe111a176c22ec4aa515ae5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-07-09T11:56:47Z",
      "updated_at": "2021-05-09T18:29:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU datacenter, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Supported APM agents If you have a .NET Windows application on IIS, the guided install configures and enables an APM agent. Guided install for .NET EU Guided install for .NET Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to what’s being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration JMX newrelic install -n jmx-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration Troubleshoot common problems As we identify areas where the guided install fails, we'll document them here and provide some troubleshooting guidance. MySQL: Incorrect user permissions To monitor MySQL health data, you need a valid username and password with specific permissions. These commands will create a user and grant the required permissions: Create a user newrelic@localhost with a specific password. sudo mysql -e \"CREATE USER 'newrelic'@'localhost' IDENTIFIED BY 'YOUR_SELECTED_PASSWORD';\" Copy Give replication privileges to newrelic@localhost with a maximum of 5 connections. sudo mysql -e \"GRANT REPLICATION CLIENT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Give select privileges to newrelic@localhost with a maximum of 5 connections. sudo mysql -e \"GRANT SELECT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Once done, your next guided install attempt should work. NGINX: No status URL To monitor your NGINX server, you'll need to configure a valid status URL. status_url: The URL set up to provide the metrics using the status module. If the default value of 127.0.0.1 is incorrect, substitute the address/FQDN/URL for your system. Example: status_url: http://127.0.0.1/status You can read more about the status_url in these NGINX docs: For NGINX Open Source: HTTP stub status module For NGINX Plus: HTTP status module and HTTP API module There are different ways to set status_url, depending on how NGINX was installed: If enabled via Kubernetes: See Monitor services running on Kubernetes. If enabled via Amazon ECS: See Monitor services running on ECS. If installed on-host: Edit the config in the integration's YAML config file, nginx-config.yml.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 416.81296,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "On-host <em>integration</em> (OHI) recipes",
        "body": "-<em>source</em>-<em>integration</em> ElasticSearch newrelic install -n elasticsearch-<em>open</em>-<em>source</em>-<em>integration</em> HAProxy newrelic install -n haproxy-<em>open</em>-<em>source</em>-<em>integration</em> HashiCorp Consul newrelic install -n hashicorp-consul-<em>open</em>-<em>source</em>-<em>integration</em> JMX newrelic install -n jmx-<em>open</em>-<em>source</em>-<em>integration</em> <em>Memcached</em> newrelic"
      },
      "id": "604130a7e7b9d299cb2a07c0"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once you’re in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.0711,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> telemetry <em>integrations</em>",
        "body": "After you import <em>Open</em>Telemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relic’s endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.57932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> telemetry <em>integrations</em>",
        "body": " to configure your <em>integration</em>. Since you want New Relic to analyze all your traces, make sure to verify that your <em>Open</em>Telemetry integrations use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/not-seeing-host-integration-data": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.1829,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.74997,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/pass-infrastructure-agent-parameters-host-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.1829,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.74997,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/run-integrations-manually": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.18277,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumer’s name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.37787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and configuration process. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials and configure how data"
      },
      "id": "6043a32364441f6fef378f1c"
    },
    {
      "sections": [
        "JMX monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Important",
        "Windows",
        "Configure the integration",
        "Integration configuration files",
        "Metrics collection files",
        "Tip",
        "Optional: Custom connector",
        "Example configurations",
        "Example host connection file",
        "Example metrics collection file",
        "Tips for naming your data",
        "Find and use data",
        "Metric data",
        "Example NRQL query",
        "Metrics data attributes",
        "Inventory data",
        "Troubleshooting",
        "Search logs for errors",
        "Metrics limit exceeded",
        "Missing metrics",
        "Dashboard not appearing in Infrastructure monitoring",
        "Troubleshooting via jmxterm",
        "Check the source code"
      ],
      "title": "JMX monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "e324733f10b7695dbebae46573e183aabf9079ae",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration/",
      "published_at": "2021-07-14T01:54:25Z",
      "updated_at": "2021-07-03T01:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our JMX integration allows users to monitor any application that exposes metrics with JMX. The integration includes a default collection file that automatically collects key metrics from the JVM. You can also customize your metric collection with YAML files to collect any subset of metrics. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Java 8 or higher. If you need to use a different Java version than the one configured in PATH, follow New Relic's configuration documentation on GitHub. Before installing the integration, make sure that you meet the following requirements: If JMX is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running JMX. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. This integration does not support the IIOP protocol. Quick start Instrument your JMX-enabled application quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the JMX integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your JMX-enabled application. Install and activate To install the JMX integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-jmx. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp jmx-config.yml.sample jmx-config.yml Copy Copy the JVM configuration file: sudo cp jvm-metrics.yml.sample jvm-metrics.yml Copy Optional: If you're interested in monitoring Tomcat, use this sample metrics file: sudo cp tomcat-metrics.yml.sample tomcat-metrics.yml Copy Edit the jmx-config.yml file as described in the configuration settings. Restart the infrastructure agent. Important If the sample files are not present in your installation, you can download them directly from the GitHub repository. Windows Download the nri-jmx .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-jmx/nri-jmx-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-jmx-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp jmx-config.yml.sample jmx-config.yml Copy Edit the jmx-config.yml configuration file using the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin. The java binary must be in one of those paths. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, jmx-config.yml. The JMX integration defines and collects integration data using two types of YAML files: the integration configuration options and the metrics collection options. Configuration options are below. For examples, see Example config. Integration configuration files This is where the host connection and collection file information is defined. This configuration accepts the following arguments. For an example, see Host connection file example. collection_files: A comma-separated list of full file paths to the metric collection definition files. For on-host install, the default JVM metrics collection file is at /etc/newrelic-infra/integrations.d/jvm-metrics.yml. Connection arguments: jmx_host: the host JMX is running on. Default: localhost. jmx_pass: the password for the JMX connection. Default: \"\". jmx_port: the port JMX is running on. Default: 9999. jmx_remote: (JBoss specific) whether or not to use the JMX remote URL connection format, connection defaults to JBoss Domain-mode if true. Default: false. jmx_remote_jboss_standlone: (JBoss specific) whether or not to use the JBoss standalone connection format, only relevant if jmx_remote is set. Default: false. connection_url:: full JMX endpoint URL. This replaces all connection arguments (above) by providing all parameters on one line. Example: \"service:jmx:rmi:///jndi/rmi://localhost:7199/jmxrmi\" . jmx_user: the username for the JMX connection. Default: \"\". key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the SSL key store. local_entity: collect all metrics on the local entity. Only use when monitoring localhost. Default: false. timeout: the timeout for individual JMX queries, in milliseconds. trust_store: the filepath of the keystore containing the JMX server's SSL certificate. trust_store_password: the password for the trust store. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Metrics collection files The metrics collection definition files are structured YAML files which tell the integration what metrics to collect. For an example configuration, see the metrics collection file example. Default JVM metrics collection file: /etc/newrelic-infra/integrations.d/jvm-metrics.yml Tip You can write different collection files to ease organization and maintenance. See configuration file for an example. Domains The integration collects and organizes metrics according to domains. All metrics defined per domain will be sent to New Relic and can be found in a corresponding event type. This event type is either auto-generated or can be set by the user. Each file contains a single collect: block which contains an array of domains. For each domain, the following keys are defined: domain: The JMX domain; for example, java.lang. You can use wildcards to match multiple domains; for example, java.*. If you use a wildcard, event_type is required, and must be unique. This field is required. event_type: The event type name for a collection from this domain. If the domain is wildcarded, this is required, and must be unique. If the domain is not wildcarded and this is undefined by the user, this will be auto generated. For example, the domain java.lang will have event type JavaLangSample. For more information, see Naming tips. beans: An array of beans to collect in this domain. Important There is a limit of 200 metrics per instance in the configuration file. If you exceed the limit for a particular instance, it will not be sent to New Relic. If you're not seeing your data in New Relic, review the troubleshooting procedures to identify if you have exceeded the limit. Beans Each domain contains an array of beans to be collected. For each bean, the following keys are defined: query: The bean name to collect; for example,type=GarbageCollector,name=YoungGen. You can use wildcards; for example, type=GarbageCollector,name=*. This field is required. exclude_regex: An optional list of regex patterns that match beans to exclude from collection; for example, type=GarbageCollector,name=.*. attributes: A list of attributes to collect. If unspecified, collects all attributes. Important The HashMap and ArrayList data types are not supported. Attributes Each bean can contain attributes, an optional list of beans that can be excluded from collection. For each attribute, the following keys are defined: Important For map attributes, you must define either an attr or an attr_regex key. attr: An exact match of the attribute name. Composite attributes can be collected by appending the composite member name to the attribute name with a dot; for example, HeapMemoryUsage.Max. attr_regex: A regex pattern that matches the attributes to be collected. metric_type: The New Relic metric type to collect this attribute as. Options are: gauge: data will be collected as an instantaneous numeric measurement. rate: data will be collected as the change in that metric per second. delta: data will be collected as the change in that metric since the last measurement. attribute: data will be collected as a string literal. If left unspecified, the JMX integration will attempt to infer the metric type based on the value returned. For example, if the metric is a number, it will collect it as gauge. If the metric is a string, it will collect it as attribute. If metrics are collected with an incorrect metric type, you can manually specify the correct metric type in the collection file. metric_name: The name under which the metric will appear in New Relic. If unspecified, it will default to the attribute name. For more information about JMX queries, see the Oracle ObjectName documentation. Optional: Custom connector JMX allows the use of custom connectors to communicate with the application. In order to use a custom connector, you have to include the custom connectors in the nrjmx classpath. By default, the sub-folder connectors is in the classpath. If this folder does not exist, create it under the folder where nrjmx is installed. For example, to add support for JBoss, create a folder named connectors under the default (Linux) library path /usr/lib/nrjmx/ (/usr/lib/nrjmx/connectors/) and copy the custom connector jar ($JBOSS_HOME/bin/client/jboss-cli-client.jar) into it. You can now execute JMX queries against JBoss. Example configurations Example file configurations for an on-host install: Example host connection file integration_name: com.newrelic.jmx instances: - name: jmx command: all_data arguments: jmx_host: jmx-host.localnet jmx_port: 9999 jmx_user: admin jmx_pass: admin collection_files: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml,/etc/newrelic-infra/integrations.d/tomcat-metrics.yml\" Copy Example metrics collection file collect: # The event type for this domain will be JavaLangSample - domain: java.lang beans: # Collect all beans of type Threading - query: type=Threading # Attributes can be either a string or a map attribute: # When unspecified, the metric_type is inferred # and the metric name is just the attribute name - ThreadCount # If using a map attribute, a custom metric name can be set - attr: TotalStartedThreadCount metric_name: ThreadsStarted # Attributes can be collected with regex matches and # the metric type can be overridden if the integration # can not correctly infer the type - attr_regex: \"ThreadCpu.*Enabled\" metric_type: attribute - query: type=Memory attributes: # Composite attributes can be collected with this syntax - HeapMemoryUsage.Max - NonHeapMemoryUsage.Max # Queries can be wildcarded where - query: type=GarbageCollector,name=* # If a specific bean is unwanted, it can be excluded # with a regex match pattern. Useful if using a wildcard query exclude_regex: # This will match any bean where the name is YoungGen - name=YoungGen attributes: - attr: LastGcInfo.GcThreadCount metric_type: gauge metric_name: GCThreadCount # Domains can be wildcarded - domain: java.util.* # If the domain is wildcarded, a custom event must be defined event_type: JavaUtilSample beans: # If no attributes are defined, all are collected by default - query: type=Logging Copy For more about the general structure of on-host integration configuration, see Configuration. Tips for naming your data Metrics are sent and stored in the form of samples. This is a list of key-value pairs that include metric data and metadata. Each sample is stored as an event in New Relic’s event database. You are responsible for creating and naming the JMX data reported to New Relic. For this reason, New Relic strongly recommends following these conventions when naming your event types. To ensure you have a consistent naming scheme: Use camel case. Use a name that clearly identifies what data it contains. Example: MyorgApplicationSample Recommendation: Use the same naming scheme for similar metrics across different applications. Find and use data Data from this service is reported to an integration dashboard. JMX data is attached to the user-defined event type specified in the configuration file. For example, if you are interested in monitoring Tomcat using the JMX integration, define an event_type called TomcatSample, and query that event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The metrics generated by the integration include metadata associated with the MBean they are collecting from. You can use this metadata in NRQL queries to filter and facet the data so that the query returns only the data for the desired beans. It can also be used to uniquely identify the metrics, since the metric name is not necessarily unique between all beans. Each event contains the following metadata: Name Description displayName The JMX domain name for these metrics. entityName The JMX domain name for these metrics with the entity type “domain:” prepended. host The JMX host the metrics are being collected from. query The query used to collect these metrics. bean The bean whose attributes these metrics were collected from. key:<mbean_key> For each key in the bean name, an attribute is added to the metric set called “key:<mbean_key> with the value of the bean’s key. Example NRQL query Here's an example NRQL query taking advantage of metadata monitor all the collected JVM garbage collectors: SELECT latest(CollectionTime) FROM JVMSample FACET `key:name` WHERE `key:type` = 'GarbageCollector' Copy Metrics data attributes The JMX integration collects the following metric data attributes: Name Description HeapMemoryUsage.Used The total Java heap memory used. HeapMemoryUsage.Committed The total Java heap memory committed to be used. HeapMemoryUsage.Init The initial Java heap memory allocated. HeapMemoryUsage.Max The maximum Java heap memory available. NonHeapMemoryUsage.Used The total Java non-heap memory used. NonHeapMemoryUsage.Committed The total Java non-heap memory committed to be used. NonHeapMemoryUsage.Init The initial Java non-heap memory allocated. NonHeapMemoryUsage.Max The maximum Java non-heap memory available. ThreadCount The number of live threads. CollectionCount The total number of garbage collections that have occurred. CollectionTime The approximate accumulated garbage collection time elapsed. Inventory data The JMX integration captures the configuration parameters of the JMX integration. The data is available on the Inventory page, under the config/jmx source. For more about inventory data, see Understand integration data. Troubleshooting Troubleshooting tips: Search logs for errors If you are having trouble with the integration, first enable and search the logs for errors. Metrics limit exceeded If you suspect there is a domain sending more than 200 metrics, check the log file for this message: \"Domain x has n metrics, the current limit is 200. This domain will not be reported.\" Copy If you see this error message, lower the number of metrics being sent for the reported domain. Missing metrics If you have missing metrics, ensure that the MBean query is valid by attempting to run it with the nrjmx tool, or use your preferred tool for ensuring the query is valid in the JMXConsole. Dashboard not appearing in Infrastructure monitoring Confirm that the configuration jvm-metrics.yml file has been updated, and that the path to the file is enumerated in the jmx-config.yml file. Troubleshooting via jmxterm JMXTerm is a CLI interactive tool bundled within the package. Docs for JMXTerm can be found at our nrjmx page in GitHub. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.74994,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "JMX monitoring <em>integration</em>",
        "sections": "JMX monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ". When the Infrastructure agent executes the nri-jmx binary, it sets the path to PATH=&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin. The java binary must be in one of those paths. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package"
      },
      "id": "6043a8c3196a679722960f3f"
    }
  ],
  "/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.08807,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and our Event API to ingest <em>data</em>. To <em>use</em> these APIs, you&#x27;ll need an Insert API key. The integration adheres to the Metric API requirements and <em>data</em> limits. The default rate limit is 100,000 <em>data</em> points per minute (DPM). If you think you&#x27;re missing metrics or sending more than 100K DPM, see Request"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Remote monitoring in on-host integrations",
        "Important",
        "Effects of activating remote_monitoring",
        "Alert verification",
        "New entity attributes",
        "Changes in recorded metrics",
        "Unrecorded attributes",
        "Updated hostname"
      ],
      "title": "Remote monitoring in on-host integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "1cfea4c65b855ce9ac5078d2a36ba11b63a6101b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-16T06:05:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has data you can monitor. Integrations can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an integration will be considered a local entity, and the data related to it will be attached to the host entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis integrations, remote monitoring (and multi-tenancy) is enabled by activating the configuration parameter remote_monitoring. Important If your Apache, Cassandra, MySQL, NGINX, or Redis service is located in the same host as the agent, when you activate remote monitoring the resulting entity will be considered as remote, regardless of its actual location. This may affect alerts, alter attributes, and have other effects, as explained here. Effects of activating remote_monitoring By enabling remote_monitoring, the integration becomes a different entity which is no longer attached to the infrastructure agent. As a result, the following items may be affected: Alert verification Enabling remote monitoring can affect your configured alerts in case they are using any of the values that are affected by this new feature. We strongly recommend checking your existing alerts to make sure they keep on working as expected. New entity attributes These attributes are modified in the resulting entity: Display name: New entity unique key (instead of using the display name) Entity GUID: New entity GUID Entity ID: New entity ID Entity key: New entity unique key (instead of using the display name) External key: Using integration entity name (instead of using the agent display) Changes in recorded metrics When remote monitoring is enabled, we will add the hostname and port values to all metrics. If the nricluster name or nriservice are defined in the integration configuration file, they will also be decorated. Unrecorded attributes Since the integration is now an independent entity which is not attached to the agent, the following agent attributes are not collected: agentName agentVersion coreCount criticalViolationCount fullHostname instanceType kernelVersion linuxDistribution entityType operatingSystem processorCount systemMemoryBytes warningViolationCount Your custom attributes Updated hostname For the ApacheSample, RedisSample, CassandraSample, and NginxSample integration metrics, we will use the integration configuration hostname instead of the short hostname from the agent. When the integration hostname is a loopback address, the agent will replace it in order to guarantee uniqueness.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.35298,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Remote monitoring in <em>on</em>-<em>host</em> <em>integrations</em>",
        "sections": "Remote monitoring in <em>on</em>-<em>host</em> <em>integrations</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " will be considered a local entity, and the <em>data</em> related to it will be attached to the <em>host</em> entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis <em>integrations</em>, remote monitoring (and multi-tenancy"
      },
      "id": "603ec000e7b9d216732a07ef"
    },
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-07-09T19:26:52Z",
      "updated_at": "2021-04-12T16:05:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries in its advanced mode. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event in New Relic Insights to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event in New Relic Insights to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event in New Relic Insights to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event in New Relic Insights for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.35187,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": " collected by any New Relic integration you <em>use</em>, including the Kubernetes integration: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a Kubernetes (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message"
      },
      "id": "603eb9a4196a678bfca83dbb"
    }
  ],
  "/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations": [
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.88826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>On</em>-<em>host</em> <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "<em>On</em>-<em>host</em> <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-<em>host</em> <em>integrations</em> can help you customize your integration, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-<em>host</em> <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-<em>host</em> integration. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Here’s an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relic’s StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.08807,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " and our Event API to ingest <em>data</em>. To <em>use</em> these APIs, you&#x27;ll need an Insert API key. The integration adheres to the Metric API requirements and <em>data</em> limits. The default rate limit is 100,000 <em>data</em> points per minute (DPM). If you think you&#x27;re missing metrics or sending more than 100K DPM, see Request"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-07-09T19:26:52Z",
      "updated_at": "2021-04-12T16:05:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries in its advanced mode. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event in New Relic Insights to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event in New Relic Insights to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event in New Relic Insights to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event in New Relic Insights for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.35187,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": " collected by any New Relic integration you <em>use</em>, including the Kubernetes integration: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a Kubernetes (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message"
      },
      "id": "603eb9a4196a678bfca83dbb"
    }
  ],
  "/docs/integrations/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard": [
    {
      "sections": [
        "Configure polling frequency and data collection for cloud integrations",
        "Tip",
        "Overview of settings",
        "Caution",
        "Change polling frequency",
        "Specify data to be fetched",
        "Data collection",
        "Filters",
        "Potential impact on alerts and charts"
      ],
      "title": "Configure polling frequency and data collection for cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "b900b7545f9032201c212449be114e10176bf789",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-06-15T13:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our cloud integrations get data from cloud provider APIs. In New Relic, you can change some of the data collection-related settings for your cloud integrations. Read on to see what changes you can make and the reasons for making them. Tip To use integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Overview of settings New Relic cloud integrations get data from cloud providers' APIs. Data is generally collected from monitoring APIs such as AWS CloudWatch, Azure Monitor, and GCP Stackdriver, and inventory metadata is collected from the specific services' APIs. You can use the account status dashboard to see how your cloud integrations are handling data from a cloud service provider. If you want to report more or less data from your cloud integrations, or if you need to control the use of the cloud providers' APIs to prevent reaching rate and throttling limits in your cloud account, you can change the configuration settings to modify the amount of data they report. The two main controls are: Change polling frequency Change what data is reported Examples of business reasons for wanting to change your polling frequency include: Billing: If you need to manage your AWS CloudWatch bill, you may want to decrease the polling frequency. Before you do this, make sure that any alert conditions set for your cloud integrations are not affected by this reduction. New services: If you are deploying a new service or configuration and you want to collect data more often, you may want to increase the polling frequency temporarily. Caution Changing the configuration settings for your integrations may impact alert conditions and chart trends. Change polling frequency The polling frequency configuration determines how often New Relic reports data from your cloud provider for each service. By default, the polling frequency is set to the maximum frequency that is available for each service. To change the polling frequency for a cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Use the dropdowns next to Data polling interval every to select how frequently you want New Relic to capture your cloud integration data. Specify data to be fetched You can specify which information you want captured for your cloud integration by enabling the collection of additional data and by applying multiple filters to each integration. To change this settings for your cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Under Data collections and filters, turn the toggles you want On. For filters, select or enter the values that you want included in your reported data. Data collection For some cloud integrations, an additional number of calls to the cloud provider APIs are needed in order to collect data. For example, to fetch tags for AWS Elastic Map Reduce clusters, an additional call to the service API is required. To better control the amount of API calls that are sent to your cloud account for these integrations, you can specify when you need these types of data to be collected. Different data collection toggles are available, depending on the integration. Toggle Description Collect tags Some integrations require additional API calls to the cloud provider to report tags. Tag collection is enabled by default. Switch this to Off if you don't want the integration to collect your cloud resource tags and thus reduce the volume of API calls. Collect extended inventory Some integrations can collect extended inventory metadata about your cloud resources by making additional API calls to the cloud provider. The metadata included within the extended inventory for each cloud integration is described in the integration documentation. Extended inventory collection is disabled by default. Switch this to On if you want to monitor extended inventory. This will increase the volume of API calls. Collect shards data Available for AWS Kinesis Streams integration. By default, we don't report shard metrics. Switch this to On if you want to monitor shard metrics in addition to data stream metrics. Collect Lambda@Edge data Available for AWS CloudFront integration. By default, we don't report Lambda@Edge data. Switch this to On if you're using Lambda@Edge in AWS CloudFront and want to get Lambda execution location metadata. Collect node data Available for AWS Elasticsearch integration. By default, we don't report Elasticsearch node metrics. Switch this to On if you want to monitor node metrics in addition to cluster metrics. Collect NAT Gateway data and Collect VPN data Available for AWS VPC integration. By default, we don't report NAT Gateway nor VPN metrics. Switch these to On if you want to monitor NAT Gateway and VPN metrics and inventory, in addition to other VPC related entities inventory. Collect IP addresses Available for AWS EC2 integration. By default, we collect EC2 instance metadata that includes public and private IP addresses, and network interface details. Switch this to Off if you don't want New Relic to store and display these IP data. Filters When a filter is On, you specify the data that you want to be collected; for example, if the Limit to AWS region is On, the regions that you select will be the ones that data will be collected for. There are different filters available, depending on the integration: Filter Description Region Select the regions that include the resources that you want to monitor. Queue prefixes Available for AWS SQS integration. Enter each name or prefix for the queues that you want to monitor. Filter values are case-sensitive. Load balancer prefixes Available for AWS ALB integration. Enter each name or prefix for the application load balancers that you want to monitor. Filter values are case-sensitive. Stage name prefixes Available for AWS API Gateway integration. Enter each name or prefix for the stages that you want to monitor. Filter values are case-sensitive. Tag key Enter one tag key that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag value filter. Tag value Enter one tag value that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag key. Resource group Select the resource groups that are associated with the resources that you want to monitor. Potential impact on alerts and charts If you change an integration's configuration, it can impact alert conditions and charts. Here are some things to consider: If you change this setting... It may have this impact... Any configuration setting When you change the configuration settings, the data that New Relic displays in infrastructure charts, on the inventory page, and in the events feed changes as well. Any filters When you create alert conditions after you set filters, make sure that your alerts are not triggered by resources that you filtered out. Filter for regions If you filter for specific regions, it may lower the amount of data reported to New Relic, which could trigger an alert. If you create an alert condition for a specific region and then filter that region out, the region would no longer report data and would never trigger the alert. Polling frequency When you create an alert, make sure that you define the threshold for a time period that is longer than the polling frequency. Tags and extended inventory If you turn on tags and/or extended inventory, New Relic makes more API calls to the cloud provider, which could increase your cloud provider API usage bill.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.6407,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "sections": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Our <em>cloud</em> <em>integrations</em> get data from <em>cloud</em> provider APIs. In New Relic, you can change some of the data collection-related settings for your <em>cloud</em> <em>integrations</em>. Read on to see what changes you can make and the reasons for making them. Tip To use <em>integrations</em> and the rest of our observability"
      },
      "id": "603e8eef64441fcc7e4e8853"
    },
    {
      "sections": [
        "Metric data gaps with cloud integrations",
        "Problem",
        "Solution",
        "Amazon (AWS)",
        "Microsoft Azure",
        "Google Cloud Platform (GCP)",
        "Tip",
        "Cause"
      ],
      "title": "Metric data gaps with cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "ee3473d0cbc9059b6b36503949d08d1e76c7fc06",
      "image": "https://docs.newrelic.com/static/dfa79b9e3086b81f216d306ba0afe557/c1b63/screen-metric-gap.png",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-29T21:18:28Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You've set up your AWS, Azure, or GCP integration and are monitoring your metrics. However, you notice gaps in your metric data charts. This screenshot shows a metric data chart with gaps. Solution Here’s a list of metrics which might show gaps in your metric data. If possible, avoid setting up alerts for these metrics because we know they can generate false positives. Amazon (AWS) Integration Provider Event Type Metric SNS SnsTopic QueueSample provider.subscriptionsConfirmed SnsTopic QueueSample provider.subscriptionsPending SnsTopic QueueSample provider.subscriptionsDeleted EFS EfsFileSystem BlockDeviceSample provider.lastKnownSizeInBytes ECS EcsCluster ComputeSample provider.registeredContainerInstancesCount EcsCluster ComputeSample provider.activeServicesCount EcsCluster ComputeSample provider.pendingTasksCount EcsCluster ComputeSample provider.runningTasksCount EcsService ComputeSample provider.pendingCount EcsService ComputeSample provider.runningCount EcsService ComputeSample provider.desiredCount DynamoDB DynamoDbTable DatastoreSample provider.itemCount DynamoDbTable DatastoreSample provider.tableSizeBytes AutoScaling AutoScalingInstance AutoScalingInstanceSample healthStatus Billing BillingBudget FinanceSample provider.actualAmount Billingbudget FinanceSample provider.forecastedAmount BillingBudget FinanceSample provider.limitAmount Microsoft Azure Integration Provider Event Type Metric SQL AzureSqlDatabase AzureSqlDatabaseSample databaseSizeCurrentBytes AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google Cloud Platform (GCP) Tip We're currently reviewing the GCP metrics that can cause data gaps. Tip This list isn't complete. We're currently reviewing the full list of metrics that can cause data gaps. Cause Some metrics aren’t present in the usual cloud provider APIs (CloudWatch, Stackdriver, Azure Monitor) and are fetched from the service APIs instead. Each cloud service provider has a unique service API that processes data and interacts with the service. For example, if a metric isn’t present in AWS CloudWatch, New Relic will fetch the metric from the AWS ECS service API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.444405,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "sections": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google <em>Cloud</em> Platform (GCP) Tip We&#x27;re currently reviewing the GCP metrics that can cause data gaps. Tip This list isn&#x27;t complete. We&#x27;re currently"
      },
      "id": "603e821e196a67a042a83df3"
    },
    {
      "sections": [
        "Adopt cloud services",
        "1. Identify services and technologies",
        "2. Deploy New Relic Infrastructure",
        "3. Configure cloud integrations",
        "4. Track data on your dashboards",
        "Tip",
        "AWS EC2 monitoring integration dashboard",
        "Azure VMs monitoring integration dashboard",
        "GCP Compute Engine monitoring integration dashboard",
        "Example modern and cloud services dashboard",
        "5. Add alerts for cloud-based metrics",
        "6. Set up additional monitoring",
        "7. CI/CD pipeline integration",
        "8. AWS Lambda Monitoring"
      ],
      "title": "Adopt cloud services",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "New Relic solutions",
        "Optimize your cloud native environment"
      ],
      "external_id": "4a8e675e4231295287e69402c8c5ff9b05a6103b",
      "image": "https://docs.newrelic.com/static/4159294d05ee078268a7b287af99a72f/c1b63/adopt_cloud.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/new-relic-solutions/optimize-your-cloud-native-environment/adopt-cloud-services/",
      "published_at": "2021-07-09T10:34:08Z",
      "updated_at": "2021-07-09T10:34:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you've recently completed your cloud migration, have been using cloud-based services for a while, or have always been in the cloud, you may find yourself deploying modern, cutting-edge technologies and services. It's important to develop the ability to adopt new services easily and with confidence; innovation never stops for companies operating in the cloud, and a company's willingness to embrace new technology can give it a major competitive differentiator. These modern technologies could be container solutions such as Docker, Kubernetes, and Amazon AWS ECS or Fargate, for example. Or they could be serverless services such as AWS Lambda, Microsoft Azure, or Google Cloud Platform Functions. They may be cloud-based databases, or any number of cloud services that abstract the service away from an operations-maintained infrastructure. In these situations, you want to monitor, query, and alert on the performance and usage metrics for both modern technologies and cloud-based services. This allows your team to deploy faster, to adopt new services with confidence, to make better business decisions, and in general to expand its technology horizons. Here are the steps to using the New Relic Platform to monitor your modern technologies and cloud services: 1. Identify services and technologies Determine the components you need to monitor by answering the following questions: What cloud-based applications do I have? What are the underlying cloud-based services, technologies, and infrastructure supporting those applications? When you have a full understanding of your architecture, you reduce the possibility of missing dependencies and new services that might impact your organization's application environment. 2. Deploy New Relic Infrastructure After reviewing the requirements for New Relic Infrastructure, install the Infrastructure agent on the hosts you identified in Step One so that you can monitor your cloud services. If you use Ansible, Chef, or Puppet for automation, you can use those tools to deploy Infrastructure agents to your hosts. 3. Configure cloud integrations As you integrate new cloud services, you can use New Relic to monitor and report data about these services; giving you a single, comprehensive overview of your entire architecture. To configure cloud service integrations, link your Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) service provider account with New Relic. 4. Track data on your dashboards New Relic Infrastructure integrations auto-populate dashboards with metrics from cloud providers like AWS, Azure, and GCP so you can track the data that is critical to your cloud adoption success. Tip If you adopt a hybrid cloud of multiple cloud providers, New Relic can provide a holistic perspective that is agnostic to cloud providers. AWS EC2 monitoring integration dashboard In this default dashboard for the AWS EC2 monitoring integration, New Relic captures metrics for EC2 instances per region, instance state, and instance type. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Amazon Web Services: View AWS EC2 data on the default dashboard for the AWS EC2 monitoring integration. Azure VMs monitoring integration dashboard The default Azure virtual machine integration dashboard shows data for VM sizes, VMs per region, and VMs per resource group. one.newrelic.com > Infrastructure > Integrations > Microsoft Azure: View Azure virtual machine data on the default dashboard for the Azure VMs monitoring integration. GCP Compute Engine monitoring integration dashboard In this default dashboard for the Google Cloud Platform, New Relic captures metrics for instances per zone, instance status, firewall dropped packets, reserved cores, and disk throttled operations. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Google Cloud Platform: View GCP Compute Engine data on the default dashboard for the GCP Compute Engine monitoring integration. Example modern and cloud services dashboard In this example dashboard, three different cloud vendors, modern technologies, cloud services, infrastructure instance locations, and DevOps widgets are combined for an overall view. one.newrelic.com > Dashboards: Here is an example of an Insights dashboard with data about vendors, technologies, services, instances, and other important details for DevOps teams. 5. Add alerts for cloud-based metrics When monitoring cloud-based services, it is essential to keep track of all changes to a system by alerting on them. Integrations with New Relic Infrastructure allow you to create alerts on the metrics that are the most important to you. Here is an example of a baseline alert that will notify you based on the number of requests received on all AWS Elastic Load Balancing (ALB) systems for the ALB monitoring integration: alerts.newrelic.com > Alert policies > (selected policy) > Alert conditions: Create a NRQL baseline alert to monitor the number of requests received your ALB systems. 6. Set up additional monitoring For full-stack visibility into every aspect of your app, we recommend deploying other types of New Relic monitoring: Use APM to report application-tier performance metrics. Use browser monitoring to report front-end web metrics. Use mobile monitoring to report front-end mobile app metrics. Use synthetic monitoring to monitor websites, critical business transactions, and API endpoints. 7. CI/CD pipeline integration It's important to track deployments and the impact that code and infrastructure changes have on your end-user experience. New Relic APM deployment markers allow you to record deployments for each application. A deployment marker is an event indicating that a deployment happened. You can pair markers with metadata from your source code management (SCM) system ( including user IDs, revisions, and change logs.). APM displays a vertical line, or marker, on charts and graphs at the deployment event's timestamp. When you hover over the line, APM displays the associated metadata for that deployment. Tracking deployments is an invaluable way to determine the root causes of immediate, long-term, or gradual degradations in your applications. Recommendation: Make POST requests to the New Relic REST API as the final step of a successful CI/CD deployment, as described in the API documentation. The following tools have integrations or plugins available to help automate CI/CD deployments: Chef (see newrelic_deployment) Jenkins Ansible Puppet 8. AWS Lambda Monitoring New Relic One features an updated APM agent that is highly optimized from a cost and time perspective for ephemeral Lambda functions. Enable New Relic monitoring of AWS Lambda to to assess invocations, error rate, function duration, cold starts, and more. You can also take advantage of New Relic's Infrastructure integration with Lambda for additional reporting capabilities.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.73267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Adopt <em>cloud</em> services",
        "sections": "3. Configure <em>cloud</em> <em>integrations</em>",
        "tags": "Optimize your <em>cloud</em> native environment",
        "body": " Platform (GCP) service provider account with New Relic. 4. Track data on your dashboards New Relic <em>Infrastructure</em> <em>integrations</em> auto-populate dashboards with metrics from <em>cloud</em> providers like AWS, Azure, and GCP so you can track the data that is critical to your <em>cloud</em> adoption success. Tip If you adopt"
      },
      "id": "603ebf09e7b9d2071a2a0806"
    }
  ],
  "/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations": [
    {
      "sections": [
        "Metric data gaps with cloud integrations",
        "Problem",
        "Solution",
        "Amazon (AWS)",
        "Microsoft Azure",
        "Google Cloud Platform (GCP)",
        "Tip",
        "Cause"
      ],
      "title": "Metric data gaps with cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "ee3473d0cbc9059b6b36503949d08d1e76c7fc06",
      "image": "https://docs.newrelic.com/static/dfa79b9e3086b81f216d306ba0afe557/c1b63/screen-metric-gap.png",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-29T21:18:28Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You've set up your AWS, Azure, or GCP integration and are monitoring your metrics. However, you notice gaps in your metric data charts. This screenshot shows a metric data chart with gaps. Solution Here’s a list of metrics which might show gaps in your metric data. If possible, avoid setting up alerts for these metrics because we know they can generate false positives. Amazon (AWS) Integration Provider Event Type Metric SNS SnsTopic QueueSample provider.subscriptionsConfirmed SnsTopic QueueSample provider.subscriptionsPending SnsTopic QueueSample provider.subscriptionsDeleted EFS EfsFileSystem BlockDeviceSample provider.lastKnownSizeInBytes ECS EcsCluster ComputeSample provider.registeredContainerInstancesCount EcsCluster ComputeSample provider.activeServicesCount EcsCluster ComputeSample provider.pendingTasksCount EcsCluster ComputeSample provider.runningTasksCount EcsService ComputeSample provider.pendingCount EcsService ComputeSample provider.runningCount EcsService ComputeSample provider.desiredCount DynamoDB DynamoDbTable DatastoreSample provider.itemCount DynamoDbTable DatastoreSample provider.tableSizeBytes AutoScaling AutoScalingInstance AutoScalingInstanceSample healthStatus Billing BillingBudget FinanceSample provider.actualAmount Billingbudget FinanceSample provider.forecastedAmount BillingBudget FinanceSample provider.limitAmount Microsoft Azure Integration Provider Event Type Metric SQL AzureSqlDatabase AzureSqlDatabaseSample databaseSizeCurrentBytes AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google Cloud Platform (GCP) Tip We're currently reviewing the GCP metrics that can cause data gaps. Tip This list isn't complete. We're currently reviewing the full list of metrics that can cause data gaps. Cause Some metrics aren’t present in the usual cloud provider APIs (CloudWatch, Stackdriver, Azure Monitor) and are fetched from the service APIs instead. Each cloud service provider has a unique service API that processes data and interacts with the service. For example, if a metric isn’t present in AWS CloudWatch, New Relic will fetch the metric from the AWS ECS service API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.444405,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "sections": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google <em>Cloud</em> Platform (GCP) Tip We&#x27;re currently reviewing the GCP metrics that can cause data gaps. Tip This list isn&#x27;t complete. We&#x27;re currently"
      },
      "id": "603e821e196a67a042a83df3"
    },
    {
      "sections": [
        "Cloud integrations: Account status dashboard",
        "Why it matters",
        "Understand dashboard data",
        "Find account status dashboard"
      ],
      "title": "Cloud integrations: Account status dashboard",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "3457b96bec1ab5f0252c061f4ef82bc15f9b61e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-16T06:05:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Each cloud account you link to New Relic infrastructure monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that cloud service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use cloud APIs to collect metrics and inventory data. The dashboards show counts and frequencies for those API calls. You may want to monitor these for two reasons: API calls can cost money, and your API usage may be throttled at certain usage levels. To decrease usage, you can configure integration settings to reduce the number of components you monitor or how frequently they're monitored. Troubleshoot missing data or other data issues. The dashboard displays information that affects how integrations report data, including: Errors that affect New Relic collecting data, like permission errors or quota exhaustion errors. Changes to integration configuration, like polling interval changes, or tag-collection being disabled or enabled. Understand dashboard data The specific data displayed on the account status dashboard will differ by cloud service provider. Common charts include: Data updates: Shows updates to metric data or inventory data (updates shown as a 1 value). Account changes: Actions affecting how the integration works. For example: renaming or unlinking a cloud account, changing polling intervals, and other configuration options. Data freshness: Timestamp of the last data point collected for each integration. Fetching errors: These indicate issues with collecting data. (This may be a problem on the cloud-provider side.) Options for better understanding chart data include: Mouse over a chart’s icon to see a chart description (if available). View the chart's underlying NRQL query. Find account status dashboard To find the account status dashboard for a cloud service provider: From one.newrelic.com > Infrastructure, and select a cloud service provider (for example, AWS). Select Account status dashboard for the cloud account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.27459,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "sections": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Each <em>cloud</em> account you link to New Relic <em>infrastructure</em> monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that <em>cloud</em> service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use"
      },
      "id": "603eb14764441f1cda4e8874"
    },
    {
      "sections": [
        "Adopt cloud services",
        "1. Identify services and technologies",
        "2. Deploy New Relic Infrastructure",
        "3. Configure cloud integrations",
        "4. Track data on your dashboards",
        "Tip",
        "AWS EC2 monitoring integration dashboard",
        "Azure VMs monitoring integration dashboard",
        "GCP Compute Engine monitoring integration dashboard",
        "Example modern and cloud services dashboard",
        "5. Add alerts for cloud-based metrics",
        "6. Set up additional monitoring",
        "7. CI/CD pipeline integration",
        "8. AWS Lambda Monitoring"
      ],
      "title": "Adopt cloud services",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "New Relic solutions",
        "Optimize your cloud native environment"
      ],
      "external_id": "4a8e675e4231295287e69402c8c5ff9b05a6103b",
      "image": "https://docs.newrelic.com/static/4159294d05ee078268a7b287af99a72f/c1b63/adopt_cloud.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/new-relic-solutions/optimize-your-cloud-native-environment/adopt-cloud-services/",
      "published_at": "2021-07-09T10:34:08Z",
      "updated_at": "2021-07-09T10:34:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you've recently completed your cloud migration, have been using cloud-based services for a while, or have always been in the cloud, you may find yourself deploying modern, cutting-edge technologies and services. It's important to develop the ability to adopt new services easily and with confidence; innovation never stops for companies operating in the cloud, and a company's willingness to embrace new technology can give it a major competitive differentiator. These modern technologies could be container solutions such as Docker, Kubernetes, and Amazon AWS ECS or Fargate, for example. Or they could be serverless services such as AWS Lambda, Microsoft Azure, or Google Cloud Platform Functions. They may be cloud-based databases, or any number of cloud services that abstract the service away from an operations-maintained infrastructure. In these situations, you want to monitor, query, and alert on the performance and usage metrics for both modern technologies and cloud-based services. This allows your team to deploy faster, to adopt new services with confidence, to make better business decisions, and in general to expand its technology horizons. Here are the steps to using the New Relic Platform to monitor your modern technologies and cloud services: 1. Identify services and technologies Determine the components you need to monitor by answering the following questions: What cloud-based applications do I have? What are the underlying cloud-based services, technologies, and infrastructure supporting those applications? When you have a full understanding of your architecture, you reduce the possibility of missing dependencies and new services that might impact your organization's application environment. 2. Deploy New Relic Infrastructure After reviewing the requirements for New Relic Infrastructure, install the Infrastructure agent on the hosts you identified in Step One so that you can monitor your cloud services. If you use Ansible, Chef, or Puppet for automation, you can use those tools to deploy Infrastructure agents to your hosts. 3. Configure cloud integrations As you integrate new cloud services, you can use New Relic to monitor and report data about these services; giving you a single, comprehensive overview of your entire architecture. To configure cloud service integrations, link your Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) service provider account with New Relic. 4. Track data on your dashboards New Relic Infrastructure integrations auto-populate dashboards with metrics from cloud providers like AWS, Azure, and GCP so you can track the data that is critical to your cloud adoption success. Tip If you adopt a hybrid cloud of multiple cloud providers, New Relic can provide a holistic perspective that is agnostic to cloud providers. AWS EC2 monitoring integration dashboard In this default dashboard for the AWS EC2 monitoring integration, New Relic captures metrics for EC2 instances per region, instance state, and instance type. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Amazon Web Services: View AWS EC2 data on the default dashboard for the AWS EC2 monitoring integration. Azure VMs monitoring integration dashboard The default Azure virtual machine integration dashboard shows data for VM sizes, VMs per region, and VMs per resource group. one.newrelic.com > Infrastructure > Integrations > Microsoft Azure: View Azure virtual machine data on the default dashboard for the Azure VMs monitoring integration. GCP Compute Engine monitoring integration dashboard In this default dashboard for the Google Cloud Platform, New Relic captures metrics for instances per zone, instance status, firewall dropped packets, reserved cores, and disk throttled operations. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Google Cloud Platform: View GCP Compute Engine data on the default dashboard for the GCP Compute Engine monitoring integration. Example modern and cloud services dashboard In this example dashboard, three different cloud vendors, modern technologies, cloud services, infrastructure instance locations, and DevOps widgets are combined for an overall view. one.newrelic.com > Dashboards: Here is an example of an Insights dashboard with data about vendors, technologies, services, instances, and other important details for DevOps teams. 5. Add alerts for cloud-based metrics When monitoring cloud-based services, it is essential to keep track of all changes to a system by alerting on them. Integrations with New Relic Infrastructure allow you to create alerts on the metrics that are the most important to you. Here is an example of a baseline alert that will notify you based on the number of requests received on all AWS Elastic Load Balancing (ALB) systems for the ALB monitoring integration: alerts.newrelic.com > Alert policies > (selected policy) > Alert conditions: Create a NRQL baseline alert to monitor the number of requests received your ALB systems. 6. Set up additional monitoring For full-stack visibility into every aspect of your app, we recommend deploying other types of New Relic monitoring: Use APM to report application-tier performance metrics. Use browser monitoring to report front-end web metrics. Use mobile monitoring to report front-end mobile app metrics. Use synthetic monitoring to monitor websites, critical business transactions, and API endpoints. 7. CI/CD pipeline integration It's important to track deployments and the impact that code and infrastructure changes have on your end-user experience. New Relic APM deployment markers allow you to record deployments for each application. A deployment marker is an event indicating that a deployment happened. You can pair markers with metadata from your source code management (SCM) system ( including user IDs, revisions, and change logs.). APM displays a vertical line, or marker, on charts and graphs at the deployment event's timestamp. When you hover over the line, APM displays the associated metadata for that deployment. Tracking deployments is an invaluable way to determine the root causes of immediate, long-term, or gradual degradations in your applications. Recommendation: Make POST requests to the New Relic REST API as the final step of a successful CI/CD deployment, as described in the API documentation. The following tools have integrations or plugins available to help automate CI/CD deployments: Chef (see newrelic_deployment) Jenkins Ansible Puppet 8. AWS Lambda Monitoring New Relic One features an updated APM agent that is highly optimized from a cost and time perspective for ephemeral Lambda functions. Enable New Relic monitoring of AWS Lambda to to assess invocations, error rate, function duration, cold starts, and more. You can also take advantage of New Relic's Infrastructure integration with Lambda for additional reporting capabilities.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.73267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Adopt <em>cloud</em> services",
        "sections": "3. Configure <em>cloud</em> <em>integrations</em>",
        "tags": "Optimize your <em>cloud</em> native environment",
        "body": " Platform (GCP) service provider account with New Relic. 4. Track data on your dashboards New Relic <em>Infrastructure</em> <em>integrations</em> auto-populate dashboards with metrics from <em>cloud</em> providers like AWS, Azure, and GCP so you can track the data that is critical to your <em>cloud</em> adoption success. Tip If you adopt"
      },
      "id": "603ebf09e7b9d2071a2a0806"
    }
  ],
  "/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations": [
    {
      "sections": [
        "Configure polling frequency and data collection for cloud integrations",
        "Tip",
        "Overview of settings",
        "Caution",
        "Change polling frequency",
        "Specify data to be fetched",
        "Data collection",
        "Filters",
        "Potential impact on alerts and charts"
      ],
      "title": "Configure polling frequency and data collection for cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "b900b7545f9032201c212449be114e10176bf789",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-06-15T13:00:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our cloud integrations get data from cloud provider APIs. In New Relic, you can change some of the data collection-related settings for your cloud integrations. Read on to see what changes you can make and the reasons for making them. Tip To use integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Overview of settings New Relic cloud integrations get data from cloud providers' APIs. Data is generally collected from monitoring APIs such as AWS CloudWatch, Azure Monitor, and GCP Stackdriver, and inventory metadata is collected from the specific services' APIs. You can use the account status dashboard to see how your cloud integrations are handling data from a cloud service provider. If you want to report more or less data from your cloud integrations, or if you need to control the use of the cloud providers' APIs to prevent reaching rate and throttling limits in your cloud account, you can change the configuration settings to modify the amount of data they report. The two main controls are: Change polling frequency Change what data is reported Examples of business reasons for wanting to change your polling frequency include: Billing: If you need to manage your AWS CloudWatch bill, you may want to decrease the polling frequency. Before you do this, make sure that any alert conditions set for your cloud integrations are not affected by this reduction. New services: If you are deploying a new service or configuration and you want to collect data more often, you may want to increase the polling frequency temporarily. Caution Changing the configuration settings for your integrations may impact alert conditions and chart trends. Change polling frequency The polling frequency configuration determines how often New Relic reports data from your cloud provider for each service. By default, the polling frequency is set to the maximum frequency that is available for each service. To change the polling frequency for a cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Use the dropdowns next to Data polling interval every to select how frequently you want New Relic to capture your cloud integration data. Specify data to be fetched You can specify which information you want captured for your cloud integration by enabling the collection of additional data and by applying multiple filters to each integration. To change this settings for your cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Under Data collections and filters, turn the toggles you want On. For filters, select or enter the values that you want included in your reported data. Data collection For some cloud integrations, an additional number of calls to the cloud provider APIs are needed in order to collect data. For example, to fetch tags for AWS Elastic Map Reduce clusters, an additional call to the service API is required. To better control the amount of API calls that are sent to your cloud account for these integrations, you can specify when you need these types of data to be collected. Different data collection toggles are available, depending on the integration. Toggle Description Collect tags Some integrations require additional API calls to the cloud provider to report tags. Tag collection is enabled by default. Switch this to Off if you don't want the integration to collect your cloud resource tags and thus reduce the volume of API calls. Collect extended inventory Some integrations can collect extended inventory metadata about your cloud resources by making additional API calls to the cloud provider. The metadata included within the extended inventory for each cloud integration is described in the integration documentation. Extended inventory collection is disabled by default. Switch this to On if you want to monitor extended inventory. This will increase the volume of API calls. Collect shards data Available for AWS Kinesis Streams integration. By default, we don't report shard metrics. Switch this to On if you want to monitor shard metrics in addition to data stream metrics. Collect Lambda@Edge data Available for AWS CloudFront integration. By default, we don't report Lambda@Edge data. Switch this to On if you're using Lambda@Edge in AWS CloudFront and want to get Lambda execution location metadata. Collect node data Available for AWS Elasticsearch integration. By default, we don't report Elasticsearch node metrics. Switch this to On if you want to monitor node metrics in addition to cluster metrics. Collect NAT Gateway data and Collect VPN data Available for AWS VPC integration. By default, we don't report NAT Gateway nor VPN metrics. Switch these to On if you want to monitor NAT Gateway and VPN metrics and inventory, in addition to other VPC related entities inventory. Collect IP addresses Available for AWS EC2 integration. By default, we collect EC2 instance metadata that includes public and private IP addresses, and network interface details. Switch this to Off if you don't want New Relic to store and display these IP data. Filters When a filter is On, you specify the data that you want to be collected; for example, if the Limit to AWS region is On, the regions that you select will be the ones that data will be collected for. There are different filters available, depending on the integration: Filter Description Region Select the regions that include the resources that you want to monitor. Queue prefixes Available for AWS SQS integration. Enter each name or prefix for the queues that you want to monitor. Filter values are case-sensitive. Load balancer prefixes Available for AWS ALB integration. Enter each name or prefix for the application load balancers that you want to monitor. Filter values are case-sensitive. Stage name prefixes Available for AWS API Gateway integration. Enter each name or prefix for the stages that you want to monitor. Filter values are case-sensitive. Tag key Enter one tag key that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag value filter. Tag value Enter one tag value that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag key. Resource group Select the resource groups that are associated with the resources that you want to monitor. Potential impact on alerts and charts If you change an integration's configuration, it can impact alert conditions and charts. Here are some things to consider: If you change this setting... It may have this impact... Any configuration setting When you change the configuration settings, the data that New Relic displays in infrastructure charts, on the inventory page, and in the events feed changes as well. Any filters When you create alert conditions after you set filters, make sure that your alerts are not triggered by resources that you filtered out. Filter for regions If you filter for specific regions, it may lower the amount of data reported to New Relic, which could trigger an alert. If you create an alert condition for a specific region and then filter that region out, the region would no longer report data and would never trigger the alert. Polling frequency When you create an alert, make sure that you define the threshold for a time period that is longer than the polling frequency. Tags and extended inventory If you turn on tags and/or extended inventory, New Relic makes more API calls to the cloud provider, which could increase your cloud provider API usage bill.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.64069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "sections": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Our <em>cloud</em> <em>integrations</em> get data from <em>cloud</em> provider APIs. In New Relic, you can change some of the data collection-related settings for your <em>cloud</em> <em>integrations</em>. Read on to see what changes you can make and the reasons for making them. Tip To use <em>integrations</em> and the rest of our observability"
      },
      "id": "603e8eef64441fcc7e4e8853"
    },
    {
      "sections": [
        "Cloud integrations: Account status dashboard",
        "Why it matters",
        "Understand dashboard data",
        "Find account status dashboard"
      ],
      "title": "Cloud integrations: Account status dashboard",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "3457b96bec1ab5f0252c061f4ef82bc15f9b61e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-16T06:05:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Each cloud account you link to New Relic infrastructure monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that cloud service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use cloud APIs to collect metrics and inventory data. The dashboards show counts and frequencies for those API calls. You may want to monitor these for two reasons: API calls can cost money, and your API usage may be throttled at certain usage levels. To decrease usage, you can configure integration settings to reduce the number of components you monitor or how frequently they're monitored. Troubleshoot missing data or other data issues. The dashboard displays information that affects how integrations report data, including: Errors that affect New Relic collecting data, like permission errors or quota exhaustion errors. Changes to integration configuration, like polling interval changes, or tag-collection being disabled or enabled. Understand dashboard data The specific data displayed on the account status dashboard will differ by cloud service provider. Common charts include: Data updates: Shows updates to metric data or inventory data (updates shown as a 1 value). Account changes: Actions affecting how the integration works. For example: renaming or unlinking a cloud account, changing polling intervals, and other configuration options. Data freshness: Timestamp of the last data point collected for each integration. Fetching errors: These indicate issues with collecting data. (This may be a problem on the cloud-provider side.) Options for better understanding chart data include: Mouse over a chart’s icon to see a chart description (if available). View the chart's underlying NRQL query. Find account status dashboard To find the account status dashboard for a cloud service provider: From one.newrelic.com > Infrastructure, and select a cloud service provider (for example, AWS). Select Account status dashboard for the cloud account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.27459,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "sections": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Each <em>cloud</em> account you link to New Relic <em>infrastructure</em> monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that <em>cloud</em> service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use"
      },
      "id": "603eb14764441f1cda4e8874"
    },
    {
      "sections": [
        "Adopt cloud services",
        "1. Identify services and technologies",
        "2. Deploy New Relic Infrastructure",
        "3. Configure cloud integrations",
        "4. Track data on your dashboards",
        "Tip",
        "AWS EC2 monitoring integration dashboard",
        "Azure VMs monitoring integration dashboard",
        "GCP Compute Engine monitoring integration dashboard",
        "Example modern and cloud services dashboard",
        "5. Add alerts for cloud-based metrics",
        "6. Set up additional monitoring",
        "7. CI/CD pipeline integration",
        "8. AWS Lambda Monitoring"
      ],
      "title": "Adopt cloud services",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "New Relic solutions",
        "Optimize your cloud native environment"
      ],
      "external_id": "4a8e675e4231295287e69402c8c5ff9b05a6103b",
      "image": "https://docs.newrelic.com/static/4159294d05ee078268a7b287af99a72f/c1b63/adopt_cloud.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/new-relic-solutions/optimize-your-cloud-native-environment/adopt-cloud-services/",
      "published_at": "2021-07-09T10:34:08Z",
      "updated_at": "2021-07-09T10:34:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you've recently completed your cloud migration, have been using cloud-based services for a while, or have always been in the cloud, you may find yourself deploying modern, cutting-edge technologies and services. It's important to develop the ability to adopt new services easily and with confidence; innovation never stops for companies operating in the cloud, and a company's willingness to embrace new technology can give it a major competitive differentiator. These modern technologies could be container solutions such as Docker, Kubernetes, and Amazon AWS ECS or Fargate, for example. Or they could be serverless services such as AWS Lambda, Microsoft Azure, or Google Cloud Platform Functions. They may be cloud-based databases, or any number of cloud services that abstract the service away from an operations-maintained infrastructure. In these situations, you want to monitor, query, and alert on the performance and usage metrics for both modern technologies and cloud-based services. This allows your team to deploy faster, to adopt new services with confidence, to make better business decisions, and in general to expand its technology horizons. Here are the steps to using the New Relic Platform to monitor your modern technologies and cloud services: 1. Identify services and technologies Determine the components you need to monitor by answering the following questions: What cloud-based applications do I have? What are the underlying cloud-based services, technologies, and infrastructure supporting those applications? When you have a full understanding of your architecture, you reduce the possibility of missing dependencies and new services that might impact your organization's application environment. 2. Deploy New Relic Infrastructure After reviewing the requirements for New Relic Infrastructure, install the Infrastructure agent on the hosts you identified in Step One so that you can monitor your cloud services. If you use Ansible, Chef, or Puppet for automation, you can use those tools to deploy Infrastructure agents to your hosts. 3. Configure cloud integrations As you integrate new cloud services, you can use New Relic to monitor and report data about these services; giving you a single, comprehensive overview of your entire architecture. To configure cloud service integrations, link your Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) service provider account with New Relic. 4. Track data on your dashboards New Relic Infrastructure integrations auto-populate dashboards with metrics from cloud providers like AWS, Azure, and GCP so you can track the data that is critical to your cloud adoption success. Tip If you adopt a hybrid cloud of multiple cloud providers, New Relic can provide a holistic perspective that is agnostic to cloud providers. AWS EC2 monitoring integration dashboard In this default dashboard for the AWS EC2 monitoring integration, New Relic captures metrics for EC2 instances per region, instance state, and instance type. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Amazon Web Services: View AWS EC2 data on the default dashboard for the AWS EC2 monitoring integration. Azure VMs monitoring integration dashboard The default Azure virtual machine integration dashboard shows data for VM sizes, VMs per region, and VMs per resource group. one.newrelic.com > Infrastructure > Integrations > Microsoft Azure: View Azure virtual machine data on the default dashboard for the Azure VMs monitoring integration. GCP Compute Engine monitoring integration dashboard In this default dashboard for the Google Cloud Platform, New Relic captures metrics for instances per zone, instance status, firewall dropped packets, reserved cores, and disk throttled operations. The dashboard also shows inventory for different software packages and configurations that are installed on those instances. one.newrelic.com > Infrastructure > Integrations > Google Cloud Platform: View GCP Compute Engine data on the default dashboard for the GCP Compute Engine monitoring integration. Example modern and cloud services dashboard In this example dashboard, three different cloud vendors, modern technologies, cloud services, infrastructure instance locations, and DevOps widgets are combined for an overall view. one.newrelic.com > Dashboards: Here is an example of an Insights dashboard with data about vendors, technologies, services, instances, and other important details for DevOps teams. 5. Add alerts for cloud-based metrics When monitoring cloud-based services, it is essential to keep track of all changes to a system by alerting on them. Integrations with New Relic Infrastructure allow you to create alerts on the metrics that are the most important to you. Here is an example of a baseline alert that will notify you based on the number of requests received on all AWS Elastic Load Balancing (ALB) systems for the ALB monitoring integration: alerts.newrelic.com > Alert policies > (selected policy) > Alert conditions: Create a NRQL baseline alert to monitor the number of requests received your ALB systems. 6. Set up additional monitoring For full-stack visibility into every aspect of your app, we recommend deploying other types of New Relic monitoring: Use APM to report application-tier performance metrics. Use browser monitoring to report front-end web metrics. Use mobile monitoring to report front-end mobile app metrics. Use synthetic monitoring to monitor websites, critical business transactions, and API endpoints. 7. CI/CD pipeline integration It's important to track deployments and the impact that code and infrastructure changes have on your end-user experience. New Relic APM deployment markers allow you to record deployments for each application. A deployment marker is an event indicating that a deployment happened. You can pair markers with metadata from your source code management (SCM) system ( including user IDs, revisions, and change logs.). APM displays a vertical line, or marker, on charts and graphs at the deployment event's timestamp. When you hover over the line, APM displays the associated metadata for that deployment. Tracking deployments is an invaluable way to determine the root causes of immediate, long-term, or gradual degradations in your applications. Recommendation: Make POST requests to the New Relic REST API as the final step of a successful CI/CD deployment, as described in the API documentation. The following tools have integrations or plugins available to help automate CI/CD deployments: Chef (see newrelic_deployment) Jenkins Ansible Puppet 8. AWS Lambda Monitoring New Relic One features an updated APM agent that is highly optimized from a cost and time perspective for ephemeral Lambda functions. Enable New Relic monitoring of AWS Lambda to to assess invocations, error rate, function duration, cold starts, and more. You can also take advantage of New Relic's Infrastructure integration with Lambda for additional reporting capabilities.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.73263,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Adopt <em>cloud</em> services",
        "sections": "3. Configure <em>cloud</em> <em>integrations</em>",
        "tags": "Optimize your <em>cloud</em> native environment",
        "body": " Platform (GCP) service provider account with New Relic. 4. Track data on your dashboards New Relic <em>Infrastructure</em> <em>integrations</em> auto-populate dashboards with metrics from <em>cloud</em> providers like AWS, Azure, and GCP so you can track the data that is critical to your <em>cloud</em> adoption success. Tip If you adopt"
      },
      "id": "603ebf09e7b9d2071a2a0806"
    }
  ],
  "/docs/integrations/infrastructure-integrations/get-started/infrastructure-integration-dashboards-charts": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration/",
      "sections": [
        "Prometheus remote write integration",
        "Why it matters",
        "Compatibility",
        "Scale your data and get moving quickly",
        "What's next",
        "For more help"
      ],
      "published_at": "2021-07-14T15:02:55Z",
      "title": "Prometheus remote write integration",
      "updated_at": "2021-07-10T23:03:06Z",
      "type": "docs",
      "external_id": "aaf44eb9ee0ffc4b6f751ca18c5dd5b34cd11649",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which scrape data from Prometheus endpoints, the remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. You can leverage the full range of options for setup and management, from raw data to queries and dashboards and beyond. With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional) Compatibility New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Scale your data and get moving quickly Once logged in to New Relic, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. View your data. What's next Ready to get started? Read the setup documentation. Configure a Prometheus data source in Grafana. Set up the integration on New Relic US EU For more help Recommendations for learning more: See the Docs site's landing page for Infrastructure integrations documentation. Browse New Relic's Explorers Hub for community discussions about our Infrastructure integrations. Find additional help or file a support ticket. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.75238,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Prometheus remote write <em>integration</em>",
        "sections": "Prometheus remote write <em>integration</em>",
        "body": "You can use the Prometheus remote write integration to <em>get</em> data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics <em>integrations</em>, which"
      },
      "id": "60ea272a196a670c6038adbf"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.30547,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>infrastructure</em> agent",
        "sections": "Install the <em>infrastructure</em> agent",
        "tags": "<em>Get</em> <em>started</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided install The quickest way to <em>get</em> <em>started</em> with our <em>infrastructure</em> monitoring agent is through our guided install. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided install EU Guided install"
      },
      "id": "603e79bd64441f99814e8888"
    },
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-07-09T19:21:35Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.89334,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; <em>Infrastructure</em> &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    }
  ],
  "/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration/",
      "sections": [
        "Prometheus remote write integration",
        "Why it matters",
        "Compatibility",
        "Scale your data and get moving quickly",
        "What's next",
        "For more help"
      ],
      "published_at": "2021-07-14T15:02:55Z",
      "title": "Prometheus remote write integration",
      "updated_at": "2021-07-10T23:03:06Z",
      "type": "docs",
      "external_id": "aaf44eb9ee0ffc4b6f751ca18c5dd5b34cd11649",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which scrape data from Prometheus endpoints, the remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. You can leverage the full range of options for setup and management, from raw data to queries and dashboards and beyond. With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional) Compatibility New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Scale your data and get moving quickly Once logged in to New Relic, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. View your data. What's next Ready to get started? Read the setup documentation. Configure a Prometheus data source in Grafana. Set up the integration on New Relic US EU For more help Recommendations for learning more: See the Docs site's landing page for Infrastructure integrations documentation. Browse New Relic's Explorers Hub for community discussions about our Infrastructure integrations. Find additional help or file a support ticket. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.75232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Prometheus remote write <em>integration</em>",
        "sections": "Prometheus remote write <em>integration</em>",
        "body": "You can use the Prometheus remote write integration to <em>get</em> data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics <em>integrations</em>, which"
      },
      "id": "60ea272a196a670c6038adbf"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.3054,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>infrastructure</em> agent",
        "sections": "Install the <em>infrastructure</em> agent",
        "tags": "<em>Get</em> <em>started</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided install The quickest way to <em>get</em> <em>started</em> with our <em>infrastructure</em> monitoring agent is through our guided install. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided install EU Guided install"
      },
      "id": "603e79bd64441f99814e8888"
    },
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-07-09T19:21:35Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.89333,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; <em>Infrastructure</em> &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    }
  ],
  "/docs/integrations/infrastructure-integrations/get-started/understand-use-data-infrastructure-integrations": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration/",
      "sections": [
        "Prometheus remote write integration",
        "Why it matters",
        "Compatibility",
        "Scale your data and get moving quickly",
        "What's next",
        "For more help"
      ],
      "published_at": "2021-07-14T15:02:55Z",
      "title": "Prometheus remote write integration",
      "updated_at": "2021-07-10T23:03:06Z",
      "type": "docs",
      "external_id": "aaf44eb9ee0ffc4b6f751ca18c5dd5b34cd11649",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which scrape data from Prometheus endpoints, the remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. You can leverage the full range of options for setup and management, from raw data to queries and dashboards and beyond. With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional) Compatibility New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Scale your data and get moving quickly Once logged in to New Relic, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. View your data. What's next Ready to get started? Read the setup documentation. Configure a Prometheus data source in Grafana. Set up the integration on New Relic US EU For more help Recommendations for learning more: See the Docs site's landing page for Infrastructure integrations documentation. Browse New Relic's Explorers Hub for community discussions about our Infrastructure integrations. Find additional help or file a support ticket. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.75232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Prometheus remote write <em>integration</em>",
        "sections": "Prometheus remote write <em>integration</em>",
        "body": "You can use the Prometheus remote write integration to <em>get</em> data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics <em>integrations</em>, which"
      },
      "id": "60ea272a196a670c6038adbf"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.3054,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>infrastructure</em> agent",
        "sections": "Install the <em>infrastructure</em> agent",
        "tags": "<em>Get</em> <em>started</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided install The quickest way to <em>get</em> <em>started</em> with our <em>infrastructure</em> monitoring agent is through our guided install. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided install EU Guided install"
      },
      "id": "603e79bd64441f99814e8888"
    },
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-07-09T19:21:35Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.89333,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; <em>Infrastructure</em> &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    }
  ],
  "/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration/",
      "sections": [
        "Prometheus remote write integration",
        "Why it matters",
        "Compatibility",
        "Scale your data and get moving quickly",
        "What's next",
        "For more help"
      ],
      "published_at": "2021-07-14T15:02:55Z",
      "title": "Prometheus remote write integration",
      "updated_at": "2021-07-10T23:03:06Z",
      "type": "docs",
      "external_id": "aaf44eb9ee0ffc4b6f751ca18c5dd5b34cd11649",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which scrape data from Prometheus endpoints, the remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. You can leverage the full range of options for setup and management, from raw data to queries and dashboards and beyond. With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional) Compatibility New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Scale your data and get moving quickly Once logged in to New Relic, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. View your data. What's next Ready to get started? Read the setup documentation. Configure a Prometheus data source in Grafana. Set up the integration on New Relic US EU For more help Recommendations for learning more: See the Docs site's landing page for Infrastructure integrations documentation. Browse New Relic's Explorers Hub for community discussions about our Infrastructure integrations. Find additional help or file a support ticket. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.75232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Prometheus remote write <em>integration</em>",
        "sections": "Prometheus remote write <em>integration</em>",
        "body": "You can use the Prometheus remote write integration to <em>get</em> data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics <em>integrations</em>, which"
      },
      "id": "60ea272a196a670c6038adbf"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.3054,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>infrastructure</em> agent",
        "sections": "Install the <em>infrastructure</em> agent",
        "tags": "<em>Get</em> <em>started</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided install The quickest way to <em>get</em> <em>started</em> with our <em>infrastructure</em> monitoring agent is through our guided install. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided install EU Guided install"
      },
      "id": "603e79bd64441f99814e8888"
    },
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Tip",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-07-09T19:21:33Z",
      "updated_at": "2021-03-13T03:24:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are what we call integrations that you can run directly on your host or server. They typically connect to core services in your servers: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including Kubernetes, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures SDK integrations: Procedures to create a custom integration Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 113.42891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "New Relic offers various <em>integrations</em> for reporting data to our platform. One category of <em>integrations</em> is our <em>Infrastructure</em> <em>integrations</em>. Tip To use <em>integrations</em> and <em>infrastructure</em> monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free"
      },
      "id": "60450a39e7b9d2de845799cd"
    }
  ],
  "/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.87718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 120.93713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Kubernetes</em> events <em>integration</em>",
        "sections": "Install <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " collector itself, install the New Relic Prometheus OpenMetrics <em>integration</em>. To <em>get</em> <em>started</em>: Install the New Relic Prometheus OpenMetrics <em>integration</em>. Label the deployment of your <em>Kubernetes</em> events <em>integration</em> with your configured scrape label (default is prometheus.io&#x2F;scrape=true). Optional: Define"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if you’re looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You won’t have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and there’ll be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free — we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relic’s Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.86804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Prometheus OpenMetrics <em>integrations</em>",
        "tags": "<em>Integrations</em>",
        "body": " to <em>get</em> <em>started</em>, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics <em>integrations</em> gather all your data in one place, and New Relic stores the metrics from Prometheus. This <em>integration</em> helps remove the overhead of managing"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements": [
    {
      "sections": [
        "Introduction to the Kubernetes integration",
        "Get started: Install the Kubernetes integration",
        "Tip",
        "Why it matters",
        "Navigate all your Kubernetes events",
        "Bring your cluster logs to New Relic",
        "Check the source code"
      ],
      "title": "Introduction to the Kubernetes integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "c641d1367f1f8fd2b589a2707112759becae609b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration/",
      "published_at": "2021-07-14T01:51:19Z",
      "updated_at": "2021-06-02T01:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. With our cluster explorer, you can cut through layers of complexity to see how your cluster is performing, from the heights of the control plane down to applications running on a single pod. one.newrelic.com > Kubernetes cluster explorer: The cluster explorer is our powerful, fully visual answer to the challenges associated with running Kubernetes at a large scale. You can see the power of the Kubernetes integration in the cluster explorer, where the full picture of a cluster is made available on a single screen: nodes and pods are visualized according to their health and performance, with pending and alerting nodes in the innermost circles. Predefined alert conditions help you troubleshoot issues right from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes integration We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you with installations in managed services or platforms, but you'll need to review a few preliminary notes before getting started. Here's what the automated installer does: Asks for the cluster name and namespace of the integration. Asks for additional setup options, such as Kube state metrics. Asks for the installation method: manifest file or Helm. Generates either the manifest or Helm chart. Read the install docs Start the installer Tip If your New Relic account is in the EU region, access the automated installer from one.eu.newrelic.com. Why it matters Governing the complexity of Kubernetes can be challenging; there's so much going on at any given moment, with containers being created and deleted in a matter of minutes, applications crashing, and resources being consumed unexpectedly. Our integration helps you navigate Kubernetes abstractions across on-premises, cloud, and hybrid deployments. In New Relic, you can build your own charts and query all your Kubernetes data, which our integration collects by instrumenting the container orchestration layer. This gives you additional insight into nodes, namespaces, deployments, replica sets, pods, and containers. one.newrelic.com > Dashboards: Using the query builder you can turn any query on Kubernetes data to clear visuals. With the Kubernetes integration you can also: Link your APM data to Kubernetes to measure the performance of your web and mobile applications, with metrics such as request rate, throughput, error rate, and availability. Monitor services running on Kubernetes, such as Apache, NGINX, Cassandra, and many more (see our tutorial for monitoring Redis on Kubernetes). Create new alert policies and alert conditions based on your Kubernetes data, or extend the predefined alert conditions. These features are in addition to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your Kubernetes events The Kubernetes events integration, which is installed separately, watches for events happening in your Kubernetes clusters and sends those events to New Relic. Events data is then visualized in the cluster explorer. To set it up, check the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into application logs and infrastructure data. Bring your cluster logs to New Relic Our Kubernetes plugin for log monitoring can collect all your cluster's logs and send them to our platform, so that you can set up new alerts and charts. To set it up, check the Log data box in step 3 of our install wizard, or follow the instructions. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or you can create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 129.09204,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Kubernetes</em> <em>integration</em>",
        "sections": "<em>Get</em> <em>started</em>: Install the <em>Kubernetes</em> <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " from the <em>start</em>. Clicking each node reveals its status and how each app is performing. <em>Get</em> <em>started</em>: Install the <em>Kubernetes</em> <em>integration</em> We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you"
      },
      "id": "6043a212196a678d86960f46"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 120.93713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Kubernetes</em> events <em>integration</em>",
        "sections": "Install <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " collector itself, install the New Relic Prometheus OpenMetrics <em>integration</em>. To <em>get</em> <em>started</em>: Install the New Relic Prometheus OpenMetrics <em>integration</em>. Label the deployment of your <em>Kubernetes</em> events <em>integration</em> with your configured scrape label (default is prometheus.io&#x2F;scrape=true). Optional: Define"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if you’re looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You won’t have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and there’ll be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free — we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relic’s Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.86804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Prometheus OpenMetrics <em>integrations</em>",
        "tags": "<em>Integrations</em>",
        "body": " to <em>get</em> <em>started</em>, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics <em>integrations</em> gather all your data in one place, and New Relic stores the metrics from Prometheus. This <em>integration</em> helps remove the overhead of managing"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.39073,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.3774,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> events, Prometheus OpenMetrics, and New Relic log monitoring. Tip"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.92862,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "sections": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/configure-kubernetes-proxy": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.39073,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.3774,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> events, Prometheus OpenMetrics, and New Relic log monitoring. Tip"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.92862,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "sections": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/install-fargate-integration": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/whats-new/2021/06/monitoring-amazon-eks-on-aws-fargate/",
      "sections": [
        "Monitor Amazon EKS on AWS Fargate integration with our public beta"
      ],
      "published_at": "2021-07-09T21:34:40Z",
      "title": "Monitor Amazon EKS on AWS Fargate integration with our public beta",
      "updated_at": "2021-07-07T14:12:51Z",
      "type": "docs",
      "external_id": "a7899b6302aa05b943053c2230e2787799a0fef4",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We are introducing New Relic's integration for Amazon EKS on AWS Fargate public beta. If you are interested in checking out the beta, please follow the steps found in the documentation. Our EKS Fargate integration supports any Fargate setup, whether the cluster is only composed of Fargate nodes or if it also coexists with EC2 nodes. The integration is also compatible with the New Relic One Kubernetes cluster explorer, providing a holistic view of a Kubernetes cluster and rapid troubleshooting. We've also improved the Kubernetes dashboard to list Fargate nodes and distinguish between standard and Fargate serverless nodes. Check it out today.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1093.0304,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor Amazon <em>EKS</em> on AWS <em>Fargate</em> integration with our public beta",
        "sections": "Monitor Amazon <em>EKS</em> on AWS <em>Fargate</em> integration with our public beta",
        "body": "We are introducing New Relic&#x27;s integration for Amazon <em>EKS</em> on AWS <em>Fargate</em> public beta. If you are interested in checking out the beta, please follow the steps found in the documentation. Our <em>EKS</em> <em>Fargate</em> integration supports any <em>Fargate</em> setup, whether the cluster is only composed of <em>Fargate</em> nodes"
      },
      "id": "60e5b66364441f2e58c42399"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 403.35413,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ", depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster <em>EKS</em> (EC2 nodes or <em>Fargate</em>) Compatible with version 1.11 or higher Kubernetes"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/whats-new/",
      "sections": [
        "What's new in New RelicRSS"
      ],
      "published_at": "2021-07-14T01:42:28Z",
      "title": "What's new in New Relic",
      "updated_at": "2021-07-09T02:24:51Z",
      "type": "docs",
      "external_id": "49467ca3d09d7a4f1c1c69fd16be4e20050a7686",
      "document_type": "views_page_content",
      "popularity": 1,
      "body": "July 8 Real Time Profiling for Java enabled by default with Java Agent v7.1 Low-overhead, continuous profiling of your Java code in production environments July 7 HIPAA-enabled Observability Platform The first to support metrics, events, logs, and traces July 1 Monitor Amazon EKS on AWS Fargate integration with our public beta Receive telemetry from Kube State Metrics, Kubelet, and cAdvisor for full observability for Kubernetes clusters running on EKS in Fargate. June 30 Control data ingest costs with ingest drill-down Analyze the data that you send to New Relic June 23 Errors Inbox: Error tracking across your entire stack Track and triage your errors from a single screen May 28 NRQL Alert Conditions, with no NRQL required Now create alert conditions directly from the Query Builder May 27 Diagnose Issues Faster with AIOps Issue Maps and More! Introducing Issue Maps, Incident Analysis, and Relevant Dashboards New observability specialist certification for developers! Get certified in programmability with this certification course May 26 Instant Kubernetes observability with Pixie Without updating code or sampling data May 25 Dynamic Baselines for all your services and infrastructure Easily apply intelligent alerting to all of your services May 21 Keep track of What's New, security bulletins, and more with RSS! Get What's New posts, security bulletins, agent release notes, and Nerdlog and Nerd Bytes videos with RSS. May 14 Nerdlog Roundup - April Check out our monthly recap of some of the new products and features we released in April May 12 Level up at FutureStack May 25-27! Attend the observability event of the year! May 7 Attend FutureStack May 25-27, the observability event of the year! Level up at the observability event of the year! May 4 Pixie is now open source!! Instantly troubleshoot your Kubernetes applications without code changes April 27 Lightning fast search response with data partitions for log data Control how you segment your log data April 23 PHP agent now supports PHP version 8.0!! Monitor your PHP(v8.0) application with New Relic One April 21 Create synthetic monitors without code Synthetic monitors have never been easier to build! April 16 Native support for OpenTelemetry (Early access available now!!) Ingest OpenTelemetry data without adding any New Relic software into your services April 14 Distributed tracing for Mobile Get visibility to the entire journey of requests, originating in your mobile app as they travel through distributed systems April 12 A simpler, more intuitive log analytics experience Check out the changes we've made to the UI to help you see details and debug faster! Create alert conditions from any chart Now you can create an alert condition from just about anywhere in New Relic! Detect patterns and outliers in log data Create queries, alerts, and dashboards using log patterns and outliers April 9 Level up at FutureStack on May 25-27! Connect with Nerds from across the globe to learn, share, and get inspired at our free, virtual event April 8 Aggregated health and activity for your Workloads New Relic Workloads are now easier to read. April 5 Guided install for Java and .NET APM agents We’ve made it simple to set up APM (Java and .NET) using our recently-launched guided installation flow, so you can instrument your systems and start analyzing your telemetry data in 5 minutes - no instrumentation expertise required. April 1 NRQL Updates You can now use Regex within your NRQL queries, and sliding time windows March 31 Amazon CloudWatch Metric Streams More metrics, more often - fill gaps in your observability with Amazon CloudWatch Metric Streams and New Relic One. FedRAMP: Logs and Metrics now certified Protecting your data is our highest priority, which is why we achieved the US Government’s rigorous FedRAMP Moderate certification in 2020. And now we’re adding support for Logs and Metrics to our long list of supported services. March 30 Kubernetes: Metric Update Moving to container_memory_working_set_bytes and away from container_memory_usage_bytes metric Slow transactions on the service Summary Quickly dive into the most time-consuming requests to your application or service March 25 Nerdlog Roundup: New Relic Lookout, Guided Install, and Open Source Docs Get real-time visibility into your stack, simplify your instrumentation, and edit our docs March 18 Nerdlog Roundup: Root Cause Analysis, Topology (Relationship-Based) Correlation, and more! Reduce your MTTR with free Automatic Proactive Anomaly Detection, find problems fast with root cause analysis, and increase context for issues using topology correlation. March 17 AIOps Made Easy: Get to root cause and respond faster Automatically find root cause and respond faster than ever March 16 Heroku cloud integration for log management Detailed log data made easier than ever Visualize log details in Dashboards with our new Logs Table Widget More flexibility and control for log message visualizations March 15 AIOps Made Easy: Cut down on alert noise Reduce alert fatigue and prioritize what’s important March 10 AIOps Made Easy: Detect unusual changes instantly Automatic proactive anomaly detection for free Nerdlog Roundup: See hosts, services, containers, and more in one view with New Relic Navigator See hundreds of entities in a compact, high-level view March 8 Nerdlog Roundup: Muting Rules, Custom Data Visualizations, and more AIOps fun! Smarter incident intelligence and custom data visualizations February 25 Nerdlog Roundup: Ingesting OpenTelemetry Data, RUM, and more Ingest OpenTelemery data and logs, drop your data, and monitor your browser performance February 24 New Relic Explorer: Say goodbye to blindspots Gain unprecedented visibility into your entire system February 19 Nerdlog Roundup: K6 Load Testing and the ServiceNow and Snowflake Integrations Keep your issues in sync, view your Snowflake and k6 performance data in New Relic One. February 11 Data Dropping update: Now, drop entire dimensional metrics Filter sensitive or low-value data February 8 Check out the Nerdlog We have a new live-stream changelog on Twitch! January 28 Agentless syslog onboarding for New Relic log management Host-based log ingestion using rsyslog or syslog-ng with a new TCP endpoint January 21 New Python agent features Python agent now auto instruments HTTPX and Django ASGI January 19 New Anomalies feed as part of AI overview page View all your anomalies in a single place January 13 New Relic Snowflake Integration New Relic now integrates with Snowflake. January 6 Schedule recurring muting rules Suppress or mute notifications with ease December 7, 2020 Recap: Top 10 new observability features you need to know Our engineers and product managers share their favorite features, capabilities, and integrations to help you be more productive and collaborative. December 1, 2020 Alerts and Applied Intelligence new landing page Surface insights with a new landing page. Percentiles now available in events-to-metrics service For the events-to-metrics service, return an attribute's approximate value at a given percentile. November 12, 2020 API keys app There's a new app for managing your API keys. New Relic Lambda extension Our new extension improves observability of your Lambda data. Share dashboards and curated views with permalinks Sharing what you’re seeing just got easier. November 10, 2020 Invite a teammate See how easy it is to invite your team to New Relic. November 9, 2020 Enhanced errors experience in New Relic One We've improved the new errors experience. November 3, 2020 Troubleshoot performance and crash issues faster with New Relic's real-time Java profiling New Relic is excited to announce the availability of real-time Java profiling using Java Flight Recorder (JFR). October 30, 2020 Build high-resolution charts using sliding windows Increase your chart resolution with rolling aggregates, powered by sliding windows. Kafka Connect: Unlock open source and alternative instrumentation sources Build observability pipelines from open source tools and alternative instrumentation sources with the New Relic connector for Kafka Connect. Veneur sink: Your pipeline to 3rd party metrics Send metrics to New Relic using our Veneur sink. October 29, 2020 Three big updates for your native mobile apps Three big updates: The New Relic iOS and tvOS agents will now be distributed as the New Relic XCFramework Agent 7.0.0. For instrumenting React Native applications, we have an experimental open source React Native Module. Plus, we’ve improved reliability from our Android service and changed the way our service handles Android stack deobfuscation. October 27, 2020 Monitor ASGI apps using the Python agent The Python agent now supports monitoring Uvicorn, Starlette, and FastAPI ASGI applications. Monitor Apollo Server GraphQL Node applications Use our Node.js agent plugin to capture executed GraphQL queries. October 15, 2020 Applied Intelligence: Better, smarter webhooks Webhooks now automatically deliver anomaly charts. October 14, 2020 New Relic One now has a new UI for OpenTelemetry Check out our APM functionality for your OpenTelemetry data. October 13, 2020 Store data in an encrypted storage solution with NerdStorageVault Store and receive sensitive third-party secrets data. October 12, 2020 Saved views for log management Save your table column, time range, etc. in the logs UI October 9, 2020 Alerting: Loss of signal detection and configurable gap-filling strategies Specify how long the system should wait before a signal is considered lost. October 7, 2020 New Relic achieves AWS Outposts Ready designation New Relic has achieved the AWS Outposts Ready designation, part of the Amazon Web Services (AWS) Service Ready Program. October 1, 2020 Applied Intelligence now features accelerated suggested decisions Reduce alert noise by using our suggested correlation decisions. September 30, 2020 Applied Intelligence: Deployment events in the issue feed Applied Intelligence now includes any relevant APM deployment data. September 29, 2020 New Relic Edge with Infinite Tracing New Relic users with Pro or Enterprise Full-Stack Observability can now access and benefit from New Relic Edge. September 11, 2020 Anomalies visible in the activity stream The activity stream shows recent events so you can quickly see what's happening in your system. September 9, 2020 Windows Logs supported Support for Windows Logs with New Relic's infrastructure agent September 4, 2020 New Relic One has dark mode! We now have dark mode! New Relic Support for Amazon Web Services (AWS) Bottlerocket Gain full visibility into your workloads and infrastructure running on AWS Bottlerocket. What's new? Your in-product destination for New Relic One updates! September 1, 2020 Issue summary and analysis in Applied Intelligence Rely on the issue summary to help you identify and resolve relevant issues. Visual issue timeline in Applied Intelligence Use our Gantt-style timeline chart to see your related events. August 26, 2020 Real-user monitoring support for W3C trace context and Google Core Web Vitals We've added support for W3C trace context and Google Core Web Vitals! August 18, 2020 Alert analysis for Applied Intelligence Alert analysis gives you greater context for your alert violations and notifications. August 16, 2020 Ingest New Relic Logs directly with Amazon Web Services (AWS) Kinesis Data Firehose New Relic can ingest data from Amazon's Kinesis Data Firehose. Tracing the gap: AWS X-Ray integration Use New Relic with AWS X-Ray, a critical distributed tracing tool for getting visibility across your AWS services. August 10, 2020 Create Grafana dashboards with Prometheus data stored in New Relic You can create Grafana dashboards with Prometheus data stored in New Relic! July 30, 2020 Overall New Relic One experience updates We've made updates across all of New Relic One! Scheduled alert muting Schedule when you want to mute alerts to avoid messages during maintenance or deployments.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 308.8543,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "July 8 Real Time Profiling for Java enabled by default with Java Agent v7.1 Low-overhead, continuous profiling of your Java code in production environments July 7 HIPAA-enabled Observability Platform The first to support metrics, events, logs, and traces July 1 Monitor Amazon <em>EKS</em> on AWS <em>Fargate</em>"
      },
      "id": "60422917196a677e2fa83ddf"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.39066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.37738,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> events, Prometheus OpenMetrics, and New Relic log monitoring. Tip"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.92859,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "sections": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-windows": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.39066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.37738,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> events, Prometheus OpenMetrics, and New Relic log monitoring. Tip"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.92859,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "sections": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.39066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.92859,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "sections": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Kubernetes integration troubleshooting: Not seeing data",
        "Problem",
        "Solution",
        "Important"
      ],
      "title": "Kubernetes integration troubleshooting: Not seeing data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "e253915bd4f1148d91d5c5587f7b7d1b1b837f62",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-not-seeing-data/",
      "published_at": "2021-07-09T17:32:49Z",
      "updated_at": "2021-03-13T03:32:16Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have completed the installation procedure for New Relic's Kubernetes integration but are not seeing Kubernetes data in your New Relic account. Solution Important Effective Wednesday, 12 August 2020, Kubernetes integration v1.7 or lower will be deprecated. To avoid losing data, upgrade to the latest version. For more information, read our New Relic Explorers Hub post or contact your New Relic account team. If you have deployed the infrastructure agent and completed the Kubernetes installation procedure but are not seeing any information in the Kubernetes dashboard, follow these steps: Check if you've access to this feature. See Factors affecting access to features and data. Confirm you have configured the CLUSTER_NAME and NRI_LICENSE_KEY variables in the deployment file before running it. See the installation procedures for more information. Confirm the deployment was successful by running: kubectl get pods --all-namespaces -o wide | grep newrelic Copy Retrieve the logs from the infrastructure agent and ensure there are no error messages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.24606,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em> troubleshooting: Not seeing data",
        "sections": "<em>Kubernetes</em> <em>integration</em> troubleshooting: Not seeing data",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Problem You have completed the <em>installation</em> procedure for New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> but are not seeing <em>Kubernetes</em> data in your New Relic account. Solution Important Effective Wednesday, 12 August 2020, <em>Kubernetes</em> <em>integration</em> v1.7 or lower will be deprecated. To avoid losing data, upgrade"
      },
      "id": "60450b2464441f178b378ee1"
    }
  ],
  "/docs/integrations/kubernetes-integration/installation/kubernetes-integration-recommended-alert-policy": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.3906,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.37737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> events, Prometheus OpenMetrics, and New Relic log monitoring. Tip"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.928566,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "sections": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    }
  ],
  "/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.63763,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 96.98417,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> <em>events</em>, Prometheus OpenMetrics, and New Relic log monitoring. Tip"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Introduction to the Kubernetes integration",
        "Get started: Install the Kubernetes integration",
        "Tip",
        "Why it matters",
        "Navigate all your Kubernetes events",
        "Bring your cluster logs to New Relic",
        "Check the source code"
      ],
      "title": "Introduction to the Kubernetes integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "c641d1367f1f8fd2b589a2707112759becae609b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration/",
      "published_at": "2021-07-14T01:51:19Z",
      "updated_at": "2021-06-02T01:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. With our cluster explorer, you can cut through layers of complexity to see how your cluster is performing, from the heights of the control plane down to applications running on a single pod. one.newrelic.com > Kubernetes cluster explorer: The cluster explorer is our powerful, fully visual answer to the challenges associated with running Kubernetes at a large scale. You can see the power of the Kubernetes integration in the cluster explorer, where the full picture of a cluster is made available on a single screen: nodes and pods are visualized according to their health and performance, with pending and alerting nodes in the innermost circles. Predefined alert conditions help you troubleshoot issues right from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes integration We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you with installations in managed services or platforms, but you'll need to review a few preliminary notes before getting started. Here's what the automated installer does: Asks for the cluster name and namespace of the integration. Asks for additional setup options, such as Kube state metrics. Asks for the installation method: manifest file or Helm. Generates either the manifest or Helm chart. Read the install docs Start the installer Tip If your New Relic account is in the EU region, access the automated installer from one.eu.newrelic.com. Why it matters Governing the complexity of Kubernetes can be challenging; there's so much going on at any given moment, with containers being created and deleted in a matter of minutes, applications crashing, and resources being consumed unexpectedly. Our integration helps you navigate Kubernetes abstractions across on-premises, cloud, and hybrid deployments. In New Relic, you can build your own charts and query all your Kubernetes data, which our integration collects by instrumenting the container orchestration layer. This gives you additional insight into nodes, namespaces, deployments, replica sets, pods, and containers. one.newrelic.com > Dashboards: Using the query builder you can turn any query on Kubernetes data to clear visuals. With the Kubernetes integration you can also: Link your APM data to Kubernetes to measure the performance of your web and mobile applications, with metrics such as request rate, throughput, error rate, and availability. Monitor services running on Kubernetes, such as Apache, NGINX, Cassandra, and many more (see our tutorial for monitoring Redis on Kubernetes). Create new alert policies and alert conditions based on your Kubernetes data, or extend the predefined alert conditions. These features are in addition to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your Kubernetes events The Kubernetes events integration, which is installed separately, watches for events happening in your Kubernetes clusters and sends those events to New Relic. Events data is then visualized in the cluster explorer. To set it up, check the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into application logs and infrastructure data. Bring your cluster logs to New Relic Our Kubernetes plugin for log monitoring can collect all your cluster's logs and send them to our platform, so that you can set up new alerts and charts. To set it up, check the Log data box in step 3 of our install wizard, or follow the instructions. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or you can create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 85.0705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Kubernetes</em> <em>integration</em>",
        "sections": "Introduction to the <em>Kubernetes</em> <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your <em>Kubernetes</em> <em>events</em> The <em>Kubernetes</em> <em>events</em> <em>integration</em>, which is installed separately, watches for <em>events</em> happening in your <em>Kubernetes</em> clusters and sends those <em>events</em> to New Relic. <em>Events</em> data"
      },
      "id": "6043a212196a678d86960f46"
    }
  ],
  "/docs/integrations/kubernetes-integration/kubernetes-events/kubernetes-integration-predefined-alert-policy": [
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.3906,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.37737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "The easiest way to install the <em>Kubernetes</em> <em>integration</em> is to use our automated installer to generate a manifest. It bundles not just the <em>integration</em> DaemonSets, but also other New Relic <em>Kubernetes</em> configurations, like <em>Kubernetes</em> events, Prometheus OpenMetrics, and New Relic log monitoring. Tip"
      },
      "id": "60450ae964441f0603378f15"
    },
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.928566,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "sections": "<em>Install</em> <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    }
  ],
  "/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes": [
    {
      "sections": [
        "Link your applications to Kubernetes",
        "Tip",
        "Compatibility and requirements",
        "Kubernetes requirements",
        "Network requirements",
        "APM agent compatibility",
        "Openshift requirements",
        "Important",
        "Configure the injection of metadata",
        "Default configuration",
        "Custom configuration",
        "Manage custom certificates",
        "Validate the injection of metadata",
        "Disable the injection of metadata",
        "Troubleshooting"
      ],
      "title": "Link your applications to Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "2ae58989813695b48f4924529d6fd6ea17e5f6c5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/",
      "published_at": "2021-07-09T17:32:03Z",
      "updated_at": "2021-05-28T06:30:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can surface Kubernetes metadata and link it to your APM agents as distributed traces to explore performance issues and troubleshoot transaction errors. For more information, see this New Relic blog post. You can quickly start monitoring Kubernetes clusters using Auto-telemetry with Pixie, which is currently a beta release. This Pixie integration into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here's the code to link APM and infrastructure data and the code to automatically manage certificates. Compatibility and requirements Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements: Kubernetes requirements Network requirements APM agent compatibility OpenShift requirements Kubernetes requirements To link your applications and Kubernetes, your cluster must have the MutatingAdmissionWebhook controller enabled, which requires Kubernetes 1.9 or higher. To verify that your cluster is compatible, run the following command: kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 admissionregistration.k8s.io/v1beta1 Copy If you see a different result, follow the Kubernetes documentation to enable admission control in your cluster. Network requirements For Kubernetes to speak to our MutatingAdmissionWebhook, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster. This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc). Tip Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute. APM agent compatibility The following New Relic agents collect Kubernetes metadata: Go 2.3.0 or higher Java 4.10.0 or higher Node.js 5.3.0 or higher Python 4.14.0 or higher Ruby 6.1.0 or higher .NET 8.17.438 or higher Openshift requirements To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires Openshift 3.9 or higher. During the process, install a resource that requires admin permissions to the cluster. Run this to log in as admin: oc login -u system:admin Copy Check that webhooks are correctly configured. If they are not, update the master-config.yaml file. admissionConfig: pluginConfig: MutatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission ValidatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission location: \"\" Copy Important Add kubeConfigFile: /dev/null to address some issues in Openshift. Enable certificate signing by editing the YAML file and updating your configuration: kubernetesMasterConfig: controllerArguments: cluster-signing-cert-file: - \"/etc/origin/master/ca.crt\" cluster-signing-key-file: - \"/etc/origin/master/ca.key\" Copy Restart the Openshift services in the master node. Configure the injection of metadata By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see Validate the injection of metadata). This default configuration also uses the Kubernetes certificates API to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates. Default configuration To proceed with the default injection of metadata, follow these steps: Download the YAML file: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-latest.yaml Copy Custom configuration You can limit the injection of metadata only to specific namespaces by using labels. To enable this feature, edit your YAML file by finding and uncommenting the following lines: # namespaceSelector: # matchLabels: # newrelic-metadata-injection: enabled Copy With this option, injection is only applied to those namespaces that have the newrelic-metadata-injection label set to enabled: kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled Copy Manage custom certificates To use custom certificates you need a specific YAML file: Download the YAML file without automatic certificate management: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-custom-certs-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-custom-certs-latest.yaml Copy Once you have the correct YAML file, you can proceed with the custom certificate management option. You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format. If you have them in the standard certificate format (X.509), install openssl, and run the following: openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem Copy If your certificate/key pair are in another format, see the Digicert knowledgebase for more help. Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands: kubectl create secret tls newrelic-metadata-injection-secret \\ --key=PEM_ENCODED_SERVER_KEY \\ --cert=PEM_ENCODED_CERTIFICATE \\ --dry-run -o yaml | kubectl -n default apply -f - caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d '\\n') kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p \"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]\" Copy Important Certificates signed by Kubernetes have an expiration of one year. For more information, see the Kubernetes source code in GitHub. Validate the injection of metadata In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables. Create a dummy pod containing Busybox by running: kubectl create -f https://git.io/vPieo Copy Check if New Relic environment variables were injected: kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0 NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox Copy Disable the injection of metadata To disable/uninstall the injection of metadata, use the following commands: Delete the Kubernetes objects using the yaml file: kubectl delete -f k8s-metadata-injection-latest.yaml Copy Delete the TLS secret containing the certificate/key pair: kubectl delete secret/newrelic-metadata-injection-secret Copy Troubleshooting Follow these troubleshooting tips as needed. No Kubernetes metadata in APM or distributed tracing transactions Problem The creation of the secret by the k8s-webhook-cert-manager job used to fail due to the kubectl version used by the image when running in Kubernetes version 1.19.x, The new version 1.3.2 fixes this issue, therefore it is enough to run again the job using an update version of the image to fix the issue. Solution Update the image k8s-webhook-cert-manager (to a version >= 1.3.2) and re-run the job. The secret will be correctly created and the k8s-metadata-injection pod will be able to start. Note that the new version of the manifest and of the nri-bundle are already updated with the correct version of the image. Problem In OpenShift version 4.x, the CA that is used in order to patch the mutatingwebhookconfiguration resource is not the one used when signing the certificates. This is a known issue currently tracked here. In the logs of the Pod nri-metadata-injection, you'll see the following error message: TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate Copy Workaround Manually update the certificate stored in the mutatingwebhookconfiguration object. The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret csr-signer in the namespace openshift-kube-controller-manager. Problem There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing. Solution Verify that the environment variables are being correctly injected by following the instructions described in the Validate your installation step. If they are not present, get the name of the metadata injection pod by running: kubectl get pods | grep newrelic-metadata-injection-deployment kubectl logs -f pod/podname Copy In another terminal, create a new pod (for example, see Validate your installation), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like: {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.107Z\",\"caller\":\"server/main.go:139\",\"msg\":\"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\\\" from 10.11.49.2:32836\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.110Z\",\"caller\":\"server/webhook.go:168\",\"msg\":\"received admission review\",\"kind\":\"/v1, Kind=Pod\",\"namespace\":\"default\",\"name\":\"\",\"pod\":\"busybox1\",\"UID\":\"6577519b-7a61-11ea-965e-0e46d1c9335c\",\"operation\":\"CREATE\",\"userinfo\":{\"username\":\"admin\",\"uid\":\"admin\",\"groups\":[\"system:masters\",\"system:authenticated\"]}} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:182\",\"msg\":\"admission response created\",\"response\":\"[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env\\\",\\\"value\\\":[{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\\\",\\\"value\\\":\\\"adn_kops\\\"}]},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"spec.nodeName\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\\\",\\\"value\\\":\\\"busybox\\\"}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\\\",\\\"value\\\":\\\"busybox\\\"}}]\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:257\",\"msg\":\"writing response\"} Copy If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication. To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like: failed calling webhook \"metadata-injection.newrelic.com\": ERROR_REASON Copy To get the apiserver logs: Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running. kubectl proxy --port=8001 Copy Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox. kubectl create -f https://git.io/vPieo Copy Retrieve the apiserver logs. curl localhost:8001/logs/kube-apiserver.log > apiserver.log Copy Delete the busybox container. kubectl delete -f https://git.io/vPieo Copy Inspect the logs for errors. grep -E 'failed calling webhook' apiserver.log Copy Remember that one of the requirements for the metadata injection is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster. If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered. Ensure the metadata injection setup job ran successfully by inspecting the output of: kubectl get job newrelic-metadata-setup Copy If the job is not completed, investigate the logs of the setup job: kubectl logs job/newrelic-metadata-setup Copy Ensure the CertificateSigningRequest is approved and issued by running: kubectl get csr newrelic-metadata-injection-svc.default Copy Ensure the TLS secret is present by running: kubectl get secret newrelic-metadata-injection-secret Copy Ensure the CA bundle is present in the mutating webhook configuration: kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json Copy Ensure the TargetPort of the Service resource matches the Port of the Deployment's container: kubectl describe service/newrelic-metadata-injection-svc kubectl describe deployment/newrelic-metadata-injection-deployment Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.81627,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "sections": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is currently a beta release. This Pixie <em>integration</em> into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our <em>Kubernetes</em> metadata injection project is open source. Here&#x27;s the code to <em>link</em> APM and infrastructure data and the code to automatically"
      },
      "id": "603ebb94196a674fd1a83df3"
    },
    {
      "sections": [
        "Tutorial: Monitor Redis running on Kubernetes",
        "What you need",
        "Step 1: Set up an example Redis application",
        "Step 2: Enable monitoring of Redis instances"
      ],
      "title": "Tutorial: Monitor Redis running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "30d0c7b52a792c21a50f98931d05a0665ff19fa1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes/",
      "published_at": "2021-07-09T17:32:03Z",
      "updated_at": "2021-03-16T04:18:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have a service running on Kubernetes, and it's a service we support, you can enable monitoring of that service by adding a configuration section for that integration to the Kubernetes integration's config. This tutorial shows how to enable monitoring for a Redis service running on the Kubernetes PHP Guestbook. For the general procedure, see Monitor a Kubernetes-running service. What you need See the general requirements for this feature, including supported services. The kubectl command-line tool must be configured to communicate with your cluster. If you don't have a cluster, you can create one using Minikube. Step 1: Set up an example Redis application This tutorial builds on the Kubernetes tutorial Deploying a PHP Guestbook application with Redis. Skip the Kubernetes tutorial and run the following command to set up the application needed for our tutorial: kubectl create -f https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml Copy If you'd like to first complete the Kubernetes tutorial, follow their tutorial instructions but do not follow the instructions in the Cleaning up section. Step 2: Enable monitoring of Redis instances The PHP Guestbook application has three Redis instances: one master and two slave instances. Each instance is tagged with a label where app=redis. For this example, we're using our Redis monitoring integration. It can monitor both master and slave instances of Redis, so we don’t have to distinguish between them. In the Kubernetes integration's YAML config file (newrelic-infrastructure-k8s-latest.yaml), you need to update the nri-integration-cfg section. From the list of integration configs, get the Redis integration YAML and add it to the Kubernetes config. The Redis YAML is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label \"app=redis\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: redis integrations: - name: nri-redis env: # using the discovered IP as the hostname address HOSTNAME: ${discovery.ip} PORT: 6379 KEYS: '{\"0\":[\"<KEY_1>\"],\"1\":[\"<KEY_2>\"]}' REMOTE_MONITORING: true labels: env: production Copy Deploy the updated service: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy You should be able to see the following in the logs for the pod newrelic-infra: time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check starting\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check finished with success\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations Copy If there are no errors, you should see Redis data in the Infrastructure UI. To find the Redis dashboards, go to one.newrelic.com > Infrastructure > Third party services, and select the Redis dashboard. For the general procedure of how to monitor services running on Kubernetes, including more detail about how configuration works, see Monitor a Kubernetes-running service.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.61269,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "sections": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-<em>integration</em>-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label &quot;<em>app</em>=redis&quot; # https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>integrations</em>&#x2F;host-<em>integrations</em>&#x2F;installation&#x2F;container-auto-discovery discovery"
      },
      "id": "603e7e8264441f332a4e8879"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.33931,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility <em>and</em> requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility <em>and</em> requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    }
  ],
  "/docs/integrations/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes": [
    {
      "sections": [
        "Link your applications to Kubernetes",
        "Tip",
        "Compatibility and requirements",
        "Kubernetes requirements",
        "Network requirements",
        "APM agent compatibility",
        "Openshift requirements",
        "Important",
        "Configure the injection of metadata",
        "Default configuration",
        "Custom configuration",
        "Manage custom certificates",
        "Validate the injection of metadata",
        "Disable the injection of metadata",
        "Troubleshooting"
      ],
      "title": "Link your applications to Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "2ae58989813695b48f4924529d6fd6ea17e5f6c5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/",
      "published_at": "2021-07-09T17:32:03Z",
      "updated_at": "2021-05-28T06:30:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can surface Kubernetes metadata and link it to your APM agents as distributed traces to explore performance issues and troubleshoot transaction errors. For more information, see this New Relic blog post. You can quickly start monitoring Kubernetes clusters using Auto-telemetry with Pixie, which is currently a beta release. This Pixie integration into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here's the code to link APM and infrastructure data and the code to automatically manage certificates. Compatibility and requirements Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements: Kubernetes requirements Network requirements APM agent compatibility OpenShift requirements Kubernetes requirements To link your applications and Kubernetes, your cluster must have the MutatingAdmissionWebhook controller enabled, which requires Kubernetes 1.9 or higher. To verify that your cluster is compatible, run the following command: kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 admissionregistration.k8s.io/v1beta1 Copy If you see a different result, follow the Kubernetes documentation to enable admission control in your cluster. Network requirements For Kubernetes to speak to our MutatingAdmissionWebhook, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster. This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc). Tip Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute. APM agent compatibility The following New Relic agents collect Kubernetes metadata: Go 2.3.0 or higher Java 4.10.0 or higher Node.js 5.3.0 or higher Python 4.14.0 or higher Ruby 6.1.0 or higher .NET 8.17.438 or higher Openshift requirements To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires Openshift 3.9 or higher. During the process, install a resource that requires admin permissions to the cluster. Run this to log in as admin: oc login -u system:admin Copy Check that webhooks are correctly configured. If they are not, update the master-config.yaml file. admissionConfig: pluginConfig: MutatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission ValidatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission location: \"\" Copy Important Add kubeConfigFile: /dev/null to address some issues in Openshift. Enable certificate signing by editing the YAML file and updating your configuration: kubernetesMasterConfig: controllerArguments: cluster-signing-cert-file: - \"/etc/origin/master/ca.crt\" cluster-signing-key-file: - \"/etc/origin/master/ca.key\" Copy Restart the Openshift services in the master node. Configure the injection of metadata By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see Validate the injection of metadata). This default configuration also uses the Kubernetes certificates API to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates. Default configuration To proceed with the default injection of metadata, follow these steps: Download the YAML file: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-latest.yaml Copy Custom configuration You can limit the injection of metadata only to specific namespaces by using labels. To enable this feature, edit your YAML file by finding and uncommenting the following lines: # namespaceSelector: # matchLabels: # newrelic-metadata-injection: enabled Copy With this option, injection is only applied to those namespaces that have the newrelic-metadata-injection label set to enabled: kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled Copy Manage custom certificates To use custom certificates you need a specific YAML file: Download the YAML file without automatic certificate management: curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-custom-certs-latest.yaml Copy Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file. Apply the YAML file to your Kubernetes cluster: kubectl apply -f k8s-metadata-injection-custom-certs-latest.yaml Copy Once you have the correct YAML file, you can proceed with the custom certificate management option. You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format. If you have them in the standard certificate format (X.509), install openssl, and run the following: openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem Copy If your certificate/key pair are in another format, see the Digicert knowledgebase for more help. Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands: kubectl create secret tls newrelic-metadata-injection-secret \\ --key=PEM_ENCODED_SERVER_KEY \\ --cert=PEM_ENCODED_CERTIFICATE \\ --dry-run -o yaml | kubectl -n default apply -f - caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d '\\n') kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p \"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]\" Copy Important Certificates signed by Kubernetes have an expiration of one year. For more information, see the Kubernetes source code in GitHub. Validate the injection of metadata In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables. Create a dummy pod containing Busybox by running: kubectl create -f https://git.io/vPieo Copy Check if New Relic environment variables were injected: kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0 NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox Copy Disable the injection of metadata To disable/uninstall the injection of metadata, use the following commands: Delete the Kubernetes objects using the yaml file: kubectl delete -f k8s-metadata-injection-latest.yaml Copy Delete the TLS secret containing the certificate/key pair: kubectl delete secret/newrelic-metadata-injection-secret Copy Troubleshooting Follow these troubleshooting tips as needed. No Kubernetes metadata in APM or distributed tracing transactions Problem The creation of the secret by the k8s-webhook-cert-manager job used to fail due to the kubectl version used by the image when running in Kubernetes version 1.19.x, The new version 1.3.2 fixes this issue, therefore it is enough to run again the job using an update version of the image to fix the issue. Solution Update the image k8s-webhook-cert-manager (to a version >= 1.3.2) and re-run the job. The secret will be correctly created and the k8s-metadata-injection pod will be able to start. Note that the new version of the manifest and of the nri-bundle are already updated with the correct version of the image. Problem In OpenShift version 4.x, the CA that is used in order to patch the mutatingwebhookconfiguration resource is not the one used when signing the certificates. This is a known issue currently tracked here. In the logs of the Pod nri-metadata-injection, you'll see the following error message: TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate Copy Workaround Manually update the certificate stored in the mutatingwebhookconfiguration object. The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret csr-signer in the namespace openshift-kube-controller-manager. Problem There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing. Solution Verify that the environment variables are being correctly injected by following the instructions described in the Validate your installation step. If they are not present, get the name of the metadata injection pod by running: kubectl get pods | grep newrelic-metadata-injection-deployment kubectl logs -f pod/podname Copy In another terminal, create a new pod (for example, see Validate your installation), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like: {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.107Z\",\"caller\":\"server/main.go:139\",\"msg\":\"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\\\" from 10.11.49.2:32836\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.110Z\",\"caller\":\"server/webhook.go:168\",\"msg\":\"received admission review\",\"kind\":\"/v1, Kind=Pod\",\"namespace\":\"default\",\"name\":\"\",\"pod\":\"busybox1\",\"UID\":\"6577519b-7a61-11ea-965e-0e46d1c9335c\",\"operation\":\"CREATE\",\"userinfo\":{\"username\":\"admin\",\"uid\":\"admin\",\"groups\":[\"system:masters\",\"system:authenticated\"]}} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:182\",\"msg\":\"admission response created\",\"response\":\"[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env\\\",\\\"value\\\":[{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\\\",\\\"value\\\":\\\"adn_kops\\\"}]},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"spec.nodeName\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\\\",\\\"value\\\":\\\"busybox\\\"}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\\\",\\\"value\\\":\\\"busybox\\\"}}]\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:257\",\"msg\":\"writing response\"} Copy If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication. To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like: failed calling webhook \"metadata-injection.newrelic.com\": ERROR_REASON Copy To get the apiserver logs: Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running. kubectl proxy --port=8001 Copy Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox. kubectl create -f https://git.io/vPieo Copy Retrieve the apiserver logs. curl localhost:8001/logs/kube-apiserver.log > apiserver.log Copy Delete the busybox container. kubectl delete -f https://git.io/vPieo Copy Inspect the logs for errors. grep -E 'failed calling webhook' apiserver.log Copy Remember that one of the requirements for the metadata injection is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster. If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered. Ensure the metadata injection setup job ran successfully by inspecting the output of: kubectl get job newrelic-metadata-setup Copy If the job is not completed, investigate the logs of the setup job: kubectl logs job/newrelic-metadata-setup Copy Ensure the CertificateSigningRequest is approved and issued by running: kubectl get csr newrelic-metadata-injection-svc.default Copy Ensure the TLS secret is present by running: kubectl get secret newrelic-metadata-injection-secret Copy Ensure the CA bundle is present in the mutating webhook configuration: kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json Copy Ensure the TargetPort of the Service resource matches the Port of the Deployment's container: kubectl describe service/newrelic-metadata-injection-svc kubectl describe deployment/newrelic-metadata-injection-deployment Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.81627,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "sections": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is currently a beta release. This Pixie <em>integration</em> into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our <em>Kubernetes</em> metadata injection project is open source. Here&#x27;s the code to <em>link</em> APM and infrastructure data and the code to automatically"
      },
      "id": "603ebb94196a674fd1a83df3"
    },
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "4c67f6272bda36eda4ad7883e89697a203aa2153",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2021-07-09T19:24:57Z",
      "updated_at": "2021-05-16T04:41:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.13.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 234.07285,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor <em>services</em> running on <em>Kubernetes</em>",
        "sections": "Monitor <em>services</em> in our <em>Kubernetes</em> <em>integration</em> installed with Helm",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "With New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> you can monitor both <em>Kubernetes</em> and the <em>services</em> running on it, such as Cassandra, Redis, MySQL, and other supported <em>services</em>. Get started Our <em>Kubernetes</em> <em>integration</em> comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "6044e50c196a676012960f35"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.33931,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility <em>and</em> requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility <em>and</em> requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    }
  ],
  "/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes": [
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "4c67f6272bda36eda4ad7883e89697a203aa2153",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2021-07-09T19:24:57Z",
      "updated_at": "2021-05-16T04:41:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.13.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 234.07285,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor <em>services</em> running on <em>Kubernetes</em>",
        "sections": "Monitor <em>services</em> in our <em>Kubernetes</em> <em>integration</em> installed with Helm",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "With New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> you can monitor both <em>Kubernetes</em> and the <em>services</em> running on it, such as Cassandra, Redis, MySQL, and other supported <em>services</em>. Get started Our <em>Kubernetes</em> <em>integration</em> comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "6044e50c196a676012960f35"
    },
    {
      "sections": [
        "Tutorial: Monitor Redis running on Kubernetes",
        "What you need",
        "Step 1: Set up an example Redis application",
        "Step 2: Enable monitoring of Redis instances"
      ],
      "title": "Tutorial: Monitor Redis running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "30d0c7b52a792c21a50f98931d05a0665ff19fa1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes/",
      "published_at": "2021-07-09T17:32:03Z",
      "updated_at": "2021-03-16T04:18:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have a service running on Kubernetes, and it's a service we support, you can enable monitoring of that service by adding a configuration section for that integration to the Kubernetes integration's config. This tutorial shows how to enable monitoring for a Redis service running on the Kubernetes PHP Guestbook. For the general procedure, see Monitor a Kubernetes-running service. What you need See the general requirements for this feature, including supported services. The kubectl command-line tool must be configured to communicate with your cluster. If you don't have a cluster, you can create one using Minikube. Step 1: Set up an example Redis application This tutorial builds on the Kubernetes tutorial Deploying a PHP Guestbook application with Redis. Skip the Kubernetes tutorial and run the following command to set up the application needed for our tutorial: kubectl create -f https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml Copy If you'd like to first complete the Kubernetes tutorial, follow their tutorial instructions but do not follow the instructions in the Cleaning up section. Step 2: Enable monitoring of Redis instances The PHP Guestbook application has three Redis instances: one master and two slave instances. Each instance is tagged with a label where app=redis. For this example, we're using our Redis monitoring integration. It can monitor both master and slave instances of Redis, so we don’t have to distinguish between them. In the Kubernetes integration's YAML config file (newrelic-infrastructure-k8s-latest.yaml), you need to update the nri-integration-cfg section. From the list of integration configs, get the Redis integration YAML and add it to the Kubernetes config. The Redis YAML is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label \"app=redis\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: redis integrations: - name: nri-redis env: # using the discovered IP as the hostname address HOSTNAME: ${discovery.ip} PORT: 6379 KEYS: '{\"0\":[\"<KEY_1>\"],\"1\":[\"<KEY_2>\"]}' REMOTE_MONITORING: true labels: env: production Copy Deploy the updated service: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy You should be able to see the following in the logs for the pod newrelic-infra: time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check starting\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check finished with success\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations Copy If there are no errors, you should see Redis data in the Infrastructure UI. To find the Redis dashboards, go to one.newrelic.com > Infrastructure > Third party services, and select the Redis dashboard. For the general procedure of how to monitor services running on Kubernetes, including more detail about how configuration works, see Monitor a Kubernetes-running service.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.61269,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "sections": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-<em>integration</em>-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label &quot;<em>app</em>=redis&quot; # https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>integrations</em>&#x2F;host-<em>integrations</em>&#x2F;installation&#x2F;container-auto-discovery discovery"
      },
      "id": "603e7e8264441f332a4e8879"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.33931,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility <em>and</em> requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility <em>and</em> requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/certificate-signed-unknown-authority": [
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.3663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Kubernetes</em> events <em>integration</em>",
        "sections": "Install <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.63751,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.65474,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " that the <em>integration</em> is working: wait a few minutes, then look for data in the New Relic <em>Kubernetes</em> cluster explorer. If you don&#x27;t see data, review the configuration procedures again, then follow the <em>troubleshooting</em> procedures. Important In the future, the number of labels collected on <em>Kubernetes</em> objects"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version": [
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.3663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Kubernetes</em> events <em>integration</em>",
        "sections": "Install <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.63751,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.65474,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " that the <em>integration</em> is working: wait a few minutes, then look for data in the New Relic <em>Kubernetes</em> cluster explorer. If you don&#x27;t see data, review the configuration procedures again, then follow the <em>troubleshooting</em> procedures. Important In the future, the number of labels collected on <em>Kubernetes</em> objects"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-error-messages": [
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.36629,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Kubernetes</em> events <em>integration</em>",
        "sections": "Install <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.63745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.654724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " that the <em>integration</em> is working: wait a few minutes, then look for data in the New Relic <em>Kubernetes</em> cluster explorer. If you don&#x27;t see data, review the configuration procedures again, then follow the <em>troubleshooting</em> procedures. Important In the future, the number of labels collected on <em>Kubernetes</em> objects"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-missing-nodes": [
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.36629,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Kubernetes</em> events <em>integration</em>",
        "sections": "Install <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.63745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.654724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " that the <em>integration</em> is working: wait a few minutes, then look for data in the New Relic <em>Kubernetes</em> cluster explorer. If you don&#x27;t see data, review the configuration procedures again, then follow the <em>troubleshooting</em> procedures. Important In the future, the number of labels collected on <em>Kubernetes</em> objects"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-not-seeing-data": [
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.36629,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Kubernetes</em> events <em>integration</em>",
        "sections": "Install <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.63745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.654724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " that the <em>integration</em> is working: wait a few minutes, then look for data in the New Relic <em>Kubernetes</em> cluster explorer. If you don&#x27;t see data, review the configuration procedures again, then follow the <em>troubleshooting</em> procedures. Important In the future, the number of labels collected on <em>Kubernetes</em> objects"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/troubleshooting/not-seeing-control-plane-data": [
    {
      "sections": [
        "Install Kubernetes events integration",
        "Requirements",
        "Install the Kubernetes events integration",
        "Tip",
        "View your Kubernetes events",
        "Query events",
        "Get attributes names",
        "See event details",
        "Search events in New Relic",
        "View events in the Kubernetes cluster explorer",
        "Optional: Collect metrics of the event collector",
        "Optional: Define custom attributes",
        "Troubleshooting",
        "Get logs on event collection",
        "Get logs on sending events"
      ],
      "title": "Install Kubernetes events integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Kubernetes events"
      ],
      "external_id": "a19259cc9ac093cc7acf401c1201e48d5daa9682",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/",
      "published_at": "2021-07-09T18:13:21Z",
      "updated_at": "2021-07-02T13:57:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kubernetes events integration watches for events happening in your Kubernetes clusters and sends those events to New Relic. To visualize your event data, use the Kubernetes cluster explorer in New Relic One platform, or use the infrastructure events UI. Requirements The Kubernetes events integration requires you to: Install and activate the New Relic Kubernetes integration. Configure a Kubernetes pod with the label app.kubernetes.io/name=nri-kube-events and two containers, one used to capture events in the Kubernetes cluster and the other to forward the events to New Relic. Install the Kubernetes events integration Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. To install the New Relic Kubernetes events integration in a Kubernetes environment: Download the integration manifest YAML file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-kube-events-latest.yaml Copy Edit the nri-kube-events-latest.yaml manifest file, adding both a cluster name, to identify your Kubernetes cluster (required), and your New Relic license key (required). clusterName: \"YOUR_CLUSTER_NAME\" [...] - name: \"NRIA_LICENSE_KEY\" value: \"YOUR_LICENSE_KEY\" Copy Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-kube-events-latest.yaml Copy To confirm that the integration has been configured correctly, wait a few minutes, then go to one.newrelic.com > Query builder, and run the following NRQL query to see if data has been reported: FROM InfrastructureEvent SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy View your Kubernetes events Once you have successfully installed the Kubernetes event integration, you can view and query your events in New Relic. To add events to your New Relic One dashboard: Add a chart to a new or existing dashboard or create a chart using the New Relic query builder. When creating or updating your chart, select the chart type table and use the following event query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind,event.involvedObject.name,event.reason,event.message WHERE clusterName = 'YOUR_CLUSTER_NAME' limit 100 Copy Query events Here are a few examples to query your data: Get attributes names To see all the attributes names, use the following query: FROM InfrastructureEvent SELECT keyset() WHERE category = 'kubernetes' Copy See event details To see details about the latest events in a cluster, use the following query, replacing YOUR_CLUSTER_NAME with the name of your cluster: FROM InfrastructureEvent SELECT event.involvedObject.kind, event.involvedObject.name, event.type, event.message, event.reason WHERE category = 'kubernetes' AND clusterName='YOUR_CLUSTER_NAME'​ Copy The events collected by New Relic will have the exact same attributes as given by Kubernetes. For a reference of these attributes, see the Kubernetes event v1 core documentation. Search events in New Relic To search events in the New Relic Infrastructure UI: Navigate to the Infrastructure event page: Go to one.newrelic.com > Infrastructure > Events. In Category, select kubernetes. Use the Search events field to look for specific events. To focus on a specific set of events, select or change the filter set. View events in the Kubernetes cluster explorer To view events in the Kubernetes cluster explorer, use either of these methods: In New Relic One: Go to one.newrelic.com > Kubernetes cluster explorer. Then, to view the pod details: In the Kubernetes cluster explorer, select a pod. Select Show pod events. Optional: Collect metrics of the event collector To collect metric data for the event collector itself, install the New Relic Prometheus OpenMetrics integration. To get started: Install the New Relic Prometheus OpenMetrics integration. Label the deployment of your Kubernetes events integration with your configured scrape label (default is prometheus.io/scrape=true). Optional: Define custom attributes To add custom attributes to the events sent by the integration these need to be specified as environment variables following the nomenclature NRI_KUBE_EVENTS_attributeKey=attributeValue. These environment variables should be defined as part of the spec for the kube-events container. For example, to add the attribute environment with value staging to all the events, add the following to your manifest: env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Be sure to specify the environment in the spec for the kube-events container, making your manifest look something like this: ... kind: Deployment ... spec: ... template: ... spec: containers: - name: kube-events ... env: - name: NRI_KUBE_EVENTS_environment value: dev Copy Troubleshooting Here are some troubleshooting tips when using the Kubernetes events integration. Get logs on event collection To check the logs of our event collector: kubectl logs deploy/nr-kube-events kube-events Copy To add more details, enable verbose mode by adding verbose: \"true\" in the config section of you configuration file, either before or after the clusterName. Get logs on sending events To check the logs of the agent responsible for sending the events to New Relic: kubectl logs deploy/nr-kube-events infra-agent Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.36626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Kubernetes</em> events <em>integration</em>",
        "sections": "Install <em>Kubernetes</em> events <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ". Install the <em>Kubernetes</em> events <em>integration</em> Tip To use <em>Kubernetes</em> <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month"
      },
      "id": "603eae45196a67b26ba83d8f"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.6374,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility Our <em>Kubernetes</em> <em>integration</em> is compatible with the following versions"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Tip",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Custom manifest",
        "Important",
        "Make sure New Relic pods can be scheduled",
        "Unprivileged installs of the Kubernetes integration",
        "Steps to complete an unprivileged install",
        "Configure the integration",
        "Select which processes should send their data to New Relic",
        "Specify the Kubernetes API host and port",
        "Kubernetes versions 1.6 to 1.7.5: Edit manifest file",
        "Use environment variables",
        "Disable kube-state-metrics parsing",
        "Caution",
        "Specify the kube-state-metrics URL",
        "Discover kube-state-metrics pods using a label",
        "Query kube-state-metrics behind RBAC",
        "kube-state-metrics timeout: Increase the client timeout",
        "Non-default namespace deployments: Edit config file",
        "Set the TTL for the Kubernetes API responses cache",
        "Specify base URLs for control plane component endpoints",
        "Configure the infrastructure agent",
        "Update to the latest version",
        "Using the automated installer",
        "Using helm",
        "Uninstall the Kubernetes integration"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "ff06c8b1d8b2940d0b23034f3057377ce571e4ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2021-07-09T19:24:58Z",
      "updated_at": "2021-06-15T13:05:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. Tip To use Kubernetes integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Use automated installer You can use the automated installer for servers, VMs, and unprivileged environments. The installer can also help you with managed services or platforms after you review a few preliminary notes. We also have separate instructions if you need a custom manifest or prefer to do a manual unprivileged installation. Start the installer If your New Relic account is in the EU region, access the installer from one.eu.newrelic.com. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate For help installing our EKS Fargate integration, see these setup options. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. Ensure you have a RoleBinding that grants you the same permissions to create Roles and ClusterRoles: kubectl create clusterrolebinding YOUR_USERNAME-cluster-admin-binding --clusterrole=cluster-admin --user=YOUR_GCP_EMAIL Copy Creating a RoleBinding is necessary because of a known RBAC issue in Kubernetes and Kubernetes Engine versions 1.6 or higher. For more information, see Google Cloud's documentation on defining permissions in a role. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Custom manifest If the Kubernetes automated installer doesn't provide the settings you need, you can download our manifest template and install the integration manually. To activate the Kubernetes integration, you must deploy the newrelic-infra agent onto a Kubernetes cluster as a DaemonSet: Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy In the DaemonSet portion of your manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Recommendation: Do not change the NRIA_PASSTHROUGH_ENVIRONMENT or NRIA_DISPLAY_NAME value in your manifest. YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. YAML key path: spec.template.spec.containers.name.env env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy If you need to adapt the manifest to fit your environment, review the configure section in this doc. Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration is working: wait a few minutes, then look for data in the New Relic Kubernetes cluster explorer. If you don't see data, review the configuration procedures again, then follow the troubleshooting procedures. Important In the future, the number of labels collected on Kubernetes objects will be limited per object type (containers, pods, nodes, etc.). If objects have labels above the limit, you will be able to configure important labels that should always be sent to New Relic. When the limitation is in place, this documentation will be updated. Make sure New Relic pods can be scheduled Some of the New Relic pods are set up as DaemonSet in the manifest file so that they can run on every host. These include newrelic-infrastructure and newrelic-logging. In rare circumstances, other pods may be scheduled first and starve the New Relic pods of resources. Since each of these pods have to run on a specific host, they will stay in pending status until that host has enough resources, even if there are other hosts available. This could end up occurring for long periods of time and result in reporting gaps. To prevent this scenario, you can configure the Kubernetes scheduler to give New Relic pods a higher priority. Using the default scheduler: Ensure kube-scheduler flag disablePreemption is not set to true (by default it is false). Create a PriorityClass for the New Relic DaemonSet pods: Set the appropriate priority value, which should generally be higher than your other pods. preemptionPolicy is set to PreemptLowerPriority by default. This allows New Relic pods assigned this priority class to remove lower-priority pods that are taking up resources. Edit the manifest file to add priorityClassName to any DaemonSet specs. In the example below, the highlighted line sets the priority class for newrelic-infrastructure: apiVersion: apps/v1 kind: DaemonSet metadata: namespace: default labels: app: newrelic-infrastructure chart: newrelic-infrastructure-1.0.0 release: nri-bundle mode: privileged name: nri-bundle-newrelic-infrastructure spec: priorityClassName: your-priority-class ... Copy If you have already deployed the New Relic pods, re-deploy them and confirm they have been created: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml kubectl create -f newrelic-infrastructure-k8s-latest.yaml kubectl get daemonsets Copy Unprivileged installs of the Kubernetes integration For platforms that have stringent security requirements, we provide an unprivileged version of the Kubernetes integration. Changes from the standard Kubernetes integration are: Runs the infrastructure agent and the Kubernetes integration as a standard user instead of root No access to the underlying host filesystem No access to /var/run/docker.sock Container's root filesystem mounted as read-only allowPrivilegeEscalation is set to false hostnetwork is set to false The tradeoff is that the solution will only collect metrics from Kubernetes, but it will not collect any metric from the underlying hosts directly. Kubernetes provides some data (metrics and metadata) about its nodes (hosts). Tip Optional: To collect the underlying host metrics, the non-containerized infrastructure agent can be deployed on the underlying host. The infrastructure agent already supports running as non-root. The combination of the Kubernetes integration in its unprivileged version and the agent running on the host will report all the metrics that our standard solution for monitoring Kubernetes receives. Steps to complete an unprivileged install Install kube-state-metrics and get it running on the cluster. For example: curl -L -o kube-state-metrics- 1.9.5 .zip https://github.com/kubernetes/kube-state-metrics/archive/v 1.9.5 .zip && unzip kube-state-metrics- 1.9.5 .zip && kubectl apply -f kube-state-metrics- 1.9.5 /examples/standard Copy Download the integration manifest: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy In the manifest, add your New Relic license key and a cluster name to identify your Kubernetes cluster. Both values are required. Important YOUR_CLUSTER_NAME is your cluster’s id in New Relic Explorer. It doesn’t need to match the name of the cluster running in your environment. env: - name: NRIA_LICENSE_KEY value: YOUR_LICENSE_KEY - name: CLUSTER_NAME value: YOUR_CLUSTER_NAME Copy YAML key path: spec.template.spec.containers.name.env Confirm that kube-state-metrics is installed. kubectl get pods --all-namespaces | grep kube-state-metrics Copy Create the DaemonSet: kubectl create -f newrelic-infrastructure-k8s-unprivileged-latest.yaml Copy Confirm that the DaemonSet has been created successfully by looking for newrelic-infra in the results generated by this command: kubectl get daemonsets Copy To confirm that the integration has been configured correctly, wait a few minutes, then run this NRQL query to see if data has been reported: SELECT * FROM K8sPodSample since 5 minutes ago Copy Configure the integration The Kubernetes integration comes with a default configuration that should work in most environments. To change the configuration, modify the manifest file: Select which processes should send their data to New Relic By default, data about the processes running on your pods is not sent to New Relic. You can enable it by setting enable_process_metrics to true. To choose what metric data you want to send to New Relic, configure the include_matching_metrics environment variable in your manifest. Specify the Kubernetes API host and port This is necessary when you are using SSL and not using the default FQDN. The Kubernetes API FQDN needs to match the FQDN of the SSL certificate. You do not need to specify both variables. For example, if you only specify the HOST, the default PORT will be used. - name: \"KUBERNETES_SERVICE_HOST\" value: \"KUBERNETES_API_HOST\" - name: \"KUBERNETES_SERVICE_PORT\" value: \"KUBERNETES_API_TCP_PORT\" Copy Kubernetes versions 1.6 to 1.7.5: Edit manifest file For Kubernetes versions 1.6 to 1.7.5, uncomment these two lines in the manifest file: - name: \"CADVISOR_PORT\" # Enable direct connection to cAdvisor by specifying the port. Needed for Kubernetes versions prior to 1.7.6. value: \"4194\" Copy Use environment variables Use environment variables that can be passed to the Kubernetes integration if you use a proxy to configure its URL. Disable kube-state-metrics parsing You can disable kube-state-metrics parsing for the DaemonSet by using the following configuration: - name: \"DISABLE_KUBE_STATE_METRICS\" value: \"true\" Copy Caution Disabling kube-state-metrics also disables data collection for the following: ReplicaSets DaemonSets StatefulSets Namespaces Deployments Services Endpoints Pods (that are pending) Additionally, disabling this affects the Kubernetes Cluster Explorer in the following ways: No pending pods are shown. No filters based on services. Specify the kube-state-metrics URL If several instances of kube-state-metrics are present in the cluster, uncomment and configure the following lines to specify which one to use: - name: \"KUBE_STATE_METRICS_URL\" value: \"http://KUBE_STATE_METRICS_IP_OR_FQDN:PORT\" Copy Important Even though a KUBE_STATE_METRICS_URL is defined, the KSM service should contain one of the following labels for the auto-discovery process: k8s-app=kube-state-metrics OR app=kube-state-metrics OR ​​app.kubernetes.io/name=kube-state-metrics Important This configuration option overrides KUBE_STATE_METRICS_POD_LABEL. If you have both defined, KUBE_STATE_METRICS_POD_LABEL has no effect. Discover kube-state-metrics pods using a label If several instances of kube-state-metrics are present in the cluster, another option to easily target one of these instances with the Kubernetes integration is to use label-based discovery. - name: \"KUBE_STATE_METRICS_POD_LABEL\" value: \"LABEL_NAME\" Copy Important When a KUBE_STATE_METRICS_POD_LABEL is defined, the label should have a value equal to true. For example, if the label name is my-ksm, ensure that my-ksm=true. Important This configuration option is incompatible with KUBE_STATE_METRICS_URL. If you have both defined, KUBE_STATE_METRICS_URL is used. Query kube-state-metrics behind RBAC If your instance of kube-state-metrics is behind kube-rbac-proxy, the integration can be configured in a compatible way using the combination of the label-based discovery and two other environment variables: - name: \"KUBE_STATE_METRICS_SCHEME\" value: \"https\" - name: \"KUBE_STATE_METRICS_PORT\" value: \"KSM_RBAC_PROXY_PORT\" Copy To confirm which port should be used as the value of KUBE_STATE_METRICS_PORT, we recommend running a describe command on the kube-state-metrics pod and look for the port exposed by the container named kube-rbac-proxy-main. Important These two configuration options only work when using the KUBE_STATE_METRICS_POD_LABEL configuration described above. kube-state-metrics timeout: Increase the client timeout To increase the client timeout of kube-state-metrics, add a new environment variable, TIMEOUT, to the manifest file: env: - name: TIMEOUT value: 5000 # The default client timeout when calling kube-state-metrics, in milliseconds Copy Then, add this new environment variable to the NRIA_PASSTHROUGH_ENVIRONMENT Non-default namespace deployments: Edit config file If you want to deploy in a different namespace from default, change all values of namespace in the manifest. Set the TTL for the Kubernetes API responses cache By default, the integration will cache any retrieved information from the Kubernetes API for 5 minutes. Use the API_SERVER_CACHE_TTL environment variable to set a custom cache duration for responses from the API server. Valid time unit values are: ns, us, ms, s, m, and h. To disable caching, set to 0s. env: - name: API_SERVER_CACHE_TTL value: \"1m\" Copy Specify base URLs for control plane component endpoints Use the following environment variables if any of the Kubernetes control plane components export metrics on base URLs that are different from the defaults. This is necessary for environments such as OpenShift when a control plane component metrics endpoint is using SSL or an alternate port. Values of these environment variables must be base URLs of the form [scheme]://[host]:[port]. URLs should not include a path component. For example: - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259\" - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy The /metrics path segment is added automatically. In addition, if the https scheme is used, authentication to the control plane component pod(s) is accomplished via service accounts. Caution If a FQDN (fully qualified domain name) is used in a multi-master cluster, inconsistent results may be returned. Therefore, it is recommended to use localhost only. Important Even though a custom base URL is defined for a given control plane component, the control plane component pod(s) must contain one of the labels supported by the auto-discovery process. Important Even though a custom ETCD_ENDPOINT_URL can be defined, ETCD will always require https and mTLS authentication to be configured. Here are some additional configurations to consider: Do more configuration for control plane monitoring Link New Relic APM to the Kubernetes integration Monitor services that run on Kubernetes Configure the infrastructure agent The Kubernetes integration image comes with a default configurations for the agent that can be modified if needed. When installing with the manifest, you can modify the infrastructure agent configuration by editing the manifest and adding any needed configuration option of the agent as environment variables of the newrelic-infrastructure DaemonSet. When installing with Helm, you can specify the needed infrastructure agent configuration options in the values.yaml as shown in the example in GitHub. The config object is used to populate the configMap that is mounted automatically in the location of the infrastructure agent configuration file in the pods created by the newrelic-infrastructure DaemonSet. Update to the latest version Using the automated installer To update a Kubernetes integration installed with the automated installer, just run the installer again. It will always offer a manifest pointing to the last released version of the integration. Using helm See Install the Kubernetes integration using Helm Custom manifest If you are already running the Kubernetes integration and want to update the newrelic-infra agent to the latest agent version: Run this NRQL query to check which version you are currently running (this will return the image name by cluster): SELECT latest(containerImage) FROM K8sContainerSample WHERE containerImage LIKE '%newrelic/infrastructure%' FACET clusterName SINCE 1 day ago Copy If you've set a name other than newrelic/infrastructure for the integration's container image, the above query won't yield results: to make it work, edit the name in the query. Download the integration manifest file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/newrelic-infrastructure-k8s-latest.yaml Copy Copy the changes you made to the manifest. At a minimum, include CLUSTER_NAME and NRIA_LICENSE_KEY, and paste your changes in the manifest you downloaded. Install the latest DaemonSet with the following command (Kubernetes will automatically do a rollout upgrade for the integration's pods): kubectl apply -f newrelic-infrastructure-k8s-latest.yaml Copy Uninstall the Kubernetes integration To uninstall the Kubernetes integration: Verify that newrelic-infrastructure-k8s-latest.yaml corresponds to the filename of the manifest as you have saved it. Example: If you are using the unprivileged version of the integration, the default filename will be newrelic-infrastructure-k8s-unprivileged-latest.yaml. After you verify the filename, use the following command: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy You only need to execute this command once, regardless of the number of nodes in your cluster.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.654724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: install and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " that the <em>integration</em> is working: wait a few minutes, then look for data in the New Relic <em>Kubernetes</em> cluster explorer. If you don&#x27;t see data, review the configuration procedures again, then follow the <em>troubleshooting</em> procedures. Important In the future, the number of labels collected on <em>Kubernetes</em> objects"
      },
      "id": "60450ae964441f0603378f15"
    }
  ],
  "/docs/integrations/kubernetes-integration/understand-use-data/auto-telemetry-pixie-data-model": [
    {
      "sections": [
        "Use Auto-telemetry with Pixie to get instant Kubernetes observability",
        "Important",
        "Why it matters",
        "Install Auto-telemetry with Pixie",
        "Beta limitations",
        "Before you begin",
        "Start the guided installation",
        "Explore your cluster",
        "Tip"
      ],
      "title": "Use Auto-telemetry with Pixie to get instant Kubernetes observability",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "218368ab3ecf86892ec64e167e19b441f8599ad8",
      "image": "https://docs.newrelic.com/static/b141876a9307e9fc517692f06e839735/c1b63/pixie-explorer.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/get-started-auto-telemetry-pixie/",
      "published_at": "2021-07-09T15:26:27Z",
      "updated_at": "2021-05-27T02:06:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Auto-telemetry with Pixie is a supported public beta. Therefore, the features and functionality are subject to change. By joining the beta for Auto-telemetry with Pixie, you agree to the terms in the New Relic Pre-Release Policy. When we say auto-telemetry, we’re not talking about cars — we’re talking about automatically collecting telemetry data for your Kubernetes clusters. With our Pixie integration, you no longer have to manually instrument your clusters or change code. With one install command, you get observability into your Kubernetes clusters and workloads. No language agents required. Pixie collects telemetry data using eBPF, a virtual machine-like construct in the Linux kernel. eBPF makes it easy to collect similar data about your clusters and workloads as with traditional New Relic language agents, but now without needing to update your code. Simply put, Auto-telemetry with Pixie offers the quickest option for getting observability into your Kubernetes services. Why it matters Our Pixie integration gives you the best of both worlds: Pixie’s fast and simple Kubernetes observability coupled with New Relic One’s incident correlation, intelligent alerting, and long-term retention. You’ll get visibility into HTTP services using golden signals, HTTP transactions, database transactions, distributed tracing, and JVM metrics. You can operate, debug, and scale your Kubernetes clusters based on the information you learn about how your clusters and services are running. Using the New Relic Explorer, you can see key metrics and events at every level, starting with the cluster, and diving down into namespaces, deployments, and pods. You can quickly spot anomalous behavior, and where it’s happening. If you need further insight, you can easily link over to Pixie's native UI for advanced debugging with PXL scripts. Install Auto-telemetry with Pixie Use our guided installation process to install Auto-telemetry with Pixie. This deploys Pixie with New Relic's Kubernetes integration on your cluster. You don't need to do any further configuration or installation to start using Pixie. If you want to install Auto-telemetry with Pixie on multiple clusters, re-run the guided install for each additional cluster. Beta limitations The following are not supported in this beta version of Auto-telemetry with Pixie: OpenShift. While our New Relic infrastructure agent supports OpenShift, Pixie does not. RBAC. New Relic RBAC controls cannot be used to gate access to Pixie. This means the following: Auto-telemetry with Pixie creates one project for all of your Kubernetes clusters users from the same email domain are automatically allowed access to the project all users have full permissions on the project Before you begin The guided install requires the following (you can get these set up before you begin, or during the install steps): a Pixie account You need a Google account to sign up for Pixie. a Pixie API key Sufficient memory: Pixie requires 2Gb of memory per node. Start the guided installation Open our New Relic One guided install. Select the account you want to use for the guided install, and click Continue. Note: if you have a single account, you won't see this option. Select Kubernetes. Type in a name for your cluster, and select the check box to Gather telemetry automatically with Pixie beta. Important Currently, Pixie performs best on clusters with up to 100 nodes (exceeding 100 nodes can lead to excessive memory usage and scripts failing to run). Friendly reminder: autoscaling can quickly drive up your node numbers. If Pixie is already running on your cluster, expand the Advanced options section and select the appropriate check box. We strongly recommend enabling Pixie auto update to maintain the latest Pixie version. Click Continue. Provide your Pixie API key. If you already have a key, simply paste it in the API key box, and click Continue. If you don't have an account and existing API key, click the link to create a Pixie account. Then, on the API key page, click New key to add a new key. In the Actions column, click the ellipsis, and then click Copy value. Switch back to the guided install UI, paste the value into the API key box, and click Continue. When you click Continue, the API key is added to your New Relic account, and a deploy key is created for Pixie. Your Pixie account information is now stored on your New Relic account; if you follow the install steps again, you won't be asked for this information. On the Choose install method page, copy the Helm command that's provided, and then run it on your command line. Helm installs a bundle containing the New Relic infrastructure agent, an integration to gather Prometheus metrics and Kubernetes events, and the Pixie integration. The deployment takes a few minutes to complete. To see the status of the install to the cluster, run kubectl get pods -n newrelic. In the New Relic install UI, click Continue to open the Listening for data page. When you get the message, See your data, click Kubernetes Cluster Explorer to see your cluster. Note that Auto-telemetry with Pixie might restart after installation. This is caused by the auto update feature. Explore your cluster In the cluster explorer, you can get a quick overview of the nodes in your cluster, including CPU, memory, and storage, as well as the status of each pod (healthy, warning, or critical). You can also find out what services are running in each container, their latency, throughput, and error rate. For more information about using the cluster explorer, see Navigate the Kubernetes cluster explorer. Tip Containers might be listed for up to four hours after they get decommissioned. You can query the Pixie data that's available in New Relic One and create dashboards for at-a-glance monitoring. Find the data model and sample queries here. In addition to analyzing the performance of your clusters in New Relic One, you might want to use Pixie's advanced debugging workflows. In Pixie's native UI, you can drill down further into header and content requests. Click Explore service in Pixie to see information about your service in the Pixie UI. Learn more about writing PXL scripts in the Pixie docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 419.4093,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> to get instant <em>Kubernetes</em> observability",
        "sections": "Use <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> to get instant <em>Kubernetes</em> observability",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": " agents required. <em>Pixie</em> collects <em>telemetry</em> data using <em>eBPF</em>, a virtual machine-like construct in the Linux kernel. <em>eBPF</em> makes it easy to collect similar data about your clusters and workloads as with traditional New Relic language agents, but now without needing to update your code. Simply put, <em>Auto</em>"
      },
      "id": "60ae8a7ee7b9d2390f3d9916"
    },
    {
      "image": "https://docs.newrelic.com/static/3b7d4417336ed5ef2eb3beb02d4affea/ae694/kubernetes-cluster-explorer.png",
      "url": "https://docs.newrelic.com/whats-new/2021/05/pixie-kubernetes-post-5-26/",
      "sections": [
        "Instant Kubernetes observability with Pixie",
        "Get started today"
      ],
      "published_at": "2021-07-09T20:15:27Z",
      "title": "Instant Kubernetes observability with Pixie",
      "updated_at": "2021-05-28T15:26:33Z",
      "type": "docs",
      "external_id": "f132310e72ece8cefc9e318b433c15cddfafa389",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We've removed the largest barriers to Kubernetes observability: The time and expertise required to manually instrument application code, by integrating Pixie Auto-Telemetry into our Kubernetes solution. Now, you can get visibility into your Kubernetes clusters and workloads instantly without installing language agents. Pixie data helps you debug faster than ever before, giving you access to everything on-cluster without sampling, then using AI/ML models to send the most relevant subset of your data to the Telemetry Data Platform for correlation with other services, intelligent alerting, and long term storage. Pixie Auto-Telemetry uses eBPF to automatically collect metrics, events, logs, and traces for your Kubernetes clusters, applications, OS, and network layers. Start fast: No code to update, new deployments, or lengthy monitoring standardization processes. Observe everything: Analyze data on-cluster using AI/ML without sampling, storing high-value telemetry data for alerting, correlation, and long term storage. Debug faster with Pixie’s developer-focused workflows, providing code-level insights. Get started today In New Relic One, choose Add more data. Choose Guided install or EU Guided install. Choose Kubernetes, and then follow the on-screen prompts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 263.7465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Instant <em>Kubernetes</em> observability with <em>Pixie</em>",
        "sections": "Instant <em>Kubernetes</em> observability with <em>Pixie</em>",
        "body": ", and long term storage. <em>Pixie</em> <em>Auto</em>-<em>Telemetry</em> uses <em>eBPF</em> to automatically collect metrics, events, logs, and traces for your <em>Kubernetes</em> clusters, applications, OS, and network layers. Start fast: No code to update, new deployments, or lengthy <em>monitoring</em> standardization processes. Observe everything: Analyze"
      },
      "id": "60aeed8a28ccbc146b77a392"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.36943,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> integration: compatibility and requirements",
        "sections": "<em>Kubernetes</em> integration: compatibility and requirements",
        "tags": "<em>Kubernetes</em> integration",
        "body": " cluster AKS Compatible with version 1.11 or higher <em>Kubernetes</em> cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane <em>monitoring</em> Compatible with version 1.11 or higher <em>Service</em> <em>monitoring</em> Compatible with version 1.13 or higher Requirements The New Relic"
      },
      "id": "603e92dc64441f3a974e8891"
    }
  ],
  "/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data": [
    {
      "sections": [
        "Navigate the Kubernetes cluster explorer",
        "Meet the cluster explorer",
        "Cluster dashboard",
        "Cluster explorer node table",
        "Search and filter your cluster data",
        "Browse your Kubernetes events"
      ],
      "title": "Navigate the Kubernetes cluster explorer",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "a3ef4aa459ed4503201c686000dff3a75331c2f7",
      "image": "https://docs.newrelic.com/static/34f90215b59ab8d7b4ec986bbb110805/9b7bd/nr1-cluster-explorer-node-tooltip.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/",
      "published_at": "2021-07-09T17:29:57Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes cluster explorer uses the data collected by the Kubernetes integration to show the status of your cluster, from the control plane to nodes and pods. You can find out about the health of each entity, explore logs, and see how your apps are performing. With the Events integration, everything that happens in your cluster becomes visible, and logs brought in using the logs plugin are also available. Meet the cluster explorer The cluster explorer represents your most relevant cluster data on a chart with the shape of a ship's wheel — which is also Kubernetes' logo. Outer ring: Contains up to 24 nodes of your cluster, the most relevant based on the amount of alerts. Hover over each node to check resource consumption and the percentage of allocable pods used. Inner rings: Contain the pods ( ) of each node. Pods with active alerts are shown in the third innermost ring, and pods that are pending or unable to run are in the center. Hover the mouse over each node or pod to get a quick overview of its resource usage. You can click each node and pod to view its resource usage over time or to get more information about its health and active alerts. Colors are based on predefined alert conditions: Yellow pods have active warning alerts, while red pods have active critical alerts. one.newrelic.com > Kubernetes cluster explorer: Click any pod to get more information about its status and health, and to dig deeper into application data and traces, logs, and events. Click a node to see the following data: Pod statistics CPU, memory, and storage consumption against allocatable amounts Amount of pods used by the node against the allocatable amount of pods For each pod, depending on the integrations and features you've enabled, you can see: Pod status and metadata, including namespace and deployment Container status and statistics Active alerts (both warning and critical) Kubernetes events that happened in that pod APM data and traces (if you've linked your APM data) A link to the pods' and containers' logs, collected using the Kubernetes plugin for New Relic Logs Cluster and control plane statistics are always visible on the left side. Cluster dashboard The cluster dashboard can be accessed at any time from the cluster explorer by clicking Kubernetes dashboard. It provides a curated dashboard experience for your Kubernetes cluster. one.newrelic.com > Kubernetes cluster explorer > Kubernetes dashboard: The Kubernetes dashboard can be accessed from the Kubernetes cluster explorer. It shows useful Kubernetes metric data. Cluster explorer node table Below the cluster explorer is the node table, which shows all the nodes of the cluster, namespace, or deployment. Like all other usage indicators, the table shows consumption against allocatable resources. Search and filter your cluster data The main way to modify the data view in the cluster explorer is by using the top bar to search for specific attributes or values. All the attributes and values collected by the Kubernetes integration can be combined to narrow down the cluster view. one.newrelic.com > Kubernetes cluster explorer: All your Kubernetes cluster's attributes and data points can be used to filter the cluster explorer view. You can also change the time frame using the time picker in the upper right corner. The Auto-refresh box turns the cluster explorer into a real-time dashboard that refreshes every 60 seconds. one.newrelic.com > Kubernetes cluster explorer: The time picker lets you select several predefined time spans. To reload the data every minute, check the auto-refresh box. Browse your Kubernetes events If you’ve enabled the Kubernetes events integration, you can click the Events tab to browse everything that happened in your cluster, from warnings to normal events. To set it up, select the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into logs and infrastructure data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.66483,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Navigate the <em>Kubernetes</em> cluster explorer",
        "sections": "Navigate the <em>Kubernetes</em> cluster explorer",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> cluster explorer uses the <em>data</em> collected by the <em>Kubernetes</em> <em>integration</em> to show the status of your cluster, from the control plane to nodes and pods. You can find out about the health of each entity, explore logs, and see how your apps are performing. With the Events"
      },
      "id": "603eb9a364441f82484e8879"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 143.17427,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": ", the <em>integration</em> executable exits. File structure and specifications Understanding the file structure of New Relic on-host <em>integrations</em> can help you customize your <em>integration</em>, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-host <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host <em>integration</em>. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "Remote monitoring in on-host integrations",
        "Important",
        "Effects of activating remote_monitoring",
        "Alert verification",
        "New entity attributes",
        "Changes in recorded metrics",
        "Unrecorded attributes",
        "Updated hostname"
      ],
      "title": "Remote monitoring in on-host integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "1cfea4c65b855ce9ac5078d2a36ba11b63a6101b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-16T06:05:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has data you can monitor. Integrations can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an integration will be considered a local entity, and the data related to it will be attached to the host entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis integrations, remote monitoring (and multi-tenancy) is enabled by activating the configuration parameter remote_monitoring. Important If your Apache, Cassandra, MySQL, NGINX, or Redis service is located in the same host as the agent, when you activate remote monitoring the resulting entity will be considered as remote, regardless of its actual location. This may affect alerts, alter attributes, and have other effects, as explained here. Effects of activating remote_monitoring By enabling remote_monitoring, the integration becomes a different entity which is no longer attached to the infrastructure agent. As a result, the following items may be affected: Alert verification Enabling remote monitoring can affect your configured alerts in case they are using any of the values that are affected by this new feature. We strongly recommend checking your existing alerts to make sure they keep on working as expected. New entity attributes These attributes are modified in the resulting entity: Display name: New entity unique key (instead of using the display name) Entity GUID: New entity GUID Entity ID: New entity ID Entity key: New entity unique key (instead of using the display name) External key: Using integration entity name (instead of using the agent display) Changes in recorded metrics When remote monitoring is enabled, we will add the hostname and port values to all metrics. If the nricluster name or nriservice are defined in the integration configuration file, they will also be decorated. Unrecorded attributes Since the integration is now an independent entity which is not attached to the agent, the following agent attributes are not collected: agentName agentVersion coreCount criticalViolationCount fullHostname instanceType kernelVersion linuxDistribution entityType operatingSystem processorCount systemMemoryBytes warningViolationCount Your custom attributes Updated hostname For the ApacheSample, RedisSample, CassandraSample, and NginxSample integration metrics, we will use the integration configuration hostname instead of the short hostname from the agent. When the integration hostname is a loopback address, the agent will replace it in order to guarantee uniqueness.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.19017,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Remote monitoring in on-host <em>integrations</em>",
        "sections": "Remote monitoring in on-host <em>integrations</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has <em>data</em> you can monitor. <em>Integrations</em> can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an <em>integration</em>"
      },
      "id": "603ec000e7b9d216732a07ef"
    }
  ],
  "/docs/integrations/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer": [
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-07-09T19:26:52Z",
      "updated_at": "2021-04-12T16:05:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries in its advanced mode. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event in New Relic Insights to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event in New Relic Insights to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event in New Relic Insights to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event in New Relic Insights for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.02592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your <em>Kubernetes</em> <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your <em>Kubernetes</em> <em>data</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " collected by any New Relic <em>integration</em> you <em>use</em>, including the <em>Kubernetes</em> <em>integration</em>: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a <em>Kubernetes</em> (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message"
      },
      "id": "603eb9a4196a678bfca83dbb"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 143.17427,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": ", the <em>integration</em> executable exits. File structure and specifications Understanding the file structure of New Relic on-host <em>integrations</em> can help you customize your <em>integration</em>, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-host <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host <em>integration</em>. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "Remote monitoring in on-host integrations",
        "Important",
        "Effects of activating remote_monitoring",
        "Alert verification",
        "New entity attributes",
        "Changes in recorded metrics",
        "Unrecorded attributes",
        "Updated hostname"
      ],
      "title": "Remote monitoring in on-host integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "1cfea4c65b855ce9ac5078d2a36ba11b63a6101b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations/",
      "published_at": "2021-07-09T19:20:08Z",
      "updated_at": "2021-03-16T06:05:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has data you can monitor. Integrations can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an integration will be considered a local entity, and the data related to it will be attached to the host entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis integrations, remote monitoring (and multi-tenancy) is enabled by activating the configuration parameter remote_monitoring. Important If your Apache, Cassandra, MySQL, NGINX, or Redis service is located in the same host as the agent, when you activate remote monitoring the resulting entity will be considered as remote, regardless of its actual location. This may affect alerts, alter attributes, and have other effects, as explained here. Effects of activating remote_monitoring By enabling remote_monitoring, the integration becomes a different entity which is no longer attached to the infrastructure agent. As a result, the following items may be affected: Alert verification Enabling remote monitoring can affect your configured alerts in case they are using any of the values that are affected by this new feature. We strongly recommend checking your existing alerts to make sure they keep on working as expected. New entity attributes These attributes are modified in the resulting entity: Display name: New entity unique key (instead of using the display name) Entity GUID: New entity GUID Entity ID: New entity ID Entity key: New entity unique key (instead of using the display name) External key: Using integration entity name (instead of using the agent display) Changes in recorded metrics When remote monitoring is enabled, we will add the hostname and port values to all metrics. If the nricluster name or nriservice are defined in the integration configuration file, they will also be decorated. Unrecorded attributes Since the integration is now an independent entity which is not attached to the agent, the following agent attributes are not collected: agentName agentVersion coreCount criticalViolationCount fullHostname instanceType kernelVersion linuxDistribution entityType operatingSystem processorCount systemMemoryBytes warningViolationCount Your custom attributes Updated hostname For the ApacheSample, RedisSample, CassandraSample, and NginxSample integration metrics, we will use the integration configuration hostname instead of the short hostname from the agent. When the integration hostname is a loopback address, the agent will replace it in order to guarantee uniqueness.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.19017,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Remote monitoring in on-host <em>integrations</em>",
        "sections": "Remote monitoring in on-host <em>integrations</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has <em>data</em> you can monitor. <em>Integrations</em> can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an <em>integration</em>"
      },
      "id": "603ec000e7b9d216732a07ef"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95363,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.6607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-app-service-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95363,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.6607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ]
}